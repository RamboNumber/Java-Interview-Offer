## 面试

### 01、先一起来看看阿里、美团、滴滴、京东等一线大厂的面试真题

一般大厂怎么问？

redis连环炮、mq连环炮、dubbo连环炮、分布式连环炮、elasticsearch连环炮

Java并发连环炮、mysql连环炮、网络连环炮、JDK集合连环炮、jvm连环炮、spring源码连环炮、tomcat连环炮、linux连环炮、系统设计连环炮、生产实践连环炮

反过来去考虑一下，站在一个公司面试官的角度，你会怎么去考察候选人呢？

主要就是用dubbo来写的，mysql分库分表 -> 不匹配

三轮面试 -> 考察各种技术广度，技术基础 -> 考察一下技术的深度，还有项目经验 -> 看看你聊聊人生和理想，综合性的考察，技术到项目 -> HR，学历，履历，薪资

### 02、面试突击第一季、第二季以及第三季各自的侧重点是什么？

redis连环炮、mq连环炮、dubbo连环炮、分布式连环炮、elasticsearch连环炮

Java并发连环炮、mysql连环炮、网络连环炮、JDK集合连环炮、jvm连环炮、spring源码连环炮、tomcat连环炮、linux连环炮、系统设计连环炮、生产实践连环炮

面试突击第一季：面试官基本上是必考，而且会问很多比较实践性的一些问题

面试突击第二季：针对分布式架构这个专题，做了一个专题性的讲解，偏重于一些生产上的一些实践问题，展开性的、带一点点小深度、带一些生产实践的问题，基本上都是很多同学出去面试，回来问我

你们有几台服务器，都是什么配置的，每个服务部署几台机器，多少访问量，为什么要如此部署，类似这样的一些问题还是比较多的

面试突击第三季：对于每一个小专题，我们都会挑选几个最最高频的问题出来，给大家来讲解一下，尤其是并发、mysql优化、jvm优化，一些常规性的问题

分布式专题、微服务专题、海量数据专题、高性能专题、高并发专栏、高可用专题

### 03、来看几个并发、JVM和MySQL的面试连环炮

### 04、学习完三季面试突击课程之后，你能拿下什么样的Offer？

从技术角度而言：技术广度（面试突击一二三季）、技术深度（阅读源码、项目深度、开源社区）、项目经验（2C、2B、政企、金融、电信、CRUD）、架构设计（负责过的架构有多大，你能设计多大多复杂的架构）、基础知识（数据结构和算法、计算机组成原理、操作系统、网路协议）

从综合角度而言：管理能力，学历，履历，软素质（表达能力、沟通能力、团队协作、价值观、性格），薪资要求

面试突击三季都学完 + 自己在面试过程中如果发现有一些问题是自己不会，要额外看一些书籍、或者别的课程、或者别的资料做一些补充

学历是211的本科、软素质、履历（之前就待过较为知名的公司），进BAT都可以

学历不是太出彩，软素质也较为一般，履历都是在一些小公司，起码就是说帮你面试中小型公司拿下offer，绝对是有很大的帮助的

学历不错，211、985之类的，软素质一般，履历之前反而是在国企、小公司，技术还可以，可能可以进一个独角兽企业

一线城市，20k+，20多k都是可以的

二线城市，十多k是可以的

### 05、除了学习面试突击课程之外，为了面试你还应该做哪些准备呢？

面试，比较多的一些新的热门和行情，无法涉及到

数据结构和算法，知识付费的平台，文字专栏、视频课程，踏踏实实的找两个专栏或者课程，都学好了，出去面试

在业务这块，DDD，领域驱动进行业务模型的设计，BAT大厂，美团之类的大厂，都在用DDD做复杂业务的设计，比较难，找到好的资料

目前得益于很多的一些在线教育机构，会把spring源码作为核心去讲解，外面很多公司都会拷问你的spring源码有没有读过，把这个spring源码自己找一些书籍，去看一下，做一点积累

设计秒杀系统，大路货，网上的文章和资料太多了

紧跟行业趋势和热点走向，平时多看一些书，多做一些积累，狸猫技术窝，大白话的视频课程，文字专栏，几十篇干货文章，几千个字，大白话一些，专栏设计出来一些别的地方没有的一些实战性的内容

## Java集合包

### 06、为什么在Java面试中一定会深入考察HashMap？

HashMap的深入考察，必然是面试中的一个核心的点

都是写Java代码，基于Java都是来构建各种各样的系统的，软件的，基于Java写出来一大堆的代码，可能会访问很多其他的东西，数据库，缓存，消息中间件，核心还是来写Java代码实现一些逻辑的运转

接收到一个请求，可能会创建一些数据结构，来存放一些数据，做一些循环、跳转、判断、加加减减，数据处理，逻辑，通过一大堆的逻辑就可以完成一些系统功能，或者是软件的功能

HashMap，数据结构，进行一定的逻辑的处理

一句话总结：你是Java工程师，你写代码的时候必然会用到一些数据结构，其中尤为经典的就是HashMap，别人必然会考察你

### 07、你知道HashMap底层的数据结构是什么吗？

```
HashMap<String, String> map = new HashMap<String, String>();
map.put(“张三”, “测试数据”);
map.put(“李四”, “测试数据”); 

{
   “张三”: “测试数据”,
   “李四”: “测试数据”
}
```

底层最核心的数据结构并不是你想的这样的

数组

对张三计算出来一个hash值，根据这个hash值对数组进行取模，就会定位到数组里的一个元素中去

[<>, <>, <>, <>,<张三, 测试数据>, <>,<>,<李四, 测试数据>,<>, <>, <>, <>,<>, <>, <>, <>]

假设可以放16个元素，取模，index

array[4] = <张三, 测试数据>

map.get(“张三”) -> hash值 -> 对数组长度进行取模 -> return array[4]

总结：

jdk1.6 1.7 采用 数组+链表，把key 进行取模获取 小标，到对应的链表 进行put/get操作

jdk1.8 采用 数组+链表+红黑树（当链表超过8 时，转红黑树【1.符合二叉树结果 2.不是黑色 就红色 3.叶子节点都是黑色 或者nil 3.左右子树 高度差小于 1 】）

### 为什么数组容量会是2的倍数，以及扩容为什么是扩成两倍

可以减少碰撞几率，2的倍数 -1 得到值所有位都是1，和计算值相与后能保证结果单一，如果位上的0越多，碰撞概率越大 举个例子。。 比如容量是2的4次方 减1就是 1111.。。那么计算值1110过来和他相与， 结果是1110，另一个计算值1111和他相与结果仍是1111 各自结果不一样 不会碰撞 如果容量不是2的4次方 比如15 减1就是1110. 那么计算值1110过来和他相与， 结果是1110，另一个计算值1111和他相与结果是1110 跟前一个一样 发生碰撞了。

### 08、JDK 1.8中对hash算法和寻址算法是如何优化的？

map.put(“张三”, “测试数据”)

对“张三”这个key计算他的hash值，是有一定的优化的

hash算法优化

```
// JDK 1.8以后的HashMap里面的一段源码
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}    
```

比如说：有一个key的hash值

1111 1111 1111 1111 1111 1010 0111 1100

0000 0000 0000 0000 1111 1111 1111 1111

1111 1111 1111 1111 0000 0101 1000 0011 -> int值，32位

hash值一样 -> 他们其实都会在数组里放在一个位置，进行复杂的hash冲突的处理

[16个元素] -> hash值对数组长度取模，定位到数组的一个位置，塞进去就ok了

高低16位都参与运算

寻址算法优化

(n - 1) & hash -> 数组里的一个位置

1111 1111 1111 1111 1111 1010 0111 1100（没有经过优化的hash值）

0000 0000 0000 0000 0000 0000 0000 1111

取模运算，他是性能比较差一些，为了优化这个数组寻址的过程

hash & (n - 1) -> 效果是跟hash对n取模，效果是一样的，但是与运算的性能要比hash对n取模要高很多，数学问题，数组的长度会一直是2的n次方，只要他保持数组长度是2的n次方

hash对n取模的效果 -> hash & (n - 1)，效果是一样的，后者的性能更高

1111 1111 1111 1111 1111 1010 0111 1100（没有经过优化的hash值）

0000 0000 0000 0000 0000 0000 0000 1111

相当于，你直接这么搞，高16位之间的与运算，是可以忽略的，核心点在于低16位的与运算，hash值的高16位没有参与到与运算里来啊

假设有两个hash值

1111 1111 1111 1111 1111 1010 0111 1100 -> 1111 1111 1111 1111 0000 0101 1000 0011

1111 1111 1111 1110 1111 1010 0111 1100 -> 1111 1111 1111 1110 0000 0101 1000 0010



1111 1111 1111 1111 0000 0101 1000 0011（经过优化和二进制位运算的新的hash值）

0000 0000 0000 0000 0000 0000 0000 1111

配合起来讲

hash算法的优化：对每个hash值，在他的低16位中，让高低16位进行了异或，让他的低16位同时保持了高低16位的特征，尽量避免一些hash值后续出现冲突，大家可能会进入数组的同一个位置

寻址算法的优化：用与运算替代取模，提升性能

总结：

hash值算法优化是将hashcode转为int型后向右位移16后得到的值与原hashcode进行异或运算得到新的hash值；寻址优化是将原来的hash值跟数组长度取模优化算法优化为用新的hash值跟数组的（n-1）做与运算得到数组的index位置；这里为什么hash值算法要优化，因为数组长度一般都比较小，即一般情况下，（n-1）高16位都是0，由于与运算是只有两个数字都是1结果才为1，其他情况均为0，所以hash值高16位跟数组（n-1）高16位做与运算，0和1结果都会0，这样子hashcode寻址冲突的概率就比较大了，除非数组长度很大，撑满整个32位最好都为1，才不会有冲突，但一般不可能这么长，通过hash值优化后，hash值高16位与低16位融合，保留高低16位的特征，再跟（n-1）做与运算，hash冲突的概率就降低了！

### 09、你知道HashMap是如何解决hash碰撞问题的吗？

hash冲突问题，链表+红黑树，O(n)和O(logn)

map.put和map.get -> hash算法优化（避免hash冲突），寻址性能优化

算出key的hash值，到数组中寻址，找到一个位置，把key-value对放进数组，或者从数组里取出来

两个key，多个key，他们算出来的hash的值，与n-1，与运算之后，发现定位出来的数组的位置还是一样的，hash碰撞，hash冲突

[<> -> <> -> <>, ]

array[0]这个位置，就是一个链表

会在这个位置挂一个链表，这个链表里面放入多个元素，让多个key-value对，同时放在数组的一个位置里

get，如果定位到数组里发现这个位置挂了一个链表，此时遍历链表，从里面找到自己的要找的那个key-value对就可以了

假设你的链表很长，可能会导致遍历链表，性能会比较差，O(n)

优化，如果链表的长度达到了一定的长度之后，其实会把链表转换为红黑树，遍历一颗红黑树找一个元素，此时O(logn)，性能会比链表高一些

为什么不直接用红黑树就可以了，还要转化为链表再转化成红黑树呢？难道红黑树还有什么不好的地方吗？

红黑树得每次增加去除元素都比较复杂得,伴随着整个树得左旋右旋还有元素得重新排列,因此,它得出现是为了针对某种特殊情况下得一种优雅得操作.是一种兜底得操作.理论上讲,链表长度超过8得情况发生得概率不超过百万分之六.小概率事件得发生意味着某种错误操作得出现,所以这种情况还会继续发生,为了应对以后要到来得这种恶劣得情况才会转化成红黑树,这个玩意儿其实并不好,理想情况下不发生哈希碰撞得才是最完美得,要锤子红黑树

长度为8时树化 长度为6时链表化；

当节点的个数小于等于 6 时，红黑树会自动转化成链表，主要还是考虑红黑树的空间成本问题，当节点个数小于等于 6 时，遍历链表也很快，所以红黑树会重新变成链表

为什么会在8转为红黑树

为什么会在8转为红黑树，可以看一下代码的注释，注释上说了作者是根据概率学的角度来决定的，因为根据统计，一个桶位置上的节点数目的分布式泊松分布，长度超过8的概率十分小，所以作者选用了8作为链表转为红黑树的阈值

### 10、说说HashMap是如何进行扩容的可以吗？

底层是一个数组，当这个数组满了之后，他就会自动进行扩容，变成一个更大的数组，让你在里面可以去放更多的元素

2倍扩容

[16位的数组，<> -> <> -> <>]

[32位的数组，<> -> <>, <>]

数组长度=16

n - 1 0000 0000 0000 0000 0000 0000 0000 1111

hash1 1111 1111 1111 1111 0000 1111 0000 0101

&结果 0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）

n - 1 0000 0000 0000 0000 0000 0000 0000 1111

hash2 1111 1111 1111 1111 0000 1111 0001 0101

&结果 0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）

在数组长度为16的时候，他们两个hash值的位置是一样的，用链表来处理，出现一个hash冲突的问题

如果数组的长度扩容之后 = 32，重新对每个hash值进行寻址，也就是用每个hash值跟新数组的length - 1进行与操作

n-1 0000 0000 0000 0000 0000 0000 0001 1111

hash1 1111 1111 1111 1111 0000 1111 0000 0101

&结果 0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）

n-1 0000 0000 0000 0000 0000 0000 0001 1111

hash2 1111 1111 1111 1111 0000 1111 0001 0101

&结果 0000 0000 0000 0000 0000 0000 0001 0101 = 21（index = 21的位置）

判断二进制结果中是否多出一个bit的1，如果没多，那么就是原来的index，如果多了出来，那么就是index + oldCap，通过这个方式，就避免了rehash的时候，用每个hash对新数组.length取模，取模性能不高，位运算的性能比较高

总结：

hashmap的底层是数组，因此需要在元素个数达到一定的阈值以后进行扩容的操作。 扩容的过程就涉及到rehash的操作，也就是所有元素的坐标再次定位到新的数组上的位置上。 如果单纯的对每个值重新进行定位运算，这样效率太低了，因此hashmap贯穿始终的长度为2的N次幂又站了出来。 通过当前的元素的hash&n是否为0来判断该元素是否需要变化，如果不为0的话就定位到当前index+n的位置即可。 由此可见长度为2的N次幂的设计是十分精妙的，这也是为何扩容一定是2倍的原因。 最后，关于扩容还有一个点，table = newTab;就是直接将新创建的空数组赋值给了table，这里也是一个优化点，为了减少进入扩容函数的线程数量，尽可能减小线程并发对扩容函数造成的影响。这个操作减小了并发的影响的同时，也带了一小段时间可能出现不可重复读的问题。

1、首先是扩容的时机：是在put的最后一步来判断要不要扩容的。 2、其次是扩容的方式：比如把24扩容成25，寻址时n-1就由1111变为11111，那么key的hash和它与运算，最高位要么多个1，要么多个0，后面四位不变的，那么它针对hash桶上某个index上对应的链表，这个链表会拆分成两部分，一部分留在原地，另一部分被移走了，这边就把这条链表拆分成了高位和低位，判断哪一部分是要移动的，并且移动的时候顺序是保持的，不会再像JDK 7中出现环，完事后，把高位链表和低位链表赋到新的hash桶中的index下去

2倍扩容，0.75阈值，rehash逻辑： 如果该桶单元只有一个数据：直接e.hash & (newCap - 1)重新计算新桶的位置 如果该桶单元是一个链表：(e.hash & oldCap) == 0 根据与旧桶的容量，判断在新桶中的位置是原位置，还是原位置+oldCap，链表顺序不变，分成两部分，一部分放到新桶中原位置，一部分放到新桶中原位置+oldCap，可以看到这里并不是避免了定位的与运算，而是避免了链表数据进入新桶多次的hash冲突。 如果该桶单元是一个红黑树：(e.hash & bit) == 0与桶单元是链表逻辑类似，判断在新桶中的位置是原位置，还是原位置+oldCap，红黑树顺序不变，分成两部分，一部分放到新桶中原位置，一部分放到新桶中原位置+oldCap，同时将根节点放到桶单元中会判断树中数据长度，小于等于6转换成链表。原理一致，多了转换成链表的判断。

size16,map放多少元素会去扩容

容量*负载因子 默认容量为16负载因子为0.75 所以达到12时会扩容

如果是hash碰撞，后加入的值是加入在链表头还是链表尾呢

看jdk源码，1.7是表头，1.8是表尾。表头插入会导致死循环的问题，造成CPU百分百。

## Java并发编程

### 11、BAT面试官为什么都喜欢问并发编程的问题？

synchronized实现原理、CAS无锁化的原理、AQS是什么、Lock锁、ConcurrentHashMap的分段加锁的原理、线程池的原理、java内存模型、volatile说一下吗、对java并发包有什么了解？一连串的问题

写一些java web系统，运用一些框架和一些第三方技术，写一些类似于crud的业务逻辑，把各种技术整合一下，写一些crud而已，没什么技术含量。很多人可能写好几年的代码，都不会用到多少java并发包下面的东西

如果说你要面试一些稍微好一点的公司，技术稍微好一点，你只要去做一个技术含量稍微高一点的系统，并发包下面的东西还是很容易会用到的。尤其是BAT，中大厂，有一定规模的公司，做出来的系统还是有一定的技术含量的

### 12、说说synchronized关键字的底层原理是什么？

其实synchronized底层的原理，是跟jvm指令和monitor有关系的

你如果用到了synchronized关键字，在底层编译后的jvm指令中，会有monitorenter和monitorexit两个指令

monitorenter

// 代码对应的指令

monitorexit

那么monitorenter指令执行的时候会干什么呢？

每个对象都有一个关联的monitor，比如一个对象实例就有一个monitor，一个类的Class对象也有一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联的monitor的lock锁

他里面的原理和思路大概是这样的，monitor里面有一个计数器，从0开始的。如果一个线程要获取monitor的锁，就看看他的计数器是不是0，如果是0的话，那么说明没人获取锁，他就可以获取锁了，然后对计数器加1

这个monitor的锁是支持重入加锁的，什么意思呢，好比下面的代码片段

// 线程1

synchronized(myObject) { -> 类的class对象来走的

// 一大堆的代码

synchronized(myObject) {

// 一大堆的代码

}

}

加锁，一般来说都是必须对一个对象进行加锁

如果一个线程第一次synchronized那里，获取到了myObject对象的monitor的锁，计数器加1，然后第二次synchronized那里，会再次获取myObject对象的monitor的锁，这个就是重入加锁了，然后计数器会再次加1，变成2

这个时候，其他的线程在第一次synchronized那里，会发现说myObject对象的monitor锁的计数器是大于0的，意味着被别人加锁了，然后此时线程就会进入block阻塞状态，什么都干不了，就是等着获取锁

接着如果出了synchronized修饰的代码片段的范围，就会有一个monitorexit的指令，在底层。此时获取锁的线程就会对那个对象的monitor的计数器减1，如果有多次重入加锁就会对应多次减1，直到最后，计数器是0

然后后面block住阻塞的线程，会再次尝试获取锁，但是只有一个线程可以获取到锁。

总结：

synchronized一般会用来做同步控制，已经配合wait/notify做线程间通讯。该关键字在编译成class文件后可以看到对应的语句是monitorEnter与monitorExit，实际上是对应到了OS的互斥量。而1.6之前这个关键字性能不佳，所以在1.6之后引入了一系列的优化，包括偏向锁，轻量级锁，以及自适应自旋锁等等，其实核心理念都是根据当前的并发程度去尽量避免直接用到OS的互斥量去完成同步操作，因为这样会导致线程在用户态与内核态之间来回切换，比较重。

synchronized是用来做线程同步的,可以对类和对象进行加锁,它依赖底层的jvm指令,线程在执行被修饰的方法时,底层是根据是否设置了ACC_synchronized访问标志.执行被修饰的代码块的时候是会先执行monitorenter指令,退出方法后会执行monitorexit指令.执行效果是一样的,只是体现的形式不一样. 执行过程就是每个对象都会关联一个monitor,它里面有一个计数器,初始是0,如果有线程要获取monitor的锁,会先看计数器是不是0,如果是0说明没有线程在获取锁,那这个线程就可以获取lock锁,然后对计数器加1.如果不为0,判断线程是不是自己,是的话就重入锁,计数器+1,如果不是的话线程会陷入block阻塞状态.线程执行完代码退出后,底层会执行monitorexit指令,这个时候对应的monitor计数器会减1,如果是多次重入锁,那么就会多次减1.直到为0.然后block状态的线程可以再次获取锁.

### 13、能聊聊你对CAS的理解以及其底层实现原理可以吗？

取值，询问，修改

多个线程他们可能要访问同一个数据

HashMap map = new HashMap();

此时有多个线程要同时读写类似上面的这种内存里的数据，此时必然出现多线程的并发安全问题，几个月培训班的同学，都应该知道

我们可能要用到并发包下面的很多技术，synchronized

synchronized(map) {

// 对map里的数据进行复杂的读写处理

}

并发包下面的其他的一些技术

CAS

一段代码：

此时，synchronized他的意思就是针对当前执行这个方法的myObject对象进行加锁

只有一个线程可以成功的堆myObject加锁，可以对他关联的monitor的计数器去加1，加锁，一旦多个线程并发的去进行synchronized加锁，串行化，效率并不是太高，很多线程，都需要排队去执行

CAS，compare and set

CAS在底层的硬件级别给你保证一定是原子的，同一时间只有一个线程可以执行CAS，先比较再设置，其他的线程的CAS同时间去执行此时会失败

总结：

CAS：比较后替换，由硬件提供的原语保证其操作原子性。 CAS一般会存在ABA问题，本质上是因为数据可回环，因此解决掉数据回环问题即可，加上版本号，或者对应时间戳之类的。 CAS一般与volatile关键字一起去实现无锁化的同步操作，可以有效的提升一些场景下的性能，比如在concurrentHashMap的元节点控制上就是用的CAS去做替换。 但是CAS+volatile却不适合做高并发场景下的同步，主要原因是因为底层走的MESI协议，以及CAS通常情况下会伴随着自旋，在高并发情况下，自旋基本很难完成更新值的操作，反而白白浪费CPU资源。

CAS虽然高效的解决了原子操作问题，但仍然存在三大问题： 1.ABA问题：如果变量V初次读取的时候值是A，后来变成了B，然后又变成了A，你本来期望的值是第一个A才会设置新值，第二个A跟期望不符合，但却也能设置新值。针对这种情况，java并发包中提供了一个带有标记的原子引用类AtomicStampedReference，它可以通过控制变量值的版本号来保证CAS的正确性，比较两个值的引用是否一致，如果一致，才会设置新值。 打一个比方，如果有一家蛋糕店，为了挽留客户，绝对为贵宾卡里余额小于20元的客户一次性赠送20元，刺激消费者充值和消费。但条件是，每一位客户只能被赠送一次。此时，如果很不幸的，用户正好正在进行消费，就在赠予金额到账的同时，他进行了一次消费，使得总金额又小于20元，并且正好累计消费了20元。使得消费、赠予后的金额等于消费前、赠予前的金额。这时，后台的赠予进程就会误以为这个账户还没有赠予，所以，存在被多次赠予的可能，但使用AtomicStampedReference就可以很好的解决这个问题。 2.无限循环问题（自旋）：看源码可知，Atomic类设置值的时候会进入一个无限循环，只要不成功，就会不停的循环再次尝试。在高并发时，如果大量线程频繁修改同一个值，可能会导致大量线程执行compareAndSet()方法时需要循环N次才能设置成功，即大量线程执行一个重复的空循环（自旋），造成大量开销。解决无线循环问题可以使用java8中的LongAdder，分段CAS和自动分段迁移。 3.多变量原子问题：只能保证一个共享变量的原子操作。一般的Atomic类，只能保证一个变量的原子性，但如果是多个变量呢？可以用AtomicReference，这个是封装自定义对象的，多个变量可以放一个自定义对象里，然后他会检查这个对象的引用是不是同一个。如果多个线程同时对一个对象变量的引用进行赋值，用AtomicReference的CAS操作可以解决并发冲突问题。 但是如果遇到ABA问题，AtomicReference就无能为力了，需要使用AtomicStampedReference来解决。

在多线程下使用synchronized关键字保证线程安全会影响性能。所以引入并发包下的一些其他操作。例如CAS（compare and set），第一步先读旧值，第二步CAS，拿旧值与当前值进行比较，如果一致则可进行修改；如果不一致说明，在第一步读取旧值后，有线程将数据修改，则此次修改失败。CAS在底层的硬件级别保证一定是原子的，同一时间只有一个线程可以执行CAS，先比较再设置，其他的线程的CAS同时间去执行此时会失败

CAS全称是CompareAndSet,(比较和设置),CAS在底层的硬件级别保证了原子性,可以使用底层是基于CAS实现的类完成一些线程同步的累加操作,例如AtomicInteger类.假设有个AtomicInteger(0)变量,有一个线程执行方法incrementAndGet方法获取当前值是0,尝试去累加1,这里会先比较旧值是否为0,如果是0说明没有其他线程改过设置为1,如果是1了,CAS失败了,会重新读取并进行CAS,如果成功累加1,值为2.CAS虽然可以高效解决原子操作问题,但有些场景有ABA问题,可以通过增加版本号或者是使用AtomicStampedReference类来解决.CAS的空循环情况会比较消耗资源.

硬件级别是如何保证cas是原子性的？

总线锁和缓存锁来保证原子性;总线的概念即volatile的原理+cpu的锁隔离,volatile当内存中变量被改变后会往总线发一个信号和将数据写会主存,其他线程读到总线的标记信号后将当前信息内的内存变量标记失效重新从共享的主存中读取最新的变量,cpu隔离:cpu1写的时候,cpu2写不了;缓存锁:将CPU和内存之间的通信锁住了

### 14、ConcurrentHashMap实现线程安全的底层原理到底是什么？

JDK 1.8以前，多个数组，分段加锁，一个数组一个锁

JDK 1.8以后，优化细粒度，一个数组，每个元素进行CAS，如果失败说明有人了，此时synchronized对数组元素加锁，链表+红黑树处理，对数组每个元素加锁

多个线程要访问同一个数据，synchronized加锁，CAS去进行安全的累加，去实现多线程场景下的安全的更新一个数据的效果，比较多的一个场景下，可能就是多个线程同时读写一个HashMap

synchronized，也没这个必要

HashMap的一个底层的原理，本身是一个大的一个数组，[有很多的元素]

ConcurrentHashMap map = new ConcurrentHashMap();

// 多个线程过来，线程1要put的位置是数组[5]，线程2要put的位置是数组[21]

map.put(xxxxx,xxx);

明显不好，数组里有很多的元素，除非是对同一个元素执行put操作，此时呢需要多线程是需要进行同步的

JDK并发包里推出了一个ConcurrentHashMap，他默认实现了线程安全性

在JDK 1.7以及之前的版本里，分段

[数组1] , [数组2]，[数组3] -> 每个数组都对应一个锁，分段加锁

// 多个线程过来，线程1要put的位置是数组1[5]，线程2要put的位置是数组2[21]

JDK 1.8以及之后，做了一些优化和改进，锁粒度的细化

[一个大的数组]，数组里每个元素进行put操作，都是有一个不同的锁，刚开始进行put的时候，如果两个线程都是在数组[5]这个位置进行put，这个时候，对数组[5]这个位置进行put的时候，采取的是CAS的策略

同一个时间，只有一个线程能成功执行这个CAS，就是说他刚开始先获取一下数组[5]这个位置的值，null，然后执行CAS，线程1，比较一下，put进去我的这条数据，同时间，其他的线程执行CAS，都会失败

分段加锁，通过对数组每个元素执行CAS的策略，如果是很多线程对数组里不同的元素执行put，大家是没有关系的，如果其他人失败了，其他人此时会发现说，数组[5]这位置，已经给刚才又人放进去值了

就需要在这个位置基于链表+红黑树来进行处理，synchronized(数组[5])，加锁，基于链表或者是红黑树在这个位置插进去自己的数据

如果你是对数组里同一个位置的元素进行操作，才会加锁串行化处理；如果是对数组不同位置的元素操作，此时大家可以并发执行的

总结：

jdk1.8之前ConcurrentHashMap实现线程安全使用的是分段锁技术，即将一个大数组分成几个小数组，当并发put时，处于同一个小数组的put操作会串行；不同小数组间的put操作不受影响 jdk1.8ConcurrentHashMap优化了锁的细粒度，并发操作时，对数组中每一个位置元素进行CAS。当并发put时，对同一位置进行put操作，如果put失败，说明在这之前有线程对这个位置进行了put成功操作，则对这个位置上的链表或者红黑树使用synchronized加锁；对不同位置put操作是不受影响的。

ConcurrentHashMap进行put操作的时候，元素为null，则进行cas,不为null，则进行synchronized同步。

put时，对key取hashcode，然后计算要放的hash桶，如果这个桶还不存在，则cas方式去把自己弄成hash桶的首节点；如果不为null（桶已存在），则直接加锁，然后在链表或者红黑树中插入。 如果正在该节点正在扩容，则去领任务（比如领到的任务是扩容第10到第20个桶的node），然后开始协助扩容

1.8以前是多个组数进行分段加锁,一个数组一个锁.在1.8之后,优化了细粒度,对数组的每个元素进行CAS,假设,多个线程同时操作Map但不是Map中的同一个数组,那么是没有关系的.如果是多个线程同时put操作Map中的一个数组,那么只有一个会成功,其他都会失败,此时需要基于链表或红黑树来处理,使用synchronized对当前数组加锁,然后进行操作.所以只要不是同时操作同一个位置的元素就不会触发同步的串行化操作,性能不会受影响.

### 15、你对JDK中的AQS理解吗？AQS的实现原理是什么？

![ReentrantLock](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/15/01.png)

ReentrantLock

state变量 -> CAS -> 失败后进入队列等待 -> 释放锁后唤醒

非公平锁，公平锁

多线程同时访问一个共享数据，sychronized，CAS，ConcurrentHashMap（并发安全的数据结构可以来用），Lock

synchronized就有点不一样了，你可以自己上网看一下 => AQS，Abstract Queue Synchronizer，抽象队列同步器

Semaphore、其他一些的并发包下的

ReentrantLock lock = new ReentrantLock(true); => 非公平锁

// 多个线程过来，都尝试

lock.lock();

lock.unlock();

总结：

AQS有两种功能：独占和共享。 独占锁，每次只能有一个线程持有锁，老师讲的ReentrantLock就是以独占方式实现的互斥锁。 共享锁，允许多个线程同时获取锁，并发访问共享资源，比如ReentrantLock。 AQS队列内部维护的是一个FIFO的双向链表。 ReentrantLock内部类Sync继承自AQS，Sync有两个具体实现类，就是公平锁和非公平锁。 NofairSync非公平锁，存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁。 FairSync公平锁，表示所有线程严格按照FIFO来获取锁

### 16、说说线程池的底层工作原理可以吗？

系统是不可能说让他无限制的创建很多很多的线程的，会构建一个线程池，有一定数量的线程，让他们执行各种各样的任务，线程执行完任务之后，不要销毁掉自己，继续去等待执行下一个任务

频繁的创建线程，销毁线程，创建线程，销毁线程

```
ExecutorService threadPool = Executors.newFixedThreadPool(3) -> 3: corePoolSize

threadPool.submit(new Callable() {
    public void run() {}
})； 
```

提交任务，先看一下线程池里的线程数量是否小于corePoolSize，也就是3，如果小于，直接创建一个线程出来执行你的任务

如果执行完你的任务之后，这个线程是不会死掉的，他会尝试从一个无界的LinkedBlockingQueue里获取新的任务，如果没有新的任务，此时就会阻塞住，等待新的任务到来

你持续提交任务，上述流程反复执行，只要线程池的线程数量小于corePoolSize，都会直接创建新线程来执行这个任务，执行完了就尝试从无界队列里获取任务，直到线程池里有corePoolSize个线程

接着再次提交任务，会发现线程数量已经跟corePoolSize一样大了，此时就直接把任务放入队列中就可以了，线程会争抢获取任务执行的，如果所有的线程此时都在执行任务，那么无界队列里的任务就可能会越来越多

fixed，队列，LinkedBlockingQueue，无界阻塞队列

总结：

线程池初始时是没有线程的，但是可以在初始化线程池的时候设置一开始就创建几条线程在池子里。首先任务过来会去判断当前线程池里面的线程数是否大于核心线程数，如果小于则新建线程执行任务。执行完任务后的线程会去阻塞到任务队列，等待任务队列中有任务到来。如果任务过来发现当前线程池里面的线程数等于核心线程数，就将任务放在任务队列中，等待有线程来处理。如果任务队列放满了，就去判断当前线程池里面的线程数是否小于最大线程数，如果小于就创建线程执行任务；如果等于最大线程数，且此时任务队列也放满了，就执行拒绝策略，抛异常或者是其他处理小于核心线程数，创建线程；大于核心线程数且小于设置的最大线程数，则继续创建线程执行任务；大于最大线程数，则任务进入队列；如果任务队列满了，则使用拒绝策略。任务执行完，大于核心线程数小于最大线程数的线程会被回收，只留核心线程数的线程在线程池

有几个点不太准确，第一点是新建的核心线程执行完毕后是不会销毁的，会去阻塞队列看看有没有任务要执行，如果没有任务要执行了，就会被线程池回收到池子里。第二点是如果任务队列满了，新建普通线程执行任务，执行完毕之后是会销毁线程的。第三点是只要线程达到最大线程数，那么任务队列早就满了，而不是且的关系，才去执行拒绝策略

### 17、那你再说说线程池的核心配置参数都是干什么的？平时我们应该怎么用？

newFixedThreadPool(3)

![newFixedThreadPool](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/17/01.png)

代表线程池的类是ThreadPoolExecutor

创建一个线程池就是这样子的，corePoolSize，maximumPoolSize，keepAliveTime，queue，这几个东西，如果你不用fixed之类的线程池，自己完全可以通过这个构造函数就创建自己的线程池

corePoolSize：3

maximumPoolSize：Integer.MAX_VALUE

keepAliveTime：60s

new ArrayBlockingQueue<Runnable>(200)

如果说你把queue做成有界队列，比如说new ArrayBlockingQueue<Runnable>(200)，那么假设corePoolSize个线程都在繁忙的工作，大量任务进入有界队列，队列满了，此时怎么办？

这个时候假设你的maximumPoolSize是比corePoolSize大的，此时会继续创建额外的线程放入线程池里，来处理这些任务，然后超过corePoolSize数量的线程如果处理完了一个任务也会尝试从队列里去获取任务来执行

如果额外线程都创建完了去处理任务，队列还是满的，此时还有新的任务来怎么办？

只能reject掉，他有几种reject策略，可以传入RejectedExecutionHandler

(1)AbortPolicy

(2)DiscardPolicy

(3)DiscardOldestPolicy

(4)CallerRunsPolicy

(5)自定义

如果后续慢慢的队列里没任务了，线程空闲了，超过corePoolSize的线程会自动释放掉，在keepAliveTime之后就会释放

根据上述原理去定制自己的线程池，考虑到corePoolSize的数量，队列类型，最大线程数量，拒绝策略，线程释放时间

一般比较常用的是：fixed线程，

总结：

core：核心worker数量，线程池会一直维持这个数量的worker，哪怕没有任务。 queue：如果core用完了会先将task放到queue中。 max：如果queue也满了才会再次创建新的worker直到这个max限值。 reject：如果max都满了，那就执行对应的拒绝策略了。 keepalive：超过core数量时，而queue又为空，这个时候多余的woker会等待空闲时间后就被回收掉。

### 18、如果在线程池中使用无界阻塞队列会发生什么问题？

在远程服务异常的情况下，使用无界阻塞队列，是否会导致内存异常飙升？

调用超时，队列变得越来越大，此时会导致内存飙升起来，而且还可能会导致你会OOM，内存溢出

### 19、你知道如果线程池的队列满了之后，会发生什么事情吗？

有界队列，可以避免内存溢出

corePoolSize: 10

maximumPoolSize : 200

ArrayBlockingQueue(200)

自定义一个reject策略，如果线程池无法执行更多的任务了，此时建议你可以把这个任务信息持久化写入磁盘里去，后台专门启动一个线程，后续等待你的线程池的工作负载降低了，他可以慢慢的从磁盘里读取之前持久化的任务，重新提交到线程池里去执行

你可以无限制的不停的创建额外的线程出来，一台机器上，有几千个线程，甚至是几万个线程，每个线程都有自己的栈内存，占用一定的内存资源，会导致内存资源耗尽，系统也会崩溃掉

即使内存没有崩溃，会导致你的机器的cpu load，负载，特别的高

8核16G的内存 一般可以创建多少线程呢

根据你的业务场景来去设计,核心线程数=cpu核数*(执行时间/(执行时间+等待时间)) 执行时间:代码中运算 等待时间:比如调用dubbo接口等待响应

总结：

如果使用无界队列，那么可能会导致OOM甚至宕机。 如果使用有界队列，然后设置max线程数=max那么会导致创建很多线程，也可能导致服务器崩溃。 所以要根据具体的场景以及具体的压测数据，来设定这些参数。最后就是我们可以手动去实现一个拒绝策略，将请求持久化一下，然后后台线程去等线程池负载降下来了后再读出来继续执行。

### 20、如果线上机器突然宕机，线程池的阻塞队列中的请求怎么办？

必然会导致线程池里的积压的任务实际上来说都是会丢失的

如果说你要提交一个任务到线程池里去，在提交之前，麻烦你先在数据库里插入这个任务的信息，更新他的状态：未提交、已提交、已完成。提交成功之后，更新他的状态是已提交状态

系统重启，后台线程去扫描数据库里的未提交和已提交状态的任务，可以把任务的信息读取出来，重新提交到线程池里去，继续进行执行

总结：

因为请求数据都在内存中的，因此宕机就会丢失，但是我们可以记录对应的执行状态，然后针对不同类型的请求，去做幂等或者去重的一些操作。

### 21、谈谈你对Java内存模型的理解可以吗？

![img](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/21/01.png)

read、load、use、assign、store、write

后台留言，并发这块讲解的好像有的地方有点浅，面试突击第一季和第二季，面试突击第一季，扫盲的作用，对并发、mysql、网络比较基础的知识，常见的面试题，根本就不太了解，4个月的培训班里出来的

直接楞住了，说，不好意思，concurrenthashmap从来没用过，crud

总结：

java内存模型是对计算机的一个抽象，将整个计算过程分成了6步去执行。因为每个线程都对应一个工作内存，所以导致主存中的数据值可能并不是最新的，因此多线程情况下，data++的这个操作就会被覆盖掉。 为什么要有工作内存的？它带来了内存不可见性与伪共享这些问题。是因为CPU的速度远高于内存的读写速度，因此为了充分利用CPU资源，设计了对应的缓存，还有一些其他的加载机制。 避免内存不可见用volatie就行，但是volatile的语义并不能保证data++能达到预期效果，因为它没办法保证这个执行的原子性。

read操作是将主存中的数据读取到cpu的cache中,

write将cpu cache中数据写回到主存中，

load操作将cpu的cache中数据加载到jvm的寄存器中,

store将jvm寄存器中数据存储到cpu的cache中

assign:接收到赋值指令时给工作内存变量赋值。

### 22、你知道Java内存模型中的原子性、有序性、可见性是什么吗？

连环炮：Java内存模型 -> 原子性、可见性、有序性 -> volatile -> happens-before / 内存屏障

也就是并发编程过程中，可能会产生的三类问题

**（1）可见性**

之前一直给大家代码演示，画图演示，其实说的就是并发编程中可见性问题

没有可见性，有可见性

**（2）原子性**

有原子性，没有原子性

原子性：data++，必须是独立执行的，没有人影响我的，一定是我自己执行成功之后，别人才能来进行下一次data++的执行

**（3**）有序性

对于代码，同时还有一个问题是指令重排序，编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序，就是说比如下面的代码

具备有序性，不会发生指令重排导致我们的代码异常；不具备有序性，可能会发生一些指令重排，导致代码可能会出现一些问题

重排序之后，让flag = true先执行了，会导致线程2直接跳过while等待，执行某段代码，结果prepare()方法还没执行，资源还没准备好呢，此时就会导致代码逻辑出现异常。

总结：

Java的内存模型是分为主内存和线程的工作内存两部分进行工作的，工作内存从主内存read数据，load到工作内存中，线程对数据进行use，然后将数据assign到工作内存，从工作内存store处理过的数据，最后将新数据write进主内存。 默认的这种情况下，不同线程并发对同一个数据进行操作是没有可见性的的，会发生数据错误的情况。如果能保证原子性就是在线程1修改了数据后，线程2立马能将自己工作线程的数据刷新进行后续操作。 原子性是指：线程1对数据读取并操作是一个原子过程，线程1在处理过程中，其他线程不能对数据进行操作 java虚拟机会对写好的代码进行指令重排，在多线程情况就可能会因为代码顺序调整出现问题。比如线程1判断flag准备数据，线程2判断flag确定是否准备好，依赖准备好的数据进行业务操作。如果指令重排序就可能导致线程1还未准备好数据，线程2就开始执行业务操作，发生错误。有序性是指：通过一定手段，保证不会对代码进行指令重排序

### 23、能从Java底层角度聊聊volatile关键字的原理吗？

内存模型 -> 原子性、可见性、有序性 -> volatile

讲清楚volatile关键字，直接问你volatile关键字的理解，对前面的一些问题，这个时候你就应该自己去主动从内存模型开始讲起，原子性、可见性、有序性的理解，volatile关键字的原理

volatile关键字是用来解决可见性和有序性，在有些罕见的条件之下，可以有限的保证原子性，他主要不是用来保证原子性的

可见性，概念进行了加强和深化，volatile在可见性上的作用和原理，有一个很清晰的了解

在很多的开源中间件系统的源码里，大量的使用了volatile，每一个开源中间件系统，或者是大数据系统，都多线程并发，volatile

总结：

volatile主要是用来解决多线程场景下变量的可见性以及顺序性。 如何实现可见性呢？如果在一个线程中修改了被volatile修饰的变量值，那么会让其他线程的工作内存中的变量值失效，在下一次需要用到该变量的时候，重新去主存中去加载最新的变量值。

比如有一个data变量被volatile修饰，线程1将data的值改变，并将data修改后的值刷回主内存。它就会去将其他线程的工作内存中原本的加载的data旧值全部失效，在其他线程想使用自身工作内存中的内存值时，会发现已经失效，必须从主内存中去读取data值，这时读取的就是data修改后的值了。

被volatile修饰之后的变量就不会被缓存到CUP级别的缓存中了？

都会缓存到cpu缓存中的, 被volatile修饰后, 其他变量的的修改, 通过mesi协议和内存屏障作用, 当前缓存的变量会被置为失效, 这样cpu核读取变量发现是已失效, 会重新从内存中获取

### 24、你知道指令重排以及happens-before原则是什么吗？

volatile关键字和有序性的关系，volatlie是如何保证有序性的，如何避免发生指令重排的

java中有一个happens-before原则：

编译器、指令器可能对代码重排序，乱排，要守一定的规则，happens-before原则，只要符合happens-before的原则，那么就不能胡乱重排，如果不符合这些规则的话，那就可以自己排序

1. **程序次序规则**：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作。
2. **锁定规则**：一个unLock操作先行发生于后面对同一个锁的lock操作，比如说在代码里有先对一个lock.lock()，lock.unlock()，lock.lock()。
3. **volatile变量规则**：对一个volatile变量的写操作先行发生于后面对这个volatile变量的读操作，volatile变量写，再是读，必须保证是先写，再读。
4. **传递规则**：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C。
5. **线程启动规则**：Thread对象的start()方法先行发生于此线程的每个一个动作，thread.start()，thread.interrupt()。
6. **线程中断规则**：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生。
7. **线程终结规则**：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行。
8. **对象终结规则**：一个对象的初始化完成先行发生于他的finalize()方法的开始。

上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。

很多同学说：好像没听懂，模模糊糊，这些规则写的非常的拗口，晦涩难懂，在面试的时候比如面试官问你，happens-before原则，你必须把8条规则都背出来，反问，没有任何一个人可以随意把这个规则背出来的

规则制定了在一些特殊情况下，不允许编译器、指令器对你写的代码进行指令重排，必须保证你的代码的有序性

但是如果没满足上面的规则，那么就可能会出现指令重排，就这个意思。

这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。

比如这个例子，如果用volatile来修饰flag变量，一定可以让prepare()指令在flag = true之前先执行，这就禁止了指令重排。

因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。

指令重排 -> happens-before -> volatile起到避免指令重排

总结：

指令重排：两个没有关联关系的代码可能会被打乱顺序去执行，主要原因是为了充分利用CPU的算力。指令重排的不良影响主要发生多线程并发的情况，因此需要用到volatile关键字去保证在volatile变量赋值前的语句的顺序性。 happens-before：这个主要是由java自身去保证一些场景下代码执行的顺序性，而不用程序员去操心。里面比较重要的就是关于volatile的读写顺序，保证是先写再读。

### 25、volatile底层是如何基于内存屏障保证可见性和有序性的？

连环炮：内存模型 -> 原子性、可见性、有序性 - > volatile+可见性 -> volatile+有序性（指令重排 + happens-before） -> voaltile+原子性 -> volatile底层的原理（内存屏障级别的原理）

volatile + 原子性：不能够保证原子性，虽然说有些极端特殊的情况下有保证原子性的效果，杠精，拿着一些极端场景下的例子，说volatile也可以原子性，oracle，64位的long的数字进行操作，volatile

保证原子性，synchronized，lock，加锁

volatile底层原理，如何实现保证可见性的呢？如何实现保证有序性的呢？

（1）lock指令：volatile保证可见性

对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改 。

如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了

lock前缀指令 + MESI缓存一致性协议

（2）内存屏障：volatile禁止指令重排序

volatille是如何保证有序性的？加了volatile的变量，可以保证前后的一些代码不会被指令重排，这个是如何做到的呢？指令重排是怎么回事，volatile就不会指令重排，简单介绍一下，内存屏障机制是非常非常复杂的，如果要讲解的很深入

Load1：

int localVar = this.variable

Load2：

int localVar = this.variable2

LoadLoad屏障：Load1；LoadLoad；Load2，确保Load1数据的装载先于Load2后所有装载指令，他的意思，Load1对应的代码和Load2对应的代码，是不能指令重排的

Store1：

this.variable = 1

StoreStore屏障

Store2：

this.variable2 = 2

StoreStore屏障：Store1；StoreStore；Store2，确保Store1的数据一定刷回主存，对其他cpu可见，先于Store2以及后续指令 。

LoadStore屏障：Load1；LoadStore；Store2，确保Load1指令的数据装载，先于Store2以及后续指令。

StoreLoad屏障：Store1；StoreLoad；Load2，确保Store1指令的数据一定刷回主存，对其他cpu可见，先于Load2以及后续指令的数据装载 。

volatile的作用是什么呢？

volatile variable = 1

this.variable = 2 => store操作

int localVariable = this.variable => load操作

对于volatile修改变量的读写操作，都会加入内存屏障 。

每个volatile写操作前面，加StoreStore屏障，禁止上面的普通写和他重排；每个volatile写操作后面，加StoreLoad屏障，禁止跟下面的volatile读/写重排 。

每个volatile读操作前面，加LoadLoad屏障，禁止上面的普通读和voaltile读重排；每个volatile读操作后面，加LoadStore屏障，禁止下面的普通写和volatile读重排

并发这块，往深了讲，synchronized、volatile，底层都对应着一套复杂的cpu级别的硬件原理，大量的内存屏障的原理；lock API，concurrenthashmap，都是各种复杂的jdk级别的源码，技术深度是很深入的。

threadlocal内存泄漏问题以及解决方案

ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏

总结：

原子性：操作一个变量宏观上分为：读取/计算/写入。volatile只能保证读取和写入的原子性，但是整个操作的原子性无法保证。 可见性：主要是lock指令+MESI协议，让修改后的值强制刷入主存，并且让其他CPU的换成全部失效。 有序性：主要是因为有内存屏障。

### 67、再谈原子性：Java规范规定所有变量写操作都是原子的

Applications apps;

线程1：

apps = loadedApps; // 原子的，不需要AtomicReference来处理

java语言规范里面，int i = 0，resource = loadedResoures，flag = true，各种变量的简单的赋值操作，规定都是原子的

包括引用类型的变量的赋值写操作，也是原子的

你赋值的时候，要保证没有人先赋值过，没有人修改过，你才能赋值，AtomicReference的CAS操作来实现了，之前给大家讲解过的

但是很多复杂的一些操作，i++，先读取i的值，再跟新i的值，i = y + 2，先读取y的值，再更新i的值，这种复杂操作，不是简单赋值写，他是有计算的过程在里面的，此时java语言规范默认是不保证原子性的

volatile，保证的可见性和有序性，原子性，杠精，偷换概念，胡说八道；i++，i = y + 2，不是volatile可以保证原子性的

### 68、32位Java虚拟机中的long和double变量写操作为何不是原子的？

原子性这块，特例，32位虚拟机里的long/double类型的变量的简单赋值写操作，不是原子的，long i = 30，double c = 45.0，在32位虚拟机里就不是原子的，因为long和double是64位的

0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000

如果多个线程同时并发的执行long i = 30，long是64位的，就会导致有的线程在修改i的高32位，有的线程在修改i的低32位，多线程并发给long类型的变量进行赋值操作，在32位的虚拟机下，是有问题的

就可能会导致多线程给long i = 30赋值之后，导致i的值不是30，可能是-3333344429，乱码一样的数字，就是因为高低32位赋值错了，就导致二进制数字转换为十进制之后是一个很奇怪的数字

### 69、volatile原来还可以保证long和double变量写操作的原子性

volatile对原子性保障的语义，在java里很有限的，几乎可以忽略不计。32位的java虚拟机里面，对long/double变量的赋值写是不原子的，此时如果对变量加上了volatile，就可以保证在32位java虚拟机里面，对long/double变量的赋值写是原子的了

int i = 0，原子性，volatile，java语言规范就规定了，原子性的

volatile long i;

多个线程执行：i = 30，此时就不要紧了，因为volatile修饰了，就可以保证这个赋值操作是原子的了

你以后出去面试也可能会遇到杠精面试官，你要说volatile是保证可见性和有序性的，不保证原子性，杠精面试官，素质差，很二，心胸很狭隘，volatile可以保证原子性，此时看过这一讲之后

i++，复杂的一些场景

resources = loadResources();

resources.execute();

ready = true;

### 70、到底有哪些操作在Java规范中是不保证原子性的呢？

所有变量的简单赋值写操作，java语言规范原生给你保证原子性的；32位java虚拟机里的long/double是不保证赋值写的原子性的；volatile可以解决这个问题；不保证原子性的一些操作呢？

i++

i = y + 1

i = x * y ==> 先把x和y分别从主内存里加载到工作内存里面来，然后再从工作内存里加载出来执行计算（处理器），计算后的结果写回到工作内存里去，最后还要从工作内存里把i的最新的值刷回主内存

CAS，AtomicInteger => compareAndSet

你敢说他是原子的？

```
volatile x = 1;
  
volatile y = 2;
  
volatile i = x * y;
```

我之前给大家已经说过了，画图都演示过了

```
FSDirectory dir = ...
  
synchronized(dir) {
  
  dir.add();
  
  dir.remove();
  
  dir.insert();
  
}  
```

加锁

### 71、可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器（上）

### 72、可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器（下）

![可见性在硬件级别的说明](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/71/01.png)

![可见性在硬件级别的说明2](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/71/02.png)

从硬件的级别来考虑一下可见性的问题

每个处理器都有自己的寄存器（register），所以多个处理器各自运行一个线程的时候，可能导致某个变量给放到寄存器里去，接着就会导致各个线程没法看到其他处理器寄存器里的变量的值修改了

可见性的第一个问题，首先，就有可能在寄存器的级别，导致变量副本的更新，无法让其他处理器看到

然后一个处理器运行的线程对变量的写操作都是针对写缓冲来的（store buffer）并不是直接更新主内存，所以很可能导致一个线程更新了变量，但是仅仅是在写缓冲区里罢了，没有更新到主内存里去

这个时候，其他处理器的线程是没法读到他的写缓冲区的变量值的，所以此时就是会有可见性的问题，这是第二个可见性发生的场景

然后即使这个时候一个处理器的线程更新了写缓冲区之后，将更新同步到了自己的高速缓存里（cache，或者是主内存），然后还把这个更新通知给了其他的处理器，但是其他处理器可能就是把这个更新放到无效队列里去，没有更新他的高速缓存

此时其他处理器的线程从高速缓存里读数据的时候，读到的还是过时的旧值

可见性发生的问题

如果要实现可见性的话，其中一个方法就是通过MESI协议，这个MESI协议实际上有很多种不同的时间，因为他不过就是一个协议罢了，具体的实现机制要靠具体底层的系统如何实现

根据具体底层硬件的不同，MESI协议的实现是有区别的

比如说MESI协议有一种实现，就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样大家的缓存不就实现同步了，然后各个处理器的线程看到的数据就一样了

为了实现MESI协议，有两个配套的专业机制要给大家说一下：flush处理器缓存、refresh处理器缓存。

flush处理器缓存，他的意思就是把自己更新的值刷新到高速缓存里去（或者是主内存），因为必须要刷到高速缓存（或者是主内存）里，才有可能在后续通过一些特殊的机制让其他的处理器从自己的高速缓存（或者是主内存）里读取到更新的值

除了flush以外，他还会发送一个消息到总线（bus），通知其他处理器，某个变量的值被他给修改了

refresh处理器缓存，他的意思就是说，处理器中的线程在读取一个变量的值的时候，如果发现其他处理器的线程更新了变量的值，必须从其他处理器的高速缓存（或者是主内存）里，读取这个最新的值，更新到自己的高速缓存中

所以说，为了保证可见性，在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一整套机制来保障的

要记住，flush和refresh，这两个操作，flush是强制刷新数据到高速缓存（主内存），不要仅仅停留在写缓冲器里面；refresh，是从总线嗅探发现某个变量被修改，必须强制从其他处理器的高速缓存（或者主内存）加载变量的最新值到自己的高速缓存里去

内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh，MESI协议是如何与内存屏障搭配使用的（flush、refresh）

volatile boolean isRunning = true;

isRunning = false; => 写volatile变量，就会通过执行一个内存屏障，在底层会触发flush处理器缓存的操作；while(isRunning) {}，读volatile变量，也会通过执行一个内存屏障，在底层触发refresh操作

之前给大家讲过那个volatile关键字的作用，对一个变量加了volatile修饰之后，对这个变量的写操作，会执行flush处理器缓存，把数据刷到高速缓存（或者是主内存）中，然后对这个变量的读操作，会执行refresh处理器缓存，从其他处理器的高速缓存（或者是主内存）中，读取最新的值

当然跟我们之前说的有一点点不一样，因为之前说的是写volatile变量的时候，一个是强制刷主内存，一个是过期掉其他处理器的高速缓存中的数据；读volatile变量的时候，会发现高速缓存中的值过期，然后强制从主内存加载最新值

其实这个东西吧，你没发现么，效果是一样的，他其实本质都是让一个线程写了volatie变量之后，另外一个变量立马可以读到volatile变量的值，只不过MESI协议的底层具体实现，根据cpu等硬件的不同，有多种不同的实现方式罢了

### 73、深入探秘有序性：Java程序运行过程中发生指令重排的几个地方

![指令重排的几个指令](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/73/01.png)

我们写好的代码在实际执行的时候那个顺序可能在很多环节都会被人给重排序，一旦重排序之后，在多线程并发的场景下，就有可能会出现一些问题

（1）自己写的源代码中的执行顺序：这个是我们自己写的代码，一般来说就是按照我们自己脑子里想的样子来写

（2）编译后的代码的执行顺序：java里有两种编译器，一个是静态编译器（javac），一个是动态编译器（JIT）。javac负责把.java文件中的源代码编译为.class文件中的字节码，这个一般是程序写好之后进行编译的。JIT负责把.class文件中的字节码编译为JVM所在操作系统支持的机器码，一般在程序运行过程中进行编译。

在这个编译的过程中，编译器是很有可能调整代码的执行顺序的，为了提高代码的执行效率，很可能会调整代码的执行顺序，JIT编译器对指令重排的还是挺多的

（3）处理器的执行顺序：哪怕你给处理器一个代码的执行顺序，但是处理器还是可能会重排代码，更换一种执行顺序，JIT编译好的指令的时候，还是可能会调整顺序

（4）内存重排序：有可能你这个处理器在实际执行指令的过程中，在高速缓存和写缓冲器、无效队列等等，硬件层面的组件，也可能会导致你的指令的执行看起来的顺序跟想象的不太一样

上述就是在我们写好java代码之后，从编译到执行的过程中，可能代码的执行顺序可能会有指令重排的地方，只要有指令重排就有一定可能造成程序执行异常

但是编译器和处理器不是胡乱的重排序的，他们会遵循一个关键的规则，就是数据依赖规则，如果说一个变量的结果依赖于之前的代码执行结果，那么就不能随意进行重排序，要遵循数据的依赖

比如说：

int a = 3;

int b = 5;

int c = a * b;

那第三行代码依赖于上面两行代码，第一行和第二行代码可以重排序，但是第三行代码必须放在最下面

此外，之前给大家介绍过happens-before原则，就是有一些基本的规则是要遵守的，不会让你胡乱的重排序

在遵守一定的规则的前提下，有好几个层面的代码和指令都可能出现重排序

### 74、JIT编译器对创建对象的指令重排以及double check单例实践

JIT动态编译的时候，有可能会造成一个非常经典的指令重排

```
public class MyObject {
    private Resource resource;
    public MyObject() {
        // 从配置文件里加载数据构造Resource对象
        this.resource = loadResource(); 
    
    }

    public void execute() {
        this.resource.execute();
    }
}

// 线程1:
MyObject myObj = new MyObject(); => 这个是我们自己写的一行代码



// 线程2：
myObj.execute();


// 步骤1：以MyObject类作为原型，
// 给他的对象实例分配一块内存空间，

//objRef就是指向了分配好的内存空间的地址的引用，指针

objRef = allocate(MyObject.class);

// 步骤2：就是针对分配好内存空间的一个对象实例，执行他的构造函数，对这个对象实例进行初始化的操作，执行我们自己写的构造函数里的一些代码，对各个实例变量赋值，初始化的逻辑
invokeConstructor(objRef);

// 步骤3：上两个步骤搞定之后，一个对象实例就搞定了，此时就是把objRef指针指向的内存地址，赋值给我们自己的引用类型的变量，myObj就可以作为一个类似指针的概念指向了MyObject对象实例的内存地址

myObj = objRef;
```

有可能JIT动态编译为了加速程序的执行速度，因为步骤2是在初始化一个对象实例，这个步骤是有可能很耗时的，比如说你可能会在里面执行一些网络的通信，磁盘文件的读写，都有可能

JIT动态编译，指令重排，为了加速程序的执行性能和效率，可能会重排为，步骤1 -> 步骤3 -> 步骤2

线程1，刚刚执行完了步骤1和步骤3，步骤2还没执行，此时myObj已经不是null了，但是MyObject对象实例内部的resource是null

线程2，直接调用myObj.execute()方法， 此时内部会调用resource.execute()方法，但是此时resource是null，直接导致空指针

double check单例模式里面，就是可能会出现这样的JIT指令重排，如果你不加volatile关键字，会导致一些问题的发生，volatile是避免说步骤1、步骤3、步骤2，必须全部执行完毕了，此时才能试用myObj对象实例

### 75、现代处理器为了提升性能的指令乱序和猜测执行的机制！

![处理器的指令乱序执行的机制](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/75/01.png)

指令乱序机制

指令不一定说是拿到了一个指令立马可以执行的，比如有的指令是要进行网络通信、磁盘读写，获取锁，很多种，有的指令不是立马就绪可以执行的，为了提升效率，在现代处理器里面都是走的指令的乱序执行机制

把编译好的指令一条一条读取到处理器里，但是哪个指令先就绪可以执行，就先执行，不是按照代码顺序来的。每个指令的结果放到一个重排序处理器中，重排序处理器把各个指令的结果按照代码顺序应用到主内存或者写缓冲器里

这就导致处理器可能压根儿就是乱序在执行我们代码编译后的指令

另外还有一个猜测执行，比如说if判断中有一坨代码，很可能先去执行if里的代码算出来结果，然后最后再来判断if是否成立

```
int sum = 0
if(flag) {
    for(int i = 0; i < 10; i++) {
    
    }
}
```

### 76、高速缓存和写缓冲器的内存重排序造成的视觉假象

![store指令重排实例](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/76/01.png)

处理器会将数据写入写缓冲器，这个过程是store；从高速缓存里读数据，这个过程是load。写缓冲器和高速缓存执行load和store的过程，都是按照处理器指示的顺序来的，处理器的重排处理器也是按照程序顺序来load和store的

但是有个问题，就是在其他的处理器看到的一个视觉假象而言，有可能会出现看到的load和store是重排序的，也就是内存重排序

处理器的乱序执行和推测执行，都是指令重排序，这次说的是内存重排序，因为都是发生在内存层面的写缓冲器和高速缓存中的

这个内存重排序，有4种可能性：

（1）LoadLoad重排序：一个处理器先执行一个L1读操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再L1

（2）StoreStore重排序：一个处理器先执行一个W1写操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再W1

（3）LoadStore重排序：一个处理器先执行一个L1读操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再L1

（3）StoreLoad重排序：一个处理器先执行一个W1写操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再W1

给大家举个例子，比如说写缓冲器为了提升性能，有可能先后到来W1和W2操作了之后，他先执行了W2操作，再执行了W1操作。那这个时候其他处理器看到的可不就是先W2再W1了，这就是StoreStore重排序

共享变量：

Resource resource = null;

Boolean resourceLoaded = false;

处理器0：

resource = loadResoureFromDisk();

resourceLoaded = true;

处理器1：

while(!resourceLoaded) {

try {

Thread.sleep(1000);

} catch(Exception) {

}

}

resource.execute();

类似上面的代码，很可能处理器0先写了resource，再写了resourceLoaded。结果呢，写缓冲器进行了内存重排序，先落地了resourceLoaded = true了，此时resource还是null。此时处理器1就会看到resourceLoaded = true，就会对resource对象执行execute()方法，此时就会有空指针异常的问题

反正类似的情况，高速缓存和写缓冲器都可以自己对Load和Store操作的结果落地到内存进行各种不同的重排序，进而造成上述4种内存重排序问题的发生

### 77、synchronized锁同时对原子性、可见性以及有序性的保证

原子性、可见性、有序性，三块东西，都重新从比较细节和底层的层面，都在硬件的级别去给大家说了一下，到底是怎么回事，为什么会发生这个问题，从底层的层面来说了一下，以及大体上有没有什么办法可以来解决这些问题

原子性，基本的赋值写操作都是可以保证原子性的，复杂的操作是无法保证原子性的

可见性，MESI协议、flush、refresh，配合起来，才可以解决可见性

有序性，三个层次，最后一个层次有4种重排（LoadLoad、StoreStore、LoadStore、StoreLoad）

synchronized关键字，同时可以保证原子性、可见性以及有序性的

原子性的层面而言，他加了以后，有一个加锁和释放锁的机制，加锁了之后，同一段代码就只有他可以执行了

可见性，可以保证可见性的，他会通过加入一些内存屏障，他在同步代码块对变量做的写操作，都会在释放锁的时候，全部强制执行flush操作，在进入同步代码块的时候，对变量的读操作，全部会强制执行refresh的操作

更新的数据，别的县城关只要进入代码块，就一定可以读到的

有序性，synchronized关键字，他会通过加各种各样的内存屏障，来保证说，解决LoadLoad、StoreStore等等重排序

### 78、深入分析synchronized是如何通过加锁保证原子性的？

总结：

1、synchronized关键字在编译以后会是monitorenter与monitorexit指令。 2、而对应的指令，再往底层走就是进入同步块执行refresh，退出同步块执行flush。 3、整体看来，synchronized的加锁、wait、释放锁与AQS差不多，都是基于一个临界资源去实现的，然后搭配上对应entryList、waitset等等就相当于是AQS中的队列以及condition队列。剩下的就是偏向锁，轻量级锁，重量级锁的差别。

### 79、synchronized是如何使用内存屏障保证可见性和有序性的？

int b = 0;

int c = 0;

synchronized(this) { -> monitorenter

Load内存屏障

Acquire内存屏障

int a = b;

c = 1; => synchronized代码块里面还是可能会发生指令重排

Release内存屏障

} -> monitorexit

Store内存屏障

java的并发技术底层很多都对应了内存屏障的使用，包括synchronized，他底层也是依托于各种不同的内存屏障来保证可见性和有序性的

按照可见性来划分的话，内存屏障可以分为Load屏障和Store屏障。

Load屏障的作用是执行refresh处理器缓存的操作，说白了就是对别的处理器更新过的变量，从其他处理器的高速缓存（或者主内存）加载数据到自己的高速缓存来，确保自己看到的是最新的数据。

Store屏障的作用是执行flush处理器缓存的操作，说白了就是把自己当前处理器更新的变量的值，都刷新到高速缓存（或者主内存）里去

在monitorexit指令之后，会有一个Store屏障，让线程把自己在同步代码块里修改的变量的值都执行flush处理器缓存的操作，刷到高速缓存（或者主内存）里去；然后在monitorenter指令之后会加一个Load屏障，执行refresh处理器缓存的操作，把别的处理器修改过的最新值加载到自己高速缓存里来

所以说通过Load屏障和Store屏障，就可以让synchronized保证可见性。

按照有序性保障来划分的话，还可以分为Acquire屏障和Release屏障。

在monitorenter指令之后，Load屏障之后，会加一个Acquire屏障，这个屏障的作用是禁止读操作和读写操作之间发生指令重排序。在monitorexit指令之前，会加一个Release屏障，这个屏障的作用是禁止写操作和读写操作之间发生重排序。

所以说，通过 Acquire屏障和Release屏障，就可以让synchronzied保证有序性，只有synchronized内部的指令可以重排序，但是绝对不会跟外部的指令发生重排序。

synchronized：

（1）原子性：加锁和释放锁，ObjectMonitor

（2）可见性：加了Load屏障和Store屏障，释放锁flush数据，加锁会refresh数据

（3）有序性：Acquire屏障和Release屏障，保证同步代码块内部的指令可以重排，但是同步代码块内部的指令和外面的指令是不能重排的

总结：

可见性保障来划分。内存屏障可分为加载屏障（Load Barrier）和存储屏障（Store Barrier）。加载屏障的作用是刷新处理器缓存，存储屏障的作用冲刷处理器缓存。Java虚拟机会在 MonitorExit （ 释放锁 ） 对应的机器码指令之后插入一个存储屏障，这就保障了写线程在释放锁之前在临界区中对共享变量所做的更新对读线程的执行处理器来说是可同步的。相应地，Java 虚拟机会在 MonitorEnter （ 申请锁 ） 对应的机器码指令之后临界区开始之前的地方插入一个加载屏障，这使得读线程的执行处理器能够将写线程对相应共享变量所做的更新从其他处理器同步到该处理器的高速缓存中。因此，可见性的保障是通过写线程和读线程成对地使用存储屏障和加载屏障实现的。 按照有序性保障来划分，内存屏障可以分为获取屏障（Acquire Barrier）和释放屏障 （ Release Barrier ）。获 取 屏 障 的 使 用 方 式 是 在 一 个 读 操 作 （ 包括 Read-Modify-Write 以及普通的读操作 ）之后插入该内存屏障，其作用是禁止该读操作与其后的任何读写操作之间进行重排序，这相当于在进行后续操作之前先要获得相应共享数据的所有权 （ 这也是该屏障的名称来源 ）。释放屏障的使用方式是在一个写操作之前插入该内存屏障，其作用是禁止该写操作与其前面的任何读写操作之间进行重排序。这相当于在对相应共享数据操作结束后释放所有权（ 这也是该屏障的名称来源 ）。 Java虚拟机会在 MonitorEnter（ 它包含了读操作 ） 对应的机器码指令之后临界区开始之前的地方插入一个获取屏障，并在临界区结束之后 MonitorExit （ 它包含了写操作 ） 对应的机器码指令之前的地方插入一个释放屏障。因此，这两种屏障就像是三明治的两层面包片把火腿夹住一样把临界区中的代码（指令序列）包括起来 由于获取屏障禁止了临界区中的任何读、写操作被重排序到临界区之前的可能性。而释放屏障又禁止了临界区中的任何读、写操作被重排序到临界区之后的可能性。因此临界区内的任何读、写操作都无法被重排序到临界区之外。在锁的排他性的作用下，这使得临界区中执行的操作序列具有原子性。因此，写线程在临界区中对各个共享变量所做的更新会同时对读线程可见，即在卖线程看来各个共享变量就像是“一下子”

### 80、再看volatile关键字对原子性、可见性以及有序性的保证

volatile对原子性的保证真的是非常的有限，其实主要就是32位jvm中的long/double类型变量的赋值操作是不具备原子性的，加上volatile就可以保证原子性了

volatile boolean isRunning = true;

线程1：

Release屏障

isRunning = false;

Store屏障 => 对于之前的讲解，更进了一步，原理，没有过多的牵扯到内存屏障的一些东西，可见性和有序性，主要都是基于各种内存屏障来实现的

线程2：

Load屏障

while(isRunning) {

Acquire屏障

// 代码逻辑

}

在volatile变量写操作的前面会加入一个Release屏障，然后在之后会加入一个Store屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作

在volatile变量读操作的前面会加入一个Load屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；

在之后会加入一个Acquire屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序

跟之前讲解的volatie读写内存屏障的知识对比一下，其实你看一下是类似的意思的

那个Acquire屏障其实就是LoadLoad屏障 + LoadStore屏障，Release屏障其实就是StoreLoad屏障 + StoreStore屏障

好像有点不太一样，对吧？

其实不要对内存屏障这个东西太较真，因为说句实话，不同版本的JVM，不同的底层硬件，都可能会导致加的内存屏障有一些区别，所以这个本来就没完全一致的。你只要知道内存屏障是如何保证volatile的可见性和有序性的就可以了

看各种并发相关的书和文章，对内存屏障到底是加的什么屏障，莫衷一是，没有任何一个官方权威的说法，因为这个内存屏障太底层了，底层到了涉及到了硬件，硬件不同对内存屏障的实现是不一样的

内存屏障这个东西，大概来说，其实就是大概的给你说一下这个意思，尤其是Release屏障，Store屏障和Load屏障还好理解一些，比较简单，Acqurie屏障，莫衷一是，我也没法给你一个官方的定论

具体底层的硬件实现

如果你一定 要杠到底，到底加的准确的屏障是什么？到底是如何跟上下的指令避免重排的，你自己去研究吧。我之前看过很多的资料，做过很多的研究，硬件对这个东西的实现和承诺，莫衷一是，没有标准和官方定论。

两点：volatile读写前后会加屏障，避免跟前后的读写操作发生指令重排

volatile和synchronized保证可见性和有序性，原来都是通过各种内存屏障来实现的，因为加了内存屏障，就会有一些特殊的指令和实现，就可以保证可见性和有序性了，有序性在几个阶段的指令重排的问题

内存屏障对应的底层的一些基本的硬件级别的原理，也都讲清楚了

### 81、高速缓存的数据结构：拉链散列表、缓存条目以及地址解码（上）

### 82、高速缓存的数据结构：拉链散列表、缓存条目以及地址解码（下）

![硬件级别的原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/81/01.png)

如果这周的课不讲，只是靠着之前的课，volatile和synchronized的原理，也能说，但是说的比较浅层一些，主要是从基础的层面来聊一下他的原理，底层的细节肯定是不行的，但是这周的课讲完了

volatile和synchronized

原子性、可见性和有序性三个方面分别来聊，这两个关键字对那几个“性”的保障是通过什么来实现的。聊到他们会加哪些内存屏障，怎么加，这些内存屏障的效果，结合底层硬件层面的一个初步的原理，来给面试官聊一下。

还是有一些遗憾的，内存屏障在硬件层面的实现的原理，到底是怎么回事，能不能再细一点，再深入一些，让大家在面试的时候聊到volatile和synchronized，直接震慑式的回答。硬件层面的一些原理

MESI协议在硬件层面的实现机制，光靠初步的MESI协议是无法保证可见性和有序性的

内存屏障在硬件层面的细致的原理，到底是如何控制那些硬件的交互和行为，最终实现可见性和有序性的保障的

volatile和synchronized，底层，彻底通透

synchronized的一些JVM对锁的优化，讲解一下；CAS底层其实也是要靠这套硬件级别的原理来给说清楚，compareAndSwap操作到底是如何在底层实现原子性的，这个东西我之前也没讲

ThreadLocal，源码基本；ReentrantLock，读写锁，源码级别

处理器高速缓存的底层数据结构实际是一个拉链散列表的结构，就是有很多个bucket，每个bucket挂了很多的cache entry，每个cache entry由三个部分组成：tag、cache line和flag，其中的cache line就是缓存的数据

tag指向了这个缓存数据在主内存中的数据的地址，flag标识了缓存行的状态，另外要注意的一点是，cache line中可以包含多个变量的值

处理器会操作一些变量，怎么在高速缓存里定位到这个变量呢？

那么处理器在读写高速缓存的时候，实际上会根据变量名执行一个内存地址解码的操作，解析出来3个东西，index、tag和offset。index用于定位到拉链散列表中的某个bucket，tag是用于定位cache entry，offset是用于定位一个变量在cache line中的位置

如果说可以成功定位到一个高速缓存中的数据，而且flag还标志着有效，则缓存命中；否则不满足上述条件，就是缓存未命中。如果是读数据未命中的话，会从主内存重新加载数据到高速缓存中，现在处理器一般都有三级高速缓存，L1、L2、L3，越靠前面的缓存读写速度越快

### 83、结合硬件级别的缓存数据结构深入分析缓存一致性协议（上）

### 84、结合硬件级别的缓存数据结构深入分析缓存一致性协议（下）

因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）

MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个处理器可以执行，其实也会通过排他锁的机制保证就一个处理器能写

之前说过那个cache entry的flag代表了缓存数据的状态，MESI协议中划分为：

（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用

（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已

（3）exclusive：独占的，标记为E，这个意思就是当前处理器对这个数据独占了，只有他可以有这个副本，其他的处理器都不能包含这个副本

（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache entry，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

MESI协议规定了一组消息，就说各个处理器在操作内存数据的时候，都会往总线发送消息，而且各个处理器还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个处理器的协作

下面来详细的图解MESI协议的工作原理，处理器0读取某个变量的数据时，首先会根据index、tag和offset从高速缓存的拉链散列表读取数据，如果发现状态为I，也就是无效的，此时就会发送read消息到总线

接着主内存会返回对应的数据给处理器0，处理器0就会把数据放到高速缓存里，同时cache entry的flag状态是S

在处理器0对一个数据进行更新的时候，如果数据状态是S，则此时就需要发送一个invalidate消息到总线，尝试让其他的处理器的高速缓存的cache entry全部变为I，以获得数据的独占锁。

其他的处理器1会从总线嗅探到invalidate消息，此时就会把自己的cache entry设置为I，也就是过期掉自己本地的缓存，然后就是返回invalidate ack消息到总线，传递回处理器0，处理器0必须收到所有处理器返回的ack消息

接着处理器0就会将cache entry先设置为E，独占这条数据，在独占期间，别的处理器就不能修改数据了，因为别的处理器此时发出invalidate消息，这个处理器0是不会返回invalidate ack消息的，除非他先修改完再说

接着处理器0就是修改这条数据，接着将数据设置为M，也有可能是把数据此时强制写回到主内存中，具体看底层硬件实现

然后其他处理器此时这条数据的状态都是I了，那如果要读的话，全部都需要重新发送read消息，从主内存（或者是其他处理器）来加载，这个具体怎么实现要看底层的硬件了，都有可能的

这套机制其实就是缓存一致性在硬件缓存模型下的完整的执行原理

![硬件级别的原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/83/01.png)

### 85、采用写缓冲器和无效队列优化MESI协议的实现性能

![硬件级别的原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/85/01.png)

MESI协议如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了

所以为了解决这个问题，硬件层面引入了写缓冲器和无效队列

写缓冲器的作用是，一个处理器写数据的时候，直接把数据写入缓冲器，同时发送invalidate消息，然后就认为写操作完成了，接着就干别的事儿了，不会阻塞在这里。接着这个处理器如果之后收到其他处理器的ack消息之后

才会把写缓冲器中的写结果拿出来，通过对cache entry设置为E加独占锁，同时修改数据，然后设置为M

其实写缓冲器的作用，就是处理器写数据的时候直接写入缓冲器，不需要同步阻塞等待其他处理器的invalidate ack返回，这就大大提升了硬件层面的执行效率了

包括查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发

引入无效队列，就是说其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

通过引入写缓冲器和无效队列，一个处理器要写数据的话，这个性能其实很高的，他直接写数据到写缓冲器，发送一个validate消息出去，就立马返回，执行别的操作了；其他处理器收到invalidate消息之后直接放入无效队列，立马就返回invalidate ack

### 86、硬件层面的MESI协议为何会引发有序性和可见性的问题？

MESI协议在硬件层面的原理其实大家都已经了解的很清晰了，对不对

可见性和有序性的问题

可见性：写缓冲器和无效队列导致的，写数据不一定立马写入自己的高速缓存（或者主内存），是因为可能写入了写缓冲器；读数据不一定立马从别人的高速缓存（或者主内存）刷新最新的值过来，invalidate消息在无效队列里面

有序性：

（1）StoreLoad重排序

int a = 0;

int c = 1;

线程1：

a = 1;

int b = c;

这个很简单吧，第一个是Store，第二个是Load。但是可能处理器对store操作先写入了写缓冲器，此时这个写操作相当于没执行，然后就执行了第二行代码，第二行代码的b是局部变量，那这个操作等于是读取a的值，是load操作

这就导致好像第二行代码的load先执行了，第一行代码的store后执行

第一个store操作写到写缓冲器里去了，导致其他的线程是读不到的，看不到的，好像是第一个写操作没执行一样；第二个load操作成功的执行了

StoreLoad重排，Store先执行，Load后执行；Load先执行，Store后执行

（2）StoreStore重排序

resource = loadResource();

loaded = true;

两个写操作，但是可能第一个写操作写入了写缓冲器，然后第二个写操作是直接修改的高速缓存，这个时候不就导致了两个写操作顺序颠倒了？

诸如此类的重排序，都可能会因为MESI的机制发生

可见性问题也是一样的，写入写缓冲器之后，没刷入高速缓存，导致别人读不到；读数据的时候，可能invalidate消息在无效队列里，导致没法立马感知到过期的缓存，立马加载最新的数据

### 87、内存屏障在硬件层面的实现原理以及如何解决各种问题

可见性问题：

Store屏障 + Load屏障

如果加了Store屏障之后，就会强制性要求你对一个写操作必须阻塞等待到其他的处理器返回invalidate ack之后，对数据加锁，然后修改数据到高速缓存中，必须在写数据之后，强制执行flush操作

他的效果，要求一个写操作必须刷到高速缓存（或者主内存），不能停留在写缓冲里

如果加了Load屏障之后，在从高速缓存中读取数据的时候，如果发现无效队列里有一个invalidate消息，此时会立马强制根据那个invalidate消息把自己本地高速缓存的数据，设置为I（过期），然后就可以强制从其他处理器的高速缓存中加载最新的值了

这就是refresh操作

为了解决有序性问题

内存屏障，Acquire屏障，Release屏障，但是都是由基础的StoreStore屏障,StoreLoad屏障，可以避免指令重排序的效果

StoreStore屏障，会强制让写数据的操作全部按照顺序写入写缓冲器里，他不会让你第一个写到写缓冲器里去，第二个写直接修改高速缓存了

resource = loadResource();

StoreStore屏障

loaded = true;

StoreLoad屏障，他会强制先将写缓冲器里的数据写入高速缓存中，接着读数据的时候强制清空无效队列，对里面的validate消息全部过期掉高速缓存中的条目，然后强制从主内存里重新加载数据

a = 1; // 强制要求必须直接写入高速缓存，不能停留在写缓冲器里，清空写缓冲器里的这条数据

int b = c;

总结：

可见性和重排序问题都是由于写缓冲器和无效队列两兄弟导致的，因此针对那种一定要求可见性和顺序性的指令，就有了各种内存屏障来强制刷入和失效变量，执行原本性能不高的MESI内存一致性协议。

通过store屏障来保证,处理器对数据的写入发送invalidate消息给总线后一定会等待invalidate ack后才能去获取锁修改数据,也就是不能直接写入到写缓冲器后认为ack了,保证写数据可见性 load屏障:保证读数据的时候一定会将无效队列中的invalidate消息写入到高速缓冲区中保证读数据的可见性 storestore屏障来保证写1一定在写2之前发生来保证store store重排的有序性同理storeload屏障保证store load重排问题

硬件级别加入写高速缓存+无效队列是为了提高系统的运行速度，但是任何缓存的引入都会引起系统级别的延迟，为了解决延迟只能强制加上了flush必须刷到高速缓存或主存中通知其他处理器，refresh操作就是读取了invalidate让自己的值过去后，重新取其他处理器或者主内存中重新获取新值，store和load就是主要对应的发送这2个指令的，后期的acquire还有release其实就是store和load屏障的两两组合，他们唯一的目的就是保证（本来处理器在执行代码时可能为了提交执行的效率，会让一些不太耗时的先执行，这样就会导致一些耗时的可能就后执行，这样它们就是乱序执行的）加上了此类的屏障，它们必须和前面的指令保持，相同的硬件执行过程，比如强制走主内存呀，强制读取新值呀，这样就能保证其执行的先后顺序不再发生改变（由于它们的执行效率差不多）。

### 88、在复杂的硬件模型之上的Java内存模型是如何大幅简化的？

java内存模型是对底层的硬件模型，cpu缓存模型，做了大幅度的简化，提供一个抽象和统一的模型给java程序员易于理解，很多时候如果要理解一些技术的本质，还是要深入到底层去研究的

volatile，原子性，可见性，有序性，加了一些内存屏障可以避免前后各种读写指令重排

synchronized，原子性，可见性，有序性，没有提到

CAS，cas指令到硬件级别，实现了一个原子性的cas操作

先把通俗易懂，简单的原理和模型给大家说一下，然后立马大量的实战，实战出真知，没感觉，整天听我讲内存屏障，枯燥死的

总结：

java虚拟机定义了java内存模型，它是对底层的硬件和操作系统内存访问差异进行了简化封装。它规范了java虚拟机与计算机内存是如何协同工作的，规定了共享变量存在于主存中，一个线程对应一个工作内存，每个线程如何同步访问主存中的共享变量和在什么时间如何看见其它线程修改主存中共享变量的值，最终实现让java程序在各种平台下达到一致的并发效果。

### 89、面试的时候如何从内存屏障、硬件层面的原理来震慑面试官。

volatile、synchronized

原子性这块，直接把底层的一些东西喷出来

硬件层面的原理 -> MESI协议在硬件层面运行的原理 -> 这套原理为何会导致可见性和有序性的问题 -> 各种内存屏障是如何在硬件层面解决可见性和有序性的问题 -> volatile和synchroized是如何加各种内存屏障来分别保证可见性和有序性的

行业里对并发这块知识掌握到这个层面的人，不多

很多人写并发的书，如果你把我们的课看完了，有并发的书，你去看看，XX并发实战，书里的内容很浅，你的水平可能已经超过部分写并发书籍的作者了

是个面试官，主要不是技术特别牛的，一般的人多会被你给震慑到，从硬件层面开始画图

总结：

1.首先硬盘和内存的发展数据远不及cpu的发展速度，而要保证cpu的告速运行就对cpu增加一个高速缓存。但是增加高速缓存以后这就导致了各个cpu之间内的高速缓存造成数据不一致的现象， 2.为了解决这种数据不一致的现象就提出了MESI协议，通过修改各个高速缓存中数据的状态来保证数据一致性的问题。M是修改状态、E是独占状态也就是加锁、S是共享状态、I是无效状态。 3.虽然通过MESI可以保证数据的一直性，但是却会大大的影响cpu的处理速度，因为cpu在修改一个处于S状态的数据时，首先会对总线发出一条invalid的通知，告诉所有其他的cpu(持有该数据)数据失效，然后等到接受到其他cpu的invalid ack消息才会对这条数据进行修改，等待ack这个时候其实是阻塞的，cpu只有等待所有ack返回之后才会执行其他的指令，而相对于cpu修改数据而言，等待ack消息的耗时是特别长的，这就体现出了cpu性能下降这个问题。 4.为了解决这个问题，就又对cpu内增加写缓存和失效队列这两个概念，cpu要写一个数据的时候首先发送invalid指令，然后把接收ack这个工作交给写缓存器，然后cpu自己去执行其他的指令。其他的cpu收到invalid消息之后直接把invalid消息扔到失效队列中然后返回invalid ack消息，这样cpu就不用因为等待ack指令而降低处理速度。 5.但是这种情况又会引发可见性和有序性的问题，被扔到写缓存里的数据不会保证什么时候完成，这就可能cpu顺序写入指令1、指令2，将他们扔到写缓存中，但是指令2先执行完成，而其他线程先看到该线程的执行顺序为指令2、指令1，这是有序性的问题。可见性也就不用说了，要修改的数据还在写缓存中等着没执行呢。 6.为了解决这种可见性和有序性的问题就引入了内存屏障的概念，在修改一个数据之后强制cpu执行完写缓存中关于该变量的指令。在读取一个数据之前强制执行时效队列中对该数据时效的指令，然后从其他高速缓存或者主内存中读取最新数据。

### 90、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（上）

### 91、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（下）

从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化

有个别同学完全没搞明白并发到底是怎么回事，一直追着问，什么是偏向锁，什么是自旋锁，锁是一种单独的锁类别。真是不懂并发技术，小白，小菜，too young too simple。其实是synchronized底层的优化和实现

synchronized说是锁，但是他的底层加锁的方式 可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁

这些东西本身你只要了解一个概念就可以了，JDK 1.6开始对synchronized关键字做过哪些优化，有哪些加锁的方式，效果是什么，作用是什么，在实际的开发和使用中，根本就不需要你去过多的care一些东西

synchronized(this) {

}

（1）锁消除

锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令

这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁

（2）锁粗化

synchronized(this) {

}

synchronized(this) {

}

synchronized(this) {

}

这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁

（3）偏向锁

这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS

性能会提升很多

但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好

可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小

如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好

（4）轻量级锁

如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁

如果是自己加的锁，那就执行代码就好了

如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁

（5）适应性锁

这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒

也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大

所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了

一直追问我，什么自旋锁，不是什么事儿，当然，如果要站在jvm的底层层面，去说清楚的话，确实是比较复杂的，但是我觉得起码目前为止，暂时也没必要，各种锁底层是如何来实现的，完全可以等到以后jvm那块都讲过之后

再回过头来深入jvm底层的原理来剖析：偏向锁、自旋锁、轻量级锁，jvm层面的概念，栈侦，Load Record，不一定能听懂，基础的知识没有铺垫好，需要通过调节jvm的一些参数来优化底层synchronized里的各种加锁方式的使用

这样可以大幅度减少线程上下文的切换，而这种自旋等待获取锁的方式，就是所谓自旋锁，就是不断的自旋尝试获取锁

如果一个线程持有锁的时间很长，那么其他线程获取不到锁，就会暂停，发生上下文切换，让其他线程来执行，这种自己暂停获取锁的方式，就是所谓的重量级锁

这个根据不同情况自动调整的过程，就是适应锁的意思

### 92、再来看看CAS是如何基于MESI协议在底层硬件层面实现加锁的？

无法发出指令来执行一个原子性的cas，先查出数据，比较一下，如果一样，就写数据。MESI协议有关系

volatile、synchronized、CAS、ThreadLocal、ReentrantReadWriteLock、锁优化、锁生产故障

并发的核心和关键的技术都到了硬件和源码的级别，大家都应该掌握的很好了

先讲线程安全的并发包下的集合，同步器组件，线程池，并发的核心技术，并发编程设计模式完全结合我们后续的微服务注册中心的项目完善、开发和实战来演练

## Spring

### 26、说说你对Spring的IOC机制和AOP机制的理解可以吗？

![img](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/26/01.png) 集合的面试题，并发的面试题，你在写代码的时候，必然都是会去用到集合包下的API，并发包下的API，基本功，你对Java最基础的一些技术和API的理解，决定了你能不能用好Java这门语言，去写好对应的程序

spring ioc

写一套系统，web服务器，tomcat，一旦启动之后，他就可以监听一个端口号的http请求，然后可以把请求转交给你的servlet，jsp，配合起来使用的，servlet处理请求

比如在我们的一个tomcat+servlet的这样的一个很low的系统里，有几十个地方，都是直接用MyService myService = new MyServiceImpl()，直接创建、引用和依赖了一个MyServiceImpl这样的一个类的对象。

我们这个low系统里，有几十个地方，都跟MyServiceImpl类直接耦合在一起了

我现在不想要用MyServiceImpl了，我们希望用的是NewServiceManagerImpl，implements MyService这个接口的，所有的实现逻辑都不同了，此时我们很麻烦，我们需要在很low的系统里，几十个地方，都去修改对应的MyServiceImpl这个类，切换为NewServiceManagerImpl这个类

改动代码成本很大，改动完以后的测试的成本很大，改动的过程中可能很复杂，出现一些bug，此时就会很痛苦，归根结底，代码里，各种类之间完全耦合在一起，出现任何一丁点的变动，都需要改动大量的代码，重新测试，可能还会有bug

Spring IOC框架，控制反转，依赖注入

xml文件来进行一个配置，进化到了基于注解来进行自动依赖注入

我们只要在这个工程里通过maven引入一些spring框架的依赖，ioc功能

tomcat在启动的时候，直接会启动spring容器

spring ioc，spring容器，根据xml配置，或者是你的注解，去实例化你的一些bean对象，然后根据xml配置或者注解，去对bean对象之间的引用关系，去进行依赖注入，某个bean依赖了另外一个bean

底层的核心技术，反射，他会通过反射的技术，直接根据你的类去自己构建对应的对象出来，用的就是反射技术

spring ioc，系统的类与类之间彻底的解耦合

现在这套比较高大上的一点系统里，有几十个类都使用了@Resource这个注解去标注MyService myService，几十个地方都依赖了这个类，如果要修改实现类为NewServiceManagerImpl

总结：

1. toma在启动时会触发容器初始化事件 。
2. spring的contextLoaderListener监听到这个事件后会执行contextinitialized方法,在该方法中会初始化spring的根容器即ioc容器,初始化完成之后,会将这个容器放入到servletContext中,以便获取 。
3. tomcat在启动过程中还会去扫描加载servlet,比如springmvc的dispatchServlet(前端控制器),用来处理每一个servlet 。
4. servlet一般采用延时加载,当一个请求过来的时候发现dispatchservlet还未初始化,则会调用其init方法,初始化时候会建立自己的容器spring mvc容器,同时spring mvc容器通过servletcontext上下文来获取到spring ioc容器将ioc容器设置为其父容器;
5. 注意事项:spring mvc中容器可以访问spring ioc容器中的bean反之不能即在controller中可以注入service bean对象,在service中不能注入controller容器。

### 27、说说你对Spring的AOP机制的理解可以吗？

spring核心框架里面，最关键的两个机制，就是ioc和aop，根据xml配置或者注解，去实例化我们所有的bean，管理bean之间的依赖注入，让类与类之间解耦，维护代码的时候可以更加的轻松便利

spring已经管理了我们代码里所有的这个类的对象实例，bean

我们有几十个Service组件，类似的一样的代码，重复的代码，必须在几十个地方都去写一模一样的东西。

spring aop机制出马了

他有几个概念，可以做一个切面，语法、用法、术语和概念，完整、详细的了解，上网再去搜一些资料 。

做一个切面，如何定义呢？MyServiceXXXX的这种类，在这些类的所有方法中，都去织入一些代码，在所有这些方法刚开始运行的时候，都先去开启一个事务，在所有这些方法运行完毕之后，去根据是否抛出异常来判断一下，如果抛出异常，就回滚事务，如果没有异常，就提交事务 => AOP。

面向切面编程，Aspect

建议大家看完这个视频，自己百度一下，spring aop代码示例，基于spring aop，切面如何来做，如何来定义增强的代码，如何来限定对哪些类的哪些方法进行增强

spring在运行的时候，动态代理技术，AOP的核心技术，就是动态代理

他会给你的那些类生成动态代理

事务，mysql，数据库里都提供一个事务机制，我们如果开启一个事务，在这个事务里执行多条增删改的sql语句，这个过程中，如果任何一个sql语句失败了，会导致这个事务的回滚，把其他sql做的数据更改都恢复回去 。

在一个事务里的所有sql，要么一起成功，要么一起失败，事务功能可以保证我们的数据的一致性，在业务逻辑组件里去加入这个事务。

总结 :

面向切面,通过动态代理的方式来生成其代理对象在方法执行前后加入自己的逻辑 - 代理方式:jdk动态代理(接口) cglib动态代理(基于类) - 相关名词: 1. JoinPoint连接点:拦截的接口的方法 2. Pointcut切入点:对哪些连接点进行拦截 3. Advice通知:比如前置通知 后置通知 环绕通知 4. aspect切面:切入点和通知组成 - 切入点 execution表达式 1. execution 权限修饰符 返回值类型 包名.类名.方法名(参数) - 通知类型 1. 前置通知:方法执行之前 2. 后置通知:在方法正常执行完毕后(提交事务) 3. 最终通知:在方法正常执行完毕或者出现异常 4. 异常通知:执行过程中出现异常(事物回滚) 5. 环绕通知:方法执行前后,目标方法默认不执行需自己调用方法

### 28、了解过cglib动态代理吗？他跟jdk动态代理的区别是什么？

优先是jdk动态代理，其次是cglib动态代理，网上搜一下两种动态代理的代码示例

其实就是动态的创建一个代理类出来，创建这个代理类的实例对象，在这个里面引用你真正自己写的类，所有的方法的调用，都是先走代理类的对象，他负责做一些代码上的增强，再去调用你写的那个类

spring里使用aop，比如说你对一批类和他们的方法做了一个切面，定义好了要在这些类的方法里增强的代码，spring必然要对那些类生成动态代理，在动态代理中去执行你定义的一些增强代码

如果你的类是实现了某个接口的，spring aop会使用jdk动态代理，生成一个跟你实现同样接口的一个代理类，构造一个实例对象出来，jdk动态代理，他其实是在你的类有接口的时候，就会来使用

很多时候我们可能某个类是没有实现接口的，spring aop会改用cglib来生成动态代理，他是生成你的类的一个子类，他可以动态生成字节码，覆盖你的一些方法，在方法里加入增强的代码

总结：

jdk基于接口，cglib基于子类.cglib创建的动态代理对象比JDK的动态代理对象的性能高，但是创建对象的时间长.对于单例无需频繁创建对象,用cglib合适

### 29、能说说Spring中的Bean是线程安全的吗？

Spring容器中的bean可以分为5个范围：

（1）singleton：默认，每个容器中只有一个bean的实例

（2）prototype：为每一个bean请求提供一个实例

一般来说下面几种作用域，在开发的时候一般都不会用，99.99%的时候都是用singleton单例作用域

（3）request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收

（4）session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效

（5）global-session

答案是否定的，绝对不可能是线程安全的，spring bean默认来说，singleton，都是线程不安全的，java web系统，一般来说很少在spring bean里放一些实例变量，一般来说他们都是多个组件互相调用，最终去访问数据库的

### 30、Spring的事务实现原理是什么？能聊聊你对事务传播机制的理解吗？

spring ioc和aop，动态代理技术，bean的线程安全问题，事务机制

事务的实现原理，事务传播机制，如果说你加了一个@Transactional注解，此时就spring会使用AOP思想，对你的这个方法在执行之前，先去开启事务，执行完毕之后，根据你方法是否报错，来决定回滚还是提交事务



// 开启一个事务

// 执行方法A的代码，接着执行方法B的代码

// 提交或者回滚事务



// 开启一个事务1

// 执行方法A里的一些代码，doSomethingPre()

// 开启一个事务2

// 执行方法B里的一些代码

// 提交或者回滚事务2

// 执行方法A里的一些代码，doSomethingPost()

// 提交或者回滚事务1



// 开启一个事务

// 执行方法A里的一些代码，doSomethingPre()

// 设置一个回滚点，savepoint

// 执行方法B里的一些代码

// 如果方法B里抛出了异常，此时进行回滚，回滚到之前的savepoint

// 执行方法A里的一些代码，doSomethingPost()

// 提交或者回滚事务

嵌套事务，外层的事务如果回滚，会导致内层的事务也回滚；但是内层的事务如果回滚，仅仅是回滚自己的代码

① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。

② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘

③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。

④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。

⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。

⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。

⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。

出去面试，事务传播机制

比如说，我们现在有一段业务逻辑，方法A调用方法B，我希望的是如果说方法A出错了，此时仅仅回滚方法A，不能回滚方法B，必须得用REQUIRES_NEW，传播机制，让他们俩的事务是不同的

方法A调用方法B，如果出错，方法B只能回滚他自己，方法A可以带着方法B一起回滚，NESTED嵌套事务

### 31、能画一张图说说Spring Boot的核心架构吗？

![](C:\Users\zy199005\Desktop\中华石杉\images\31-01.png)

spring的时候，带一下spring boot，有时候出去面试，也会问到spring boot，提的一个点，spring boot启动的时候一个流程图，本身他是spring这个项目发展到一定阶段之后的一个产物

spring框架，mybatis，spring mvc，去做一些开发，打包部署到线上的tomcat里去，tomcat启动了，他就会接收http请求，转发给spring mvc框架，调用controller -> service -> dao -> mybatis（sql语句）

java web开发的时候，在这里整合进来redis、elasticsearch、还有很多其他的一些东西，rabbitmq、zookeeper，等等，诸如此类的一些东西

国外的spring开源社区，就发起了一个项目，spring boot，我们基于spring boot直接进行开发，里面还是使用spring + spring mvc + mybatis一些框架，我们可以一定程度上来简化我们之前的开发流程

做很多的配置，自己去定义一些bean，流程比较繁琐

spring boot内嵌一个tomcat去直接让我们一下子就可以把写好的java web系统给启动起来，直接运行一个main方法，spring boot就直接把tomcat服务器给跑起来，把我们的代码运行起来了

自动装配，比如说我们可以引入mybatis，我其实主要引入一个starter依赖，他会一定程度上个自动完成mybatis的一些配置和定义，不需要我们手工去做大量的配置了，一定程度上简化我们搭建一个工程的成本

引入一些mybatis的jar包，还有mybatis依赖的一些其他的jar包，然后动手编写一些xml配置文件，然后定义一些bean，写一些sql语句，写一些dao代码，此时就可以使用mybatis去执行sql语句了

只要引入一个starter，他会自动给你引入需要的一些jar包，做非常简单的、必须的一些配置，比如数据库的地址，几乎就不用你做太多的其他额外的配置了，他会自动帮你去进行一些配置，定义和生成对应的bean

生成的bean自动注入比如你的dao里去，让你免去一些手工配置+定义bean的一些工作

spring boot + spring + spirng mvc + mybatis + XXX之类的技术去进行开发，后续很多配置和定义的一些繁琐的重复性的工作就免去了，自动装配的一些功能，自动给你把一些事情干完了，不需要你去做了

spring boot这个框架，面试突击第三季，仅仅只是扫盲，源码流程

spring boot关键的一些原理，和架构，画一张图，10来分钟的小视频，对spring boot的来龙去脉，有一个基本的了解和认识，基于spring boot开发的时候，他大致的一个工作流程是什么样子的

main，他自动启动一个内嵌的tomcat

### 32、能画一张图说说Spring的核心架构吗？

spring核心源码，spring核心架构图，里面包含了各种类和API之间的调用，引入一个别的点，把spring的核心的东西再梳理一下

spring bean生命周期，从创建 -> 使用 -> 销毁

你在系统里用xml或者注解，定义一大堆的bean

（1）实例化Bean：如果要使用一个bean的话

（2）设置对象属性（依赖注入）：他需要去看看，你的这个bean依赖了谁，把你依赖的bean也创建出来，给你进行一个注入，比如说通过构造函数，setter

（3）处理Aware接口：

如果这个Bean已经实现了ApplicationContextAware接口，spring容器就会调用我们的bean的setApplicationContext(ApplicationContext)方法，传入Spring上下文，把spring容器给传递给这个bean

（4）BeanPostProcessor：

如果我们想在bean实例构建好了之后，此时在这个时间带你，我们想要对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。

（5）InitializingBean 与 init-method：

如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。

（6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法

（7）DisposableBean：

当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法；

（8）destroy-method：

最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。

创建+初始化一个bean -> spring容器管理的bean长期存活 -> 销毁bean（两个回调函数）

总计：

1、spring核心点主要是IOC和AOP，核心技术分别是反射与代理。 2、bean的声明周期可以分为：根据配置生成bean对象 -> 为bean传入参数 -> 实现了aware接口可以拿到IOC容器 -> 实现beanpostProcessor接口可以在bean完成创建的前后去做一些操作 -> 最后还可以配置init-method会调用对应的函数 -> 如果配置了destory则会在bean被回收的时候调用对应函数。 3、关于bean的循环依赖问题，单例作用域的bean，通过构造器注入会时循环依赖会报错，因为创建实例对象时无法完成。而通过set/get注入不会报错，因为先创建了实例，spring会缓存一下创建好的bean去注入到对应依赖的bean中。

### 33、能说说Spring中都使用了哪些设计模式吗？

工厂，单例，代理

不可能有任何的技术深度，面试突击第二季里，我尝试着做了一些技术深度的讲解，但是我觉得还是很困难，只能稍微给你讲解一些原理

Spring源码底层去看，很多种设计模式的一个运用

工厂模式，单例模式，代理模式

工厂模式，spring ioc核心的设计模式的思想提现，他自己就是一个大的工厂，把所有的bean实例都给放在了spring容器里（大工厂），如果你要使用bean，就找spring容器就可以了，你自己不用创建对象了

![工厂模式](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/33/01.png)

spring默认来说，对每个bean走的都是一个单例模式，确保说你的一个类在系统运行期间只有一个实例对象，只有一个bean，用到了一个单例模式的思想，保证了每个bean都是单例的

![单例模式](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/33/02.png)

稍微还算是有点含金量的

如果说你要对一些类的方法切入一些增强的代码，会创建一些动态代理的对象，让你对那些目标对象的访问，先经过动态代理对象，动态代理对象先做一些增强的代码，调用你的目标对象

在设计模式里，就是一个代理模式的体现和运用，让动态代理的对象，去代理了你的目标对象，在这个过程中做一些增强的访问，你可以把面试突击的内容作为一个抛砖引玉的作用，去更加深入的学习一些技术

### 34、能画一张图说说Spring Web MVC的核心架构吗？

（1）tomcat的工作线程将请求转交给spring mvc框架的DispatcherServlet

（2）DispatcherServlet查找@Controller注解的controller，我们一般会给controller加上你@RequestMapping的注解，标注说哪些controller用来处理哪些请求，此时根据请求的uri，去定位到哪个controller来进行处理

（3）根据@RequestMapping去查找，使用这个controller内的哪个方法来进行请求的处理，对每个方法一般也会加@RequestMapping的注解

（4）他会直接调用我们的controller里面的某个方法来进行请求的处理

（5）我们的controller的方法会有一个返回值，以前的时候，一般来说还是走jsp、模板技术，我们会把前端页面放在后端的工程里面，返回一个页面模板的名字，spring mvc的框架使用模板技术，对html页面做一个渲染；返回一个json串，前后端分离，可能前端发送一个请求过来，我们只要返回json数据

（6）再把渲染以后的html页面返回给浏览器去进行显示；前端负责把html页面渲染给浏览器就可以了

总结：

1、服务启动时，将Dispatcherservlet注入到tomcat中。 2、tomcat工作线程接收http请求后，将请求转发给Dispatcherservlet。 3、Dispatcherservlet会查找有@Controller注解标记的类，然后根据uri找到对应目标Controller。 4、然后在controller中再次找到对应的方法，最后进行一系列的调用。 5、处理完毕后，分为两种方式返回。一是前端页面放在后端工程时，可以返回对应的页面模板名字，然后springmcv使用模板技术进行渲染后返回。二是前后端分离以后，直接返回前端所需JSON串即可。

### 35、能画一张图说说Spring Cloud的核心架构吗？

![SpringCloud](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/35/01.png)

spring boot、spring、spring mvc、spring cloud，让你开发那种单体架构的系统，spring cloud是让你去开发分布式系统，让你把系统拆分为很多的子系统，子系统互相之间进行请求和调用

面试突击第二季，有完整的spring cloud架构原理的讲解

eureka、ribbon、feign、zuul、hystrix、链路追踪、其他组件，服务于分布式系统的，hystrix主要用于服务之间调用的熔断、隔离、降级，在狸猫技术窝上，在我的课程的目录里，有一个文档，标识出来了我的一些之前的课程，csdn上去搜索，亿级流量里面带有hystrix讲解，看一下

## JVM

### 36、JVM中有哪几块内存区域？Java 8之后对内存分代做了什么改进？

集合、并发、spring框架，期待着我对这些基础的东西做一些很深入的，很牛X的讲解，基于框架写一些代码，完事儿了之后，就会把代码进行一个部署，一般来说是通过tomcat、jetty来部署java web系统

tomcat部署，tomcat自己就是基于java来开发的，我们启动的不是自己的系统，是一个tomcat是一个jvm进程，我们写的系统只不过是一些代码，放在tomcat的目录里，tomcat会去加载我们的代码到jvm里去

tomcat去负责接收请求，执行我们写好的代码，基于spring框架的一大堆代码

狸猫技术窝，有我好朋友写的《从0开始带你成为JVM实战高手》

面试突击第三季的定位，在10讲的jvm内容，会带着大家把jvm最最基础和最最常用的一些概念和原理，过一遍，起到一个复习的作用，起到一个抛砖引玉的效果，会给大家植入一些硬广，jvm那个专栏的一些内容的介绍

跟面试常问的一些思路结合起来，让大家可以站在面试的角度去思考一下，jvm平时面试会怎么来问呢，如何为了面试去好好准备jvm的东西呢

执行我们的一些对象的方法，执行代码的时候肯定会有很多的线程，tomcat里就有很多自己的工作线程，去执行我们写的代码，每个工作线程都会有自己的一块数据结构，栈内存，这个里面是存放一些东西

java 8以后的内存分代的改进，永久代里放了一些常量池+类信息，常量池 -> 堆里面，类信息 -> metaspace（元区域）

元数据区取代了永久代,两者类似,都是对JVM规范中方法区的实现,元数据空间不在虚拟机中，而是使用本地内存

总结：

jdk1.7 java 堆，java栈，本地方法栈，程序计数器，方法区

jdk1.8 java堆，java栈，本地方法栈，程序计数器，元空间

### 37、你知道JVM是如何运行起来的吗？我们的对象是如何分配的？

![01](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/37/01.png)

一定会有线程去执行我们写的代码

比如说我们有一个类里面包含了一个main方法，你去执行这个main方法，此时会自动一个jvm进程，他会默认就会有一个main线程，这个main线程就负责执行这个main方法的代码，进而创建各种对象

tomcat，类都会加载到jvm里去，spring容器而言都会对我们的类进行实例化成bean，有工作线程会来执行我们的bean实例对象里的方法和代码，进而也会创建其他的各种对象，实现业务逻辑

### 38、说说JVM在哪些情况下会触发垃圾回收可以吗？

我们的jvm的内存其实是有限制的，不可能是无限的，昂贵的资源，2核4G的机器，堆内存也就2GB左右，4核8G的机器，堆内存可能也就4G左右，栈内存也需要空间，metaspace区域放类信息也需要空间

在jvm里必然是有一个内存分代模型，年轻代和老年代

比如说给年轻代一共是2GB内存，给老年代是2GB内存，默认情况下eden和2个s的比例：8:1:1，eden是1.6GB，S是0.2GB

如果说eden区域满了，此时必然触发垃圾回收，young gc，ygc，谁是可以回收的垃圾对象呢？就是没有人引用的对象就是垃圾对象

总结：

1、垃圾回收的主要作用区域是堆区，堆区分为年轻代和老年代，其中年轻代又分为eden与两个s区，这样分代的原因主要是方便根据各个区域的对象特点选择更合适的回收算法。 2、在分区清晰后，GC的触发条件是这两个区域对象装不下了，因此就要通过GC回收掉没有引用的对象。 3、可以分为两个区域去总结。年轻代：没有足够的内存去分配给新的对象了。老年代：没有足够的内存去分配给新晋升的对象了，或者是因为分配担保机制导致老年代触发GC。 4、最后的metaspace也会因为内存不足从而触发GC。

### 39、说说JVM的年轻代垃圾回收算法？对象什么时候转移到老年代？

![01](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/39/01.png)

如果说你让代码一边运行，一边有变动，一边判断哪些对象是可以回收的，这个是不现实的，垃圾回收的时候有一个概念，叫做stop the world，停止你的jvm里的工作线程的运行，然后扫描所有的对象，判断哪些可以回收，哪些不可以回收的

年轻代，大部分情况下，对象生存周期是很短的，可能在0.01ms之内，线程执行了3个方法，创建了几个对象，0.01ms之后就方法都执行结束了，此时那几个对象就会在0.01ms之内变成垃圾，可以回收的

100个对象，可能90个对象都是垃圾对象，10个对象是存活的对象，5个

复制算法，一次young gc，年轻代的垃圾回收

三种场景，第一种场景，有的对象在年轻代里熬过了很多次垃圾回收，15次垃圾回收，此时会认为这个对象是要长期存活的对象 。

Spring容器里，对每个bean实例对象就一个，长期存活，一直给我们来使用

比如说有一个对象自己就有100MB，此时如果他是长期存活的，每次ygc，他都要在年轻代里反复移动

### 40、说说老年代的垃圾回收算法？常用的垃圾回收器都有什么？

老年代对象越来越多，是不是会发现说，老年代的内存空间也会满的，可以不可以使用类似年轻代的复制算法，不合适的，因为老年代里的对象，很多都是被长期引用的，spring容器管理的各种bean 。

长期存活的对象是比较多的，可能甚至有几百MB 。

对老年代而言，他里面垃圾对象可能是没有那么多的，标记-清理，找出来那些垃圾对象，然后直接把垃圾对象在老年代里清理掉，标记-整理，把老年代里的存活对象标记出来，移动到一起，存活对象压缩到一片内存空间里去

剩余的空间都是垃圾对象整个给清理掉，剩余的都是连续的可用的内存空间，解决了内存碎片的一个问题 。

parnew+cms的组合，g1直接分代回收，新版本，慢慢的就是主推g1垃圾回收器了，以后会淘汰掉parnew+cms的组合，jdk 8~jdk 9比较居多一些，parnew+cms的组合比较多一些，是这么一个情况。

分成好几个阶段，初始标记，并发标记，并发清理，等等，老年代垃圾回收是比较慢的，一般起码比年轻代垃圾回收慢个10倍以上，cms的垃圾回收算法，刚开始用标记-清理，标记出来垃圾对象，清理掉一些垃圾对象，整理，把一些存活的对象压缩到一起，避免内存碎片的产生。

执行一个比较慢的垃圾回收，还要stop the world，需要100mb，此时就会让系统停顿100ms，不能处理任何请求，尽可能的让垃圾回收和工作线程的运行，并发着来执行。

总结：

1、老年代的垃圾回收算法根据老年代的特性有两类，标记清除和标记整理。 2、常用的垃圾回收器组合有parnew+cms和G1，前者目前使用比较多，后者主要适用于大内存机器。

### 41、你们生产环境中的Tomat是如何设置JVM参数的？如何检查JVM运行情况？

面试的时候，面试官很多时候都是针对jvm的一些运行原理去深扣，结合我讲的东西，然后去把jvm专栏里面的内容仔细看一下，应付面试都是很容易的，一般来说都会这么问，你们线上系统的生产环境的jvm参数是怎么来配置的，为什么要这么配置，在你们配置的这个参数之下，线上系统jvm运行的情况如何

你确实必须得去看一下你当前生产系统的jvm参数都是如何设置的，如果说你是tomcat部署的java web系统，jvm进程对应的tomcat自己，你的系统仅仅是在tomcat的jvm进程来执行

tomcat的一个配置脚本，catalina脚本里去找一下，jvm专栏都有说明的，里面是有对应的tomcat启动的一些jvm参数的设置

比如通过java命令直接启动你的一个main方法跑起来的系统，就是你自己启动的时候，java命令可以带上一些jvm参数

对你自己系统的jvm参数有一个了解，内存区域大小的分配，每个线程的栈大小，metaspace大小，堆内存的大小，年轻代和老年代分别的大小，eden和survivor区域的大小分别是多少，如果没有设置，会有一些默认值

jvm专栏里，在中间有一些地方，他是讲了一些命令的，可以查看jvm的启动默认参数

垃圾回收器，年轻代是用了什么，老年代，每种垃圾回收器是否有对应的一些特殊的参数有设置，那些特殊的参数分别都是用来干什么的

为什么要这么设置呢？当前线上系统运行的时候，jvm的表现如何？

救火队队长的《从0开始带你成为jvm实战高手》，有大量的实战案例的讲解，业务背景引出，在一定的业务背景之下，如何去进行系统运行时的对象数量的预估，对内存的压力进行预估，对整个jvm运行的状况进行预估

预估完毕之后，根据预估的情况，可以去设置一些jvm参数

进行压测，在压测的时候，其实就需要去观察jvm运行的情况，jstat工具去分析jvm运行的情况，他的年轻代里的eden区域的对象增长的情况，ygc的频率，每次ygc过后有多少对象存活，s能否放的下，老年代对象增长速率，老年代多久会触发一次fgc

就可以根据压测的情况去进行一定的jvm参数的调优，一个系统的QPS，一个是系统的接口的性能，压测到一定程度的时候 ，机器的cpu、内存、io、磁盘的一些负载情况，jvm的表现

可能需要对一些代码进行优化，比如优化性能，或者减轻一点cpu负担，减轻io和磁盘负担，发现jvm的gc过于频繁，内存泄漏，此时就需要对jvm的各个内存区域的大小以及一些参数进行调优

跑到线上实际生产环境里去，运行的过程中，也需要基于一些监控工具，或者是jstat，除了观察系统的QPS和性能，接口可用性，调用成功率，机器的负载，jvm的表现，gc的频率，gc耗时，内存的消耗

总结：

1、根据自己线上系统参数而定。 2、可以用第三方工具，也可以用JDK自带的jstat等工具查看JVM运行情况。 3、最后压测调优。

### 42、你在实际项目中是否做过JVM GC优化，怎么做的?

如何通过预估 + 压测，做一份生产环境的jvm参数出来的，如何去观察jvm运行的情况，jvm出现频繁full gc的问题，你有没有尝试过生产环境的系统去进行gc的一个优化，对于这个问题

狸猫技术窝公众号上的救火队队长的《从0开始带你成为jvm实战高手》，有非常详细的案例的分析，通过很多个案例，去分析如何在各种各样奇葩的背景之下，发现jvm的gc很频繁，导致系统卡顿问题

如何一步一步去分析系统的jvm的性能问题，如何去进行jvm gc调优

假设你没看过jvm专栏，自己做过jvm gc的生产调优，恭喜你了，直接实话实说，你当时怎么调优，你们的问题如何暴露出来的，你如何一步一步定位问题的，如何进行调优，最后的结果是什么 。

你看了jvm专栏，在过程中，或者看完以后，在自己生产环境中根据专栏学习到的知识，去调优过jvm，这个时候，你可以专栏里学习到的知识，去讲。最好对自己系统的生产环境的jvm，进行一个分析，gc频繁的问题

你尽可能的去调优一下参数，如果效果比较好

发现分析了一下生产环境的jvm的运行情况，非常好，并发量很低，几十分钟才一次young gc，存活的对象特别少，几乎都在s区域，老年代几乎没什么对象，几天或者几周才发生一次full gc

在自己本地单机部署，测试环境里，去压测，每秒单机有500并发请求，去观察jvm的运行情况，这个时候他会不会存在频繁gc的问题，你就去调优一下，你就可以基于这个压测的例子去说了

一定要结合你自己的业务，系统，接口，干什么，并发请求，jvm运行的情况，问题出在哪儿，如何调优，效果如何

### 43、你知道发生OOM之后，应该如何排查和处理线上系统的OOM问题？

oom可能发生在哪几个区域，解决的一个思路，在jvm里可以设置几个参数，如果一旦jvm发生了oom之后，就会导出一份内存快照，就会有当时的线上内存里的对象的一个情况，可以用MAT这样的工具，可以去分析

无非就是找出来当时的时候占用内存最大的对象都是谁，找出来那些对象是在代码中哪些地方创建出来的，一般来说就是可能会对内存去做一个调优

还是得去参考jvm专栏里的大量的案例背景，从业务背景出发，一步步去说明，在什么样的业务背景之下，为什么会产生oom的问题呢？必然会导致系统可能就是崩溃了，客服会反馈说，XX功能不能用了，说某个系统崩溃了

找他自动导出的内存快照，分析，XX对象，直接去定位代码，修改代码

你一定要把案例的业务、背景和思想给吸收了，就得融入到自己的业务里去，我负责的业务系统，在什么样的情况下，可能说会出现一大批的对象卡在内存里，无法回收，导致我系统没法放更多的对象了

产生OOM，内存泄漏的问题，少数场景在互联网公司，超高并发下的oom问题，瞬时大量存活对象占据内存， 导致没法创建更多的对象了

你也得去思考，甚至去模拟一下，最好可以模拟出来，oom不是你自己的代码，可能是你依赖的第三方的组件，netty导致的，结合自己的项目去一步一步的分析，oom问题的产生，和解决的过程

总结：

1、排查主要是通过GC日志和快照分析工具，定位到巨大的对象以及对应的代码。 2、OOM主要是由两种，内存溢出：就是原本需要放这么多，内存的确放不下。内存泄漏：原本只想放50m，结果因为代码的原因导致实际上放了2G都还没被回收，典型的大的map/list等结构，或者threadlocal等等。

### 90、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（上）

### 91、Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁（下）

总结：

1、锁消除：JIT编译器通过逃逸分析等技术发现有些被加锁的代码不会出现线程安全问题，那么动态编译的时候就会消除掉这个加锁的操作。（一般是有些框架里面自己加的synchronized而我们作为程序员并不知道，主要是优化这个） 2、锁粗化：多个同步块合并在一起去执行。 3、偏向锁：偏向于第一个加锁的线程，下一次这个线程再来加锁就不用加锁了，提升性能。但是仅仅适用于非常低的并发场景，因为一旦有第二个线程去尝试加锁，原本偏向的那个线程会被挂起来释放锁，偏向锁也就失效了，升级为轻量级锁。 4、轻量级锁：主要是基于对象头里面的mark word进行cas，防止每一次加锁都用到os互斥量的重量级锁。这个也仅仅适用于只有少量并发的情况，因为一旦第二个线程加锁失败，进入自旋，仍然失败，就会升级到重量级锁。 5、自旋锁：为了尽量少使用os的互斥量所做的最后努力（如果重量级锁竞争失败了，会进入自适应自旋），如果自旋也失败了，就会被挂起导致上下文切换。 6、重量级锁：就是直接使用OS互斥量来进行加锁操作的一种锁，涉及到内核态和用户态的相互转换。

## 网络

### 44_你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！(上)

### 45_你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！(中)

### 46_你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！(下)

**面试官心理分析**

为啥要问这个？

坦白讲，一些大的公司，计算机基础必面，尤其是针对薪资30k以内的工程师，因为薪资30k以内，你还是要干活儿的吧，还没上升到就设计架构就可以的程度吧，你还没到那个高度吧。

所以只要你干活儿，你就不可避免要跟机器、网络、cpu、磁盘、内存，成天打交道。而线上系统，计算基础的一些东西，网络、cpu、磁盘、内存，都是关联很大的，比如说你线上系统会不会因为网络故障导致一些问题？cpu负载达到100%了咋办？磁盘读写很慢快满了咋办？内存使用率过高咋办？

你起码得有一套自己的计算机功底去支撑你玩儿线上系统吧。所以很多人呢，都说计算机基础没啥用，那这个话呢，也对，也不对。对就在于，你如果毕业出来干简单的crud，这些东西你确实不需要；不对就在于，你如果当个高工，带几个小弟干高并发有压力的线上系统，机器负载很高，很容易出问题，结果你连机器都不敢摸，或者也不知道怎么摸，那不是尴尬了么。

所以说，计算机基础，网络、磁盘、cpu、内存，还是得会一点儿基础的

作为一个大公司的面试官，一定会考察你这些东西

**剖析**

首先要说一下，四层模型和七层模型，我们往往是可以一块儿来聊的。

**（1）首先我问要明白，为啥要有协议**

设想一下，各个电脑厂商，比如IBM、苹果啥的，都弄自己的协议，结果就苹果电脑和苹果电脑自己可以通信，和IBM电脑就不可以通信，这不是尴尬么。所以搞一个国际通行的协议，大家都按照这个来，所有电脑都可以通信，不是很好么。

此时就必须搞一个标准的网络模型出来，大家都按照这个来走，大家都要遵守统一的规范。这就是所谓OSI七层模型，他们分别是：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。那么在这个基础上，又简化出了TCP/IP四层模型，数据链路层、网络层、传输层、应用层。

**（2）从底向上的网络分层**

1）物理层

物理层，物理层干啥的，就是电脑之间要联网，一般咋弄？类似于说，你有台电脑，现在要联网，咋联？以前N年前，大家记不记得都是在电脑上插根线是吧，然后才能上网，结果现在就是联个wifi就行了，还有中国美国之前联网靠的是海底的光缆。所以物理层就指的这个，就是怎么把各个电脑给联结起来，形成一个网络，这就是物理层的含义，物理层负责传输0和1的电路信号。学过一些计算机的同学，计算机的最最底层，就是0/1，电信号。

2）数据链路层

00000011（从电脑1出发，要到电脑2去）

00101（从电脑1出发，要到电脑3去）

0101（从电脑2触发，要到电脑4去）

01（从电脑3出发，要到电脑5去）

很多年前，每个公司都定义自己的电路信号分组方式，但是后来出来了以太网协议，以太网。一组电信号是一个数据包，叫一个帧（frame），每个帧分成两个部分，标头（head）和数据（data），标头包含一些说明性的东西，比如说发送者、接收者和数据类型之类的。

每台电脑要往另外一台电脑发送数据，一堆0/1电路信号，封装成数据包，包含头和数据，头里包含了从哪儿来到哪儿去，必须从一台电脑的一个网卡，发送到另外一个电脑的一个网卡，所以以太网发送的数据包必须得指定，目标电脑的网卡的mac地址。

以太网规定了，每个网卡必须得包含一个mac地址，mac地址就是这个网卡的唯一标识，

以太网协议规定了，接入网络里的所有设备，都得有个网卡，以太网协议里的那个数据包，在数据链路层传输的数据包，必须从一个电脑的网卡传输到另外一个电脑的网卡，而这个网卡地址就叫做所谓的mac地址。每块网卡出厂的时候，就有一个唯一的mac地址，48位的二进制，但是一般用12个16进制数字表示，前6个16进制是厂商编号，后6个16进制是网卡流水号。

windows上，ipconfig /all，看看物理地址，就是mac地址，7C-67-A2-20-AB-5C

所以在以太网里传输数据包的时候，必须指定接收者的mac地址才能传输数据。

但是以太网的数据包怎么从一个mac地址发送到另一个mac地址？这个不是精准推送的，以太网里面，如果一个电脑发个数据包出去，会广播给局域网内的所有电脑设备的网卡，然后每台电脑都从数据包里获取接收者的mac地址，跟自己的mac地址对比一下，如果一样，就说明这是发给自己的数据包。

但是上面这种广播的方式，仅仅针对一个子网（局域网）内的电脑，会广播，否则一个电脑不能广播数据包给全世界所有的其他电脑吧，是仅仅广播给一个子网里面的电脑的。

如下图：

![以太网的数据包发送mac地址](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/44/02.png)

3）网络层

上面说到，子网内的电脑，通过以太网发个数据包，对局域网内的电脑，是广播出去的。那么怎么知道哪些电脑在一个子网内呢？这就得靠网络层了，这里就有一套IP地址，IP地址就可以让我们区分哪些电脑是一个子网的。

网络层里有IP协议，IP协议定义的地址就叫做IP地址。IP地址有IPv4和IPv6两个版本，目前广泛使用的是IPv4，是32个二进制数字组成的，但是一般用4个十进制数字表示，范围从0.0.0.0到255.255.255.255之间。

每台计算机，都会分配一个ip地址，ip地址的前24位（就是前面3个十进制数字），代表了网络，后8位（就是最后1个十进制数字），代表了主机。

如果几台电脑是一个子网的，那么前面的3个十进制数字一定是一样的。举个例子，大家平时做实验，玩儿虚拟机吧，自己win上开几个linux虚拟机，你会发现，win上的ip地址可能是192.168.0.103，然后几个虚拟机的ip地址是192.168.0.182，192.168.0.125，192.168.0.106，类似这样的。

这个win机器和几个虚拟机，前面3个十进制数字都是192.168.0，就代表大家是一个子网内的，最后那个数字是这个子网的不同主机的编号。

但是实际上上面就是举个例子，其实单单从ip地址是看不出来哪些机器是一个子网的，因为从10进制是判断不出来的。需要通过ip地址的二进制来判断，结合一个概念来判断，叫做子网掩码。

比如说ip地址是192.168.56.1，子网掩码是255.255.255.0。知道了子网掩码之后，如果要判断两个ip地址是不是一个子网的，就分别把两个ip地址和自己的子网掩码进行二进制的与运算，与运算之后，比较一下代表网络的那部分。

192.168.56.1和192.168.32.7，判断是不是一个子网的，拿子网掩码255.255.255.0，跟两个ip地址的二进制做与运算

11000000.10101000.00111000.00000001

11111111.11111111.11111111.00000000

子网掩码的二进制是：11111111.11111111.11111111.00000000，然后就跟ip地址的二进制做与好了，通过二进制来比较网络部分的地址是不是一模一样的。

有了网络层的ip地址之后，两台在子网内的电脑终于可以通过广播+mac地址判断来传输数据包进行通信了。

但是如果发现要接受数据包的计算机不在子网内，那么就不能通过广播来发送数据包，需要通过路由来发送数据包。

看到路由，就想到了路由器了，对了，路由器大家都熟悉吧，自己平时也会去买对吧，比如小米的路由器啥的，家里上网一般都会弄个路由器对吧，ok。路由器负责将多个子网进行连接，因为比如你在自己家里，其实你就只是你自己的一个子网，你要是访问网站啥的，是跟那个网站机器所在的子网进行通信。

每个电脑都可以搞多个网卡的，不是只有一个网卡，一般笔记本电脑都有以太网网卡和wifi网卡，发送数据包的时候要决定走哪个网卡。路由器，其实就是配置了多个网卡的一个专用设备，可以通过不同的网卡接入不同的网络。

网关其实是就是路由器的一种，运作在网络层，这个概念不多解释了，大家可以就把路由器上的ip地址认为是网关，路由器上每个网卡都有mac地址和对应的ip地址。路由器虽然有mac地址，但是不能通过mac地址寻址的，必须通过ip地址寻址，所以路由器其实是工作在网络层的设备。

网络交换机，也是一种设备，是工作在数据链路层的，路由器是工作在网路层的。

网络交换机是通过mac地址来寻址和传输数据包的；但是路由器是通过ip地址寻址和传输数据包的。网络交换机主要用在局域网的通信，一般你架设一个局域网，里面的电脑通信是通过数据链路层发送数据包，通过mac地址来广播的，广播的时候就是通过网络交换机这个设备来把数据广播到局域网内的其他机器上去的；路由器一般用来让你连入英特网。

LAN，就是local area network，就是局域网；WAN，就是wide area network，就是广域网。WLAN是wireless local area network，就是无线局域网，也就是wifi，在局域网内，直接通过wifi无线联网。

家里的路由器是包含了交换机和路由的两个功能的，如果是连接到局域网内的设备就把线插LAN那儿；如果是连接到英特网，就把线插在WAN那儿。

这儿给大家举个例子，就是两个局域网之间，如果要是通过一个路由器进行通信的话，是怎么弄的。

大概过程就是，路由器配置了两块网卡，每个网卡可以连到一个局域网内。

局域网1内的电脑，要发送数据包到局域网2内的电脑，在数据包里写上自己的ip地址和对方的ip地址。但是他们俩不在一个局域网内，于是局域网1内的电脑，先通过交换机将数据包发送给路由器，这个过程需要将路由器的一块网卡的ip地址对应的mac地址写到数据包的头部，然后才能通过交换机广播出去，路由器接收到之后比较自己一块网卡的mac地址，就知道是来找自己的。

接着路由器接收到数据包之后，就会在局域网2内，将目标机器的ip地址对应的mac地址写入头部，接着再次通过交换机发送广播通知，发送给局域网2内的电脑。

一个局域网内的每台机器都有自己的ARP cache，这个ARP就是用来在一个局域网内让各个设备都知道每个设备的ip地址和mac地址的对应关系的，一般就是某个机器发送广播通知自己的ip地址和mac地址的对应关系，然后每个机器给他一个回应。以此类推，大家都互相这样广播一把，ip地址和mac地址的对应关系，大家不就都知道了吗？

所以大家在上面可以看到，一个子网内的机器之间通信，就是在数据包里写上对方的mac地址，然后交换机广播出去ok了；但是如果是跨子网的通信，就是写上对方的ip地址，然后先通过mac地址广播到路由器，让路由器再根据另外一个子网的ip地址转换为mac地址，通过另外一个子网的交换机广播过去。就这个意思。

![跨子网的通信](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/44/03.png)

4）传输层

上面我们大概明白了通过网络层的ip地址怎么划分出来一个一个的子网，然后在子网内部怎么通过mac地址广播通信；跨子网的时候，怎么通过ip地址 -> mac地址 -> 交换机 -> 路由器 -> ip地址 -> mac地址 -> 交换机的方式来通过路由器进行通信。

但是这里还有一个问题，就是一台机器上，是很多个程序用一个网卡进行网络通信的，比如说浏览器、QQ、视频直播，这些软件都用了一个网卡往外面发送数据，然后从网卡接收数据，对吧。

所以还需要一个端口号的概念，就是你得发送数据包到某个机器的一个网卡的某个端口上去，然后那个机器上监听那个端口的程序，就可以提取发送到这个端口的数据，知道是自己的数据。端口号是065536的范围内，01023被系统占用了，别的应用程序就用1024以上的端口就ok了。

电脑1，是在端口48362监听的，通过网卡发送了一条数据 -> 电脑2的ip地址的20386这个端口 -> 电脑2的上面的某个QQ，监听着20386的端口 -> 电脑2的网卡接收到一条数据之后，发现人家找的是20386这个端口，就去找谁哪个哥儿们在监听20386端口，QQ在监听，我就把这个网卡过来的数据，传递给QQ，通过端口知道，哪条数据是给你的 。

所以其实大家会发现一点，网络层，是基于ip协议，进行主机和主机间的寻址和通信的，然后传输层，其实是建立某个主机的某个端口，到另外一个主机的某个端口的连接和通信的。

这个通信，就是通过socket来实现的，通过socket就可以基于tcp/ip协议完成刚才上面说的一系列的比如基于ip地址和mac地址转换和寻址啊，通过路由器通信啊之类的，而且会建立一个端口到另外一个端口的连接。

udp和tcp都是传输层的协议，作用就是在数据包里加入端口号，可以通过端口号进行点对点的通信了。udp协议是不可靠的，发出去人家收到没有就不知道了；tcp协议是可靠的，要求三次握手，而且要求人家接收到数据必须回复你。

传输层的tcp协议，仅仅只是规定了一套基于端口的点对点的通信协议，包括如何建立连接，如何发送和读取消息，但是实际上如果你要基于tcp协议来开发，你一般是用socket，java socket网络编程。

![java socket网络编程](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/44/04.png)

5）应用层

通过传输层的tcp协议可以传输数据，但是人家收到数据之后，怎么来解释？比如说收到个邮件你怎么处理？收到个网页你怎么处理？类似这个意思，所以针对各种不同的应用，邮件、网页之类的，都是定义不同的应用层协议的。这个应用层，我们就假设综合了会话层、表示层和应用层了，3层合成1层。

电脑1走tcp协议发送了一段东西过来，发送到电脑2的20386端口

GET http://localhost:8080/ http/1.1

key:valuel

key:value

电脑2走tcp协议读取到了属于自己这个20386端口 的一段数据

GET http://localhost:8080/ http/1.1

key:valuel

key:value

发送了一段响应

200

key;value

key:value

又通过底层的tcp发了出去，电脑1的30987端口，ip

电脑1，网卡，走以太网协议收到一个数据包

200

key;value

key:value

比如最常见的，应用层的协议就是http协议，进行网络通信。

然后我们看下自己的网络设置，一般包含了ip地址、子网掩码、网关地址、DNS地址。前面3个我们其实都知道啥意思了。ip地址和子网掩码用来划分子网的，判断哪些ip地址在一个子网内。同时你的ip地址和mac地址关联起来的，唯一定位了你的网卡。网关地址，你就认为是路由器上的那个网卡的ip地址吧，路由器的网卡也有mac地址，mac地址对应了一个ip地址。

DNS地址是啥呢？Domain Name System。因为我们一般定位是通过ip地址+mac地址+端口号来定位一个通信目标的，但是如果在浏览器上输入一个[www.baidu.com](www.baidu.com)，咋整？这个时候是先把[www.baidu.com](www.baidu.com)发给DNS服务器，然后DNS服务器告诉你[www.baidu.com](www.baidu.com)对应的ip地址的。

总结：

1、物理层：物理设备，传输0/1电路信号。

2、数据链路层：解析/组包电路信号，走以太网协议，封装对应带有mac地址的数据包。

3、网络层：通过IP再次对底层网络进行抽象，将网络划分为局域网，广域网等等。

4、传输层：TCP/UDP等协议作用层。

5、应用层：HTTP/STMP等协议作用层。

6、交换机：作用于数据链路层，作用是广播以太网数据包。

7、路由器(网关)：作用于网络层，连接多个子网。

8、ARP协议：作用于数据链路层，主要是通过广播的方式，拿到目标IP的对应mac地址，并缓存一段时间。

9、DNS协议：作用于应用层，主要是通过UDP的方式拿到域名对应的IP地址。

9、整个过程：数据包由应用层对应的协议产生，然后向下逐一封装，TCP报文 -> IP报文 -> 以太网报文。最后由交换机广播在整个子网内，如果目标mac地址不在同一子网内，会由路由器去一次次转发，最后目标子网的网关接收到，发现目标电脑的mac地址在自己的子网内，于是再通过交换机在自己的子网内广播，最终目标电脑接收到报文。

前提：网络中主机与主机之间是通过Mac地址+IP地址+端口号来定位通信目标的

1.物理层：作用是把主机之间连接起来，传输0/1电信号

2.数据链路层：基于以太网协议，对物理层传输的电信号进行分组，每个数据包为一个帧，每个帧包含标头和数据，标头封装了要进行通信的主机的Mac地址。工作在这层的硬件为交换机，作用域在一个局域网内，基于Mac地址进行寻址

3.网络层：通过IP地址+子网掩码确定与目标主机是否在同一个局域网内：在同一个子网内，则通过广播+Mac地址来传输数据包进行网络通信；不在同一个子网内，则通过路到其他子网内继续判断与目标主机是否在同一个局域网 4.传输层：通过socket协议，将数据包加入端口号，实现主机的某个端口号与目标主机的某个端口号之间的点对点通信

5.应用层：HTTP协议、FTP协议等。解析tcp层传输的数据

### 47、浏览器请求[www.baidu.com](www.baidu.com)的全过程大概是怎么样的？（上）

### 48、浏览器请求[www.baidu.com](www.baidu.com)的全过程大概是怎么样的？（下）

**面试官心里分析**

这个问题，其实就是跟之前的那个一样，他就是考察考察你的基本功，看你对基本的网络通信知识有没有了解。

而且话说回来，考察一个人的基本功，就这个问题应该是相当经典和直接的一个问题，你理解清楚了，那么网络通信这块的一些基本概念，你基本都了解了。

**剖析**

如果你阅读过昨天发布文章，就应该知道网络七层模型大概都是怎么回事了，然后四层模型其实就是会话层、表示层和应用层，合并为了一个应用层，同时没把物理层算在内

并且我们也大概知道每一层的协议和作用，网络通信的时候都是怎么回事了，现在我们来看看假设通过浏览器发送一个请求，你访问到那个网站对应的机器，然后人家再给你一个响应的全过程。

现在我们先假设，我们给电脑设置了几个东西：

ip地址：192.168.31.37

子网掩码：255.255.255.0

网关地址：192.168.31.1

DNS地址：8.8.8.8

这时，我们打开一个浏览器，请求[www.baidu.com](www.baidu.com)地址，这个时候找DNS服务器，DNS服务器解析域名之后，返回一个ip地址，比如172.194.26.108。

接着会判断两个ip地址是不是一个子网的，用子网掩码255.255.255.0，对两个ip地址做与运算，拿到192.168.31.0和172.194.26.0，明显不是一个子网的。

那就得发送一个数据包给网关，其实你就认为是我们的路由器吧，就是192.168.31.1，而且我们是可以拿到网关ip地址的mac地址的，现在我们从应用层出发，通过浏览器访问一个网站，是走应用层的http协议的，并且要把浏览器发出的请求打包成数据包，要把哪些东西给放到数据包中去呢？

http协议分为几个部分：

请求方法+URL地址+http版本

比如

GEThttp://172.194.26.108/testHTTP/1.1，类似这种请求头，类似下面这种：

Host:upload.jiangsu.io

Proxy-Connection:keep-alive

User-Agent:Mozilla/5.0

等等。。。

请求体，比如常见的可以放一个json这就构成了一个http请求报文浏览器请求一个地址，先按照应用层的http协议，封装一个应用层数据包，数据包里就放了http请求报文，这个时候会将这个http请求报文打包成一个数据包，仅仅只是数据包的数据部分，此时是数据包是没有头的。上面根据http协议搞一个http请求报文，然后搞一个数据包出来，就是网络模型中到的应用层干的事儿了。

接着就是跑传输层来了，这个层是tcp协议，这个tcp协议会让你设置端口，发送方的端口随机选一个，接收方的端口一般是默认的80端口。

这个时候，会把应用层数据包给封装到tcp数据包中去，而且会加一个tcp头，这个tcp数据包是对应一个tcp头的，这个tcp头里就放了端口号信息。

接着跑到网络层来了，走ip协议，这个时候会把tcp头和tcp数据包，放到ip数据包里去，然后再搞一个ip头，ip头里本机和目标机器的ip地址。

这里本机ip地址是192.168.31.37，

目标机器是172.194.26.108。

因为，通过ip协议，可以判断说，两个ip地址不是在一个子网内的，所以此时只能将数据包先通过以太网协议广播到网关上去，通过网关再给他发送出去。

接着是数据链路层，这块走以太网协议，这里是把ip头和ip数据包封到以太网数据包里去，然后再加一个以太网数据包的头，头里放了本机网卡mac地址，和网关的mac地址。但是以太网数据包的限制是1500个字节，但是假设这个时候ip数据包都5000个字节了，那么需要将ip数据包切割一下。

这个时候一个以太网数据包要切割为4个数据包，每个数据包包含了以太网头、ip头和切割后的ip数据包，4个数据包的大小分别是1500，1500,1500，560。ip头里包含了每个数据包的序号。

这4个以太网数据包都会通过交换机发到你的网关上，然后你的路由器是可以联通别的子网的，这个是时候你的路由器就会转发到别的子网的可能也是某个路由器里去，然后以此类推吧，N多个路由器或者你叫网关也行，N多个网关转发之后，就会跑到百度的某台服务器，接收到4个以太网数据包。

百度服务器接收到4个以太网数据包以后，根据ip头的序号，把4个以太网数据包里的ip数据包给拼起来，就还原成一个完整的ip数据包了。接着就从ip数据包里面拿出来tcp数据包，再从tcp数据包里取出来http数据包，读取出来http数据包里的各种协议内容，接着就是做一些处理，然后再把响应结果封装成htp响应报文，封装在http数据包里，再一样的过程，封装tcp数据包，封装ip数据包，封装以太网数据包，接着通过网关给发回去。

总结：

1、DNS域名解析，通过UDP广播的方式拿到对应域名的IP。

2、拿到IP以后进行子网判断，如果不在同一个子网内，则向下进行组包。

3、应用层组http的请求报文，运输层组TCP报文，网络层组网络协议报文，数据链路层组以太网协议报文。

4、组好报文后由网关进行路由发出，经过层层的转发，最终到达目标服务器。

5、目标服务器收到对应的报文后，层层解析，最终交付应用层HTTP报文，由对应的服务处理后响应。

6、最终服务器返回的网络资源经过同样的过程到达用户电脑，并由浏览器展示。

发送请求过程：数据包的一层层封装过程

应用层：基于HTTP协议，将请求报文（请求行+请求头+请求体）打包成数据包传送给传输层

传输层：传输层基于tcp协议，将应用层数据包封装到tcp数据包中，加一个tcp头，头部信息存放发送者的端口号（随机选一个）以及接受者的端口号，将tcp数据包传递到网络层

网络层：网络层基于IP协议。将tcp头+tcp数据包封装到IP数据包,加一个IP头，IP头中包含本机和目标主机的IP地址，将IP数据包传递到数据链路层

数据链路层：基于以太网协议，把ip头和ip数据包封到以太网数据包里去，然后再加一个以太网数据包的头，头里放了本机网卡mac地址，和网关的mac地址。但是以太网数据包的限制是1500个字节，需将IP数据包分割成多个数据包，每个数据包包含了以太网头、ip头和切割后的ip数据包。通过ip协议+子网掩码，可以判断两个ip地址不是在一个子网内的，同一子网内则传送数据；不在一个子网内，只能将数据包先通过以太网协议广播到网关上去，通过网关再给他发送出去

返回数据过程：数据包的一层层拆封过程

### 49、画一下TCP三次握手流程图？为啥是三次而不是二次或者四次呢？

TCP三次握手和四次握手的工作流程是什么（画一下流程图）？为什么不是五次握手或者两次握手？

（1）tcp三次握手过程

通过传输层的tcp协议建立网络连接的时候，其实走的是三次握手的过程

建立三次握手的时候，TCP报头用到了下面几个东西，ACK、SYN、FIN。

第一次握手，客户端发送连接请求报文，此时SYN=1、ACK=0，这就是说这是个连接请求，seq = x，接着客户端处于SYN_SENT状态，等待服务器响应。

第二次握手，服务端收到SYN=1的请求报文，需要返回一个确认报文，ack = x + 1，SYN=1，ACK = 1，seq = y，发送给客户端，自己处于SYN_RECV状态。

第三次握手，客户端收到了报文，将ack = y + 1，ACK = 1，seq = x + 1

![01_tcp三次握手](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/49/01.png)

（2）为啥不是2次或者4次握手呢？

假设两次握手就ok了，要是客户端第一次握手过去，结果卡在某个地方了，没到服务端；完了客户端再次重试发送了第一次握手过去，服务端收到了，ok了，大家来回来去，三次握手建立了连接。

结果，尴尬的是，后来那个卡在哪儿的老的第一次握手发到了服务器，服务器直接就返回一个第二次握手，这个时候服务器开辟了资源准备客户端发送数据啥的，结果呢？客户端根本就不会理睬这个发回去的二次握手，因为之前都通信过了。

但是如果是三次握手，那个二次握手发回去，客户端发现根本不对，就会发送个复位的报文过去，让服务器撤销开辟的资源，别等着了。

因为3次握手就够了，不需要4次或者5次浪费资源了。

![02_tcp连接为什么是3次握手](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/49/02.png)

（3）tcp断开连接的4次挥手

第一次挥手，客户端发送报文，FIN=1，seq=u，此时进入FIN-WAIT-1状态

第二次挥手，服务端收到报文，这时候进入CLOSE_WATI状态，返回一个报文，ACK=1，ack=u+1，seq=v。客户端收到这个报文之后，直接进入FIN-WAIT-2状态，此时客户端到服务端的连接就释放了。

第三次挥手，服务端发送连接释放报文，FIN=1，ack=u+1，seq=w，服务端进入LAST-ACK状态

第四次挥手，客户端收到连接释放报文之后，发应答报文，ACK=1，ack=w+1，seq=u+1，进入TIME_WAIT状态，等待一会儿客户端进入CLOSED状态，服务端收到报文之后就进入CLOSED状态。

![03_tcp的4次挥手断开连接](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/49/03.png)

总结：

三次握手、四次挥手、以及其他TCP协议的控制套件都是为了保证可靠传输。

1、三次握手：保证client-server都可以进行可靠收发，第三次是为了确认服务端的可靠收发。还有一种说法就是如果两次就建立连接的话，会因为网络的原因，有些第一次握手的请求滞后了，导致服务端额外的开销，因此出现这种请求，就需要第三次握手去复位连接，释放掉服务端资源。

2、四次挥手，主要是第二次挥手后服务端依旧可以发送数据，然后client在发出第四次挥手后需要等待2MSL的时间才会进入closed状态。具体原因也是出于可靠传输，一是为了防止第四次挥手的消息超时，2MSL的时间可以处理超时重发。 二是为了防止该client在服务端确认关闭前再次连接上该server，保证该连接被正确关闭。挥手过程耗时相对较长，所以说频繁的创建和关闭TCP连接的开销比较大。

### 50、聊聊HTTP协议的工作原理！

说一下http的工作流程？http 1.0、http 1.1、http 2.0具体有哪些区别？

http发起请求的底层原理，大家其实都知道了，理解了那个原理，就一通百通了。那么来聊下http请求和响应的规范吧。其实请求的报文，就是请求头、请求方法、请求正文，GET/POST啥的，应该都知道；请求头，自己百度一下吧，作为一个工程师必须知道。响应，状态行，响应头，响应正文，状态行，200,400,500，实在不想讲了；响应头，自己查一下。

http请求封装到应用层数据包，封装在tcp数据包，封装在ip数据包，封装在以太网数据包，如果过大，可能会拆成几个包，走以太网协议+交换机 -> 广播 -> 网关 -> 多个网关 -> 目标的机器 -> 一层一层拆包 -> http请求报文 -> 传递给tomcat -> spring mvc -> http响应 -> 一样的路径会去

最最底层，这个数据如何传输？走的是物理层，网线、光缆，所有数据都是0/1电路信号

http协议，其实是每个搞java必须会的基础。

互联网初期，一般一个网页几乎都没什么图片，当时就是挂一些文字，一个网页里就是一大坨的文字。http 1.0版本。

浏览器 -> 网站，互相之间是先要通过tcp三次握手，建立一个连接，浏览器和网站互相都给对方留出一份资源，浏览器发起http请求 -> tcp -> ip -> 以太网，网站上面去，网站返回一个响应，连接关闭，tcp四次挥手。释放掉浏览器和网站各自给对方保持的一份资源。

http 1.0要指定keep-alive来开启持久连接，默认是短连接，就是浏览器每次请求都要重新建立一次tcp连接，完事儿了就释放tcp连接。早期的网页都很low，没啥东西，就一点文字，就用这个没问题。但是现在，一个网页打开之后，还要加载大量的图片、css、js，这就坑爹了，发送多次请求。

早期，2000年之前，那个时候网页，都很low，当时你打开一个网页，就是说现场底层tcp三次握手，跟网站建立一个tcp连接，然后通过这个tcp连接，发送一次http请求，网站返回一个http响应（网页的html，里面有一大段文字），浏览器收到html渲染成网页，浏览器就走tcp四次挥手，跟网站断开连接了

到了后面，发现说2000之后，2010之后更不用说了，网页发展很迅猛，一个网页包含着大量的css、js、图片等资源。比如你请求一个网页，这个网页的html先过来，过来之后，浏览器再次发起大量的请求去加载css、js、图片，打开一个网页可能浏览器要对网站服务器发送几十次请求。

http 1.0，疯了，刚开始请求网页的html，tcp三次握手建立连接 -> 请求/响应 -> tcp四次挥手断开连接，接着再次要加载css、js、图片，要发送30个请求，上面的过程来30次，30次频繁的建立tcp连接以及释放tcp连接。很慢很慢。

其实最慢的不是说发送请求和获取响应，打开和释放连接，这都是很重的过程 。

http 1.1默认支持长连接，就是说，浏览器打开一个网页之后，底层的tcp连接就保持着，不会立马断开，之后加载css、js之类的请求，都会基于这个tcp连接来走。http 1.1还支持host头，也就可以支持虚拟主机；而且对断点续传有支持。

浏览器，第一次请求去一个网站的一个页面的时候，就会打开一个tcp连接，接着就在一段时间内都不关闭了，然后接下来这个网页加载css、js、图片大量的请求全部走同一个tcp连接，频繁的发送请求获取响应，最后过了一段时间，这些事儿都完了，然后才会去释放那一个tcp连接。大幅度的提升复杂网页的打开的速度，性能。

http 2.0，支持多路复用，基于一个tcp连接并行发送多个请求以及接收响应，解决了http 1.1对同一时间同一个域名的请求有限制的问题。二进制分帧，将传输数据拆分为更小的帧（数据包），frame（数据包，帧），提高了性能，实现低延迟高吞吐。

总结：

http：主要是分为head+body。 http1.0：每次请求都需要单独建立连接，在获取响应后关闭连接。效率低，不利于复杂网页的加载。 http1.1：支持TCP长连接，在请求头里面加了一行keep-alive。从而提高效率。 http2.0：有一些新特性，例如多路复用，首部压缩，服务器推送等等。主要是为了进一步提高性能。

### 51、聊聊HTTPS的工作原理？为啥用HTTPS就可以加密通信？

http和https的区别是什么？https的原理是什么？

http协议都是明文的，是没有加密的，所以其实现在一般大部分应用都是用https协议的。之前是基于SSL协议对http进行加密，后来又升级到了TSL协议来加密，现在称之为SSL / TSL吧。

https的工作原理大概是这样的：

（1）浏览器把自己支持的加密规则发送给网站

（2）网站从这套加密规则里选出来一套加密算法和hash算法，然后把自己的身份信息用证书的方式发回给浏览器，证书里有网站地址、加密公钥、证书颁发机构

（3）浏览器验证证书的合法性，然后浏览器地址栏上会出现一把小锁；浏览器接着生成一串随机数密码，然后用证书里的公钥进行加密，这块走的非对称加密；用约定好的hash算法生成握手消息的hash值，然后用密码对消息进行加密，然后把所有东西都发给网站，这块走的是对称加密

（4）网站，从消息里面可以取出来公钥加密后的随机密码，用本地的私钥对消息解密取出来密码，然后用密码解密浏览器发来的握手消息，计算消息的hash值，并验证与浏览器发送过来的hash值是否一致，最后用密码加密一段握手消息，发给浏览器

（5）浏览器解密握手消息，然后计算消息的hash值，如果跟网站发来的hash一样，握手就结束，之后所有的数据都会由之前浏览器生成的随机密码，然后用对称加密来进行进行加密。

常用的非对车呢加密是RSA算法，对称加密是AES、RC4等，hash算法就是MD5

就好比，有个人说我加密的时候是用了一个公钥去加密，然后你解密的时候是用私钥去解密；我加密的时候用的算法，跟解密的时候用的算法，是一样的，对称加密

![https原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/51/01.png)

总结：

1、发送自身支持的加密规则，然后网站返回对应的证书回来。 2、浏览器校验证书后出现一把小锁，然后生成随机数，计算消息hash值，再用该随机数对消息进行加密，最后将随机数进行公钥加密，发送给网站。网站收到消息后，先对整体随机数进行私钥解密，再通过随机数解密消息，对比其hash值，确认未被篡改。 3、网站再用随机数发送确认，之后浏览器确认以后，那么以后就用随机数进行加密后进行通信。 4、这个随机数是不能被第三方获取的，因为一开始是浏览器自己生成的，然后发送出去的时候已经用公钥加密了，没有私钥就解不出来。因此，这个随机密码就只会有浏览器和目标网站能拿到。

### 52、聊聊http的长连接的工作原理到底是啥？

什么是长连接？http长连接是什么？

http本身没什么所谓的长连接短连接之说，其实说白了都是http下层的tcp连接是长连接还是短连接，tcp连接保持长连接，那么多个http请求和响应都可以通过一个链接来走。其实http 1.1之后，默认都是走长连接了，就是底层都是一个网页一个tcp连接，一个网页的所有图片、css、js的资源加载，都走底层一个tcp连接，来多次http请求即可。

http 1.0的时候，底层的tcp是短连接，一个网页发起的请求，每个请求都是先tcp三次握手，然后发送请求，获取响应，然后tcp四次挥手断开连接；每个请求，都会先连接再断开。短连接，建立连接之后，发送个请求，直接连接就给断开了

http 1.1，tcp长连接，tcp三次握手，建立了连接，无论有多少次请求都是走一个tcp连接的，走了n多次请求之后，然后tcp连接被释放掉了

总结：

http的长短连接，本质上是TCP的socket连接在一次交互完成后是否进行关闭。

### 92、再来看看CAS是如何基于MESI协议在底层硬件层面实现加锁的？

总结:

cas主要基于mesi协议中的e，也就是对该变量在硬件级别上加一个独占锁，从而来实现原子性的比较替换。

## MySQL

### 53、MySQL、MyISAM和InnoDB存储引擎的区别是啥？（上）

### 54、MySQL、MyISAM和InnoDB存储引擎的区别是啥？（下）

MySQL有哪些存储引擎啊（myisam和innodb）？都有什么区别？请详细说明一下。

mysql支持的存储引擎有很多种，innodb、myisam、memory，很多，但是我就讲其中两种，因为其实现在，常用的就一种，innodb，myisam以前可能还有一些场景会用，现在用的已经非常少了

（1）myisam

myisam，不支持事务，不支持外键约束，索引文件和数据文件分开，这样在内存里可以缓存更多的索引，对查询的性能会更好，适用于那种少量的插入，大量查询的场景。

比如说最经典的就是报表系统，比如大数据的报表系统，给大家画个图聊聊一半都是怎么玩儿的，常见的就是走hadoop生态来搞，hdfs来存储数据，然后基于hive来进行数仓建模，每次hive跑出来的数据都用sqoop从hive中导出到mysql中去。然后基于mysql的在线查询，就接上j2ee写个简单的web系统，每个报表开发一套代码，写sql查数据，组织数据，按照前端要求的格式返回数据，展现出来一个报表。

这种报表系统，是最适合mysql的myisam存储引擎的，不需要事务，就是一次性批量导入，接下来一天之内就是纯查询了。

![基于myisam存储引擎做报表系统](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/53/01.png)

这个是比较low的做法，说实在的，现在你要让我说myisam的场景其实不多了，在很多大数据场景里是不适用的，因为真正的大数据系统，很多时候hadoop跑出来的结果还是很大，一天就几千万结果数据，几十亿明细数据，那mysql是抗不住这么大量的数据的。所以现在大数据一般用kylin做离线数据的分析引擎，直接hive数据导入kylin里面去了，或者也可以走elasticsearch。

尝试过做过一个事情，用mysql分库分表来抗，抗不住了，单表一般建议是控制在几百万的数据量级，500w以内的数据量，多少表？多少库？多少台数据库服务器？sql多达几百行，各种子查询、join、函数、行转列、列传行，非常不适合用mysql -> 数据量很大 -> sql很复杂 -> 导致mysql数据库服务器cpu负载过高

比较高端一点了，我们会基于自己研发的可配置化BI系统 + kylin + elasticsearch，支持大规模数据的复杂报表的支持，做的非常好，效果远远超出基于mysql的那套方案

后来还有那种实时数据报表，就是storm或者是spark streaming，跑数据出来，来一条算一条，然后结果立马写入mysql中，这个的话，一般就保留当天数据，其实压力不会太大，但是问题在于说，可能写并发会超高，每秒并发轻易就可以几千甚至上万。所以大数据实时报表不会写mysql了，现在一般都是写es。

你可以按照我上面的这套说辞去说说，如果是java方向的同学，就说你们之前配合你们公司的数据团队开发过这种报表系统的j2ee部分，所以当时用myisam比较多，但是后来人家几乎都不用了，借此体现出你是有实际经验的，这回答的档次都不一样了。

（2）innodb

说真的，现在一般用mysql都是innodb，我真很少用其他的存储引擎，而且国内用其他存储引擎的场景和公司也不多，所以用innodb就可以了，而且这个也是mysql 5.5之后的默认存储引擎。

主要特点就是支持事务，走聚簇索引，强制要求有主键，支持外键约束，高并发、大数据量、高可用等相关成熟的数据库架构，分库分表、读写分离、主备切换，全部都可以基于innodb存储引擎来玩儿，如果真聊到这儿，其实大家就可以带一带，说你们用innodb存储引擎怎么玩儿分库分表支撑大数据量、高并发的，怎么用读写分离支撑高可用和高并发读的，用上第1季的内容就可以了。

说实话，关于存储引擎，现在因为其实真的主要就是innodb，聊到这儿就可以了，反而被问到这问题，多拓展根据你的经验来回答

总结：

1、myisam索引文件和数据文件分离，之前用着OLAP类型应用，现在已经不常用了，主要是不支持事务。

2、innodb是一个事务安全的存储引擎，可以支持行级锁，良好的事务支持，已经对各种业务场景都有成熟的解决方案。

### 55、聊聊MySQL的索引实现原理？各种索引你们平时都怎么用的？（上）

### 56、聊聊MySQL的索引实现原理？各种索引你们平时都怎么用的？（下）

**1.索引的数据结构是什么**

其实就是让你聊聊mysql的索引底层是什么数据结构实现的，弄不好现场还会让你画一画索引的数据结构，然后会问问你mysql索引的常见使用原则，弄不好还会拿个SQL来问你，就这SQL建个索引一般咋建？

至于索引是啥？这个问题太基础了，大家都知道，mysql的索引说白了就是用一个数据结构组织某一列的数据，然后如果你要根据那一列的数据查询的时候，就可以不用全表扫描，只要根据那个特定的数据结构去找到那一列的值，然后找到对应的行的物理地址即可。

那么回答面试官的一个问题，mysql的索引是怎么实现的？

答案是，不是二叉树，也不是一颗乱七八糟的树，而是一颗b+树。这个很多人都会这么回答，然后面试官一定会追问，那么你能聊聊b+树吗？

但是说b+树之前，咱们还是先来聊聊b-树是啥，从数据结构的角度来看，b-树要满足下面的条件：

（1）d为大于1的一个正整数，称为B-Tree的度。

（2）h为一个正整数，称为B-Tree的高度。

（3）每个非叶子节点由n-1个key和n个指针组成，其中d<=n<=2d。

（4）每个叶子节点最少包含一个key和两个指针，最多包含2d-1个key和2d个指针，叶节点的指针均为null 。

（5）所有叶节点具有相同的深度，等于树高h。

（6）key和指针互相间隔，节点两端是指针。

（7）一个节点中的key从左到右非递减排列。

（8）所有节点组成树结构。

（9）每个指针要么为null，要么指向另外一个节点。

（10）如果某个指针在节点node最左边且不为null，则其指向节点的所有key小于v(key1)，其中v(key1)为node的第一个key的值。

（11）如果某个指针在节点node最右边且不为null，则其指向节点的所有key大于v(keym)，其中v(keym)为node的最后一个key的值。

（12）如果某个指针在节点node的左右相邻key分别是keyi和keyi+1且不为null，则其指向节点的所有key小于v(keyi+1)且大于v(keyi)。

上面那段规则，我也是从网上找的，说实话，没几个java程序员能耐心去看明白或者是背下来，大概知道是个树就好了。就拿个网上的图给大家示范一下吧：

比如说我们现在有一张表：

(

id int

name varchar

age int

)

我们现在对id建个索引：15、56、77、20、49

select * from table where id = 49

select * from table where id = 15

反正大概就长上面那个样子，查找的时候，就是从根节点开始二分查找。大概就知道这个是事儿就好了，深讲里面的数学问题和算法问题，时间根本不够，面试官也没指望你去讲里面的数学和算法问题，因为我估计他自己也不一定能记住。

好了，b-树就说到这里，直接看下一个，b+树。b+树是b-树的变种，啥叫变种？就是说一些原则上不太一样了，稍微有点变化，同样的一套数据，放b-树和b+树看着排列不太一样的。而mysql里面一般就是b+树来实现索引，所以b+树很重要。

b+树跟b-树不太一样的地方在于：

1. 每个节点的指针上限为2d而不是2d+1。

2. 内节点不存储data，只存储key；

   叶子节点不存储指针。

select * from table where id = 15

select * from table where id>=18 and id<=49

但是一般数据库的索引都对b+树进行了优化，加了顺序访问的指针，如网上弄的一个图，这样在查找范围的时候，就很方便，比如查找18~49之间的数据：

其实到这里，你就差不多了，你自己仔细看看上面两个图，b-树和b+树都现场画一下，然后给说说区别，和通过b+树查找的原理即可。

接着来聊点稍微高级点的，因为上面说的只不过都是最基础和通用的b-树和b+树罢了，但是mysql里不同的存储引擎对索引的实现是不同的。

**2.myism存储引擎的索引实现**

先来看看myisam存储引擎的索引实现。就拿上面那个图，咱们来现场手画一下这个myisam存储的索引实现，在myisam存储引擎的索引中，每个叶子节点的data存放的是数据行的物理地址，比如0x07之类的东西，然后我们可以画一个数据表出来，一行一行的，每行对应一个物理地址。

**索引文件**

id=15，data：0x07，0a89，数据行的物理地址

数据文件单独放一个文件

select * from table where id = 15 -> 0x07物理地址 -> 15，张三，22

myisam最大的特点是数据文件和索引文件是分开的，大家看到了么，先是索引文件里搜索，然后到数据文件里定位一个行的。

**3.innodb存储引擎的索引**

好了，再来看看innodb存储引擎的索引实现，跟myisam最大的区别在于说，innodb的数据文件本身就是个索引文件，就是主键key，然后叶子节点的data就是那个数据的所在行。我们还是用上面那个索引起来现场手画一下这个索引好了，给大家来感受一下。

innodb存储引擎，要求必须有主键，会根据主键建立一个默认索引，叫做聚簇索引，innodb的数据文件本身同时也是个索引文件，索引存储结构大致如下：

15，data：0x07，完整的一行数据，（15,张三,22）

22，data：完整的一行数据，（22,李四,30）

就是因为这个原因，innodb表是要求必须有主键的，但是myisam表不要求必须有主键。另外一个是，innodb存储引擎下，如果对某个非主键的字段创建个索引，那么最后那个叶子节点的值就是主键的值，因为可以用主键的值到聚簇索引里根据主键值再次查找到数据，即所谓的回表，例如：

select * from table where name = ‘张三’

先到name的索引里去找，找到张三对应的叶子节点，叶子节点的data就是那一行的主键，id=15，然后再根据id=15，到数据文件里面的聚簇索引（根据主键组织的索引）根据id=15去定位出来id=15这一行的完整的数据

所以这里就明白了一个道理，为啥innodb下不要用UUID生成的超长字符串作为主键？因为这么玩儿会导致所有的索引的data都是那个主键值，最终导致索引会变得过大，浪费很多磁盘空间。

还有一个道理，一般innodb表里，建议统一用auto_increment自增值作为主键值，因为这样可以保持聚簇索引直接加记录就可以，如果用那种不是单调递增的主键值，可能会导致b+树分裂后重新组织，会浪费时间。

**4.索引的使用规则**

一般来说跳槽时候，索引这块必问，b+树索引的结构，一般是怎么存放的，出个题，针对这个SQL，索引应该怎么来建立

select * from table where a=1 and b=2 and c=3，你知道不知道，你要怎么建立索引，才可以确保这个SQL使用索引来查询。

好了，各位同学，聊到这里，你应该知道具体的myisam和innodb索引的区别了，同时也知道什么是聚簇索引了，现场手画画，应该都ok了。然后我们再来说几个最最基本的使用索引的基本规则。

其实最基本的，作为一个java码农，你得知道最左前缀匹配原则，这个东西是跟联合索引（复合索引）相关联的，就是说，你很多时候不是对一个一个的字段分别搞一个一个的索引，而是针对几个索引建立一个联合索引的。

给大家举个例子，你如果要对一个商品表按照店铺、商品、创建时间三个维度来查询，那么就可以创建一个联合索引：shop_id、product_id、gmt_create

一般来说，你有一个表（product）：shop_id、product_id、gmt_create，你的SQL语句要根据这3个字段来查询，所以你一般来说不是就建立3个索引，一般来说会针对平时要查询的几个字段，建立一个联合索引

后面在java系统里写的SQL，都必须符合最左前缀匹配原则，确保你所有的sql都可以使用上这个联合索引，通过索引来查询

create index (shop_id,product_id,gmt_create)

**（1）全列匹配**

这个就是说，你的一个sql里，正好where条件里就用了这3个字段，那么就一定可以用到这个联合索引的：

select * from product where shop_id=1 and product_id=1 and gmt_create=’2018-01-01 10:00:00’

**（2）最左前缀匹配**

这个就是说，如果你的sql里，正好就用到了联合索引最左边的一个或者几个列表，那么也可以用上这个索引，在索引里查找的时候就用最左边的几个列就行了：

select * from product where shop_id=1 and product_id=1，这个是没问题的，可以用上这个索引的

**（3）最左前缀匹配了，但是中间某个值没匹配**

这个是说，如果你的sql里，就用了联合索引的第一个列和第三个列，那么会按照第一个列值在索引里找，找完以后对结果集扫描一遍根据第三个列来过滤，第三个列是不走索引去搜索的，就是有一个额外的过滤的工作，但是还能用到索引，所以也还好，例如：

select * from product where shop_id=1 and gmt_create=’2018-01-01 10:00:00’

就是先根据shop_id=1在索引里找，找到比如100行记录，然后对这100行记录再次扫描一遍，过滤出来gmt_create=’2018-01-01 10:00:00’的行

这个我们在线上系统经常遇到这种情况，就是根据联合索引的前一两个列按索引查，然后后面跟一堆复杂的条件，还有函数啥的，但是只要对索引查找结果过滤就好了，根据线上实践，单表几百万数据量的时候，性能也还不错的，简单SQL也就几ms，复杂SQL也就几百ms。可以接受的。

**（4）没有最左前缀匹配**

那就不行了，那就在搞笑了，一定不会用索引，所以这个错误千万别犯

select * from product where product_id=1，这个肯定不行

**（5）前缀匹配**

这个就是说，如果你不是等值的，比如=，>=，<=的操作，而是like操作，那么必须要是like ‘XX%’这种才可以用上索引，比如说

select * from product where shop_id=1 and product_id=1 and gmt_create like ‘2018%’

**（6）范围列匹配**

如果你是范围查询，比如>=，<=，between操作，你只能是符合最左前缀的规则才可以范围，范围之后的列就不用索引了

select * from product where shop_id>=1 and product_id=1

这里就在联合索引中根据shop_id来查询了

**（7）包含函数**

如果你对某个列用了函数，比如substring之类的东西，那么那一列不用索引

select * from product where shop_id=1 and 函数(product_id) = 2

上面就根据shop_id在联合索引中查询

**5.索引的缺点以及使用注意**

索引是有缺点的，比如常见的就是会增加磁盘消耗，因为要占用磁盘文件，同时高并发的时候频繁插入和修改索引，会导致性能损耗的。

我们给的建议，尽量创建少的索引，比如说一个表一两个索引，两三个索引，十来个，20个索引，高并发场景下还可以。

字段，status，100行，status就2个值，0和1。

你觉得你建立索引还有意义吗？几乎跟全表扫描都差不多了

select * from table where status=1，相当于是把100行里的50行都扫一遍

你有个id字段，每个id都不太一样，建立个索引，这个时候其实用索引效果就很好，你比如为了定位到某个id的行，其实通过索引二分查找，可以大大减少要扫描的数据量，性能是非常好的

在创建索引的时候，要注意一个选择性的问题，select count(discount(col)) / count(*)，就可以看看选择性，就是这个列的唯一值在总行数的占比，如果过低，就代表这个字段的值其实都差不多，或者很多行的这个值都类似的，那创建索引几乎没什么意义，你搜一个值定位到一大坨行，还得重新扫描。

就是要一个字段的值几乎都不太一样，此时用索引的效果才是最好的。

还有一种特殊的索引叫做前缀索引，就是说，某个字段是字符串，很长，如果你要建立索引，最好就对这个字符串的前缀来创建，比如前10个字符这样子，要用前多少位的字符串创建前缀索引，就对不同长度的前缀看看选择性就好了，一般前缀长度越长选择性的值越高。

好了，各位同学，索引这块能聊到这个程度，或者掌握到这个程度，其实普通的互联网系统中，80%的活儿都可以干了，因为在互联网系统中，一般就是尽量降低SQL的复杂度，让SQL非常简单就可以了，然后搭配上非常简单的一个主键索引（聚簇索引）+ 少数几个联合索引，就可以覆盖一个表的所有SQL查询需求了。更加复杂的业务逻辑，让java代码里来实现就ok了。

大家要明白，SQL达到95%都是单表增删改查，如果你有一些join等逻辑，就放在java代码里来做。SQL越简单，后续迁移分库分表、读写分离的时候，成本越低，几乎都不用怎么改造SQL。

我这里给大家说下，互联网公司而言，用MySQL当最牛的在线即时的存储，存数据，简单的取出来；不要用MySQL来计算，不要写join、子查询、函数放MySQL里来计算，高并发场景下；计算放java内存里，通过写java代码来做；可以合理利用mysql的事务支持

总结：

1、索引原理B/B+树，及其区别： B-Tree/B+Tree：一长串定义可以不去背，需要能画得出具体的示意图即可，可以将它们看做是一颗优化过的平衡查找树。由于查找树的查找效率极高所以选取它们作为索引结构非常合适，但是为了保证树的高度一定而可以存储大量的数据，因此需要选取高阶的查找树。 二者的区别： B-Tree在非叶子节点中也会存放数据，因此它适合有高频查询的场景，也就是有一个KEY被频繁的查询，走B-Tree的话，可以很快速的查到。但是B-Tree有一个致命的缺点就是没办法高效的进行范围查询，因此慢慢的被B+Tree取代。 B+Tree的数据只存放在叶子结点，并且每个叶子结点都由头尾指针相连，形成了一个循环的双链表，因此B+Tree的范围查询十分高效。 2、myisam/innodb 索引结构区别 myisam：该存储引擎将索引文件和数据文件分开，索引文件中保存着数据文件所在的物理地址。 innodb：该存储引擎索引文件和数据文件都在一起，在索引文件的叶子节点中保存着完整的行数据。而innodb将索引文件分成了两大类，分别是聚集索引和非聚集索引。 3、聚集索引和非聚集索引 聚集索引：innodb下的每张表必须要指定一个索引，而聚集索引就是基于主键去建立的。聚集索引的结构类似于{"id":"1","pointer":"0007"}其中pointer指向对应的叶子结点。 非聚集索引：又叫做辅助索引，相当于是对聚集索引的辅助，我们建立的索引都是这类索引，结构类似于{"name":"zhangsan","id":"1"}其中保存着聚集索引中的id。 以上结构仅仅用于帮助理解和记忆，具体的实现结构是偏移量之类的。 4、主键的使用细节 主键的使用细节主要是根据B+Tree的特性来，如果使用随机的key来作为主键，可能会导致B+Tree频繁的分裂与合并，开销很大，还有很多碎片，因此一般建议主键是自增的，这样插入的时候达到了一定的限制后，分裂产生的新的叶子结点就是新插入的那一行数据，因此是规整的。 5、索引的使用细节，最左前缀匹配原则。 例如有联合索引(a,b,c)那么在这个索引构造的时候，是基于a，然后b，c两列有序的，因此如果不从左到右去匹配，那么索引是无效的。 其余的like/范围查/存在函数等等需要记忆。

### 57、你能说说事务的几个特性是啥？有哪几种隔离级别？（上）

### 58、你能说说事务的几个特性是啥？有哪几种隔离级别？（下）

**1.事务的ACID**

这个先说一下ACID，必须得知道：

（1）Atomic：原子性，就是一堆SQL，要么一起成功，要么都别执行，不允许某个SQL成功了，某个SQL失败了，这就是扯淡，不是原子性。

（2）Consistency：一致性，这个是针对数据一致性来说的，就是一组SQL执行之前，数据必须是准确的，执行之后，数据也必须是准确的。别搞了半天，执行完了SQL，结果SQL对应的数据修改没给你执行，那不是坑爹么。

（3）Isolation：隔离性，这个就是说多个事务在跑的时候不能互相干扰，别事务A操作个数据，弄到一半儿还没弄好呢，结果事务B来改了这个数据，导致事务A的操作出错了，那不就搞笑了。

（4）Durability：持久性，事务成功了，就必须永久对数据的修改是有效的，别过了一会儿数据自己没了，不见了，那就好玩儿了。

2.**事务隔离级别**

总之，面试问你事务，先聊一下ACID，然后聊聊隔离级别

（1）读未提交，Read Uncommitted：这个很坑爹，就是说某个事务还没提交的时候，修改的数据，就让别的事务给读到了，这就恶心了，很容易导致出错的。这个也叫做脏读。

（2）读已提交，Read Committed（不可重复读）：这个比上面那个稍微好一点，但是一样比较尴尬

就是说事务A在跑的时候， 先查询了一个数据是值1，然后过了段时间，事务B把那个数据给修改了一下还提交了，此时事务A再次查询这个数据就成了值2了，这是读了人家事务提交的数据啊，所以是读已提交。

这个也叫做不可重复读，就是所谓的一个事务内对一个数据两次读，可能会读到不一样的值。

（3）可重复读，Read Repeatable：这个比上面那个再好点儿，就是说事务A在执行过程中，对某个数据的值，无论读多少次都是值1；哪怕这个过程中事务B修改了数据的值还提交了，但是事务A读到的还是自己事务开始时这个数据的值。

（4）幻读：不可重复读和可重复读都是针对两个事务同时对某条数据在修改，但是幻读针对的是插入。

比如某个事务把所有行的某个字段都修改为了2，结果另外一个事务插入了一条数据，那个字段的值是1，然后就尴尬了。第一个事务会突然发现多出来一条数据，那个数据的字段是1。

那么幻读会带来啥问题呢？因为在此隔离级别下，例如：事务1要插入一条数据，我先查询一下有没有相同的数据，但是这时事务2添加了这条数据，这就会导致事务1插入失败，并且它就算再一次查询，也无法查询到与其插入相冲突的数据，同时自身死活都插入不了，这就不是尴尬，而是囧了。

（5）串行化：如果要解决幻读，就需要使用串行化级别的隔离级别，所有事务都串行起来，不允许多个事务并行操作。

（6）MySQL的默认隔离级别是Read Repeatable，就是可重复读，就是说每个事务都会开启一个自己要操作的某个数据的快照，事务期间，读到的都是这个数据的快照罢了，对一个数据的多次读都是一样的。

接下来我们聊下MySQL是如何实现Read Repeatable的吧，因为一般我们都不修改这个隔离级别，但是你得清楚是怎么回事儿，MySQL是通过MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。

当我们使用innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。

事务id，在mysql内部是全局唯一递增的，事务id=1，事务id=2，事务id=3

事务id=121的事务，查询id=1的这一行的时候，一定会找到创建事务id <= 当前事务id的那一行

select * from table where id=1，就可以查到上面那一行

事务id=122的事务，将id=1的这一行给删除了，此时就会将id=1的行的删除事务id设置成122

事务id=121的事务，再次查询id=1的那一行，能查到吗？

能查到，要求创建事务id <= 当前事务id，当前事务id < 删除事务id

事务id=121的事务，查询id=2的那一行，查到name=李四

事务id=122的事务，将id=2的那一行的name修改成name=小李四

事务id=121的事务，查询id=2的那一行，答案是：李四，创建事务id <= 当前事务id，当前事务id < 删除事务id

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；

同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。

那么如果某个事务执行期间，别的事务更新了一条数据呢？这个很关键的一个实现，其实就是在innodb中，是插入了一行记录，然后将新插入的记录的创建时间设置为新的事务的id，同时将这条记录之前的那个版本的删除时间设置为新的事务的id。

现在get到这个点了吧？这样的话，你的这个事务其实对某行记录的查询，始终都是查找的之前的那个快照，因为之前的那个快照的创建时间小于等于自己事务id，然后删除时间的事务id比自己事务id大，所以这个事务运行期间，会一直读取到这条数据的同一个版本。

记住，聊到事务隔离级别，必须把这套东西给喷出来，尤其是mvcc，说实话，市面上相当大比重的java程序员，对mvcc是不了解的

总结：

事务特性：ACID 原子性：事务要么成功要么失败，没有第三种状态。 一致性：事务前后数据要一致，也就是事务成功以后对数据的更改要持久化(redo)，事务失败以后要对数据进行回滚(undo)。 隔离性：多个事务之间互不影响(隔离级别)。 持久性：对数据的更改是持久性的，那么是数据库宕机了，也可以恢复过来(redo)。 脏读：A事务中读到了B事务还未提交的数据。 不可重复读：A事务中进行前后两次查询，第二次查询查到了B事务中已提交的更新数据，前后两次查询结果不一致。 幻读：A事务中进行前后两次查询，第二次查询查到了B事务中已提交的插入数据，查到了原先不存在的数据，像幻觉一样。 丢失修改：A事务的更新被B事务的更新给替代掉，导致A事务的更改从未发生。 隔离级别： 读未提交：读到了未提交事务的数据。 读已提交：读到了已提交事务的数据。 可重复读：一个事务内的查询结果都是一致的。（mysql默认的隔离级别） 串行化：事务一个个的来，相当于进了队列。 mysql-innodb可重复读的实现： innodb给每张表都加了两个隐藏列，创建事务id，删除事务id。要求查询时： 创建事务id <= 当前事务id < 删除事务id。 A事务查询，B事务删除：在A事务内，哪怕B事务删了对应的数据，A事务也能查到符合条件的数据。 A事务查询，B事务更新：在A事务内，B事务的更新会重新创建一行作为副本，A事务也能查到符合条件的数据。 mysql-innodb还可以解决掉幻读的问题，主要是通过next_key lock这个范围锁实现的，所以mysql-innodb在可重复读的隔离级别下，实现了串行化的隔离要求。 MVCC：多版本并发控制，主要是基于副本机制实现。

### 59、你能说说MySQL数据库锁的实现原理吗？如果死锁了咋办？

**1、面试题**

- 数据库锁有哪些类型？
- 锁是如何实现的？
- MySQL行级锁有哪两种？
- 一定会锁定指定的行么？为什么？
- 悲观锁和乐观锁是什么？使用场景是什么？
- mysql死锁原理以及如何定位和解决？

**2、面试官心里分析**

说实话，聊mysql的话，我肯定也是循序渐进慢慢问的，先聊下存储引擎，然后问问索引一半怎么用，一半用innodb存储引擎，加上联合索引能玩儿好，明白什么是聚簇索引，再熟悉事务那套东西，包括spring的事务传播之类的，那么数据库常见的开发都没问题了。

接着就是聊聊锁，因为如果对锁没了解的话，线上系统其实进场有时候，在高并发访问下，会出现一些死锁的问题，或者是等待锁时间过长就超时了，偶尔会有这种问题的，所以会问问你锁的问题。

**3、面试题剖析**

（1）mysql锁

先跟面试官聊下，mysql的锁类型吧，一般其实就是表锁、行锁和页锁。

一般myisam会加表锁，就是myisam引擎下，执行查询的时候，会默认加个表共享锁，也就是表读锁，这个时候别人只能来查，不能写数据的；然后myisam写的时候，也会加个表独占锁，也就是表写锁，别人不能读也不能写。

这个myisam因为很少用了，所以别去管他了，面试的时候来这么一句就ok了。

所以话说回来，大家也发现了，myisam其实在实际生产中，我们曾经就是在报表系统里用的是最多的，当年es和kylin没出来的时候，大数据系统计算好的报表数据，都是放mysql的myisam里的，一般就是每天凌晨导入一批数据，那个时候别人不需要查询，没人凌晨来看报表；然后白天也没有写入，就是别人纯查询，建好索引，查询性能还是不错的，单表支撑千万级别数据没问题。

报表系统，有一次，一般来说hadoop计算完大批量的报表数据在凌晨就算完了，没有人看报表的；但是确实有一次是hadoop出了问题，是在上午11点还在计算往表里面大规模大批量的插入数据，当时造成了很严重的锁表，别人查就查不出来，我们的报表系统的用户在查看报表的时候，点504，点504，超时。

超大的case。。。。

这个页级锁，一般几乎很少用，你提一句就ok了，我们不多说了。

其实面试官重点还是跟你聊聊行锁就好了，是innodb引擎一般用行锁，但是也有表锁。

innodb的行锁有共享锁（S）和排他锁（X），两种，其实说白了呢，共享锁就是，多个事务都可以加共享锁读同一行数据，但是别的事务不能写这行数据；排他锁，就是就一个事务可以写这行数据，别的事务只能读，不能写。

innodb的表锁，分成意向共享锁，就是说加共享行锁的时候，必须先加这个共享表锁；还有一个意向排他锁，就是说，给某行加排他锁的时候，必须先给表加排他锁。这个表锁，是innodb引擎自动加的，不用你自己去加。

insert、update、delete，innodb会自动给那一行加行级排他锁

select，innodb啥锁都不加，因为innodb大家记得么，默认实现了可重复读，也就是mvcc机制，所以多个事务随便读一个数据，一般不会有冲突，大家就读自己那个快照就可以了，不涉及到什么锁的问题

但是innodb从来不会自己主动加个共享锁的，除非你用下面的语句自己手动加个锁：

手动加共享锁：select * from table where id=1 lock in share mode，那你就给那一行加了个共享锁，其他事务就不能来修改这行数据了

手动加排他锁：select * from table where id=1 for update，那你就给那一行加了个排他锁，意思就是你准备修改，别的事务就别修改了，别的事务的修改会hang住。这个要慎用，一般我们线上系统不用这个，容易搞出问题来。

所以看到这儿，我们琢磨琢磨默认的数据库锁机制，各位同学

对一行数据，如果有人在修改，会加个排他锁，然后你不能修改，你只能等着获取这把锁，但是这个时候你可以随便select，你就是查询你的事务开始之前那行数据的某个版本而已。然后如果你修改某行数据，会同时拿这个表的排他锁，但是呢，如果不同的事务修改不同的行，会拿不同行的行级排他锁，但是大家都会拿一个表的排他锁，ok，实际上innodb的表级排他锁可以随便拿，这个是没冲突的。

所以这个就是mysql innodb存储引擎的默认锁模式，其实还挺不错的。相当于就是一行数据，同一个时刻只能一个人在修改，但是别人修改，你可以随便读，读是读某个版本的，走mvcc机制。大家理解这个就好。

（2）悲观锁和乐观锁是啥？

mysql里的悲观锁是走select * from table where id=1 for update，就这个，意思是我很悲观，我担心自己拿不到这把锁，我必须先锁死，然后就我一个人可以干这事儿，别人都干不了了，不能加共享锁，也不能加排他锁。

乐观锁，就是说我觉得应该没啥问题，我修改的时候感觉差不多可以获取到锁，不需要提前搞一把锁，我就先查出来某个数据，select id,name,version from table where id=1，接着再执行各种业务逻辑之后再修改，update table set name=’新值’,version=version+1 where id=1 and version=1，就是说每次修改，比较一下这条数据的当前版本号跟我之前查出来的版本号是不是一样的，如果是一样的就修改然后把版本号加1，否则就不会更新任何一行数据，此时就重新查询后再次更新。

一般悲观锁什么时候用呢？比如你查出来了一条数据，要在内存中修改后再更新到数据库中去，但是如果这个过程中数据被别人更新了，你是不能直接干这个操作的，这个时候，你就得走上面那个操作，查询之后就不让别人更新了，你搞完了再说。

但是真有这种场景，推荐你还是用乐观锁把，悲观锁实现简单一点，但是太有风险了，很容易很容易死锁，比如事务A拿了数据1的锁，事务B拿了数据2的锁，然后事务A又要获取数据2的锁就会等待，事务B又要获取数据1的锁，也会等待，此时尴尬了，死锁，卡死，互相等待，永不释放。

所以select ... for update这个语法，轻易不要用，我们几乎线上很少用。

（3）死锁

事务A

select * from table where id=1 for update

事务B

select * from table where id=2 for update

事务A

select * from table where id=2 for update

事务B

select * from table where id=1 for update

常见的死锁就是类似上面那种，给大家说过了，分别都持有一个锁，结果还去请求别人持有的那把锁，结果就是谁也出不来，死锁了

情况太多，不一一列举了，其实就给大家说下发现死锁的时候怎么排查吧

其实很简单，就是找dba看一下死锁日志，就ok了，然后根据对应的sql，找下对应的代码，具体判断一下为啥死锁了

总结：

1、锁类型：行锁，表锁。 2、行锁的类型：共享锁，排它锁。 3、行锁的实现算法：record lock、gap lock、next_key lock。 4、手动加锁：乐观锁，基于版本；悲观锁，基于lock。 5、死锁：相互等待。

### 60、MySQL的SQL调优一般都有哪些手段？你们一般怎么做？

**1、面试题**

SQL调优的常用手段

**2、面试官心里分析**

说实话，这个其实就是针对你有没有最最基础的线上SQL跑的慢的优化能力

**3、面试题剖析**

如果是应付面试，我们实在是不可能深入讲mysql的SQL优化，以后架构班里都会深入讲解，但是这里给大家说一句，互联网公司的系统，一般很少需要复杂的SQL优化

为啥呢？因为我说过很多次了，保持SQL简单，一般90%的SQL都建议是单表查询，join等逻辑放java代码里实现，不要放SQL里。

既然是单表查询了，你觉得还能有什么性能问题么？对吧

如果某个线上SQL跑的慢，十有八九就是因为那个SQL没有用索引，所以这个时候，第一步就是去看MySQL的执行计划，看看那个SQL有没有用到索引，如果没有，那么就改写一下SQL让他用上索引，或者是额外加个索引。

我的面试突击课里就讲这种互联网公司最经典和常用的SQL优化手段，其他的大家为了面试准备，可以临时去网上搜个帖子，MySQL SQL优化，随便记住一些到时候说说即可。

我这里其实主要就是讲下怎么看SQL的执行计划，这个是码农必备能力，必须能看懂执行计划，一般其实就是看SQL有没有走索引，你倒是可以在这个环节重点说下你对执行计划这块的理解就ok

explain select * from table，就ok了

table | type | possible_keys | key | key_len | ref | rows | Extra

- table：哪个表
- type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）
- possible_keys：显示可能使用的索引
- key：实际使用的索引
- key_len：使用索引的长度
- ref：联合索引的哪一列被用了
- rows：一共扫描和返回了多少行
- extra：using filesort（需要额外进行排序），using temporary（mysql构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据where来过滤出了结果）

总结：

1、一般讲索引的应用，如何选取索引，索引的原则，索引覆盖等等。 2、然后就用explain看一下执行计划，看到底走没走索引。

## Linux

### 61、聊聊Socket的工作原理？Socket跟TCP IP之间是啥关系？

**1、面试题**

说说socket通信的原理？

**2、面试官心里分析**

其实不知道大家发现没有，网络相关的问题，都是围绕着所谓的七层模型，或者是四层模型去走的。聊完四层模型，接着就是一次请求的全过程，紧接着就是聊传输层的tcp的连接，然后就是传输层的tcp协议之上的socket编程，接下来还会聊聊应用层的http协议。

所以说，来吧，这都是最最基础的网络知识。

**3、面试题剖析**

其实说白了，socket就是在传输层里把tcp/ip协议给封装了一下，我们程序员一般都是面向socket来编程的，比如java原生就支持socket网络编程的。

大体来说这个步骤，就是我们搞一个ServerSocket无限等待别人来连接你，然后某个机器要跟你连接，就在本地创建一个socket去连接你，然后建立连接之后，在服务器上，ServerSocket也会创建出来一个socket的。通过客户端的socket跟服务端的socket进行通信，我给你写数据，你读数据，你给我写数据，我读数据，就这个过程。

当然这个底层，比如建立连接和释放连接，都是基于tcp三次握手和四次挥手的规范来搞的，包括基于tcp协议传输数据，其实就跟我们之前说的一样，都是封装个tcp数据包，里面有tcp报头，整了端口号啥的，然后封装在ip数据包里，最后封在以太网数据包里传递。

### 62、进程间是如何通信的？线程间又如何切换呢？

**1、面试题**

进程间是如何通信的？线程间又如何切换呢？

**2、面试官心里分析**

这个问题不是高频基础问题，但是确实可能有人会问，因为怎么说呢，计算机基础，就这点儿东西，网络、cpu、磁盘、内存、进程，所以可能有人会看看你的基础知识咋样，所以问问你这个问题。

**3、面试题剖析**

进程间的通信有很多种方式，比如说：管道（pipe）、命名管道（fifo）、消息队列，共享内存（System V）

**（1）管道（pipe）**

unix操作系统里面，有一个fork操作，可以创建进程的子进程，或者说是复制一个进程完全一样的子进程，共享代码空间，但是各自有独立的数据空间，不过子进程的数据空间是拷贝父进程的数据空间的。

管道机制要求的是两个进程之间是有血缘关系的，就比如fork出来的父子进程。

linux操作系统里，管道用来缓存要在进程间传输的数据，管道是一个固定大小的缓冲区，是4kb。管道中的数据一旦被读取出来，就不在管道里了。

但是如果管道满了，那么写管道的操作就阻塞了，直到别人读了管道的数据；反之如果管道是空的，那么读操作就阻塞了。就这个意思。管道一边连着一个进程的输出，一边连着一个进程的输入，然后就一个进程写数据，另外一个进程读数据，两个进程都没了，管道也就没了。管道是半双工的，就是数据只能流向一个方向，比如说你架设一个管道，只能一个进程写，另外一个进程读。

linux里面对管道的实现，是用了两个文件，指向了一个VFS（虚拟文件系统）的索引节点inode，然后VFS索引节点指向一个物理页面，接着一个进程通过自己关联的那个文件写数据，另外一个进程通过自己关联的那个文件读数据。

**（2）命名管道（fifo）**

管道的通信，要求必须是父子关系的进程间通信，就受到了限制，所以可以用命名管理来解决这个问题。

之前的管道，是没有名字的，所以必须是有父子关系的进程才能使用。但是这个命名管道是有名字的。这个命名管道，相当于是一个有名字的文件，是有路径的，所以没有血缘关系的进程多可以通过这个命名管道来通信，名字在文件系统上，数据在内存里。其他的跟管道一样，一个进程写，一个进程读，也是半双工的，数据只能单向流动。

**（3）消息队列**

linux的消息队列可以认为是个链表结构，linux内核有一个msgque链表，这个链表里每个指针指向一个msgid_ds结构，这个结构就描述了一个消息队列。然后进程之间就通过这个消息队列通信就可以，一样是写入数据和消费数据。消息队列的好处就是对每个消息可以指定类型，消费的时候就消费指定类型的消息就行了，功能更多一些。这种方式其实用的不多的。

**（4）共享内存**

一块物理内存被映射到两个进程的进程地址空间，所以进程之间互相都可以立即看到对方在共享内存里做出的修改，但是因为是共享内存，所以需要锁来保证同步。这个说对了很复杂，我在这里就不多说了，我觉得如果被人问到这个问题，短期内突击的话，回答到这个程度就行了，就是知道有哪些方式。如果你要深入理解各种机制，那是要好好学习linux的各种东西了。

**（5）线程间如何切换**

一个进程的多个线程间切换的时候就涉及到了上下文切换，这个东西说复杂了就很复杂，但是简单来说，就是有一个时间片算法，cpu给每个线程一个时间片来执行，时间片结束之后，就保存这个线程的状态，然后切换到下一个线程去执行，这就是所谓多线程并发执行的原理，就是多个线程来回来去切换，每个线程就一个时间片里执行。太复杂的我也不讲了，大家就记住一个线程上下文切换指的是什么就行了。

### 63、你能聊聊BIO、NIO、AIO分别都是啥？有什么区别？（上）

### 64、你能聊聊BIO、NIO、AIO分别都是啥？有什么区别？（下）

**1、面试题**

nio、bio、aio都是什么以及有什么区别？说说nio的原理？

**2、面试官心里分析**

如果聊io这块，我就必问这个问题，因为io的那些过于基础的知识，各种流的使用，是用来考察应届生和培训班刚出来的同学的，正常问一个有经验的开发人员，io这块就是聊聊几种io模式，以及同步、异步、阻塞和非阻塞几种io的概念。

**3、面试题剖析**

**3.1 BIO**

这个其实就是最传统的网络通信模型，就是BIO，同步阻塞式IO，简单来说大家如果参加过几个月的培训班儿应该都知道这种BIO网络通信方式。就是服务端创建一个ServerSocket，然后客户端用一个Socket去连接那个ServerSocket，然后ServerSocket接收到一个Socket的连接请求就创建一个Socket和一个线程去跟那个Socket进行通信。

然后客户端和服务端的socket，就进行同步阻塞式的通信，客户端socket发送一个请求，服务端socket进行处理后返回响应，响应必须是等处理完后才会返回，在这之前啥事儿也干不了，这可不就是同步么。

这种方式最大的坑在于，每次一个客户端接入，都是要在服务端创建一个线程来服务这个客户端的，这会导致大量的客户端的时候，服务端的线程数量可能达到几千甚至几万，几十万，这会导致服务器端程序的负载过高，最后崩溃死掉。

要么你就是搞一个线程池，固定线程数量来处理请求，但是高并发请求的时候，还是可能会导致各种排队和延时，因为没那么多线程来处理。

**3.2 NIO**

JDK 1.4中引入了NIO，这是一种同步非阻塞的IO，基于Reactor模型。

NIO中有一些概念：

比如Buffer，缓冲区的概念，一般都是将数据写入Buffer中，然后从Buffer中读取数据，有IntBuffer、LongBuffer、CharBuffer等很多种针对基础数据类型的Buffer。

还有Channel，NIO中都是通过Channel来进行数据读写的。

包括Selector，这是多路复用器，selector会不断轮询注册的channel，如果某个channel上发生了读写事件，selector就会将这些channel获取出来，我们通过SelectionKey获取有读写事件的channel，就可以进行IO操作。一个Selector就通过一个线程，就可以轮询成千上万的channel，这就意味着你的服务端可以接入成千上万的客户端。

这块其实相当于就是一个线程处理大量的客户端的请求，通过一个线程轮询大量的channel，每次就获取一批有事件的channel，然后对每个请求启动一个线程处理即可。

这里的核心就是非阻塞，就那个selector一个线程就可以不停轮询channel，所有客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排着队而已。

这里的核心就是因为，一个客户端不是时时刻刻都要发送请求的，没必要死耗着一个线程不放吧，所以NIO的优化思想就是一个请求一个线程。只有某个客户端发送了一个请求的时候，才会启动一个线程来处理。

所以为啥是非阻塞呢？因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到selector上去罢了，一个selector线程不断的轮询所有的socket连接，发现有事件了就通知你，然后你就启动一个线程处理一个请求即可，但是这个处理的过程中，你还是要先读取数据，处理，再返回的，这是个同步的过程。

所以NIO是同步非阻塞的。

工作线程，从channel里读数据，是同步的，是工作线程自己去干这个事儿，卡在那儿，专门干读数据的这个活儿，数据没读完，你就卡死在这儿了；然后往channel里写数据，也是你自己去干这个事儿，卡死在这儿了，数据没写完，你就卡在这儿了

**3.3 AIO**

AIO是基于Proactor模型的，就是异步非阻塞模型。

每个连接发送过来的请求，都会绑定一个buffer，然后通知操作系统去异步完成读，此时你的程序是会去干别的事儿的，等操作系统完成数据读取之后，就会回调你的接口，给你操作系统异步读完的数据。

然后你对这个数据处理一下，接着将结果往回写。

写的时候也是给操作系统一个buffer，让操作系统自己获取数据去完成写操作，写完以后再回来通知你。

工作线程，读取数据的时候，是说，你提供给操作系统一个buffer，空的，然后你就可以干别的事儿了，你就把读数据的事儿，交给操作系统去干，操作系统内核，读数据将数据放入buffer中，完事儿了，来回调你的一个接口，告诉你说，ok，buffer交给你了，这个数据我给你读好了

写数据的时候也是一样的的，把放了数据的buffer交给操作系统的内核去处理，你就可以去干别的事儿了，操作系统完成了数据的写之后，级会来回调你，告诉你说，ok，哥儿们，你交给我的数据，我都给你写回到客户端去了

**3.4** **同步阻塞、同步非阻塞、异步非阻塞**

但是这里为啥叫BIO是同步阻塞呢？这个其实不是针对网络编程模型来说的，是针对文件IO操作来说的，因为用BIO的流读写文件，是说你发起个IO请求直接hang死，必须等着搞完了这次IO才能返回

BIO的这个同步阻塞，不是完全针对的网络通信模型去说的，针对的是磁盘文件的IO读写，FileInputStream，BIO，卡在那儿，直到你读写完成了才可以。

NIO为啥是同步非阻塞？就是说通过NIO的FileChannel发起个文件IO操作，其实发起之后就返回了，你可以干别的事儿，这就是非阻塞，但是接下来你还得不断的去轮询操作系统，看IO操作完事儿了没有。

你呢也可以使用FileChannel这种NIO的模型，去读写磁盘文件，读数据，发起读数据的请求之后，你不是阻塞住的，你可以干别的事儿，但是你在干别的事儿的同时，还得来时不时的自己去轮询操作系统读数据的状态，看看人家读好了没有

AIO为啥是异步非阻塞？就是说通过AIO发起个文件IO操作之后，你立马就返回可以干别的事儿了，接下来你也不用管了，操作系统自己干完了IO之后，告诉你说ok了。同步就是自己还得主动去轮询操作系统，异步就是操作系统反过来通知你。

你也可以基于AIO的文件读写的api去读写磁盘文件，你发起一个文件读写的操作之后，交给操作系统，你就不去管他了，直到操作系统自己完成之后，会来回调你的一个接口，通知你说，ok，这个数据读好了，那个数据写完了

## 生产实践

### 65、线上服务器CPU 100%了！该怎么排查、定位和解决？

**1、面试题**

线上服务器的cpu使用达到100%了，如何排查、定位和解决该问题？

**2、面试官心里分析**

说实话，这个问题是面试的时候，聊基础，最常问的一个问题，就是看看你有没有处理过高负载的线上问题场景。所以很多大公司考察你的基本功，肯定会问这个。其实这个你干过就是干过，掌握就是掌握，只要干过，所有人都是一样的步骤，没区别。

**3、面试题剖析**

其实核心思路，就是找到这台服务器上，是哪个进程的哪个线程的哪段代码，导致cpu 100了，主要就是考察你是否熟练运用一些线上的命令。

这里我可以给大家说一个我们线上的经验，就是之前有一个bug，是一个很年轻的同学写的，就是我们当时是定了异常日志是写到es里去的

public void log(String message) {

try {

// 往es去写

} catch(Exception e) {

log(message);

}

}

线上事故，es集群出了点问题，没法写，最后出现线上几十台机器，全部因为这一行代码，全体cpu 100%，卡死了

（1）定位耗费cpu的进程

top -c，就可以显示进程列表，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND

43987 root 20 0 28.2g 4.5g 68m S 99.0 24.0 44333.4 java -Xms。。。

大概类似上面这样，能看到哪个进程，CPU负载最高，还有启动这个进程的命令，比如一般就是java啥啥的。

（2）定位耗费cpu的线程

top -Hp 43987，就是输入那个进程id就好了，然后输入P，按照cpu使用率排序，你会看到类似下面的东西

大概类似上面那样，你就可以看到这个进程里的哪个线程耗费cpu最高 。

（3）定位哪段代码导致的cpu过高

printf “%x\n” 16872，把线程pid转换成16进制，比如41e8

jstack 43987 | grep ‘0x41e8’ -C5 --color

这个就是用jstack打印进程的堆栈信息，而且通过grep那个线程的16进制的pid，找到那个线程相关的东西，这个时候就可以在打印出的代码里，看到是哪个类的哪个方法导致的这个cpu 100%的问题。

总结

1.top -c 查看所有进程 2.键入P 按照cpu的使用率从上到下排序 3.根据第2步拿到的pid 执行top -Hp pid 查看pid对应的线程cpu使用率 4.键入P 线程CPU的消耗从大到小排序 5.选择第四步中最耗CPU的线程id 6.由于Linux中线程id的打印是16进制,将线程id转为16进制，printf “%x” tid 7.打印线程id对应的jstack日志 jstack pid │ grep tid -C 5 --color ：输出指定pid的线程jstack日志，过滤筛选指定的线程id，找到位置后前后打印5行 满足条件的tid字段线程颜色 8.根据堆栈线程找到对应的代码行

### 66、线上机器的一个进程用kill命令杀不死该怎么办？磁盘空间快满了又该怎么处理？

**1、面试题**

线上进程kill不掉怎么办

**2、面试官心里分析**

但是可能就是想考察一下你有没有处理过类似的问题

**3、面试题剖析**

我们公司有一套自己研发的发布系统，你每次部署，都是走发布系统，告诉他一个git仓库的地址，那个系统会自动从git仓库拉取代码，基于maven来打包，你还可以指定你要用的profile，maven打包的时候会用对应的profile打对应环境的包，打完jar包之后，就会java -jar之类的来启动。

当时那个发布系统，他自己在每台机器上有一个进程，发布和启动的时候，他启动的那个进程，不是直接java -jar来启动的，发布系统的一个进程搞了一个子进程，子进程是我们的系统进程。

这个其实就是线上可能遇到的一个问题，我们之前确实就是遇到过这个问题，kill一个进程死活杀不死，那个进程成了僵尸进程，就是zombie状态。这是因为这个进程释放了资源，但是没有得到父进程的确认。

ps aux，看看STAT那一栏，如果是Z，那么就是zombie状态的僵尸进程

ps -ef | grep 僵尸进程id，可以找到父进程id

然后先kill掉父进程即可

**1、面试题**

服务器存储空间快满了（95%），还有一个小时存储就满了，在不影响服务正常运行的情况下，该如何解决？

**2、面试官心里分析**

这个确实没什么好说的，无非就是用一些一些线上的场景和问题来考考你平时一般怎么处理的，线上机器磁盘满，一般啥原因，不就是日志太多了给写满了么。。。对吧，我们不说别的，就说说这最基本的就行了

**3、面试题剖析**

df -h，先看看磁盘使用的情况

然后就是到你的系统部署的地方，一般就是tomcat下的日志、spring boot的日志，去看看，如果过多，就删除掉一些日志就行了，自己注意让tomcat或者nginx之类的日志输出，按天切割，这样你还可以写个shell脚本，crontab定时，定期删除7天以前的日志

要是不行，那就：find / -size +100M |xargs ls -lh，找找大于100m的文件，但是如果有大量的小文件，那么这样是不行的

或者是用：du -h >fs_du.log，看看各个目录占用的磁盘空间大小，看看是不是哪个目录有大量的小文件

其实面试官无非就是看看是不是知道常见的命令罢了，如果不是。那那个面试官就得再提示多一些细节，到底要考察你什么。但是简单问一个磁盘占用排查，就是常见这几个命令罢了

## 系统安全性

### 93、为什么越来越多的公司面试的时候，喜欢问安全相关的连环炮？

（1） 能不能聊聊平时我们开发的系统，有可能被黑客以哪些方式来攻击呢？

（2） XSS攻击方式背后的原理是什么，SQL注入背后的原理是什么，等等，各种攻击方式背后的原理是什么？

（3） 针对常见的黑客攻击方式，你平时开发系统的时候都有哪些方案可以去保护你的系统安全，避免被黑客攻破。

（4） 平时你们微服务架构里，网关系统用的是什么？在网关层面如何防止黑客攻击？

（5） 哪怕网关不是你负责的，你负责的一些系统的接口，如何保证你设计的接口的安全性呢？

（6） 缓存穿透，假如说有黑客攻击你，每次使用的缓存的Key是不同的，传统的缓存穿透的方案无法防御，此时怎么办呢？

（7） 加密算法，公钥密钥是怎么回事，如何进行加密的网络通信？数据加密？

（8） 除了公钥密钥以外，你们有没有完整的一套系统安全性防御机制呢，防火墙，网站安全漏洞扫描，密钥存储是如何做的 。

技术公众号，挺知名的，是一个独角兽公司，估值也是很高的，删库跑路，威胁的都是系统的安全性，数据的安全性，就有业内很知名的互联网大厂，在某一个领域是绝对的巨头，删库跑路事件。

多年以前微博受到了XSS攻击，用户自动关注一个病毒用户，然后病毒用户自动发布病毒微博，大量的人点击之后，再度扩散；很多曾经重要的网站，都遭遇攻击，几百万、上千万的用户数据泄露。

直到现在有时候大家还会听到XX网站、XX APP的用户数据泄露的问题，曾经互联网顶级大厂早期也被黑客威胁要攻破系统 。

现在都经常还有同学会找到我说，老师，我们部署在云的几台机器感觉被攻击了，系统负载很高，有时候会挂掉

所以系统的安全问题现在是一个非常重要的系统架构设计的话题，你的系统如何保证安全性？如何避免被黑客攻击导致系统故障？如何避免你系统的核心数据遭到泄漏？这是非常重要的一个事情。

所以说，现在出去面试，其实作为面试官，有时候就会考察你对基本的网络攻击手段是否了解，对网络攻击手段背后的原理是否了解，对常见的防止网络攻击的方法是否了解，如果你一无所知的话，那么可能有时候面试就不理想了。

当然，现在我们比较推崇的，其实是应用架构师或者工程师，对网络攻击有一定的了解，但是现在更多的一个趋势，是安全这块交给专业的公司和团队去做，我们购买他们的服务，但是我们要了解常见的网络攻击手段，知道如何使用一些云公司的安全产品来保护我们的系统，同时我们自己的系统有时候也可能要做一些简单的防御措施。

常见的网络攻击手段包括了：XSS（跨站点脚本攻击）、SQL注入、CSRF（跨站点请求伪造）、错误回显、HTML注释、文件上传、路径遍历，等等，每一种攻击手段都有办法可以避免，所以我们最近就是学习一下系统安全性的问题和常见防御手段。

还会学习一下防火墙和网站安全漏洞扫描的概念。

同时我们还会学习一些数据加密技术，包括公钥密钥的机制，还有垃圾内容过滤、黑名单机制一些常见的安全防护机制

如果学习完了这些之后，你就知道平时你开发系统在安全性上哪些地方需要注意了，如果你再加上一些云安全产品的使用，那么基本系统可以说安全性很强，就固若金汤了

### 94、能不能说说一般黑客常用的XSS网络攻击的原理是什么？

![XSS攻击原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/94/01.png)

![XSS攻击原理2](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/94/02.png)

XSS攻击和SQL注入就是大部分的黑客进行网络攻击的手段，此外还有很多别的攻击方式，比如说CSRF、Session劫持之类的，今天先说说XSS网络攻击的原理

XSS的全称是Cross Site Script，就是跨站点脚本攻击，意思就是说，黑客恶意篡改你的网页的前端代码，在里面注入一些他自己的html+javascript的脚本和代码，然后你比如在访问那个网站的网页的时候，他注入的那些恶意脚本就会运行了。

恶意脚本运行的时候就会控制你的浏览器，这个时候他的脚本就可以做很多很多的事情了。

第一种XSS攻击是反射型攻击，他主要是想办法让你点击一个URL链接，在这个URL链接里就嵌入他自己的恶意脚本，你点击那个URL链接之后，那个URL指向的是黑客自己的服务器上的一段恶意脚本。

他可能给你展示的是一个什么什么图片，或者是一个flash的动图，或者是一个小视频的东西，诱惑性，引诱你去点击。

然后恶意脚本被返回到你的浏览器里就会运行，然后就可以控制你的浏览器里的行为了，这个控制行为就很恐怖了，他可以干很多的事儿，比如说脚本可以自动让你关注某个用户ID，然后控制你自动发布一个带有病毒的微博，这是比较简单的。

实际上来说，一段恶意的js脚本，几乎可以说是无恶不作的，因为他一旦控制了你的浏览器就可以得到大量的东西，大家都知道浏览器里包含了你的一些cookie，有的浏览器可能还存储了你的密码，通过知道你的cookie，就可以利用cookie伪造你的用户登录的session状态，去以你这个用户的名义干一些事儿。

另外一种XSS攻击是叫做持久型攻击，这个意思就是说，举个例子，比如是个什么论坛、或者社交网站之类的系统，不是你可以发布一些帖子啊，或者是评论啥的内容么，此时黑客就可以在里面写一段恶意脚本。

然后把恶意脚本混杂在评论内容里提交到你的网站的数据库里去。

然后后面比如其他用户在社交网站里浏览到了你的这个评论，评论内容会被返回到浏览器里去，此时评论内容是包含恶意js脚本的，马上恶意脚本运行，又可以干坏事儿了，干的坏事儿就跟之前是一样的。

如果要防止XSS攻击，一般来说手段有如下两种：

包含恶意URL链接的图片、视频、动图、flash动画，平时自己注意一下，少点，尽量使用正规的网站。

消毒机制，这就是说，如果黑客在一些评论之类的内容里混入恶意脚本，那么你的代码里必须对内容进行消毒，就是进行一些转义，比如说把>转义为&gt之类的，这样就可以把恶意脚本里的html标签、js代码之类的东西，都给转义掉，让这些恶意脚本失效。

<html> -> &lthtml&gt，这种东西在浏览器里是不会运行的。

<html><script>// 包含恶意脚本</script></html>。

这样的话，转义以后的脚本被其他用户看到的时候也不会在浏览器里运行了

HttpOnly方式，这个意思是说如果你在浏览器里存放cookie的时候，可以设置一个HttpOnly属性，比如说存放用户加密认证信息的cookie，这样的话，在浏览器里运行的js脚本是被禁止访问这些HttpOnly cookie的，他就无法窃取你在浏览器里存储的cookie了

总结：

1、XSS全称是跨站点脚本攻击，原理就是想办法把恶意脚本加载到浏览器然后运行。主要方式是反射型和持久型。 2、反射型：访问一些不怎么正规的网站，然后那个网站可以被黑客发布广告，然后用户点击广告以后，就会跳转到黑客搭建的一个后台，然后就会返回一段恶意脚本给浏览器，然后恶意脚本就可以在浏览器运行，做一些很卑劣的事情。 3、持久型：黑客可以向博客、论坛性质的网站，提交一些恶意评论，评论内容里面包含了一段恶意代码。如果该网站不进行相关的处理，那么在一个用户浏览到该评论的时候，这个恶意评论就会被读取到浏览器，进而执行。 4、解决方案： 作为用户，我们尽量少点。 作为博客、论坛的后台开发，我们需要对用户提交的评论进行特殊符号转移，让恶意代码失效。 作为一些网站的后台开发，我们需要将cookie打上HttpOnly属性，就不让恶意脚本获取cookie信息。

### 95、能不能说说我们经常听到的SQL注入攻击背后的原理是什么？

黑客如何通过SQL注入来攻击我们的系统

![SQL注入攻击的原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/95/01.png)

你的系统在数据库里执行SQL语句的时候，可能也存在漏洞，导致黑客把一些恶意的SQL语句注入进去让你的系统在你的数据库来执行这样子

http://www.xxx.com/goods?goodsSkuNo=xxxxx

select * from eshop_goods_sku where goods_sku_no=’xxxxx’

http://www.xxx.com/goods?goodsSkuNo=xxxxx’;drop table eshop_goods_sku;--

执行SQL语句的时候，手动进行拼接，比较low

select * from eshop_goods_sku where goods_sku_no=’xxxxx’;drop table eshop_goods_sku;--‘;

这样就直接恶意给你造成删库跑路的效果了，这还不算什么，关键是这种SQL语句里可以拼接进去各种支持的SQL语法，包括对数据库施加的命令，甚至通过附加一些脚本直接窃取你的数据，都是有可能的 。

但是如果要给你搞SQL注入，其实也不是那么容易的，因为必须要知道你的数据库表结构才行，一般获取数据库表结构的方式就下面几种：

（1）如果你使用的是开源软件，比如开源的博客系统，论坛系统，或者别的什么系统，那么人家自然知道你的表结构了，这种情况是比较少见的。

（2）错误回显，不知道大家有没有经历过这种，就是你有时候把系统跑在web服务器里，然后程序报错了，结果直接在浏览器页面上显示出来了你的异常堆栈信息，包括有错误的SQL语句，这就尴尬了，通过这个，黑客直接就知道你的表结构了。

（3）根据你的请求参数的名称，大致推测你的数据库表结构，这个一般不太现实。

我就经常在一些不大不小的站点会见过，5年前开始，我会在一些网站上发布我录制的一些课程，有一些有一定知名度的不大不小的站点，我就经常亲眼见过，站点可能有bug，我一点击什么东西，他系统内部直接就报错了。

执行SQL语句的时候报错了，是不是会有异常堆栈，在controller层面没有进行try catch，他在controller层面就直接把你的异常给抛出来了，被mvc框架捕获到，mvc框架就直接把这段异常堆栈信息返回给浏览器了。

在浏览器里，我居然经常见到一个站点内部的SQL语句报错的异常，直接可以看到SQL语句的语法，通过SQL语句，就可以反过来推测出来你的表结构，此时就可以观察你的系统有哪些http接口，然后可以通过postman那种工具，去构造一个请求发送过去执行。

在参数里可以拼接进去一个恶意的SQL语句进行注入。

所以要防止SQL注入，一个是别让人家知道你的数据表结构，关闭web服务器的错误回显，显示一个400，500之类的就可以了，另外一个，就是要用预编译的方法，现在mybatis、hibernate都是支持预编译的。

放到底层的JDBC里，PreparedStatement，对SQL进行预编译，如果你给SQL的某个参数传入进去的是一个恶意SQL语句，人家预编译过后，会让你的恶意SQL语句是无法执行的，所以千万不要直接自己用字符串去拼接SQL语句。

insert into xxx_table(xx,xxx,xx) values(?,?,?)，对这个SQL进行预编译，然后给他里面把各个参数设置进去，此时参数里如果带有恶意SQL是不会作为SQL去执行的。

mybatis

对这个方法比如传递进去了一个map或者是对象，mybatis，根据你的占位符的变量名字，从你的Map里或者是对象里提取出来一个一个的参数的值，进行预编译SQL的参数值的设置。

insert into xxx_table(xxx,xx,xx) values(#{xx},#{xx},#{xxx})

这个预编译，就是说把黑客在参数里混进来来的SQL语句当做一个参数，而绝对不会作为独立的SQL语句去执行，这就避免了SQL注入攻击了。

所以说，平时开发系统，我们一定要注意这两件事情，包括关闭web服务器错误回显，包括mybatis之类的用预编译，不要直接拼接SQL语句。

### 96、听说过CSRF攻击吗？你知道他背后的原理是什么吗？

Cross Site Request Forgery，垮站点请求伪造

这个就是黑客想办法去伪造成你这个用户去发送请求到某个系统上去，然后查询你的数据，转账交易之类的，伪装成你，也有很多办法，比如利用XSS搞一个恶意脚本让你执行，然后盗取你的浏览器里的cookie。

利用你的cookie伪装成你登录的状态，然后去执行一些请求。

利用XSS跨站点脚本攻击，获取cookie，然后再利用postman发送垮站点伪造请求。

防御CSRF的方法主要是以下几种：

（1）**防止cookie被窃取**：最最根本的，其实还是说防止cookie被窃取，可以给你的网站的cookie设置HttpOnly属性，禁止被别人的script脚本窃取，那么别人就无法伪造用户登录请求了。

（2）**随机token**：每次返回一个页面给你的时候，都生成一个随机token附加在页面的隐藏元素里，同时在你的redis里可以存以下，然后页面发送请求的时候附加随机token，验证通过才能执行请求，你要是自己用postman构造请求就不知道随机token是什么了。

（3）**验证码**：页面提交必须搞一个验证码，那种图形的，现在比较流行的还有拖动一个拼图什么的，必须验证码通过了才能执行你的请求，避免黑客直接伪造请求发送过来，这个其实是比较常见的，最好是在用户进行支付交易的时候，要求必须在页面上拖拽一个拼图验证码。

（4）**Referer请求头**：这个是http请求里有一个referer请求头，带有这个请求的来源，你可以验证一下这个请求是不是从自己的页面里来的，如果是的话才执行，否则就不要执行了。

### 97、如果你们的系统允许用户上传文件，可能会遭到什么样的黑客攻击？

很多时候如果我们的网站允许别人上传文件，那么文件可能是可执行的脚本，可能是病毒或者木马文件，其实这个是非常危险的，如果是脚本的话，可能会在服务器执行，搞很多破坏，比如黑客黑掉你的服务器，勒索你给他比特币之类的。

比如把自己的文件后缀改成.jpg、.txt之类的东西，来上传，其实本质病毒文件。

病毒脚本是非常的可怕的，因为原则上来说，只要黑客掌握底层的一些技术，就可以利用病毒脚本干各种各样的事情，比如连接你的数据库之类的。

对于文件上传这块，核心的就是要进行白名单校验，限制上传文件的类型，只能是我们指定的，而且要限制文件的大小，还要对文件重命名，限制文件类型不能简单的根据后缀来判断，可能后缀被篡改了，要根据文件二进制数据的开头几个字节代表的magic number来判断文件的类型。

读取这个文件的二进制数据流，读取开头的几个字节，提取这个文件的魔数，根据魔数的值去判断他是什么类型的。

FFD8FF：JEPG

89504E47：PNG

类似这样，以此类推。

比如说你的网站要求用户只能上传word类型，png类型，此时你就限制仅仅这几种文件是可以上传的，其他的类型的文件都不让上传。

网上可以查到完整的magic number列表，根据那个限制一下，哪些文件类型可以上传，这样就避免说有那种木马、病毒之类的可执行文件被上传了。

而且还要限制，不允许用户上传大文件，文件超过一定大小就不让上传了。

然后对上传好的文件进行重命名。

而且最好对文件进行一定的压缩，这样可以破坏原来的文件结构，避免文件在服务器执行，利用imagemagick这种开源包，可以很方便进行文件缩放

### 98、让所有工程师闻声色变的DDoS攻击到底是什么东西？

![DDos攻击原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/98/01.png)

DDoS，distributed denial of service，分布式拒绝服务攻击，最可怕的黑客攻击，可以把你的网站、APP、系统给搞瘫痪了

DoS攻击，就是说黑客知道你的服务器地址了，然后你的系统假设每秒就抗下1000请求，黑客就以每秒1000请求访问你，你的服务器线程资源全部打满，正常用户根本无法发送请求，你的网站就宕机了。

甚至他以每秒1万请求攻击你的服务器呢，那就的系统机器就挂了。

DoS攻击是一对一的，就是黑客搞一台高性能服务器，拼命发送请求给你的一台服务器，但是如果你的服务器配置超高，每秒抗1万请求，结果黑客的机器每秒才5000请求，那么就没用了。

DDoS的意思就是黑客控制大量的机器，比如普通人的电脑，或者是一些公司的服务器，被他的一些木马植入给控制了，就是所谓的“肉鸡”，然后黑客下达指令，让所有肉鸡一起发送请求给攻击目标，直接搞瘫你的服务器。

如何防御DDoS攻击？自己其实挺难的，这其实是非常专业的一种攻击手段，通常我们可以采购云厂商的安全服务，比如DDoS高防IP，可以把攻击流量都导入到云厂商的高防IP的服务器上去，他们有专业的技术方案和算法来防御。

### 99、基于SYN Flood模式的DDoS攻击，背后的原理是什么呢？基于SYN Flood模式的DDoS攻击，背后的原理是什么呢？

![SYN_Flood模式的DDos攻击原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/99/01.png)

TCP三次握手

1、客户端发送一个SYN请求，指明客户端的端口号以及TCP连接的初始序列号。

2、的服务器收到SYN后，返回一个SYN+ACK，表示请求被接收，TCP序列号加1。

3、客户端收到服务器的SYN+ACK后，返回一个ACK给服务器，TCP序列号加1，连接建立完毕，接着可以通信了。

如果服务器没有收到第三步的ACK，会重试返回SYN+ACK给客户端，同时处于SYN_RECV状态，把客户端放入等待列表。重试会3~5次，每隔30重试一次，遍历等待列表，再次重试发送SYN+ACK。

只要返回SYN+ACK给客户端，就会为客户端预留一部分资源，重试期间都保留，等待跟客户端建立连接；所以如果说太多的客户端来建立连接，资源耗尽，那么就无法建立新的TCP连接了。

所以黑客就会伪造大量的不同ip地址去发送SYN请求给一台服务器建立TCP连接，每次都是卡在服务器返回SYN+ACK，但是黑客是不会最终返回ACK的，所以导致服务器可能为了黑客建立了大量的半连接放在等待列表里，占用了大量的资源，还得不停的去重试。

一旦服务器的资源耗尽，那么正常的请求过来，是无非建立TCP连接的。

要知道，HTTP底层就是基于TCP实现的，一旦你无法建立TCP连接，那么这台服务器也自然接受不了任何HTTP请求。

### 100、再来看看基于DNS Query Flood和HTTP Flood的DDoS攻击

![SYN_Flood模式的DDos攻击原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/100/01.png)

这个DNS Query Flood攻击，顾名思义，就是去攻击DNS服务器，也就是伪造大量的域名解析请求发送给DNS服务器，然后DNS服务器必然没有，接着必然会去找上级DNS服务器，一直到根域名服务器

这么搞必然导致DNS服务器的资源别耗尽，其他正常人浏览网页也要解析域名的，此时就没法访问DNS服务器了。

13.68.131.42

cc攻击，HTTP flood，其实就是之前说过的那种，直接就是在互联网上找到大量的HTTP代理，说白了，其实本身就有很多公司提供HTTP代理服务，自己搜一下就知道了，就是有很多代理服务器，你可以控制那些HTTP代理服务器去给目标服务器发送大量的HTTP请求。

然后目标服务器直接肯定就挂了。

Nginx、Tomcat、Jetty，机器上都是会部署Web服务器，都是一个进程，启动多个线程，来并发的处理各种HTTP请求。

控制大量的肉鸡，控制大量的HTTP代理，TCP SYN Flood、DNS Query Flood、HTTP Flood，搞瘫痪你的服务器，或者DNS服务器，DDoS攻击，利用大量的机器进行分布式的海量请求发送，你的网站的服务不可用 。

### 101、在分布式架构中，Zuul网关是如何防止网络攻击的？

![zuul网关原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/101/01.png) XSS、CRSF、SQL注入、DDoS

XSS核心是设置cookie的http only属性，过滤脚本，CRSF也是设置cookie的http only属性，根据referer请求头来过滤，设置表单随机参数，SQL注入就是过滤恶意参数，DDoS攻击主要就是限流。

zuul网关里加一个过滤器，过滤器里去过滤一些特殊请求的脚本，根据referer请求头过滤，对请求的随机token进行校验，甚至可以对参数进行校验，如果参数里包含SQL说明要注入，全部过滤，对ip地址可以进行基于redis的访问计数，比如说一个ip地址一秒内连续访问5次，那么就直接禁止访问。

## 网络与IO

### 102、一个对技术有追求的面试官，是怎么深挖网络与IO的面试连环炮的？

1、Netty的架构原理图能画一下吗，他是如何体现Reactor架构思想的？

2、能说说你对Netty堆外内存的理解吗？什么情况下会使用堆外内存？

3、你遇到过堆外内存溢出或者堆外内存泄漏的场景吗？怎么解决的？

4、能聊聊你对零拷贝技术原理的理解吗？他到底是如何提升性能的？

5、你知道哪些开源的中间件系统用了零拷贝技术吗？为什么那些系统要使用零拷贝技术？

6、你了解过在操作系统层面，系统区内存和用户区内存的关系是什么吗？

7、说说你对序列化机制的理解，了解过Protobuf吗？他是用来干什么的知道吗？

### 103、Netty的架构原理图能画一下吗，他是如何体现Reactor架构思想的？

里面的技术的点特别的多，Netty是一款非常优秀的，高性能的，一个网络通信的框架，他底层的话呢，把NIO进行了重度的封装，如果大家连NIO，BIO，区别，最好自己先去看看NIO，以及NIO的一些示例代码。

JDK提供了网络编程的框架，NIO这套东西。

netty平时常用的一些场景，主要是一些中间件系统，比如分布式存储系统，分布式消息系统，比较典型的，RocketMQ，底层做网络通信这一块就是基于Netty来的，分布式服务框架，服务之间进行远程通信，也可以基于Netty来。

crud的业务系统，一般是不会用netty的，比如说API网关系统，IM即时通讯系统。

可以自己去网上找一个netty的入门demo示例代码，体验一下netty的入门案例，基于netty可以开发一个服务器程序，再开发一个客户端程序，两边就可以进行通讯了。

### 104、能说说你对堆外内存的理解吗？堆外内存的优势在哪里？

![堆外内存](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/104/01.png)

堆内和堆外的概念：堆内内存，heap，off-heap

硬件层面的内存，其实就是一根内存条而已，自己去购买内存条，在笔记本电脑里是可以装更多的内存条的，习惯于用32GB内存的笔记本电脑，买16GB内存，装在里面。

如何用堆外内存？

ByteBuffer buffer = ByteBuffer.allocateDirect(1024); // 传入的是你要申请的堆外内存的大小

// 你可以直接把你的数据写入到内外内存DirectByteBuffer里去

// 把这块数据通过Socket发送，就是直接发送就可以了，不需要走一个拷贝

堆外内存的优势？堆内的数据，要网络IO写出去，要先拷贝到堆外内存，再写入到socket里发送出去；如果直接数据分配在堆外内存，是不需要有一次额外的拷贝的，性能是比较高的。

读写文件也是同理的，都可以节约数据拷贝次数。

1、如果堆外内存足够，就直接预留一部分内存

2、如果堆外内存不足，则将已经被 JVM 垃圾回收的 DirectBuffer 对象的堆外内存释放

3、如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()

4、如果 9 次尝试后依旧没有足够的可用堆外内存，则抛异常。

-XX:MaxDirectMemorySize

### 105、JDK是如何对堆外内存进行分配和回收的？会发生堆外内存溢出吗？

-XX:MaxDirectMemorySize：通过JVM参数是可以设置你最大可以使用的堆外内存的大小的，比如说设置堆外内存最大可以使用1GB，此时已经使用了950MB空间了，然后呢，你此时要申请一块80MB的堆外内存。

会发现说，堆外内存已经不够了，此时不能直接分配堆外内存了。

DirectByteBuffer，这个对象是JVM堆内存里的一个对象，但是这个DirectByteBuffer里面包含指针，引用了一块堆外的内存。

1、如果堆外内存足够，就直接预留一部分内存

2、如果堆外内存不足，则将已经被 JVM 垃圾回收的 DirectBuffer 对象的堆外内存释放

3、如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()

4、如果 9 次尝试后依旧没有足够的可用堆外内存，则抛异常

5、实际分配内存

jvm专栏，或者是对jvm的垃圾回收有一定的理解的话。

jvm一般分为young gc和full gc，无论是发生哪种gc，都可能会回收掉一些没有GC roots变量引用的DirectByteBuffer对象，回收掉了之后，就会主动释放他们引用的那些堆外内存，是这样子的。

DirectByteBuffer回收，就会回收关联的堆外内存，或者是内部有一个cleaner对象，可以用反射获取他，然后调用他的clean方法来主动释放内存。

如果依靠jvm gc机制，可能DirectByteBuffer躲过N次minor gc进入了老年代，然后老年代迟迟没有放满，因此迟迟没有回收，此时可能会导致DirectByteBuffer对象一直在引用堆外内存。

这样当你要分配更多的堆外内存时，无法腾出来更多的内存，就会有堆外内存溢出了。

堆内内存的OOM一样，out of memory，内存耗尽，实在是没有空闲的内存空间给你来使用了，因为所有的内存此时都别别人在使用，你要申请一块新的内存空间，实在是没有了，所以就OOM。

堆外内存的溢出，也是一样的。

### 106、如果不使用零拷贝技术，普通的IO操作在OS层面是如何执行的？

![一个普通IO操作的底层原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/106/01.png)

![IO操作执行流程](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/106/02.png)

```
File file = new File("xxx.txt");        
RandomAccessFile raf = new RandomAccessFile(file, "rw");         

byte[] arr = new byte[(int) file.length()];        
# read读取数据,用户态切换内核态
raf.read(arr);   


Socket socket = new ServerSocket(8080).accept();        
# 
socket.getOutputStream().write(arr);
```

File file = new File("xxx.txt");

RandomAccessFile raf = new RandomAccessFile(file, "rw");

byte[] arr = new byte[(int) file.length()];

raf.read(arr);

Socket socket = new ServerSocket(8080).accept();

socket.getOutputStream().write(arr);

使用read读取数据的时候，会有一次用户态到内核态的切换，也就是说从用户角度切换到了内核角度去执行，这个时候基于DMA引擎把磁盘上的数据拷贝到内核缓冲里去；接着会从内核态切换到用户态，基于CPU把内核缓冲里的数据拷贝到用户缓冲区里去。

接着我们调用了Socket的输出流的write方法，此时会从用户态切换到内核态，同时基于CPU把用户缓冲区里的数据拷贝到Socket缓冲区里去，接着会有一个异步化的过程，基于DMA引擎从Socket缓冲区里把数据拷贝到网络协议引擎里发送出去。

都完成之后，从内核态切换回用户态。

所以说，从本地磁盘读取数据，到通过网络发送出去，用户态和内核态之间，要发生4次切换，这是其一；其二，数据从磁盘拿出来过后，一共要经过4次拷贝；所以说，这4次切换和4次拷贝，让普通的IO操作都性能较低。

 

### 107、听说过mmap吗？内存映射技术为什么可以提升IO性能？

![内存映射](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/107/01.png)

把**一个磁盘文件映射到内存里来**，然后把映射到内存里来的数据通过socket发送出去

有一种mmap技术，也就是内存映射，直接将磁盘文件数据映射到内核缓冲区，这个映射的过程是基于DMA引擎拷贝的，同时用户缓冲区是跟内核缓冲区共享一块映射数据的，建立共享映射之后，就不需要从内核缓冲区拷贝到用户缓冲区了。

光是这一点，就可以避免一次拷贝了，但是这个过程中还是会用户态切换到内核态去进行映射拷贝，接着再次从内核态切换到用户态，建立用户缓冲区和内核缓冲区的映射。

接着把数据通过Socket发送出去，还是要再次切换到内核态。

接着直接把内核缓冲区里的数据拷贝到Socket缓冲区里去，然后再拷贝到网络协议引擎里，发送出去就可以了，最后切换回用户态。

减少一次拷贝，但是并不减少切换次数，一共是4次切换，3次拷贝。

mmap技术是主要在RocketMQ里来使用的，公众号：狸猫技术窝，《从0开始带你成为消息中间件高手》的专栏，RocketMQ，里面剖析了一下，RocketMQ底层主要就是基于mmap技术来提升了磁盘文件的读写，性能。

### 108、零拷贝技术到底是什么，他是如何提升IO性能的？

![零拷贝原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/108/01.png)

linux提供了sendfile，也就是零拷贝技术

在你的代码里面，如果说你基于零拷贝技术来读取磁盘文件，同时把读取到的数据通过Socket发送出去的话，流程如下，Kafka源码，transferFrom和transferTo两个方法，从磁盘上读取文件，把数据通过网络发送出去。

这个零拷贝技术，就是先从用户态切换到内核态，在内核态的状态下，把磁盘上的数据拷贝到内核缓冲区，同时从内核缓冲区拷贝一些offset和length到Socket缓冲区；接着从内核态切换到用户态，从内核缓冲区直接把数据拷贝到网络协议引擎里去。

同时从Socket缓冲区里拷贝一些offset和length到网络协议引擎里去，但是这个offset和length的量很少，几乎可以忽略。

只要2次切换，2次拷贝，就可以了。

kafka、tomcat，都是用的零拷贝技术，rocketmq用的是mmap技术，mmap还是要多2次切换和1次拷贝的，在Java代码中如何进行mmap和零拷贝，大家可以去看一看网上的一些资料。

用户态和内核态，用户态空间，内核态空间。

## 分布式架构

### 109、一起来看一个难度升级之后的分布式架构面试连环炮

1. 你们的分布式系统是如何进行链路监控的？说说链路追踪系统架构原理？
2. 对分布式系统进行核心链路追踪的时候，链路id是怎么管理的？
3. 聊过两阶段提交了，那么分布式事务三阶段提交的思想能说一下吗？
4. 唯一id生成机制中的snowflake算法的时钟回拨问题如何解决？
5. 实施灰度发布的时候，网关是可以灰度了，可是Dubbo服务如何进行灰度呢？
6. 除了常见服务注册中心之外，你觉得Redis能作为服务注册中心吗？

### 110、你们的分布式系统是如何进行链路监控的？都监控什么？

什么是分布式链路追踪 。

下订单之后，直到返回，需要几秒钟 。

追踪了有什么用，调用链路，链路性能监控，链路故障排查 。

Google的Dapper，阿里的鹰眼，大众点评的CAT，Twitter的Zipkin，LINE的pinpoint，国产的skywalking，很多，国内一般用CAT和zipkin比较多。

其实核心架构就是做一个框架，然后每一次服务调用都要经过这个框架，框架采集调用链路的数据存储起来，然后有可视化界面展示出来每个调用链路，性能，故障，这些东西。

订单服务收到这个请求是12:00:00，商品服务收到这个请求是12:00:01，库存服务收到这个请求是12:00:10。

### 111、对分布式系统进行核心链路追踪的时候，链路id是怎么管理的？

![链路id管理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/111/01.png)

每一个请求入口，traceid，每一次服务调用，spanid，上游服务id，parenetid，调用时间，timestamp，有正向的，还有反向的，把请求发出，请求接收，业务处理，各种时间都记录下来，计算网络耗时和业务处理耗时

底层的服务框架，接收到每一层请求调用的时候都要交给链路追踪系统的客户端框架来处理一下，traceid，代表了一次请求。

链路追踪数据：

traceid=1，spanid=1，parentid=0，received_timestamp=12:00:00 300，send_timestamp=12:00:00 302

traceid=1，spanid=2，parentid=1，received_timestamp=12:00:00 303，send_timestamp=12:00:00 305

traceid=1，spanid=3，parentid=2，received_timestamp=12:00:00 306

traceid=1，spanid=3，parentid=2，send_timestamp=12:00:05 502

traceid=1，spanid=2，parentid=1，received_timestamp=12:00:05 503

### 112、聊过两阶段提交了，那么分布式事务三阶段提交的思想能说一下吗？

![分布式事务三阶段提交](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/112/01.png)

### 113、唯一id生成机制中的snowflake算法的时钟回拨问题如何解决？

snowflake

一串数字，用很多的二进制里的bit位，去代表不同的东西。

40多位都是当前机器上的时间，中间有几位是代表的是机器id，自增长的id。

分布式业务系统，分布在很多台机器上，这很多台机器都要生成唯一的id，不能重复，此时可以调用某台机器上的snowflake算法生成的唯一id。

12:00:00 500 28 1~20

12:00:04 300 28 1~20

12:00:05 308，当前你的snowflake算法部署的机器发生了本地时钟的回拨，时间回拨到了12:00:00 500这个时间。

判断是否发生了时钟回拨，当前时间比我上一次生成id的时间要小，此时就是发生了时钟回拨问题，12:00:00 500 28 1~20，snowflake算法生成的不重复的id，此时会导致生成的id是重复的，这就比较坑了。

比较简单容易理解的思路，当前的机器的可能会跟一台基准时间服务器进行时间校准，导致你的机器的时间本来跑的稍微快了一点，此时跟基准时间服务器进行了校准，你的时间回拨回去了，倒退回去了。

你在内存里把过去1个小时之内生成的每一毫秒的每台机器生成的id都在内存里保存最大的那个id。

12:00:00 500 28 20

12:00:04 300 28 8

如果发生了时钟回拨，此时你看看时钟汇报到了之前的哪一毫秒里去，直接接着在那一毫秒里的最大的id继续自增就可以了，12:00:00 500 28 21

### 114、实施灰度发布的时候，网关是可以灰度了，可是Dubbo服务如何进行灰度呢？


![网关灰度发布原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/114/01.png)

spring cloud alibaba技术栈，nacos，nacos是企业级的服务注册中心，功能还是比较强大的，灰度发布这块都是可以做到的

### 115、除了常见服务注册中心之外，你觉得Redis能作为服务注册中心吗？

![redis作为服务注册中心](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/115/01.png)

redis里的hash数据结构，类似map的数据结构

约定好，你的服务注册的key都是：service_ORDER，service_PRODUCT，scan扫描指定的前缀开头的所有的key，一次性把注册表里的几十个到几百个服务都给扫描出来，获取到一个完整的注册表。

基于Redis还可以做分布式服务注册中心，Redis Cluster，做一个集群。

## 中间件系统

### 116、我们一般到底用ZooKeeper来干什么事儿？

ZooKeeper顶尖高手课程：从实战到源码。

Kafka里面大量使用了ZooKeeper进行元数据管理、Master选举、分布式协调，Canal也是一样，ZooKeeper进行元数据管理，Master选举实现HA主备切换。

HDFS，HA也是基于ZK来做的。

6周的，zk核心原理，zk集群部署、运维和管理，zk实战开发，zk在hdfs、kafka、canal源码中的运用的分析，两周的时间研究zk的核心的内核源码和底层原理。

《001_我们一般到底用ZooKeeper来干什么事儿？》

Java架构的课，分布式架构中，分布式锁，Redis分布式锁，ZooKeeper分布式锁。

分布式锁：运用于分布式的Java业务系统中。

元数据管理：Kafka、Canal，本身都是分布式架构，分布式集群在运行，本身他需要一个地方集中式的存储和管理分布式集群的核心元数据，所以他们都选择把核心元数据放在zookeeper中的。

分布式协调：如果有人对zk中的数据做了变更，然后zk会反过来去通知其他监听这个数据的人，告诉别人这个数据变更了，kafka有多个broker，多个broker会竞争成为一个controller的角色。

如果作为controller的broker挂掉了，此时他在zk里注册的一个节点会消失，其他broker瞬间会被zk反向通知这个事情，继续竞争成为新的controller。

这个就是非常经典的一个分布式协调的场景，有一个数据，一个broker注册了一个数据，其他broker监听这个数据。

Master选举 -> HA架构。

HDFS，NameNode HA架构，部署主备两个NameNode，只有一个人可以通过zk选举成为Master，另外一个backup。

Canal，HA。

ZooKeeper，分布式协调系统，封装了分布式架构中所有核心和主流的需求和功能，分布式锁、分布式集群的集中式元数据存储、Master选举、分布式协调和通知。

### 117、有哪些开源的分布式系统中使用了ZooKeeper？

Canal、Kafka、HDFS，学习过的这些技术都用了ZooKeeper，元数据管理，Master选举。

ZooKeeper，他主要是提供哪些功能，满足哪些需求，使用在哪些场景下，最后一句话总结，ZooKeeper到底是为什么而生的，定位是什么？

三类系统

第一类：分布式Java业务系统，分布式电商平台，大部分的Java开发的互联网平台，或者是传统架构系统，都是分布式Java业务系统，Dubbo、Spring Cloud把系统拆分成很多的服务或者是子系统，大家协调工作，完成最终的功能。

ZooKeeper，用的比较少，分布式锁的功能，而且很多人会选择用Redis分布式锁。

第二类：开源的分布式系统

Dubbo，HBase，HDFS，Kafka，Canal，Storm，Solr。

分布式集群的集中式元数据存储、Master选举实现HA架构、分布式协调和通知。

Dubbo：ZooKeeper作为注册中心，分布式集群的集中式元数据存储

HBase：分布式集群的集中式元数据存储

HDFS：Master选举实现HA架构

Kafka：分布式集群的集中式元数据存储，分布式协调和通知

Canal：分布式集群的集中式元数据存储，Master选举实现HA架构

第三类：自研的分布式系统

HDFS，面向的超大文件，切割成一个一个的小块儿，分布式存储在一个大的集群里。

分布式海量小文件系统：NameNode的HA架构，仿照HDFS的NameNode的HA架构，做主备两个NameNode，进行数据同步，然后自动基于zk进行热切换。

在很多，如果你自己研发类似的一些分布式系统，都可以考虑，你是否需要一个地方集中式存储分布式集群的元数据？是否需要一个东西辅助你进行Master选举实现HA架构？进行分布式协调通知？

如果你在自研分布式系统的时候，有类似的需求，那么就可以考虑引入ZooKeeper来满足你的需求。

### 118、为什么我们在分布式系统架构中需要使用 ZooKeeper 集群？

ZooKeeper，功能和定位，满足的需求。

使用ZooKeeper去满足自己需求的项目都有哪些。

分布式集群的集中式元数据存储，Master选举实现HA架构，分布式协调和通知。

我们写一个类似ZK的系统，单机版本，就是部署在一台机器上面，里面提供了一些功能，比如说允许你在里面存储一些元数据，支持你进行Master选举，支持你分布式协调和通知，也可以做到。

单机版本的系统，万一挂掉了怎么办？

集群部署，部署一个集群出来，多台机器，保证高可用性，挂掉一台机器，都可以继续运行下去。

3台机器。

我现在要进行元数据的存储，我向机器01写了一条数据，机器01应该怎么把数据同步给其他的机器02和机器03呢？

自己写一个类似ZK的系统？不可能单机版本吧？肯定得集群部署保证高可用吧？一旦集群了之后，数据一致性怎么保证？多麻烦！

你的分布式架构中有需求，干脆就直接用工业级，久经考验的zookeeper就可以了，bug很少，功能很全面，运用在很多工业级的大规模的分布式系统中，HDFS、Kafka、HBase。

### 119、ZooKeeper为了满足分布式系统的需求要有哪些特点？

![分布式协调和通知](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/119/01.png)

ZooKeeper肯定是一套系统，这个系统可以存储元数据，支持Master选举，可以进行分布式协调和通知

集群部署：不可能单机版本。

顺序一致性：所有请求全部有序。

原子性：要么全部机器都成功，要么全部机器都别成功。

数据一致性：无论连接到哪台ZK上去，看到的都是一样的数据，不能有数据不一致。

高可用：如果某台机器宕机，要保证数据绝对不能丢失。

实时性：一旦数据发生变更，其他人要实时感知到。

### 120、为了满足分布式系统的需求，ZooKeeper的架构设计有哪些特点？

![分布式协调和通知](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/120/01.png)

为了实现需要的一些特性，ZooKeeper的架构设计需要有哪些特点？

集群化部署：3~5台机器组成一个集群，每台机器都在内存保存了zk的全部数据，机器之间互相通信同步数据，客户端连接任何一台机器都可以。

树形结构的数据模型：znode，树形结构，数据模型简单，纯内存保存。

数据结构就跟我们的文件系统是类似的，是有层级关系的树形的文件系统的数据结构。

znode可以认为是一个节点而已。

create /usr/local/uid

create /usr/local/test_file

uid：可以写入一些数据的值，比如说hello world

test_file：也可以写入一些数据的值

顺序写：集群中只有一台机器可以写，所有机器都可以读，所有写请求都会分配一个zk集群全局的唯一递增编号，zxid，保证各种客户端发起的写请求都是有顺序的。

数据一致性：任何一台zk机器收到了写请求之后都会同步给其他机器，保证数据的强一致，你连接到任何一台zk机器看到的数据都是一致的。

高性能：每台zk机器都在内存维护数据，所以zk集群绝对是高并发高性能的，如果你让zk部署在高配置物理机上，一个3台机器的zk集群抗下每秒几万请求没有问题。

高可用：哪怕集群中挂掉不超过一半的机器，都能保证可用，数据不会丢失，3台机器可以挂1台，5台机器可以挂2台。

高并发：高性能决定的，只要基于纯内存数据结构来处理，并发能力是很高的，只有一台机器进行写，但是高配置的物理机，比如16核32G，写入几万QPS，读，所有机器都可以读，3台机器的话，起码可以支撑十几万QPS。

### 121、ZooKeeper集群的三种角色：Leader、Follower、Observer

![ZooKeeper集群的三种角色](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/121/01.png)

通常来说ZooKeeper集群里有三种角色的机器

集群启动自动选举一个Leader出来，只有Leader是可以写的，Follower是只能同步数据和提供数据的读取，Leader挂了，Follower可以继续选举出来Leader，Observer也只能读但是Observer不参与选举。

### 122、客户端与ZooKeeper之间的长连接和会话是什么？

![客户端与ZooKeeper之间的长连接和会话](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/122/01.png)

zk集群启动之后，自己分配好角色，然后客户端就会跟zk建立连接，是TCP长连接

把我们的Java架构课程里的网络那块的东西，自研的分布式海量小文件存储系统的项目，我们手写了大量的底层的网络通信的代码。

也就建立了一个会话，就是session，可以通过心跳感知到会话是否存在，有一个sessionTimeout，意思就是如果连接断开了，只要客户端在指定时间内重新连接zk一台机器，就能继续保持session，否则session就超时了。

### 123、ZooKeeper的数据模型：znode和节点类型

![ZooKeeper的数据模型：znode和节点类型](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/123/01.png)

核心数据模型就是znode树，平时我们往zk写数据就是创建树形结构的znode，里面可以写入值，就这数据模型，都在zk内存里存放

有两种节点，持久节点和临时节点，持久节点就是哪怕客户端断开连接，一直存在。

临时节点，就是只要客户端断开连接，节点就没了。

还有顺序节点，就是创建节点的时候自增加全局递增的序号。

大家去看一下，之前Java架构的分布式锁里，有一个zk锁的源码分析，curator框架，zk分布式锁的实现，在里面就是基于zk的临时顺序节点来实现的，加锁的时候，是创建一个临时顺序节点。

zk会自动给你的临时节点加上一个后缀，全局递增的，编号。

如果你客户端断开连接了，就自动销毁这个你加的锁，此时人家会感知到，就会尝试去加锁。

如果你是做元数据存储，肯定是持久节点。

如果你是做一些分布式协调和通知，很多时候是用临时节点，就是说，比如我创建一个临时节点，别人来监听这个节点的变化，如果我断开连接了，临时节点消失，此时人家会感知到，就会来做点别的事情。

顺序节点，在分布式锁里用的比较经典。

每个znode还有一个Stat用来存放数据版本，version（znode的版本），cversion（znode子节点的版本），aversion（znode的ACL权限控制版本）。

### 124、ZooKeeper最核心的一个机制：Watcher监听回调

![Watcher监听回调](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/124/01.png)

ZooKeeper最核心的机制，就是你一个客户端可以对znode进行Watcher监听，然后znode改变的时候回调通知你的这个客户端，这个是非常有用的一个功能，在分布式系统的协调中是很有必要的

支持写和查：只能实现元数据存储，Master选举，部分功能

分布式系统的协调需求：分布式架构中的系统A监听一个数据的变化，如果分布式架构中的系统B更新了那个数据/节点，zk反过来通知系统A这个数据的变化

/usr/local/uid

使用zk很简单，内存数据模型（不同节点类型）；写数据，主动读取数据；监听数据变化，更新数据，反向通知数据变化

实现分布式集群的集中式的元数据存储、分布式锁、Master选举、分布式协调监听

### 125、一个关键的问题：zk到底通过什么协议在集群间进行数据一致性同步？

在整个zk的架构和工作原理中，有一个非常关键的环节，就是zk集群的数据同步是用什么协议做的？其实用的是特别设计的ZAB协议，ZooKeeper Atomic Broadcast，就是ZooKeeper原子广播协议。

原子性

通过这个协议来进行zk集群间的数据同步，保证数据的强一致性。

zk，就说他的一致性的问题，很多同学可能不理解zk的一致性。

### 126、ZAB的核心思想介绍：主从同步机制和崩溃恢复机制

![主从同步机制和崩溃恢复机制](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/126/01.png)

协议的本质而言，划分集群角色，主从架构，Leader和Follower两种角色

只有Leader可以接受写操作，Leader和Follower都可以读，Leader收到事务请求，转换为事务Proposal（提议）同步给所有的Follower，超过半数的Follower都说收到事务proposal了，Leader再给所有的Follower发一个Commit消息，让所有Follower提交一个事务。

而且如果Leader崩溃了，要重新选举Leader保证继续运行。

角色划分，2PC（两阶段），过半写机制。

### 127、从zk集群启动到数据同步再到崩溃恢复的ZAB协议流程

zk集群启动的时候，进入恢复模式，选举一个leader出来，然后leader等待集群中过半的follower跟他进行数据同步，只要过半follower完成数据同步，接着就退出恢复模式，可以对外提供服务了。

**只要有超过一半的机器，认可你是leader，你就可以被选举为leader**。

3台机器组成了一个zk集群，启动的时候，只要有2台机器认可一个人是Leader，那么他就可以成为leader了。

3台可以容忍不超过一半的机器宕机，1台。

剩余的2台机器，只要2台机器都认可其中某台机器时leader，2台 > 一半，就可以选举出来一个leader了 。

zk的leader选举算法，我们可以在后面的zk核心源码剖析的时候。

1台机器时没有办法自己选举自己的。

5台机器，3台机器认可某个人是leader；可以允许2台机器宕机，3台机器，leader选举，只要是5台机器，一半2.5，3台机器都认可某个人是leader，此时3 > 2.5，过半，leader是可以选举出来的。

2台机器，小于一半，没有办法选举新的leader出来了。

当然还没完成同步的follower会自己去跟leader进行数据同步的。

此时会进入消息广播模式。

只有leader可以接受写请求，但是客户端可以随便连接leader或者follower，如果客户端连接到follower，follower会把写请求转发给leader。

leader收到写请求，就把请求同步给所有的follower，过半follower都说收到了，就再发commit给所有的follower，让大家提交这个请求事务。

如果突然leader宕机了，会进入恢复模式，重新选举一个leader，只要过半的机器都承认你是leader，就可以选举出来一个leader，所以zk很重要的一点是主要宕机的机器数量小于一半，他就可以正常工作。

因为主要有过半的机器存活下来，就可以选举新的leader。

新leader重新等待过半follower跟他同步，完了重新进入消息广播模式。

2PC是什么，两阶段提交，Java架构里的分布式事务的课程，好好去看一下。

集群启动：恢复模式，leader选举（过半机器选举机制） + 数据同步。

消息写入：消息广播模式，leader采用2PC模式的过半写机制，给follower进行同步。

崩溃恢复：恢复模式，leader/follower宕机，只要剩余机器超过一半，集群宕机不超过一半的机器，就可以选举新的leader，数据同步。

### 128、采用了2PC两阶段提交思想的ZAB消息广播流程

![采用了2PC两阶段提交思想的ZAB消息广播流程](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/128/01.png)

每一个消息广播的时候，都是2PC思想走的，先是发起事务Proposal的广播，就是事务提议，仅仅只是个提议而已，各个follower返回ack，过半follower都ack了，就直接发起commit消息到全部follower上去，让大家提交

发起一个事务proposal之前，leader会分配一个全局唯一递增的事务id，zxid，通过这个可以严格保证顺序

leader会为每个follower创建一个队列，里面放入要发送给follower的事务proposal，这是保证了一个同步的顺序性

每个follower收到一个事务proposal之后，就需要立即写入本地磁盘日志中，写入成功之后就可以保证数据不会丢失了，然后返回一个ack给leader，然后过半follower都返回了ack，leader推送commit消息给全部follower

leader自己也会进行commit操作

commit之后，就意味这个数据可以被读取到了

### 129、ZooKeeper到底是强一致性还是最终一致性？

强一致性：只要写入一条数据，立马无论从zk哪台机器上都可以立马读到这条数据，强一致性，你的写入操作卡住，直到leader和全部follower都进行了commit之后，才能让写入操作返回，认为写入成功了。

此时只要写入成功，无论你从哪个zk机器查询，都是能查到的，强一致性。

明显，ZAB协议机制，zk一定不是强一致性。

最终一致性：写入一条数据，方法返回，告诉你写入成功了，此时有可能你立马去其他zk机器上查是查不到的，短暂时间是不一致的，但是过一会儿，最终一定会让其他机器同步这条数据，最终一定是可以查到的。

研究了ZooKeeper的ZAB协议之后，你会发现，其实过半follower对事务proposal返回ack，就会发送commit给所有follower了，只要follower或者leader进行了commit，这个数据就会被客户端读取到了。

那么有没有可能，此时有的follower已经commit了，但是有的follower还没有commit？绝对会的，所以有可能其实某个客户端连接到follower01，可以读取到刚commit的数据，但是有的客户端连接到follower02在这个时间还没法读取到。

所以zk不是强一致的，不是说leader必须保证一条数据被全部follower都commit了才会让你读取到数据，而是过程中可能你会在不同的follower上读取到不一致的数据，但是最终一定会全部commit后一致，让你读到一致的数据的。

zk官方给自己的定义：顺序一致性。

因此zk是最终一致性的，但是其实他比最终一致性更好一点，出去要说是顺序一致性的，因为leader一定会保证所有的proposal同步到follower上都是按照顺序来走的，起码顺序不会乱。

但是全部follower的数据一致确实是最终才能实现一致的。

如果要求强一致性，可以手动调用zk的sync()操作。

### 130、ZAB协议下一种可能存在的数据一致性问题

Leader收到了过半的follower的ack，接着leader自己commit了，还没来得及发送commit给所有follower自己就挂了，这个时候相当于leader的数据跟所有follower是不一致的，你得保证全部follower最终都得commit。

另外一个，leader可能会自己收到了一个请求，结果没来得及发送proposal给所有follower之前就宕机了，此时这个Leader上的请求应该是要被丢弃掉的。

所以在leader崩溃的时候，就会选举一个拥有事务id最大的机器作为leader，他得检查事务日志，如果发现自己磁盘日志里有一个proposal，但是还没提交，说明肯定是之前的leader没来得及发送commit就挂了。

此时他就得作为leader为这个proposal发送commit到其他所有的follower中去，这个就保证了之前老leader提交的事务已经会最终同步提交到所有follower里去。

然后对于第二种情况，如果老leader自己磁盘日志里有一个事务proposal，他启动之后跟新leader进行同步，发现这个事务proposal其实是不应该存在的，就直接丢弃掉就可以了。

### 131、崩溃恢复时选举出来的Leader是如何跟其他Follower进行同步的？

新选举出来一个leader之后，本身人家会挑选已经收到的事务zxid里最大的那个follower作为新的leader。

5个机器，1leader + 4个follower。

1个leader把proposal发送给4个follower，其中3个folower（过半）都收到了proposal返回ack了，第四个follower没收到proposal。

此时leader执行commit之后自己挂了，commit没法送给其他的follower，commit刚发送给一个follower。

剩余的4个follower，只要3个人投票一个人当leader，就是leader。

假设那3个收到proposal的follower都投票第四台没有收到proposal的follower当心的leader？这条数据一定永久性丢失了。

选择一个拥有事务zxid最大的机器作为新Leader。

其他的follower就会跟他进行同步，他给每个follower准备一个队列，然后把所有的proposal都发送给follower，只要过半follower都ack了，就会发送commit给那个follower。

所谓的commit操作，就是把这条数据加入内存中的znode树形数据结构里去，然后就对外可以看到了，也会去通知一些监听这个znode的人。

如果一个follower跟leader完全同步了，就会加入leader的同步follower列表中去，然后过半follower都同步完毕了，就可以对外继续提供服务了。

### 132、对于需要丢弃的消息是如何在ZAB协议中进行处理的？

每一条事务的zxid是64位的，高32位是leader的epoch，就认为是leader的版本吧；低32位才是自增长的zxid。

老leader发送出去的proposal，高32位是1，低32位是11358。

如果一个leader自己刚把一个proposal写入本地磁盘日志，就宕机了，没来得及发送给全部的follower，此时新leader选举出来，他会的epoch会自增长一位。

proposal，高32位是2，低32位是继续自增长的zxid。

然后老leader恢复了连接到集群是follower了，此时发现自己比新leader多出来一条proposal，但是自己的epoch比新leader的epoch低了，所以就会丢弃掉这条数据。

启动的时候，过半机器选举leader，数据同步。

对外提供服务的时候，2PC + 过半写机制，顺序一致性（最终的一致性）。

崩溃恢复，剩余机器过半，重新选举leader，有数据不一致的情况，针对两种情况自行进行处理，保证数据是一致的（磁盘日志文件、zxid的高32位）。

### 133、看看ZooKeeper的Observer节点是用来干什么的？

Observer节点是不参与leader选举的，他也不参与ZAB协议同步时候的过半follower ack的那个环节，他只是单纯的接收数据，同步数据，可能数据存在一定的不一致的问题，但是是只读的。

leader在进行数据同步的时候，observer是不参与到过半写机制里去。

所以大家思考一个问题了。

zk集群无论多少台机器，只能是一个leader进行写，单机写入最多每秒上万QPS，这是没法扩展的，所以zk是适合写少的场景。

但是读呢？follower起码有2个或者4个，读你起码可以有每秒几万QPS，没问题，那如果读请求更多呢？此时你可以引入Observer节点，他就只是同步数据，提供读服务，可以无限的扩展机器。

### 134、ZooKeeper为什么只能是小集群部署？为什么适合读多写少场景？

大数据的同学，Java架构，分布式架构，Eureka源码解析。

Eureka，peer-to-peer架构，master-slave。

小集群部署，每个节点收到的注册、心跳所有的信息，都必须向其他节点都进行同步，有很大的问题，他在进行同步的时候，采取的是完全的一个异步同步的机制，不管什么2PC，异步慢慢同步就可以了。

时效性是很差的，eureka，这个技术不适合大公司，大厂的场景去使用。

现在第二个问题，为什么zk的leader和follower只能是三五台机器，小集群部署？因为你想，假设你有1个leader + 20个follower，21台机器，你觉得靠谱吗？不靠谱，因为follower要参与到ZAB的写请求过半ack里去。

如果你有20个follower，一个写请求出去，要起码等待10台以上的Follower返回ack，才能发送commit，才能告诉你写请求成功了，性能是极差的。

所以zk的这个ZAB协议就决定了一般其实就是1个leader + 2个follower的小集群就够了，写请求是无法扩展的，读请求如果量大，可以加observer机器，最终就是适合读多写少的场景。

主要就是用于分布式系统的一些协调工作。

这也就让大家知道了，很多互联网公司里，不少系统乱用zk，以为zk可以承载高并发写，结果每秒几万写请求下去，zk的leader机器直接可能就挂掉了，扛不住那么大的请求量，zk一旦挂掉，连带的kafka等系统会全部挂掉。

zk适合读多写少的，zk集群挂掉了。

leader写入压力过大， 最终导致集群挂掉了，对一个公司的技术平台是有重大打击的，hbase、kafka之类的一些技术都是强依赖zk的，dubbo + zk去做服务框架的话，有上万甚至几十瓦的服务实例的时候

大量的服务的上线、注册、心跳的压力，达到了每秒几万，甚至上十万，zk的单个leader写入是扛不住那么大的压力的。

一般适合写比较少。

读比较多，observer节点去线性扩展他的高并发读的能力。

### 135、再次回头对ZooKeeper特性的总结

核心原理，比kafka、hdfs、hbase之类的要简单很多，最最底层的技术，很多人都依赖他。

《020_一清二楚：再次回头对ZooKeeper特性的总结》

集群模式部署

一般奇数节点，因为你5台机器可以挂2台，6台机器也是挂2台，不能超过一半的机器挂掉，所以5台和6台效果一致，那奇数节点可以减少机器开销，小集群部署，读多写少。

主从架构：Leader、Follower、Observer（一般刚开始没必要用）

内存数据模型：znode，多种节点类型。

客户端跟zk进行长连接，TCP，心跳，维持session。

zxid，高32位，低32位。

ZAB协议，2PC，过半ack + 磁盘日志写，commit + 写内存数据结构。

支持Watcher机制，监听回调通知。

顺序一致性：消息按顺序同步，但是最终才会一致，不是强一致。

高性能，2PC中的过半写机制，纯内存的数据结构，znode。

高可用，follower宕机没影响，leader宕机有数据不一致问题，新选举的leader会自动处理，正常运行，但是在恢复模式期间，可能有一小段时间是没法写入zk的。

高并发，单机leader写，Observer可以线性扩展读QPS。

## 系统架构优化与设计

### 136、一个身经百战的互联网面试官的系统设计优化连环炮发问！

（1）说说高并发场景下的数据库连接池应该如何进行优化？

（2）如果压测的时候发现系统的TPS不达标，此时应该如何优化系统？

（3）说说你对NoSQL的理解以及他的优缺点分别都是什么？

（4）假设让你来负责微信朋友圈这样的社交系统，应该如何设计？

（5）微信朋友圈是如何对好友显示权限进行控制的？

（6）如何设计高并发的朋友圈点赞系统架构？

（7）如何保证刚发的微信朋友圈被人点赞后，自己能及时的看到？

（8）在你的朋友圈点赞系统，如何防止用户连续点击几次进行重复点赞？

（9）设想你负责一个系统，此时某个核心服务如果挂掉，该怎么处理？

（10）如果你们公司的Nginx故障挂掉了，应该如何做才能保证他的高可用性？

（11）如果让你来设计12306售票系统，应该如何设计？

（12）如果让你来设计一个电商场景下的秒杀系统，应该如何设计？

### 137、说说高并发场景下的数据库连接池应该如何进行优化？

以druid来举例

（1）maxWait

表示从池里获取连接的等待时间，万一你暂时没有可用的连接，就可能要等待别的连接用完释放，你再去使用，通常建议设置在1000以上，就是等待1s以上，比如你可以设置1200，因为有的时候要等待建立新的TCP连接，最多在1s内，那你就得等一会儿。

如果这个参数默认设置为0，意思就是无限的等待获取连接，在高并发场景下，可能瞬间连接池耗尽，大量的请求都卡死在这里等待获取连接，进而导致你tomcat里没有可用的线程，服务就是一个假死的样子。

你还会拖累调用你的其他服务，其他服务都卡死在调用你的请求上，可能会导致整体系统大量服务的雪崩。

你设置一个靠谱点的参数，那么起码大量线程获取不到连接，1s左右快速就失败了，这个时候还不至于说拖死整个服务，也不至于说拖死其他调用你的服务，还不至于会发生服务雪崩的问题。

（2）connectionProperties

里面可以放connectionTimeout和socketTimeout，分别代表建立TCP连接的超时时间，以及发送请求后等待响应的超时时间，推荐connectionTimeout设置为1200，socketTimeout设置为3000。

之所以必须设置他们俩，是因为高并发场景下，万一遇到网络问题，可能会导致你跟数据库的Socket连接异常无法通信，此时你Socket可能一直卡死等待某个请求的响应，然后其他请求无法获取连接，只能是重启系统重新建立连接才行。

所以设置一下超时时间，可以让网络异常之后，连接自动超时断开重连。

（3）maxActive

最大连接池数量，一般建议是设置个20就够了，如果确实有高并发场景，可以适当增加到3~5倍，但是不要太多，其实一般这个数字在几十到100就很大了，因为这仅仅是你一个服务连接数据库的数量，你数据库整体能承受的连接数量是有限的。

而且连接越多不是越好，数据库连接太多了，会导致cpu负载很高，可能反而会导致性能降低的，所以这个参数你一般设置个20，最多加到个几十，其实就差不多了

更多的，你反而应该是优化你每个请求的性能，别让一个请求占用连接太长的时间。

### 138、如果压测的时候发现系统的TPS不达标，此时应该如何优化系统？

对系统进行压测，比如每秒压个几百请求到几千请求，甚至上万请求，此时发现死活压不上去，压来压去，你的系统最多每秒就处理几百个请求，根本到不了几千个请求，此时就发现系统的TPS不达标，此时如何优化？

其实这个时候，如果发现TPS不达标，通常是说明你系统肯定是每个请求处理时间太长了，所以就导致你单位时间内，在有限的线程数量下，能处理的TPS就少了，这个时候往往要先优化性能，再提TPS。

假设你一共有200个线程，结果你每个请求要耗费500ms，每个线程每秒就只能处理2个请求，200个线程每秒只能处理400个请求，比你期望的单机处理500~600个请求，要少了很多。

既然说要优化性能，那就得通过打日志的方式，或者是监控的方式，检查你服务的每个环节的性能开销，通常来说用打日志方式会细化一些，要靠监控把每个细节摸清楚，也挺难的，毕竟很多是代码细节。

把你的系统里一个请求过来，每一次数据库、缓存、ES之类的操作的耗时都记录在日志里面，把你的每个请求执行链路里的每个耗时小环节，都给记录清楚他，比如说你一个请求过来一共500ms，此时你发现就是某个SQL语句一下子耗时了300多ms，其实其他的操作都在正常范围内。

优化一下SQL语句呢？这个SQL语句搞了一个全表扫描，因为写SQL的时候没有考虑到使用索引，所以此时可以建立新的索引，或者是改写SQL语句，让他可以使用到你建立好的索引，SQL语句优化到100ms。

每个请求只要300ms就可以了，每个线程每秒可以处理3个请求，200个线程每秒可以处理600个请求。

你可以检查你核心服务的每个环节的性能，针对性的做优化，把你每个请求的时间降到最低，这样你单位时间内的TPS绝对就提高了，这个很关键。

其次就是增加机器数量，线性扩容了，比如说服务层面，每个服务单机最多抗800请求，那扩容到部署10台机器，就可以抗8000请求，但是你又得考虑你依赖的数据库，MQ，Redis能不能抗下这么多的并发。

### 139、为什么有了HDFS之后，还需要HBase呢？

hdfs，对hdfs是什么，功能，架构，源码，如何二次开发，都有了一定的理解，看懂hdfs那个课程的前提，Java架构部分的jdk集合、并发、IO、网络，自研分布式海量小文件系统的项目，最好去先做一下。

hdfs设计主要是针对什么呢？针对的是大数据，超大文件，比如说你有一个超大文件，里面要放100GB的用户行为的日志，甚至是1TB，甚至1PB，针对一个网站的每天的用户行为的日志，可以都放一个超大文件里去。

超大文件很难说放在一台服务器上，所以说此时，可以把超大文件拆散，拆成N多个128MB的小文件，每一个小文件就可以说是这个大文件的一个block。

hdfs解决的主要是一个分布式存储的问题，也就是说你有超大数据集，不可能都放在一个文件里，是不现实的，所以可以拆分为N多个128MB的block小文件，分散存储在多台机器上，对超大数据集实现分布式存储的效果。

每个block小文件还有3副本冗余存储，每个副本在不同的机器上，高可用和高容错，任何一台机器挂掉，不会导致数据丢失的。

hdfs，hadoop distributed filesystem，分布式文件系统，他存放的是文件，文件死的，静态的，最多只能是你不停的往文件的末尾追加数据，他会把你追加到文件末尾的数据其实都是分散在不同的小block里存储在机器上。

目录层级结构，创建文件，管理权限，对文件进行删除，大概就是这样的一些事情了，对大文件里的数据进行读取，对文件进行数据追加，hdfs只能做到如上一些事情。

针对你hdfs上存储的海量数据，10TB的数据，我要进行增删改查，我要往里面插入数据，还要修改数据，还有删除里面某一行数据，还有精准的查询里面某一行数据，得了，hdfs上是大量的block小文件。

虽然说帮你把超大数据集给分布式存储了，现实吗？根本就不现实。

所以呢，当当当当，hbase出马了，由hbase基于hdfs进行超大数据集的分布式存储，让海量数据分布式存储在hdfs上，但是对hdfs里的海量数据进行精准的某一行，或者某几行的数据的增删改查，由hbase来解决了。

hadoop nosql database

### 140、到底为什么把 HBase 叫做NoSQL数据库呢？

hdfs可以解决我们的一些问题，超大数据集的分布式存储，hbase作为hadoop nosql database，来解决海量数据的增删改查的问题。

到底什么是nosql，跟sql相对应。

hbase主要是就是能够帮助你对海量数据进行增删改查，跟sql是相反的，关系型数据库，mysql/oracle，一般来说都是基于SQL语法让你实现复杂的一些SQL语句，还可以支持事务，主要是开发业务系统的。

比如说你有一些需求，是要对10TB的数据，对他们进行相对较为简单的增删改查，比如说插入一行数据，查询一行数据，根据一些简单的条件查询某几行数据，删除一行数据，更新一行数据。

hbase就可以搞定，专职就是干这个的，分布式nosql数据库就可以了，他是跟关系型数据库相反的，是不支持SQL语句的，nosql，没有SQL语句的支持，没有SQL的数据库，帮你对海量数据做简单增删改查的。

nosql数据库，一般都是分布式的，解决海量数据的简单增删改查问题的，如果你要是针对少量数据做简单增删改查，也不需要nosql，其实你用mysql/oracle天然就可以搞定少量数据的增删改查。

天生不擅长sql，所以不要强行在上面用sql，就是做一些简单的增删改查就可以了。

很多场景是需要对海量数据做基础的增删改查，不需要复杂的sql语法支持，那么天然可以用hbase，海量数据可以存储，分布式的nosql支持。

### 141、HBase作为一个NoSQL数据库，有哪些架构上的特点？（上）

### 142、HBase作为一个NoSQL数据库，有哪些架构上的特点？（下）

（1）分布式架构

hbase定位是分布式nosql数据库，把自己的nosql数据库的功能是通过多台机器来实现的，有多个RegionServer，分布式管理数据，分布式执行你的各种nosql数据库的操作 。

（2）分布式数据存储和自动数据分片

这个功能是极为强大的，比如你搞一个hbase里的表，然后在表里搞很多很多的数据，这个表会分为很多的region，每个region里是一个数据分片，然后这些region数据分片就会分散在多台机器上。

假设你的表里的数据太多了，此时region会自动进行分裂，分裂成更多的region，自动分散在更多的机器上，对我们使用是极为方便的。

（3）集成hdfs作为分布式文件存储系统。

（1）强一致读写

他不是zk那种最终一致性，是强一致的，你写成功了立马就可以读。这个功能是极为实用的，他是依靠的分布式存储才做到的，zk那种是属于主从同步，你读follower机器是可能读到不一致数据的。

（2）高可用

每台机器上部署一个RegionServer，管理一大堆的region数据分片，RegionServer都是支持高可用的，一个RegionServer挂掉不会导致数据丢失，他自动可以由别的机器接管他的工作运行下去。

（3）支持mapreduce/spark这种分布式计算引擎

对hbase里的数据进行分布式计算，可以从hbase里分布式抽数据去计算，也可以把计算后的结果写入hbase分布式存储。

（4）Java API/thrift API/REST API的支持

当然支持Java API了，咱们的Java业务系统经常会有海量数据NoSQL存储的需求，此时就可以基于Java API来操作hbase里的数据了。

（5）支持协处理器，块缓存和布隆过滤器，可以用于优化查询性能。

（6）hbase现在最新版本都是支持web界面的方式来对hbase集群进行运维管理的。

### 143、HBase作为NoSQL数据库，到底适用于哪些场景？

（1）海量数据场景

表来形容，单表在千万以内级别的数据量，基本都是小数据，千万级别的数据量，最多只能说是中等数据量，MySQL搞一下分库分表，搞个两三台服务器，就可以轻松抗住千万级别的数据量的表了，每个表可能也就几万条数据了。

基于分库分表的中间件，mycat、sharding-sphere，都可以的，直接做一些路由什么的，就可以轻松搞定几千万级别的数据了，性能也是很高的。

假设几千万条数据是过去历史几年下来积累的，每年积累一千万数据，每个月也就100万数据左右，每一天几万数据量，10年才1亿数据，MySQL分库分表的技术方案，抗下小亿级别的数据量都是ok的，一两亿数据。

可能你作为这个系统的负责人，在可见的范围内，基本上单表撑死也就几千万到一两亿级别，10年、20年以后了，不用考虑这么多了，其实像这种级别的存量和增量的数据量，用MySQL分库分表就可以轻松搞定了。

要做一些跨库跨表的SQL，不太好做，可以自己查询一些数据放到内存来做定制计算也是可以的。

什么叫做海量数据？说不好听的，假设你就几百万数据，用MySQL就可以轻松高兴了，要是你有几千万数据呢？基本到MySQL的瓶颈和极限了。要是你有几亿条数据呢？而且每天数据量还在不停的增长呢？

这个时候你就可以使用hbase了，他天生就是分布式的，可以扩容很方便，数据分布式存储，自动数据分片，完善的运维管理，底层集成hdfs做分布式文件系统，增删改查的nosql功能都支持，你几亿到几十亿的数据放里面很适合，而且数据还一直在增长。

绝对比mysql分库分表要来的方便的多。

（2）只需要简单的增删改查的支持

你对海量的数据仅仅就是简单的增删改查的支持，绝对没有MySQL那种关系型数据库支持的列类型、索引、事务、SQL语法，那么多高阶的特性，你要是不需要索引、SQL和事务，那妥妥的用hbase就可以了。

虽然hbase之上有很多开源组件，可以搞二级索引、phoniex可以支持SQL，但是说实话，真的没必要，人家hbase就不是干这个的，麻烦大家别折腾他好吗。

你要海量数据下支持事务，可以用分布式数据库，比如TiDB；你要海量数据下支持复杂SQL实时分析，可以用clickhouse，或者是druid之类的。

### 144、HBase的数据模型是什么样的？（上）

### 145、HBase的数据模型是什么样的？（下）

hbase里其实也是建一个一个的表。

表里有很多行的数据，但是其实这个表说白了就是一个逻辑模型，物理上根本没那么简单的，一个表的数据当然是拆为很多region分散在不同的机器上的，要是表里数据太多了，region数量还会变多，这样你加更多机器，region可以自动迁移到不同的机器上去。

每一行都有一个rowkey，还有很多列，表里的数据行都是按照rowkey排序的，大致可以把rowkey理解为mysql里的主键id，在hbase里每一行数据都有一个rowkey行健来唯一的标识一行数据。

所以一般设计rowkey是一门讲究活，后续还会讲如何设计rowkey的，因为一般要把同一类数据的rowkey设计的相似一些，比如说用户id=1的订单，就应该叫做order_1_xx之类的，这样一个用户的订单就会在排序之后靠近在一起。

rowkey 列

order_1_110 xxx

order_1_111 xxx

order_2_256 xxx



rowkey order:base order:detail order:extent

order_1_110 xxx xxx

order_1_111 x1(t1); x2(t2) xxx xxx

每一行数据都有一些列族，就是column family，每个列族都包含一些列，每个列族都有一系列的存储属性，比如说是否把列族里的列值缓存在内存里，列族里的数据如何进行压缩，类似这种。

一个表里有固定的一些列族，每一行都有这些列族，当然有可能你一行数据在某个列族里没存什么东西，是有可能的。

然后就是列，每个列就是一个列族+分号+列限定符（column qualifier），比如说列族是order，列可能就是order:base，或者是order:detail。

每个表的列族是固定的，但是每一行数据有哪些列是不固定的，插入数据的时候可以动态可以给这行数据设定多个列，每个列都是属于一个列族，就是一个列族+分号+列限定符的形式，就可以确定一个列。

时间戳，timestamp，每一行的每个列的值写入的时候就会有一个时间戳，时间戳就代表了这一行这个列的某个版本的值，当然这个timestamp你也可以自己插入的时候指定一个timestamp也是ok的。

单元格，也就是cell，其实就是一行的某个列族下的某个列（由列限定符来确定）的某个timestamp版本对应的值，说白了就这么个东西，在hbase里，每一行的每个列的值，是有多个版本的，每个版本都是一个cell。

### 146、HBase的物理存储格式：为啥说他是列式存储？

rowkey order:base order:detail order:extent

order_1_110 xxx(t3) xxx(t4)

order_1_111 x1(t1); x2(t2) xxx(t5) xxx(t6)

hbase，列式存储的一个系统，他不是说按一行一行的格式来进行存储的，按列来进行存储的。

rowkey timestamp 列 值

order_1_110 t3 order:base xxx

order_1_110 t4 order:detail xxx

order_1_111 t1 order:base x1

order_1_111 t2 order:base x2

order_1_111 t5 order:detail xxx

order_1_111 t6 order:extent xxx

### 147、假设让你来负责微信朋友圈这样的社交系统，应该如何设计？（上）

### 148、假设让你来负责微信朋友圈这样的社交系统，应该如何设计？（下）

![朋友圈设计](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/03/images/147/01.png)

你自己可以发朋友圈，刷朋友，看到你自己发的朋友圈以及你的好友发的朋友圈，你可以对朋友圈进行点赞，进行评论，你可以去设置权限，你不看某些人的朋友圈，不让某些人看你的朋友圈，拉黑或者删除某个好友再也不用看到他的朋友圈了

首先你发送朋友圈的时候，一般是9张图片配合一些文字，组成了一条朋友前，文字还好说，但是图片就会稍微有点大了，也可以是一个短视频配合一些文字，点击发送，假设你要同步发送，可能会导致你点击发送按钮之后，弹出一个旋转框，告诉你发送中，持续好几秒种中，用户体验是比较差的。

有一个好一点的办法，可以把这些数据在客户端本地暂存一下，然后直接让你发送成功返回，走一个异步发送状态，然后立马让你自己在刷朋友圈的时候，可以把你客户端本地的刚发的朋友圈加载出来看到。

仅仅是这样，就会变成，发朋友圈成了你自己的自娱自乐，因为你的朋友圈并没有发送出去让你的好朋友看到，可以走一个异步的模式，把你的朋友圈里的图片或者视频+文字，花费几秒钟的时间传送到你的后台服务器上去存储。

之后，你的朋友就可以从后台服务器上加载你的朋友圈里的图片和视频，可以看到了。

你的那些视频和图片，是不是可以就都直接就近上传到CDN（content delivery network），不是直接到朋友圈系统的后台，这样速度是很快的；接着就是发送请求到朋友圈后端系统，请求包括图片的地址，你配的文字，发朋友圈的时候可以选择开放给谁看，这些数据写入到朋友圈发布表里去。

然后需要在相册表里写入索引数据，里面存放的是对你的发布表里的数据引用，这样你以后浏览相册的时候，都是根据相册里的索引数据到发布表里找实际对应的数据的。

接着就走一个离线批处理，通过批处理程序把这条朋友圈写入到你所有好友的时间线表里去，你好友的时间线表里就是存放了他刷朋友圈的时候，可以按照时间线刷到的所有好友的朋友圈。

你有3个好朋友，每个好朋友都在时间线表里有一个朋友圈的时间线，按照时间顺序排列了他可以查看的所有朋友圈，包括了他自己发的朋友圈以及他的好友发的朋友圈允许他看的那些，都会在这里。

然后你的好朋友刷朋友圈的时候，就会知道自己的时间线表里有个新的变化，就是有好友发了朋友圈，此时就会提示你一个红色的圆点，你就开始刷，刷的时候就根据图片url地址，去cdn拉取的。

### 149、微信朋友圈是如何对好友显示权限进行控制的？

发送朋友圈的时候，可以通过几种方式进行谁可以看你这条朋友圈的权限的控制，你发的时候可以选择屏蔽谁，对哪个标签下的人开放。

这条朋友圈的权限到了后台之后，会有一个离线批处理的程序跑起来，对最近发的一波朋友圈都找他们的朋友圈的权限的设置看一下，此时就会对你允许看到的好友，此时就在他们的时间线里插入这条朋友圈数据，那么这样的话，只有你允许的好友的时间线里才有你这条朋友圈。

比如说王五发的朋友圈16931可以允许张三和李四看到，设置了一个标签组，标签名称是老铁三人组，里面就正好有张三和李四。

张三 发表朋友圈的时间戳 朋友圈16931 王五

张三 发表朋友圈的时间戳 朋友圈16384 李四

李四 发表朋友圈的时间戳 朋友圈16931 王五

在redis里可以设置张三的朋友圈是有变动的一个状态，在上次拉取朋友圈的时间点之后的一些朋友圈都从时间线表里拉取出来，刷朋友圈的时候，如果说你的网速要是不太好的话，你会发现这样一个场景。

就是你最新的一些朋友发的朋友圈是显示出来了，但是视频和图片都是一片灰色，仅仅能看到他的文字和其他的一些东西，比如说点赞之类的，图片和视频死活看不到，都是一片灰色，反正我自己网速不好的时候经常看到这样的情况。

假设王五之前发了一条朋友圈，设置李四可以看到的，李四之前确实是看到了这条朋友圈的，但是有个问题，王五后来跟李四吵了一架，关系变得非常的不好，王五就对李四设置了一个朋友圈的权限，就是自己的朋友圈不允许李四看到，甚至可能会直接拉黑/删除李四这个好友，这个就够狠了。

你设置自己的朋友圈对所有朋友都是仅仅三天之内可见。

就是说你跟李四之间的朋友圈的权限总设置或者是朋友之间的关系，有了变化，或者是你的自己的朋友圈对外展示的总权限有了变化，此时每次如果有变动，那么这些设置，包括你对每个朋友的朋友圈权限的设置，跟朋友的关系，自己的朋友圈的总权限，这些设置都会统统的缓存起来。

包括缓存在你自己的客户端本地，也可以缓存在你的朋友的客户端本地。

但是你可能随时会拉黑、删除某个人，或者是突然设置对那个人朋友圈不可见，或者是突然你自己设置了朋友圈三天可见什么的，所以你设置的这些东西，都会被缓存起来，每次你好友刷朋友圈，查看自己的时间线表的时候，都会检查你的某条朋友圈根据你的一些行为，是否还对他可见。

李四会关注王五的各种朋友圈权限和朋友关系的一个变化，一旦说有变化了，可以缓存到自己的本地，下次在客户端里再次刷新朋友圈的时候，客户端对于王五的朋友圈会和王五的各种权限设置结合起来判断一下。

李四能否看到王五的这条朋友圈。

一般那些操作很少做的，所以做的时候更新一下缓存就行了。

一般问题不大。

### 150、如何设计高并发的朋友圈点赞系统架构？

我看到了你的朋友圈，此时我就可以对你的朋友圈去进行一个点赞，也可以取消点赞，假设要设计成支撑高并发的点赞系统，应该如何设计？

朋友圈的点赞和评论，是独立的数据，其实比如点赞，都是可以基于redis来做的，每个朋友圈里对应一个set数据结构，里面放谁给你点赞了，这样每条朋友圈的点赞人和点赞数量直接从redis出就可以了，smembers和scard。

评论也是可以存表里的，都是以朋友圈为粒度来存储。

那么刷朋友圈的时候，比如说你好友和你，另外一个好友都是好友，此时你好友刷到了你的朋友圈，就可以把另外一个好友对你的点赞和评论都拉出来，展示在客户端下面就可以了，这个展示过程可以是动态的。

你是王五，你的朋友圈被张三点赞了，李四跟你们也是好朋友，此时李四刷朋友圈看到了王五发的这条朋友圈，此时你可以在后台，对这条朋友圈的set用张三做一个sismember操作，就是判断一下你们俩的所有共同好友，有哪些人对这条朋友圈点赞了。

此时就可以看出来这条朋友圈被你们的共同好友多少人点赞了，哪些人点赞了。

比如你另外一个好友是否对这条朋友圈点赞了，直接sismember就可以判断出来，这样整个你基于redis，他都是非常高性能的。

### 151、关于重复点赞问题以及点赞查看时效性的方案设计

我看到了你的朋友圈，此时我就可以对你的朋友圈去进行一个点赞，也可以取消点赞，假设要设计成支撑高并发的点赞系统，应该如何设计？

朋友圈的点赞和评论，是独立的数据，其实比如点赞，都是可以基于redis来做的，每个朋友圈里对应一个set数据结构，里面放谁给你点赞了，这样每条朋友圈的点赞人和点赞数量直接从redis出就可以了，smembers和scard。

假设通过数据库的表来存放点赞的数据，此时可以通过联合唯一键来保证幂等性，用户id+朋友圈id是一个联合唯一索引，就可以确保一个人对一条朋友圈只能点赞一次，如果通过redis里的set来实现，set本身就是可以去重的，你多次把一个人放到一条朋友圈的set里去，是不会重复的。

评论也是可以存表里的，都是以朋友圈为粒度来存储。

那么刷朋友圈的时候，比如说你好友和你，另外一个好友都是好友，此时你好友刷到了你的朋友圈，就可以把另外一个好友对你的点赞和评论都拉出来，展示在客户端下面就可以了，这个展示过程可以是动态的。

你是王五，你的朋友圈被张三点赞了，李四跟你们也是好朋友，此时李四刷朋友圈看到了王五发的这条朋友圈，此时你可以在后台，对这条朋友圈的set用张三做一个sismember操作，就是判断一下你们俩的所有共同好友，有哪些人对这条朋友圈点赞了。

此时就可以看出来这条朋友圈被你们的共同好友多少人点赞了，哪些人点赞了。

比如你另外一个好友是否对这条朋友圈点赞了，直接sismember就可以判断出来，这样整个你基于redis，他都是非常高性能的。

### 136、ThreadLocal内存泄漏问题

ThreadLocal这个东西为什么会有内存泄漏。

我们的每个线程可以通过THreadLocal来存取自己线程专属的一个变量副本，ThreadLocalMap，Key-Value，Key是WeakReference，弱引用，value就是你自己放的变量副本。

比如说你的线程长期存活，ThreadLocal里会一直有你这个线程的key-value对，万一说出现一些内存不够的情况，进行了gc，此时就会自动把很多线程在ThreadLocal里存放的key-value对的key，弱引用，都会进行回收。

null -> value

JDK团队都有解决的方案了，你在通过ThreadLocal，set、get、remove，他会自动清理掉map里值为null的key，确保不要有很多的null值引用了你的value造成内存的泄漏问题，这个就是一个他的自己的解决方案。

你不要老是让一个长期存活的线程，线程池里的线程，要不然可能是你自己开启的线程在后台长期运行，尽量避免在ThreadLocal长期放入数据，你不使用的时候最好及时的进行remove，自己主动把数据给删除了。