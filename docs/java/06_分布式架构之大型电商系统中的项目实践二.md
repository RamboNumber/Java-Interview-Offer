# 06_分布式架构之大型电商系统中的项目实践

## 02_基于Spring Cloud的大型电商系统分布式架构重构实战

### 147_重新再来梳理一下项目阶段二的授课步骤和思路 

**第一步，146讲已经搞定了整个spring cloud核心的5个组件的源码** 

深入的学习了，在我们这种一线的大型互联网公司中，其实就是这么玩儿的，我们要用一个新技术，往往就是把这个新技术的核心组件的源码，通读一下 

**第二步，用spring cloud核心的5个组件，来将项目阶段一中开发好的电商系统改造成微服务+分布式的架构** 

整个电商系统拆散成多个小服务，整个就会形成一个分布式的系统架构 

带着大家来熟悉一下系统架构逐步完善，逐步演进的一个过程，如果说在你的公司里实际的去实践和落实微服务系统架构的时候，你都不要一步到位，一步一步来的 

微服务架构 -> 服务拆的很散，很小，一个大的系统拆分成N多个小服务 -> 就一定是分布式系统架构 

做这个事情的时候，核心点在于说，利用你之前对每个技术源码级别的一个理解，然后来融入你的架构设计的思考过程中，这个是非常关键和有价值的，是真正的架构师和普通的工程师之间的最大的区别 

普通的工程师做一个系统设计的时候，就是简单的将技术用一下，而架构师是站在一个对技术源码级别很深入的理解的背景之下，来对架构进行设计，这里的思考的深度就完全不一样了。。 

动手开始改造这个项目，是个体力活儿，很简单的，没什么特别的事情 

最最重要的是，很多参数要合理的设计，基于你对源码的这个理解来设计很多的机制和参数的使用 

在改造的过程中，会将spring cloud其他的组件都给用一下，比如说rabbitmq + spring cloud stream，zookeeper + spreing cloud zookeeper，Spring Security + OAuth2 + JWT，服务接口文档 + swaggerUI 

**第三步，分布式系统架构** 

你既然做成了微服务架构，就一定是一个分布式系统了，分布式系统就一定要用一些分布式的系统架构 

分布式事务、分布式锁、分布式会话、单点登录、分布式一致性、分布式接口幂等性，很多分布式场景下我们面临的一些技术挑战都需要在这里来做，说实话，这块完全就会做成一个大型的分布式系统的架构实战 

之前体会过，项目实战，复杂的业务来做项目，设计模式，真正能体会到一个技术在复杂业务场景里是如何来使用的，分布式技术，系统架构，也是一样的，另外一方面，是正儿八经的非常复杂的业务场景下的各种分布式技术的方案，架构，设计，实战 

**第四步，初步实现高可用的架构** 

限流、降级、超时、熔断、异常，主要是基于hystrix来做，所有的这个高可用的一些方案，全部都是基于复杂的业务场景来做，技术方案都会呈指数级的复杂增长，复杂N倍，要考虑到很多东西 

**第五步，DevOps +** **持续集成 +** **持续交付 +** **自动化部署** 

这块东西，是真的说，在微服务之后，将开发效率提升到最高，无情的自动化的一个体现，大量的工作不需要码农手工去做，大量的都是自动化来操作 

**第六步，容器技术** 

微服务一旦部署之后，基本上都是多大几十个服务实例，几百个服务实例，一般来说会用这个容器技术对大量的微服务的实例来进行管理 

**第七步，微服务支撑运维平台** 

（1）日志中心：ELK + Kafka + 自研组件，亿级大规模日志中心架构设计

（2）监控中心：机器与Metrics监控，OpenFalcon

（3）配置中心：spring cloud config源码，Apollo先剖析源码，再投入使用 

**第八步，敏捷开发和项目管理** 

敏捷式的支持快速迭代的开发流程以及项目管理流程 

**第九步，微服务治理平台** 

自研微服务治理平台，主要是参考大众点评的CAT源码，还有阿里的鹰眼的架构思想，加入很多的微服务系统的治理的很多的功能 

**第十步，生产环境的演练，部署、压测、容灾、工程** 

这套东西做微服务架构的思路，是完全合理的，就是说无论你在公司里是将一个旧的系统改造成微服务架构，还是说将一个全新的项目用微服务架构来做，都可以采用和参考这套顺序和步骤来做 

我们之前从0开始建设上面的一整套微服务架构，整个我们这个过程，可能用几年的时间搞出来的，每一块都涉及到巨复杂的系统，业务，很庞大的团队，所以说我们花费的时间来探索和积累，都是一步一步来的，都花了很多的时间来做 

在我们课程里，大家就是后面会看到，我们也是一步一步来建设的，你在公司里也是可以按照这个思路，拆分成很多个步骤，一步一步的来做，结合公司的实际情况来做，我估计这个快东西，没那么快能搞完，十月份结束 

在这个阶段二的后期，会同步启动一个JDK并发、集合、io、网络、类加载、jvm等底层技术和知识的源码剖析的这么一个阶段，阶段二最后的那个自研微服务治理平台，才可以搞定

### 148_接下来回顾一下我们的项目阶段一完成的电商系统的组成 

类似于复习的一个意思 

权限中心（这个是面向后台的权限中心，电商系统的管理后台，管控公司内部员工的使用系统的权限的）

购物车中心

评论中心

商品中心

基础模块：不会做成一个独立的系统或者是服务，属于工具包，可以做成一个依赖

客服中心

财务中心

库存中心

物流中心

会员中心

订单中心

支付中心

促销中心

采购中心

调度中心

仓储中心 

一开始就是三五个人一起开发，最快的 

10个人，15个人，20个人 

每个中心都有专门的人去负责的，最最起码的每个中心派一个人去负责，电商系统他是不断的在更新版本，在升级功能的，可能就是每个中心也许都会派一个PM来负责，不断的出需求，增加和更新功能，就是说要有对应的RD来支持PM，不断的做新的功能 

15个中心，15个人的话，最最好的就是每个中心分配一个人呗 

在刚开始进行微服务拆分的时候，就是不用想的太多，也不用过于复杂，来设计拆分成多少个服务呢？没有一个准则的，刚开始，其实就是按模块来划分，15个模块，那就拆分成15个服务 

到了后面，如果一个服务的代码越来越多，比如都几万行代码，或者是十几万行代码，采购中心，采购单管理、供应商管理，采购服务 -> 采购单服务 + 供应商服务；订单中心，订单管理 + 价格管理 -> 订单服务 + 价格服务 -> 也会不断的拆分更多的服务 

在项目阶段二，拆分成15个服务就ok了，其实在项目阶段四，就是10倍复杂的业务系统，那个时候又是纯做业务了，项目阶段三已经构造好了一个特别好的棒的一个基础框架，公司的研发框架，可以将开发效率提升10倍！ 

可能15个服务会继续拆分，可能会拆分成比如25个服务，35个服务，50个服务，都有可能，几十个服务，每个服务在线上起码部署2台以上的机器，那么其实你线上光是业务系统，都上百台机器了 

微服务架构，主要是解决复杂系统，10人以上协作开发的效率问题

### 149_初步敲定电商系统微服务架构改在一期的整体技术方案 

如何将一个单块系统改造成一个微服务架构的系统呢？ 

技术，基于spring cloud来改造整个系统，eureka、ribbon、feign、hystrix、zuul 

eureka：是肯定要用的，服务注册中心；我们基于eureka将各个模块拆成一个一个的服务，然后将服务注册到eureka中去 

ribbon + feign：ribbon作为底层的负载均衡的组件，我们知道他的核心原理，重试机制也是跟ribbon相关的，但是我们是不会直接使用ribbon的；feign，我们基于feign以声明式调用的方式，来进行服务间的一个调用 

hystrix：不在我们第一期改造的范围里面，之前给大家说过了，feign是可以独立使用的，如果你没开启feign + hystrix的整合，其实就跟hystrix是没关系的，我们改造的时候，第一步先别上hystrix 

zuul：作为网关，是肯定要用的，用zuul作为网关来屏蔽掉后端的所有的服务，对外暴露的其实就是一个一个的接口，人家在调用的时候，都是调用接口的，不care服务的，直接将接口调用的请求交给zuul，网关将这个请求转发给后端的各个服务 

第一期改造的技术方案 

eureka，ribbon + feign，zuul（默认就整合ribbon和hystrix） 

在公司里，如果要做一个事情，在做具体的技术设计之前，肯定是先出技术方案，拿着技术方案来进行评审和讨论，所以我们这一讲，相当于就是初步的先出了一个技术方案

### 150_完成电商系统微服务架构改造一期的概要设计

电商系统微服务架构改造一期架构 

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\15001.png) 

按照我们在公司里的做法，就是我们会把一个架构的升级，给他拆分成N多期，比如说架构升级改造，我们会说有一个一期，二期，三期，四期，五期，六期，等等阶段，比如说我们最近两周在做的就是这个一期 

我其实之前给大家都演示过了，在阶段一的时候，我们重点演示了很多的文档的编写的格式，等等。所以说项目阶段二开始，我们就不会再次按照特别标准和规范的方式来写文档了，写的太规范了，花费的时间太多了。 

讲项目，讲技术，文档的东西，我们走这个过程和环节，但是尽量简化一些，避免耗费太多的时间 

\-------------------------------------------------------------------------------------------------

1、微服务网关层 

我们使用zuul来构建微服务架构的网关，起码部署两台机器，双机部署是最最起码的，这样子可以保证如果一台机器挂了，还有另外一台机器可以用，我们在生产系统里面，每个服务，最最起码是双机部署 

2、业务服务层 

一共将整个电商系统拆分为15个服务，每个服务部署两台机器，双机部署，避免单点故障，如果只部署一台机器，挂了整个就挂了，单点故障。 

3、数据存储层 

所有的服务都共用一个数据存储，也就是一个数据库，这个数据库就部署在一台机器上，刚开始是不做分库分表的，然后的话呢，就是在数据库服务器中，创建15个逻辑库，在一个MySQL中，创建15个database。 

在微服务架构中，每个服务都是自己的代码仓库，每个服务都是自己的数据库，逻辑库，一个MySQL中的一个database。后面如果压力上来了，你就可以将不同的服务的数据库迁移到不同的机器上去。 

4、服务注册中心 

采用Eureka来作为服务注册中心，每个服务都是一个Eureka Client，会自动往Eureka Server去进行服务注册，eureka server是双机部署的，这样的话呢可以避免单点故障 

5、服务调用 

采用ribbon + feign进行服务调用，feign进行生命式的服务调用，ribbon进行服务调用的负载均衡 

6、开发框架 

每个服务的业务逻辑的开发，统一采用SSM框架（spring mvc + spring + mybatis + spring boot）

### 151_电商系统微服务架构改造一期的详细设计的整体思路

### 152_从源码级别验证spring cloud环境下的服务注册的毫秒级时效性

### 153_从源码级别验证eureka服务发现的分钟级时效性

服务发现的时效性分析

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\15301.png) 

### 154_从源码级别验证服务定时进行心跳通知的时效性

### 155_基于概要设计的架构图来细化和完善出来详细设计的架构图

电商系统微服务架构改造一期的详细设计

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\15501.png)

### 156_从源码级别验证服务故障时的自动感知下线的时效性

### 157_从源码级别验证服务正常下线时的感知时效性

电商系统微服务架构改造一期的详细设计(2)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\15701.png)

### 158_从源码级别分析eureka自我保护机制的稳定性以及可用性

### 159_构造eureka server集群通过源码探究服务是如何访问集群的

电商系统微服务架构改造一期的详细设计(3)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\15901.png)

### 160_从源码级别验证eureka server进行集群同步的时效性

电商系统微服务架构改造一期的详细设计(4)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16001.png)

### 161_从源码级别验证feign+ribbon+eureka服务发现与故障感知的时效性

电商系统微服务架构改造一期的详细设计(5)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16101.png)

### 162_从源码级别确认spring cloud环境下ribbon的负载均衡算法

电商系统微服务架构改造一期的详细设计(6)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16201.png)

### 163_从源码级别出发来敲定feign+ribbon的超时以及重试参数的设置

电商系统微服务架构改造一期的详细设计(7)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16301.png)

### 164_从很多年前最原始的网关出现的场景开始讲起zuul如何使用

### 165_通过ribbon预加载来解决zuul网关第一次访问总是超时的问题

### 166_从源码级别验证zuul+ribbon+eureka服务发现与故障感知的时效性

电商系统微服务架构改造一期的详细设计(8)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16601.png)

### 167_通过实验来演示一下zuul无法感知到服务故障的期间会发生什么

### 168_为zuul配置一下请求超时以及失败重试的参数以及进行实验

电商系统微服务架构改造一期的详细设计(10)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\16801.png)

### 169_完善一下数据库、工程拆分相关的详细设计的文档编写 

我们其实主要是站在之前对每个组件的核心源码的理解上，来思考每个组件的底层的工作机制和关键的运行流程，然后针对这些核心机制的关键参数，来考虑应该在生产环境下如何来配置和调整，以及做一些对应的测试 

我们要对整个这次这个架构的各种细节都做比较深入的一个思考 

**1、服务注册中心** 

eureka server来做的，我们要来思考服务注册和发现的核心运行过程中，有哪些地方是需要我们来配置和调整的 

**1.1 eureka server的请求压力** 

他是双机部署，然后eureka server两台机器互相之间都会进行注册，完成eureka server集群的一个识别和构造 

之前有同学问过我，就是说eureka server在线上的生产环境是否要部署很多台机器，没什么太大的必要，eureka server的设计原则，是纯粹基于自己的内存来设计的，也就是不会比如说依赖数据库，或者是网络请求，所以eureka server纯内存操作，都是很快的 

每一台eureka server的机器都可以承载很多的并发请求，一台普通的4核8G的机器，部署eureka server，每秒钟接收个几百的请求都是问题不大的 

你觉得eureka server的压力大吗？不大，其实各个服务都是eureka client，eureka client其实每隔30秒来一次心跳的请求，所以这个压力其实一点儿都不大，比如说你有100个服务，每隔服务部署2台机器，就有200个服务实例，30秒有200个心跳请求，每秒呢？每秒大概也就8~10个心跳请求 

你的一个系统做到200个服务实例的时候，已经算是比较不错的了 

除非是你如果是一个大的公司，大型互联网公司，上百个系统，几万个服务实例，几十万个服务实例，30秒，有10万个心跳请求，每秒要3000多的请求了，所以此时eureka server的压力就会比较大了，要部署一个比较大的eureka server的集群了 

大部分的小公司而言，其实无所谓，几百个服务实例，每秒钟eureka server承载的请求可能就10个请求，或者几十个请求，这个压力很小 

在中小型公司而言，部署2台eureka server组成一个集群，妥妥的 

**1.2** **服务注册的时效性** 

在spring cloud环境下来验证一下 

之前给大家讲过，spring cloud环境下对eureka client的封装在spring-cloud-netflix-eureka-client工程里面，在这个工程里，就会自动初始化一个DiscoveryClient，而且还有一个额外的附加的一个类，EurekaServiceRegistry，这个额外的类是负责在服务一启动的时候，就立马来向eureka server发起一个注册的请求 

我们从源码层面来验证一下，服务注册实际上是，只要这个服务一启动，就会立马向eureka server，服务注册中心发起一个注册的请求 

服务注册的时效性应该是毫秒级的，起码在1秒以内 

spring-cloud-netflix-eureka-server工程，实际上在启动的时候，就会完成eureka-server的初始化的流程，然后就让eureka core基于了jersey来提供了一些restful的http接口，供eureka client来访问和请求 

eureka-core工程里，resources包下，有一个ApplicationResource，里面有个addInstance()方法，是负责接收服务注册的请求 

接下来依次请求eureka server和serviceA，来从源码级别验证一下服务只要一启动，就会完成服务注册 

eureka server其实本质上也是一个eureka client，所以eureka server启动的时候自己本身的eureka client会尝试向自己来进行注册，但是这边是不会注册的，所以说在eureka server启动的时候不会发生这个注册

serviceA启动的时候，代表了一个eureka client，就是一个服务，肯定会初始化一个DiscoveryClient，这个东西是必须有的，我之前给大家讲过 

EurekaServiceRegistry    : Registering application ServiceA with eureka with status UP 

在服务启动的时候，通过spring cloud额外封装的一个类，立即发起一次注册的请求 

直接会发送一个请求到eureka server的ApplictionResource中去，完成一个服务的注册 

**从源码层面可以判断出来，其实就是说spring cloud对eureka做了额外的封装，只要服务一旦启动，立马就会发出注册的请求，时效性基本在毫秒级**

**1.3** **服务发现的时效性** 

服务刚启动的时候，发现其他所有服务的时效性 

模拟一个场景，比如说先启动一个服务A的一台机器，启动服务B，服务B是依赖于服务A的，所以说我们通过源码来看一下服务B能否立马发现这个服务A已有的服务实例 

服务B一启动的时候，会去抓取注册表，直接发送一个http请求到eureka server抓取已有的全量的注册表 

[Application [name=SERVICEA, isDirty=true, instances=[com.netflix.appinfo.InstanceInfo@a7d576d6], shuffledInstances=null, instancesMap={localhost:ServiceA:8080=com.netflix.appinfo.InstanceInfo@a7d576d6}]] 

**服务刚启动的时候，立马发现其他所有服务的时效性是毫秒级，立马就会去发送一个请求，拿到所有的注册表** 

后面，如果说其他服务加了一台机器，此时这个服务要发现别的服务新增了一台机器，要过多久可以发现，服务A新增了一台机器，服务B要发现服务A新增了一台机器，要过多久才可以发现 

eureka server的多级缓存的机制 

比如说，现在服务A新增了一台机器，会更新到注册表中去，而且会立马过期掉原有的缓存，会立马过期掉这个ReadWriteCacheMap 

很多人会天真的认为，服务A新增了一台机器，其他服务最多30秒之内一定会感知到服务A新增了一台机器，你如果考虑到eureka server内部的多级缓存的机制，你就会发现说，其实在极端情况下，服务A新增了一台机器，可能会要1分钟的时间，60s的时间，才能让其他服务感知到 

你要从源码层面确定，每个服务拉取增量注册表的间隔，就是在初始化调度任务的代码那里看一下，默认的时间是多少，看看我们可以通过设置哪个参数来改变这个时间 

int registryFetchIntervalSeconds = clientConfig.getRegistryFetchIntervalSeconds(); 

默认情况下，就是30秒，spring-cloud-netflix-eureka-client工程里的EurekaClientConfigBean里面，定义了所有参数的默认值，如果你要设置，就是eureka.client作为前缀，然后加上变量的名字，就可以设置自己的数值 

eureka-core工程，ResponseCacheImpl，在spring-cloud-netflix-eureka-server工程里的EurekaServerConfigBean，里面对所有的参数都进行了定义，默认情况下也是30秒 

如果要修改，eureka.server.responseCacheUpdateIntervalMs = 30000 

**eureka服务发现是分钟级的时效性** 

这个事情是否是可以接受的，其实人家eureka如此设置这个参数，是有道理的，还是可以接受的，你如果给某个服务新增一台机器，其实是在扩容，你扩容的话，就不是说很着急很着急，必须1秒之内要让其他的机器感知到 

如果说你给某个服务新增了一台机器，其他的服务过了最长1分钟才能感知到，这个事情，我们认为是可以接受的，ok的 

**1.4** **服务心跳的间隔** 

任何一个服务启动之后都会定时发送心跳的通知，通知eureka server自己还是存活的状态 

心跳的间隔是多长时间 

在spring-cloud-netflix-eureka-client工程里，有一个EurekaInstanceConfigBean，这个里面就定义了默认的发送心跳的时间，就是30秒 

eureka.client.leaseRenewalIntervalInSeconds = 30 

**1.5** **服务故障的自动感知的时效性** 

比如说财务服务故障了，首先你要先考虑一点，就是说eureka server过了多久可以感知到财务服务故障了，然后将这个服务给他下线呢？ 

首先在eureka server中，是每隔60s去执行一次evict task，去判断一下当前所有的服务实例，是否有的服务实例出现了故障，一直没有发送心跳过来，是否要将故障的服务实例给他下线呢 

eureka.server.evictionIntervalTimerInMs = 60 * 1000 

按照这套evict task执行的机制，还有eureka本身的一个bug（90s * 2），很可能在极端情况下，从一个服务宕机之后，到evict task发现和判定这个服务已经故障，可能最多要差不多4分钟，最少也得2分钟~3分钟 

发现故障了以后，从服务注册表中摘除，然后过期掉readWriteCacheMap缓存 

加上两个缓存map的同步，以及其他服务30s一次增量拉取的成本，很可能在极端情况下，服务B是要过了将近5分钟才能感知到服务A的某台机器故障，宕机了。即使不在极端情况下，其他服务要感知到某个服务实例的故障，起码也要三分钟~四分钟 

**我们一般都是按照极端情况来计算的，服务实例故障，自动感知的时效性，服务A某台机器宕机了，服务B感知服务A那台机器宕机了，时效性在5分钟以内，起码三四分钟** 

服务实例上线（新增机器）的感知的时效性在1分钟以内 

引申出来的一个问题，如果说服务A某台机器宕机了，要3分钟以后服务B才能感知到，此时3分钟以内，服务A请求服务B的那台机器就会是失败的，此时怎么办呢？所以才要做超时设置（多少秒之内请求不成功就timeout），失败重试（服务A的一台机器挂了，你可以重试服务A的其他机器） 

这个就是下面结合feign那块的参数设计要考虑的 

我们为什么要先读源码，读了源码之后，深入的理解所有的细节，站在源码的深层次和角度来思考微服务架构的详细设计，考虑清楚里面每个细节点，以及在线上生产环境可能出现的问题，通盘思考，通盘设计 

eureka.instance.leaseExpirationDurationInSeconds = 90 

**1.6** **服务下线的感知的时效性** 

服务正常下线的话，会怎么样呢？这块的时效性，跟之前说的那个服务新增了一台机器，新增了一个服务实例，那么一般感知时效性在1分钟以内。**如果服务正常下线，执行****DiscoveryClient的shutown()方法，此时其他服务感知到这个服务实例下线，也是在1分钟以内。 

**1.7 eureka server的自我保护的稳定性** 

之前已经给大家分析的非常详细了，在eureka server源码的时候 

在eureka的自我保护机制的代码里，大量的运用了hard code硬编码，惨痛一点，他默认你的心跳的时间间隔是30s，一分钟就2次心跳，也就是说你压根儿就不能去修改心跳的间隔，否则就会跟eureka自我保护机制的hard code硬编码的代码出现冲突 

在eureka的自我保护代码里，充斥了大量的问题和bug，在尤其是测试环境下，你会发现经常就是动不动就进入自我保护的模式，说实话自我保护模式非常不稳定，完全不适合生产环境来使用 

否则如果在生产环境中，动不动进入自我保护的模式，你平时服务故障了，他都不会去进行服务的故障感知和实例摘除，那这个事情就坑了 

我之前做测试的时候，包括我们的一些同学做测试的时候，经常会发现每分钟期望的心跳次数是算错了，跟我们期望的是不符合的，计算每分钟的心跳次数，如果心跳次数<期望的心跳次数，就进入自我保护机制，不让evict task摘除任何服务实例 

不要用，这么垃圾的代码，写的这么不靠谱的机制，不要用，在生产环境，测试环境都直接关了 

eureka.server.enableSelfPreservation = false 

**1.8 eureka server集群的负载均衡** 

如果eureka server部署集群的时候，各个服务在注册或者是发送心跳，是如何请求eureka server集群的呢？你必须得通过源码来思考 

其实服务，第一次注册的时候，走的就是8761那台机器 

http://peer1:8761/eureka/apps/SERVICEA 

后面又开始发送心跳，我们来看看他使用的是eureka server的哪台机器呢？走的地址，还是8761那台机器。等个30秒，下次发送心跳的时候，再看一下，走的是哪台机器呢？每次发送心跳，其实走的都是8761那台机器 

http://peer1:8761/eureka/apps/SERVICEA/192.168.56.1:ServiceA:8080 

到目前为止，我们可以做出第一个推断，也就是说，你的服务访问的是eureka server哪台机器，其实是通过你的application.yml配置的，先配置了8761那台机器，其实所有的服务都是优先就是走那台机器的 

下面我们来做个试验，如果8761那台机器挂掉了，那么会怎么样呢？会重试几次尝试访问8761那台机器，但是如果还是不行的话，就会换成8762那台机器。而且后面主要都会请求8762那台机器了，每次发送心跳都会请求8762的机器。 

如果后面8761那台机器又活过来了呢？也没什么用，其他所有的服务都是在访问8762那台机器了。除非说8762那台机器死掉了，那么其他的服务才会再次尝试8761那台机器。模拟8762已经死了，然后直接会重试8761那台机器，因为8761已经活了，所以就ok。这之后再发送心跳，走的都是8761了。 

**结论：你的每个服务里，会配置一个eureka server列表，谁配置在第一个，所有的服务优先就是访问那个eureka server。然后如果那台eureka server宕机了，那么此时所有的服务在重试过后都会访问其他的eureka server，而且此后大家都会去访问那台eureka server。** 

他优先选择哪个eureka server，如果失败了如何重试，重试之后如何选择其他的eureka server的代码在哪里呢？我跟大家说，eureka代码写的很烂，所以这块逻辑他给隐藏起来了，我们就不带着大家去找了，但是起码在两台eureka server的情况下，你通过部分源码的调试，加上做了一些实验，搞清楚了eureka server集群的时候，各个服务是如何请求和访问他们的。。。。。 

**1.9 eureka server集群同步的时效性** 

观察一下，源码调试一下集群同步的代码，接收注册请求的是8761的机器，8761的机器是将8762的机器作为自己的peer nodes，所以说你配置了多少台server机器，其中接收到请求的那条机器，就会将请求转发给其他所有的机器 

将这个集群数据同步的任务，会放入一个acceptorQueue里面去，AcceptorRunner后台线程来处理，每隔10ms会执行一次业务逻辑 

中间有一个大包的过程，他默认会将500ms内的请求，注册、心跳、下线，打成一个batch，再一次性将这个batch发送给其他的机器，减少网络通信的次数，减少网络通信的开销，集群同步的批量处理的机制 

**eureka server集群同步的时效性，基本上是在1s内，几百毫秒都是正常的** 

后面我们会专门针对每个特性的疑问和考量，构造一些demo和例子，然后站在源码的层面，去观察一下整个eureka这块各个机制的关键的度量，从源码的层面彻底理解和悟透，要做到心里有数 

然后这些核心机制对应的关键参数，在生产环境中，如何设置，是否要设置，怎么设置 

整个我给大家演示的这个过程，是完全按照我们在生产环境中做这个线上的项目，引入一个新的技术，完整的一个全流程，读源码，理解的很透彻，站在对源码的一个理解和思考的基础之上，完成架构设计的各种细节 

**2、服务调用** 

ribbon + feign，我们主要是面向feign来做，ribbon作为feign底层依赖的这么一套机制来搞的，但是我们把源码全部都读透了，都读过 

**2.1 ribbon + eureka服务发现与故障感知的时效性** 

这块非常简单的，首先我们先考虑一下，eureka client感知到其他服务上线了一个新的服务实例，1分钟以内，几十秒；eureka client感知到其他服务有个服务实例宕机了，大概是5分钟以内，三四分钟

比如说购物车服务要访问库存服务，刚开始库存服务就一台机器 

后来某天，库存服务进行服务扩容，新增了1台机器，此时购物车服务本地的eureka client大概是1分钟才感知到人家新增了1台机器，ribbon的PollingServerListUpdater，刚好是30秒过后去刷新本地的eureka client的注册表到ribbon内部去 

ribbon感知到库存服务新增1台机器，可能又过了30秒了，1.5分钟，1分30秒，如果比较快呢，1分钟组左右 

目前库存服务有2台机器，其中1台机器宕机了，此时购物车服务本地的eureka client大概也需要三四分钟，最长是5分钟时间感知到，ribbon每隔30秒刷新eureka client的注册表到ribbon内部，极限情况下，ribbon感知到库存服务某台机器宕机了，可能需要5.5分钟，正常也需要个4分钟左右 

**2.2 ribbon的负载均衡算法** 

spring cloud环境下，ZoneAwareLoadBalancer，机房感知负载均衡器，比如说如果是多机房部署的话，比如在上海和北京，各部署了一部分机器在一个机房里，你一个系统，购物车服务有10台机器，库存服务有10台机器 

在北京机房里，购物车服务放了5台机器，库存服务放了5台机器；在上海机房里，购物车服务也是5台机器，库存服务也是5台机器 

然后ZoneAwareLoadBalancer，比如说北京机房里的购物车服务，现在要访问库存服务，是针对库存服务两个机房的10台机器去做负载均衡吗？优先是同机房访问，北京机房的机器，尽可能就是优先访问自己北京机房里的库存服务的5台机器 

那么就是在北京机房的库存服务的5台机器中进行负载均衡 

对于绝对多数的中小型公司来说，没有机房的概念，抛弃掉机房的事儿不考虑的话，那么他的负载均衡的算法，就是最最基础的round robin轮询算法，每次请求下一台机器，对一个机器列表，循环往复的请求 

**2.3 feign + ribbon的服务调用的超时与重试** 

我们细细的分析，spring cloud，从源码来出发，这种先研究源码，再设计架构的感觉，真的很爽的，因为你把握住了各种底层的东西，对很多深入的东西你都会考虑的很清楚，这种才叫做架构设计 

比如库存服务有2台机器，其中一台机器故障宕机了，如果要让依赖库存服务的购物车服务感知到，可能需要几分钟的时间，三四分钟，五六分钟，都有可能 

比如说在这几分钟的时间里面，ribbon内部的保存的库存服务的server list，还是2台机器，此时不断的请求过来，ribbon负载均衡算法还是会不断的将请求流量分发给库存服务已经宕机的那台机器 

如果请求已经宕机的一台机器，一定是会有问题的，连接一定会连接不上去的，connect timeout异常和报错，如果说你不做任何处理的话，可能会在短时间内导致大量的请求都会失败 

超时和重试的参数 

ribbon:

 ConnectTimeout: 1000

 ReadTimeout: 1000

 OkToRetryOnAllOperations: true

 MaxAutoRetries: 1

 MaxAutoRetriesNextServer: 3 

ConnectTimeout：连接一台机器的超时时间

ReadTimeout：向一台机器发起请求的时间 

重点是通过实验来设置好MaxAutoRetries和MaxAutoRetriesNextServer这两个参数 

来决定一下我们应该如何进行重试 

我们知道下一次请求一定是8088那台机器的，所以此时我们手动将8088器给停掉，模拟宕机了 

要请求8088，肯定是不行的，请求失败 

第一次重试，还是8088

第二次重试，还是8088

第三次重试，还是8088

第四次重试，才是8080 

ribbon:

 ConnectTimeout: 1000

 ReadTimeout: 1000

 OkToRetryOnAllOperations: true

 MaxAutoRetries: 1

 MaxAutoRetriesNextServer: 1 

模拟8088故障 

请求，8088，失败

第一次重试，还是8088

第二次重试，就是8080 

**3、服务网关** 

剧透一下，我们之前给大家写的一个大纲，其实是在高并发亿级流量的那个环节，我们其实会构造真正的高并发 + 各种统一处理的机制的复杂的网关系统 

在spring cloud全家桶里面，初期，架构演进来的，千万不要想着，上来就把这个架构做的很复杂，很完美 

在我们好多年前，刚开始实践，大数据，微服务，当时都没有网关的这么一个概念 

刚开始，我们就是觉得很简单，单块的大系统太恶心了，5个人共同维护一个单块系统，还凑合，很勉强了。10个人维护一个单块系统，其实就很难受了，每个人都是开发里面不同的一块东西，有很多个需求并行的在做，一个单块，在git代码仓库，拉了很多版本同时在开发，测试、部署、调试、上线，各种冲突。 

代码冲突，恶心，超过5个人共同维护一个大系统之后，各种代码冲突就很恶心了 

RPC服务框架，拆，这个大系统就开始拆分，每个人维护一个服务，一般来说不会让两个人维护一个服务，每个人维护一个或者多个服务。 

前后端分离，很早很早以前开始我们就是前后端分离了，我们就是对外提供一些http接口，我们后端系统都是采用的rpc调用，当时就是先用RPC服务框架进行系统拆分，都没有网关的概念，前端就是得自己记住有哪些那些那些服务 

自己研发的rcp服务框架，前端也是支持的，前端可以通过rpc服务框架，通过rpc调用的模式，伪rpc，底层走的还是http请求，来调用后端各个服务的接口。前端还是要care很多个后端服务的。 

PC端（前端），html5端，android/ios，等等 

后端服务太多了，越来越多，一开始可能就10个服务，到了后面，服务越来越多，20个，30个，50个，100个服务 

我们一开始都是自己提供了入口服务，接口服务，API服务，=> 网关系统 

所有的前端、html5、android、ios，全部是访问一个API服务，API服务就是做各种后端服务的一个接口的调用、转发、接口合并、数据处理、打印日志 

API服务（网关的前身），就是用他来做统一的服务接口调用的入口就可以了，统一限流、统一降级、流量分发、灰度发布、统一异常、统一日志 

网关，最原始的一个作用，实际上就是对前端、android、ios屏蔽掉后台系统100+个服务，你只能对外暴露一些有业务含义的接口，人家就记住接口，就访问你一个网关服务就可以了。。。 

zuul，定位，也可以这样子，就是做最最基础的请求的转发，降低和减轻前端的一个复杂度，前端只要知道接口就可以了，不要care这个服务，直接请求网关就ok了 

**3.1 ribbon预加载** 

第一次请求zuul的时候是很慢的，很容易超时，演示过很多次了 

zuul:

 ribbon:

eager-load:

 enabled: true 

**3.2 zuul + ribbon + eureka感知服务上线和故障的时效性** 

zuul要将请求转发给购物车服务，购物车服务一开始就部署了1台机器，那么zuul就直接转发给那台机器就可以了。但是后面购物车服务又新增了1台机器，zuul要感知到购物车服务新增了1台机器，要过多久呢？ 

zuul 和 feign在这块是类似的，ribbon + eureka，eureka client感知到服务新实例上线，大概可能要个1分钟以内，ribbon那里，1.5分钟，1分钟左右，zuul也是一样的 

如果说购物车服务现在有2台机器，但是不幸的是有1台机器宕机了，此时怎么办呢？eureka client大概可能需要5分钟以内的时间才能感知到，ribbon，5.5分钟，三四分钟，四五分钟。zuul的ribbon，感知到服务实例故障了，四五分钟 

在某个服务实例宕机了，zuul还是不停的将请求转发过去会怎么样呢？肯定会失败，请求不了，此时zuul默认就是整合hystrix，hystrix会感知请求失败，异常了，直接会走hystrix的降级的逻辑，是什么都没有的，所以异常直接就会抛出来 

zuul会有一个统一的error filter，会将这个异常给打印出来，反馈到调用方那里去 

在默认情况下，某个服务实例突然宕机，zuul的整个行为，下一讲我们来做个试验在演示一下，我们就知道，我们现在担心的就是这种情况，某个服务实例宕机了，可能会有问题，配置一下ribbon的超时和重试 

**3.3** **请求超时和重试** 

为什么配置了zuul的预加载ribbon之后，第一次请求还是会超时呢？各个服务自己本身还是在第一次请求的时候会去初始化ribbon，会造成请求超时。 

基于ribbon + hystrix来做的，这里的超时的时间应该如何来设置呢？

请求失败的话，ribbon + hystrix配合的时候，这个请求重试的次数是如何处理的呢？ 

如果说某个服务实例突然挂了，zuul默认情况下是无法处理这种情况的，所以在线上生产环境下是很要命的，绝对不能让zuul直接这样子裸奔，不设置任何重试的机制直接就上了。如果某个服务实例挂了，打印error日志，给前端返回一段异常的json串： 

{

  "timestamp": 1532067539032,

  "status": 500,

  "error": "Internal Server Error",

  "exception": "com.netflix.zuul.exception.ZuulException",

  "message": "GENERAL"

} 

默认情况下，hystrix实际上虽然是用了，但是没有这个降级的策略，默认情况下，无论是被hystrix线程池拒绝（被限流）、请求超时（出现过几次请求超时的现象）、发生了异常（某个服务实例宕机了） 

抛出异常，hystrix降级没有办法做任何处理，异常只能被打印出来，封装成一个json串法功给前端浏览器来显示，或者是将json串发送给调用zuul网关的android、ios、PC前端，抛出来的这个异常，统一都是ZuulException，其实具体是什么异常，他是不告诉你的，他只告诉你内部出现了异常和错误 

不是特别好，起码针对服务实例可能会挂、或者是可能会偶尔出现网络调用超时（失败），设置个最基本的重试的策略 

人zuul也是用的hystrix + ribbon那套东西，所以说，超时这里要考虑hystrix和ribbon的，而且hystrix的超时要考虑ribbon的重试次数和单次超时时间 

ribbon的超时，hystrix的超时，zuul默认启用了hystrix，你要考虑到hystrix的超时 

hystrix是包裹了ribbno的使用的，一般来说，hystrix的超时时长必须大于ribbon的超时时长，否则如果hystrix设置了超时是1s，ribbon设置的超时时长是2s，那么ribbon其实还没超时，hystrix直接就超时了 

必须是hystrix超时时长最好是远大于ribbon超时时长，超时和重试尽量都是以ribbon为主 

hystrix的超时时间计算公式如下： 

(ribbon.ConnectTimeout + ribbon.ReadTimeout) * (ribbon.MaxAutoRetries + 1) * (ribbon.MaxAutoRetriesNextServer + 1) 

feign和zuul基于ribbon进行重试的机制给搞混了，feign和zuul重试稍微有些是不太一样的 

​          <dependency>

​         <groupId>org.springframework.retry</groupId>

​            <artifactId>spring-retry</artifactId>

​          </dependency> 

zuul:

 retryable: true

 

ribbon:

 ReadTimeout:1000

 ConnectTimeout:1000

 MaxAutoRetries:1

 MaxAutoRetriesNextServer:1

 

hystrix:

 command:

  default:

   execution:

​    isolation:

​     thread:

​      timeoutInMilliseconds: 10000 

如果不配置ribbon的超时时间，默认的hystrix超时时间是4000ms（4s） 

(1 + 1) * (1 + 1) * (1 + 1) = 8s  

直接如果你断点调试的话，直接就给你判定为hystrix层面就超时了，hystrix timeout异常，no fallback 

正常情况下是用的：RibbonLoadBalancingHttpClient

但是如果将zuul.retrayable设置为true之后：RetryableRibbonLoadBalancingHttpClient 

如果是之前，没有设置zuul的重试，retryableClient是false，但是现在设置了重试之后，这个变量就是true了 

请求，9099机器，失败

第一次重试，9099机器，失败

第二次重试，9090机器，成功 

zuul的重试机制就成功了 

否则会很坑的，因为spring cloud eureka这一块来做服务发现，服务实例故障的感知达到了几分钟的级别，这是比较差的，必须配合上超时 + 重试的机制。如果某个服务实例挂了，feign / zuul在1s之内连接不上去（超时）直接timeout，然后对故障机器重试一次，不行，就重试其他的机器 

就可以保证，如果比如说你在运维某个服务，购物车服务其中的某台机器，有点问题，或者是在做一些操作，停机了，在几分钟之内，zuul / feign是感知不到的，如果你不做超时 + 重试的机制，在几分钟之内，大量的请求是失败的 

但是如果你配置了超时 + 重试的参数，可以保证在上述情况下，在几分钟内，请求还是可以成功的，因为他会自动进行重试 

**4、开发框架** 

Spring Web MVC + Spring + Mybatis + Spring Boot 

**5、数据库** 

MySQL，一台机器，部署一个mysql服务，在里面创建15个逻辑库，15个中心，每个中心对应一个逻辑库 

在微服务的架构中，有一点是非常的重要的，服务与服务之间一般是不会共享数据库的，每个服务一般是使用自己独立的一个数据库，然后如果某个服务要访问另外一个服务的数据的话，不能直接查其他服务的数据库的，而是通过其他服务暴露出来的接口来访问 

这个是我们很多年做微服务架构血与火的经验，微服务的军规 

不同的服务，其实是不同的人来负责的，一个人负责维护一个或者两个，或者最多不要超过5个的服务，就足够了，如果是不同的人维护不同的服务的话，其实大家如何访问数据库，都是不知道的 

如果服务A和服务B共用一个数据库，共用一样的表，小A负责服务A，小B负责服务B，然后小A给数据库里某张表加了几个字段，或者修改了字段的类型，结果咔嚓一下子导致小B的服务的代码就报错了 

包括在数据库里洗数据，然后一些性能很差的SQL，服务与服务之间如果在数据存储层耦合的话，会出现大量的意向不大的问题 

所以哪怕就一台数据库服务器，每个服务都拥有自己独立的一个逻辑库 

**6、工程拆分** 

我们一共会将电商系统拆分为15个中心，15个工程，每个服务一个工程，每个服务的工程对应一个独立的git仓库，每个服务都是完全独立的，每个服务都是在eclipse里是单独的一个工程，有自己独立的代码仓库，有自己独立的集成测试机器、QA测试机器、staging测试机器、prod生产机器，独立的部署，独立的数据库（某台数据库服务器上的一个逻辑库，比如mysql中的一个database） 

每个负责服务的人，都可以在自己本地的eclipse中独立的写代码，独立的写单元测试，独立的部署在各个机器上来测试，独立的部署和发布，数据库从逻辑上起码是自己独立的

### 170_梳理一下基于将电商系统拆分为微服务架构的实施步骤 

**1、从工程层面，我们给他拆开来** 

每个服务 -> 工程，每个工程是自己完全独立的，跟其他的服务的工程都没什么关系的，在微服务架构里，我们比较强调的一点是整体自治，每个服务都可以自己管理自己，跟其他的服务是没有关系的 

**2、拆完工程以后，我们会去基于spring cloud调整里面的代码** 

各个服务之间都涉及到大量的接口的调用，我们要将每个服务转换成一个spring cloud的服务，然后将每个服务要对外暴露的接口，做成controller，这个controller里面的接口是定义在一个单独的服务API工程中的（上传到Nexus私服里面去），然后其他服务来依赖他的API接口工程，来进行接口调用 

完成这一步了，各个工程才不会报错了 

**3、给每个服务都创建一个独立的database** 

逻辑库，在mysql实例里来创建，让每个服务都是在连接自己的数据库，如果有跨服务的数据的访问，不能够各个服务交叉访问对方的数据库，而是要通过其他服务的接口来获取数据 

**4、调整各个单元测试类** 

微服务架构之后，在单元测试中，你对各个依赖的其他的服务接口必须得进行mock，让其他依赖的服务接口都是返回相对应的mock数据的，保证各个工程的额单元测试类都可以正常跑通 

**5、验证一下各个工程都可以正常的启动，启动不能报错** 

**6、分布式系统的集成测试** 

我们需要在本地将15个服务全部启动，在本地是使用的不同的端口号，完成一个基本的集成测试，让一个分布式的电商系统，跑起来，我们跑一些主要的测试用例，看一下，整个系统在分布式的环境下，能否跑通， 能否互相之间正常调用 

**7、独立的代码仓库** 

做完这些事情之后，就ok了，工程才算是拆分完成了，然后就给每个服务创建一个git代码仓库，将每个服务的工程上传到对应的git代码仓库中去就ok了 

**8、结果：完全独立的每个服务** 

每个服务 -> 工程 -> 数据库 -> 代码仓库，完全独立自治 

可以自己独立的开发、测试、数据、代码仓库、部署，跟其他人是没有任何关系的，只要你保持接口不变即可（如果说你的接口变化了，那么需要你通知其他人说你的接口变化了）

### 171_动手拆分15个服务以及1个基础类库组成的分布式电商系统 

在我们实际做项目的时候，特别是做这种微服务的系统架构，里面有大量的服务，这些服务他是随时可能变化，很松散的，所以一般我们不会说给某个大型系统搞一个父工程，你会用spring boot，你会用spring cloud 

spring boot，是作为一个parent父工程引入进来，约束了很多依赖的版本

spring cloud，是作为dependency management引入进来的，约束了很多依赖的版本 

所以说大家只要保证spring boot和spring cloud版本基本一致就ok了 

统一约定一下版本号 

之前那个雏形版本，可以认为是电商的0.1版本 

微服务架构这块，我们统一给做1.x版本，我们现在不是在做微服务架构改造的一期，后面你每次对这个架构做一次改变，我们就会升级一下版本号 

1.0.0 

我们在这里稍微做一点简化，我们就只保留了一个dev环境的profile，就可以了 

这边的话呢，购物车服务依赖了别的服务的接口还有domain类，后面我们会将每个服务对外暴露的接口，还有domain类都封装在每个服务的API工程里面，所以后面我们完成了代码层面的改造，就不会报错了 

在单块系统的开发环境下，写代码，跟微服务分布式系统环境下，写代码，是完全不一样的，包括核心java代码以及单元测试代码，不太一样的，后面第二步，就是在分布式的环境下，如何去改造对应的一些代码 

顺便再给大家说一下，我们一般在做微服务架构的程序的时候，如果图省事儿的话，一般其实就是新建一个服务，就是将之前的老服务的一堆配置，就跟现在其实一模一样，直接拷贝过来。 

如果你做的稍微高端一点点，maven课程里有一个骨架，其实你会微服务的工程，就是完全标准化的，可以做一个骨架出来，每个人要新建一个服务工程，就用骨架来创建就可以了，之前给大家都在Maven课程里演示过了 

在自己公司里可以搞一个骨架，我们这边的话，这次改造就不搞，阶段四，10倍复杂的业务逻辑，我们可以搞一个骨架，快速的创建各种新的工程 

### 172_基于spring cloud改造服务的代码以及API接口的定义（一） 

需要花个几讲的时间，我们得先保证各个服务的代码不要报错 

权限服务：不用搞了，本身就没有报错 

我们以之前的spring cloud他的demo，ServcieB来做一个参照 

spring boot可以升级一下版本，因为我们跟spring cloud来搭配的话，统一是用的spring boot的1.5.13版本 

我们可以先试一下，对eshop-auth服务的改造是否成功，能否正常启动，不要报错，可以作为一个服务注册到spring cloud上面去 

两点问题： 

1、applicaiton.yml文件中，我们一开始将spring的配置做了两份 

2、启动类必须放在com.zhss.eshop包下面，spring boot默认是从这个包开始作为一个根包去扫描对应的spring bean的。com.zhss.eshop.auth包下，那么我们依赖的eshop-common工程里，有那个com.zhss.eshop.common.util包下有那个DateProvider那个spring bean是无法扫描到的 

库存服务，本身就是自己的这个服务对外暴露的一个接口，结果你的这个接口还依赖了别人的服务里的domain对象，这个是不合理的 

如果是之前单块系统的时候，倒是无所谓，就是一个系统 

在分布式系统里，在微服务架构里，这个东西是不能这么玩儿的，库存服务，提供一个接口 

informSubmitOrderEvent接口 

输入： 

{

 xxx

 xxx

} 

输出： 

{

 xxx

 xxx

} 

spring cloud这套东西来玩儿的话，那么大家看到的都是一些json串 

那么所以这个时候，比如说你的库存服务的某个接口，要接受的就是一个订单的信息，你需要根据自己的要求，定义好你需要订单的哪些字段，然后在自己的库存服务的API工程里，定义好属于你自己的库存服务的一个订单信息的domain类 

每个一个服务，一定是高内聚，低耦合的，这个服务自己本身全部围绕自己来打造所有的东西，除非你是要调用其他服务，那么你可以去依赖其他服务的东西，否则如果是具体的实现西细节的东西，全部由你这个服务来提供 

对于库存服务内部而言，我需要一个domain类来接收这个采购入库单的json对象，json对象的数据需要灌入一个domain类的对象中，那么这个domain类的对象，本来就是应该是负责库存服务的这个同学自己来定义的

### 173_基于spring cloud改造服务的代码以及API接口的定义（二） 

我们这块封装domain，我们暂时先这么改着，如果后面改完了感觉有什么问题可以再次调整 

### 174_基于spring cloud改造服务的代码以及API接口的定义（三）

### 175_基于spring cloud改造服务的代码以及API接口的定义（四） 

各位同学，肯定会思考一个问题，我们能不能比如说OrderInfoDTO为什么要定义在各个服务里面呢？而不是说其他所有的服务都使用订单服务统一暴露出来的一个OrderInfoDTO呢？ 

调度服务API，如果依赖了订单服务API，使用的是订单服务的OrderInfoDTO 

调度服务和订单服务是不同的人在负责的，所以只要是不同的人在负责，那么这里一定会有沟通不顺畅的问题 

假如说，某一天，订单服务的负责人，他将OrderInfoDTO的定义给修改了，但是调度服务的同学是不太清楚的，此时会怎么样呢？很可能会导致调度服务会出问题，这个就是所谓的耦合导致的问题 

可能会导致调度服务中的一些代码都会出错 

调度服务是使用的订单服务的OrderInfoDTO，本来里面有一个x字段，用的好好的，代码都是在这样子跑的，结果某天订单服务API将OrderInfoDTO给修改成了删除了x字段，x字段没了，或者是修改x字段的名字为x2字段 

调度服务很被动了，他必须无时无刻的去care各种服务的domain类的字段的变化，如果有变化他自己还得重新修改 

调度服务依赖了20个服务API的domain类，20个服务API的domain类可能有几百个，只要一旦修改了，那么你调度服务必须得配合着改，这个简直是旷世灾难 

所以说我们在实际项目中使用的一个比较常见的做法，也不高明，稍微有点笨，贯彻一点，每个服务高度自治，高内聚，低耦合，除非是你必须要调用其他服务的接口，否则其他只要事关你的内部实现的东西，全部自己来定义 

就比如说，你的调度服务有个接口，要接受一个订单的信息，这个订单的信息对应的json对象你可以定义出来，告诉别人说，我就是接收这么一个订单信息的，然后这个订单信息的domain类，可以你自己来定义 

你自己定义好了这个订单信息的domain类之后，可能这个名字不是OrderInfoDTO，这个名字可能就是MyOrder，里面定义了一些订单需要的字段 

如果说这样来做的话，麻烦在于哪里呢？ 

比如有个服务来调用你，来调用你这个调度服务，人家先查询订单的信息，比如说从订单服务里查到，OrderInfoDTO，然后他必须将这个订单服务的OrderInfoDTO转换成你的调度服务需要的一个MyOrder的一个类型 

这里面的类名不一样，类里面的字段名也不一样，字段的数量也不一行 

麻烦的人不是你，是你的调用者了 

这两种方案呢，都有麻烦的地方，要不然就是所有服务都用一个domain类，OrderInfoDTO，要不就是各个服务对于订单相关的信息都有自己的定义，MyOrder，Order，OrderInfo，OrderInfoDTO 

如果你的这个系统不是特别的复杂，相对来说比较简单，就是你也可以采用说，所有的服务都用一个domain类，比如说就是OrderInfoDTO，而且团队人数比较少，在10个人左右，大家人少好交流沟通。 

但是如果是那种特别复杂的庞大的系统，几百个人做，很多人互相之间都不认识的，所有服务都共用一个domain类，负责任的告诉大家，domain类这个东西，是经常改变的，改变的频率是很高的，他是跟着业务走的，业务系统不断的在演进，不断的删除字段，增加字段，修改字段，修改字段的类型 

如果比如说20个服务都依赖了一个OrderInfoDTO，后面这个domain类不断的修改，100个服务都得跟着不断的修改自己的代码，耦合性太强了，各个服务都要紧张兮兮的去care说我依赖了别人的domain类，我整天担惊受怕，说那哥儿们会不会修改他的domain类的定义，如果他修改了，我还要去care和做对应的代码修改 

庞大的微服务架构和系统，最好是做到每个服务高度自治，每个服务尽量减少跟其他服务之间的依赖，哪怕是跟其他服务之间有大量的接口调用的依赖，很麻烦。人家的接口只要变动，你必须得去配合着care和修改 

如果是一个服务，将自己需要的所有的domain类都定义在自己本地，由自己来维护，虽然可能说会出现，一个订单，可能在100个服务中，有100种定义，但是对于服务维护本身而言，是方便的 

但是不方便在哪儿呢？人家来调用你的服务的时候，就会很麻烦了，他可能会涉及到在很多种不同的订单类之间进行转换，他要写很多的类与类之间转化的代码，将服务A返回的OrderInfoDTO的类型，转换为服务B的接口需要的MyOrder的一个类型 

起码这块不方便仅仅是停留在调用方了， 对于维护服务而言，方便多了 

我们在线上的系统里都有使用，这两种方法我们都有使用，历史原因，当时也没在这块定义太多的规范，但是后面我们尽量是建议每个服务尽量是自治，减少跟其他服务之间的耦合。在实际生产中的时候，比如说使用频率特别高的domain类，订单类，你可以找专人来维护一份就可以了 

但是如果是很多次要的一些类，退货工单，你可以让各个服务自己定义就好了 

我们之前写代码的时候，图省事儿，大量的服务与服务之间的调用，都是用的那种很复杂的domain类，传递大量的无效的字段。实际上在开发的时候，如果是分布式的系统，考虑网络间通信的效率问题，你可以尽可能的减少互相之间传递的参数的数量，可以提高网络通信的效率。 

重写一下接口的定义，使用map来传递json对象，各种参数尽可能扁平化的封装在一个map中，也可以的，或者是针对这个接口，专门定义一个特殊的domain类来接收 

取舍和权衡，如果大量的服务共用一个domain类，会导致这个domain类频繁变化的时候，会导致各个服务的维护同学可能会耗费很多时间去评估这个domain类变化对自己的影响，其次是如果变化了可能会导致一不小心就是代码出错，导致线上事故 

比如你维护的是一个调度服务，如果说增加了大量的你依赖的别人的domain类可能会对你造成的影响，就可能随时导致你的服务出现问题，系统会出现不稳定的因素 

但是如果是将这个成本放服务的调用方，别人在写代码的时候将OrderInfoDTO类型写代码转换为MyOrder类型的对象，这个东西如果有工具方法协助的话，成本还好，会多一些成本，但是成本不会太高 

这个只是耗费时间而已，不会对线上的系统造成不稳定的影响

### 176_基于spring cloud改造服务的代码以及API接口的定义（五）

### 177_基于spring cloud改造服务的代码以及API接口的定义（六）

### 178_基于spring cloud改造服务的代码以及API接口的定义（七）

### 179_基于spring cloud改造服务的代码以及API接口的定义（八） 

订单服务这里，暴露了一个接口，这个接口是接收一些coupon优惠券的一些信息，所以可以想一想，如果是一个专门负责订单服务的哥儿们在这里开发和处理，他一般来说会怎么做呢？ 

他会让订单服务去依赖一些别的促销服务，使用促销服务的domain类吗？ 

其实也不应该用别人的domain类，应该是你自己在订单服务里就专门定义一些自己的domiain类 

### 180_基于spring cloud改造服务的代码以及API接口的定义（九）

### 181_基于spring cloud改造服务的代码以及API接口的定义（十）

### 182_基于spring cloud改造服务的代码以及API接口的定义（十一）

### 183_基于spring cloud改造服务的代码以及API接口的定义（十二） 

在之前的开发过程中，实际上我们已经给大家都演示过了，如果你的服务要自己维护的比较好的话，都把控在自己手上，那么最好的一点就是自己对接口关联的domain类都来自己维护 

但是我给大家演示了一下，这么玩儿会导致，别人调用你的服务的时候，会出现很多bean类型之间的转换，很麻烦 

在实际的生产系统中，这两种都会出现，你不要以为人家给你提供的接口一定都是复用的统一的domain类，很可能就是别人自己定义的，这个时候你就需要自己进行domain类型之间的转换 

但是也很有可能人家复用的就是别人的服务的domain类 

比如说你的是调度服务，提供了接口A，使用了X类，X类是库存服务里定义的 

现在另外一个人是购物车服务，要来调用你的调度服务的接口A，他会引用你的调度服务的依赖包，发现说，没法完成对你的接口的一个调用，因为调用接口A要传入的X类，是在库存服务里定义的 

购物车服务根本不需要依赖库存服务，但是实际上在maven系统中，会一并将库存服务引用过来，你在调用A接口的时候，是使用库存服务的domain类，构造了一个这个domain类的对象，然后去调用调度服务的A接口 

这里就会出现大量的服务间耦合的情况 

一旦团队大了，人多了，很难去把控这个事情的，，所以上述两种情况都可能会出现 

在我们这里开发的时候，如果使用我们之前用的那种模式，也就是说每个服务对所有的domain类都自己类定义，这个就会导致开发业务逻辑调用服务接口的时候是很麻烦的，要做各种类型的转换 

为课程方便，干脆就是说，同一个domain类仅仅定义在一个服务中，其他的服务全部都使用这个domain类，不需要进行大量的domain类转换了 

如果说所有人都依赖于同一个工程的同一个domain类，互相之间的依赖关系会极其混乱，出现循环依赖的这种情况  

### 184_为微服务创建独立数据库以及保证单元测试运行通过（一） 

我们在运行单元测试的时候，其实是会去初始化spring相关的一些容器，eureka client就会去初始化，就会去向eureka server进行注册，但是如果说你这里eureka server没有运行起来的话，就没法注册的，就会报错 

严格意义上来说，不用去管他，只要单元测试都通过就ok了 

在开发环境可以连接的机器上，部署dev环境的eureka server，所有人跑单元测试的时候，eureka server都是可以接受到服务注册的请求，就不会报错了 

会专门有一批dev环境的机器，本地笔记本电脑都是可以连通的，只要在dev环境的机器上部署eureka server集群就ok了。我们课程里就模拟成是在本地笔记本电脑上启动dev环境的eureka server集群。 

如果说某一个服务要获取别的服务对应的数据，绝对是不能说一个服务直接查别的服务的数据库，必须是通过别的服务提供的接口来获取数据 

权限服务，库开放给了购物车服务，这是两个团队在负责 

购物车服务哪天脑子发抽写了一个bug，直接qps 2万请求权限服务的数据库，直接权限服务的库宕机 

但是如果购物车服务是通过权限服务提供的接口来请求的，那么权限服务的接口就是可以基于hystrix来进行限流和降级的，qps太高了，直接就给限流掉了，熔断掉，提供降级的请求。。。。。。  

### 185_为微服务创建独立数据库以及保证单元测试运行通过（二） 

很多同学都一直在问我，能不能在课程上给大家讲解一下解决一些报错和异常的过程还有经验，没问题 

因为单元测试运行很快，立马就结束了，所以导致LoadBalancer很快就shutdown了，导致了一个illegal state exception，ribbonLoadBalancer实例化失败，ribbonLoadBalancingClient实例化失败 

进而最终导致AutoPublishCommentTask立马报错，这个里面用了OrderService（FeignClient，基于ribbon来实现的） 

AutoPublishCommentTask会定时调度，每隔10分钟运行一次，调用OrderService（FeignClient）的接口，此时就会初始化一堆ribbon相关的bean，但是初始化ribbon bean的时候，单元测试已经运行结束了，基本上就已经关闭了，所以导致了一堆state illegal exception 

在单元测试类中，将AutoPublishCommentTask这个bean设置为@MockBean，里面依赖的OrderService就全部都失效了

### 186_为微服务创建独立数据库以及保证单元测试运行通过（三）

### 187_为微服务创建独立数据库以及保证单元测试运行通过（四）

### 188_为微服务创建独立数据库以及保证单元测试运行通过（五）

### 189_为微服务创建独立数据库以及保证单元测试运行通过（六）

### 190_为微服务创建独立数据库以及保证单元测试运行通过（七） 

库存服务和调度服务之间的交互，当时走了纯内存的异步化，这块是有问题的，因为一旦拆分成了分布式的系统之后，这种服务之间的异步的请求，必须得走MQ，所以我们在这里先操作一下，保证代码不要报错 

然后接着我们在做集成测试的时候，我们就给改造成基于RabbitMQ来作为MQ中间件，做成，库存服务将库存更新的消息发送到RabbitMQ中去，调度服务来消费RabbitMQ，包括这里面的离线存储，等等这些技术，全都做一下重构

### 191_从单块系统到分布式系统演进对MQ消息中间件的引入

单块系统中的异步通知架构

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\19101.png) 

分布式环境下的MQ的使用

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\19102.png) 

库存服务，这块，其实是将库存更新的消息，异步发送到一个内存队列中的，我们之前是用的内存队列，但是现在的话呢，我们没法用这个内存队列了，因为我们的库存服务和调度服务给隔离开来了，独立部署，可能都不在一台机器上 

库存服务发送库存更新的消息，到MQ（消息中间件），独立部署的中间件，调度服务就去消费这个MQ中的消息，拿到了额消息之后，调度服务再来更新自己本地的库存 

我们的这个技术，都不是说干讲demo，确实我们的系统架构演进的过程中，一步一步的确实需要用到那个东西了，我们才会引入到系统架构中来 

Message Queue，消息队列 

从单块系统到分布式系统演进对MQ消息中间件的引入

### 192_关于消息中间件的基础知识以及常见解决方案的预习说明 

我们之前出过免费的公开课，是讲MQ的一些技术选型，选择用哪种MQ，还有MQ中面试常问的一些技术问题以及解决方案 

《互联网Java工程师面试突击训练：第1季》 

建议大家在学习后面的课程之前，把这个课程里的MQ相关的知识都看一下，那个里面的东西讲解的多很大白话，都是画图的，都比较简单，建议大家去看一下，作为我们后面课程的一个预习 

积累一些前置的基础知识 

最最基础的一些知识： 

1、MQ的技术选型：你要对常见的几种MQ技术都有一点点的了解，而且知道在什么情况下选用哪种MQ

2、消息队列，部署为一个集群，这个东西呢，我们会在后面给大家来讲解，这块东西在项目阶段二中不会深入去讲解，但是在面试突击课程中，剖析了一下常见的MQ技术的高可用的原理

3、消息队列，消费到重复数据怎么办

4、消息队列，发送过去的数据丢失了怎么办

5、消息队列中的数据如何按照顺序来发送与消费

6、大量的消息积压在消息队列中该怎么办呢

7、消息队列的核心原理（你如何来设计一个消息中间件）

### 193_教会大家如何自己来学习开源技术以及码农的英文学习建议 

我想借着这个机会，以后的话呢，在讲课的时候，我想除了教给大家一些鱼以外，还要教给大家一些渔。我除了直接将一些知识倾倒给大家以外，我想带着大家一起来比如看看一些开源技术的官网，带着大家来查阅一些资料，带着大家来学习如何靠着自己不看任何书籍和视频，快速入门一个技术 

这块是渔而不是鱼，学习一下自己快速入门技术、学习技术、快速上手将一个技术用起来，学会这块的东西，软技能 

很多同学，rabbitmq，博客，找一些视频课程，淘宝上买一些盗版课程 

博客：首先博客里的版本不一定是最新的，可能会对你造成一些问题，你看的博客教你的是旧版本，结果你在官网里下载的是新版本 

视频课程：关于一个技术的使用这块，你看视频课程，视频课程，教你如何使用一个技术的话，技术的版本一直在更新，视频课程使用的这个版本可能都是老版本，然后你现在需要使用的是新版本。很多讲师水平很烂，课都讲不清楚，发音都不标准，普通话都说不连贯，误人子弟，浪费时间浪费金钱 

架构班视频课程的意义（好的视频课程）： 

1、将一些技术的入门、原理、使用浓缩和精华一下，用最简洁的语言教给大家，避免大家还要自己去看英文的官网，效率比较低，提高大家的效率 

2、教给大家底层的一些东西，我以我的技术功底和技术经验，带着大家来学习一些底层的源码，没人带，大部分工程师自己都学不会 

3、经验，实战，必须在真实的复杂的大型的项中，结合复杂的业务场景来使用各种技术，思考可能会遇到哪些技术问题，我会将我在实际工作中遇到的一些技术问题，揉到我们的项目中来，做真正的大型项目的实战 

最最好的，快速入门一个技术的方法以及靠着自己系统学习一门技术的方法，无他，如果说以后大家在2年过后全部结业，这个架构班的课程全部学习结束了，我觉得每个人最理想的一个技术实力 

首先自己通读一遍官方文档，照着官方文档写几个demo出来，上手开始使用了，然后将官方文档中的其他部分，包括一些高级的特性什么的，都系统通读一遍，自己可以做一些笔记总结一下 

然后直接结合几个官网的hello world小demo，来调试和阅读这个技术的核心源码，第一遍阅读，谨记，抓大放小，连蒙带猜 

立马用到项目中去实战，学完技术，立马用到项目里去，效果是最好的 

积累很多的感悟，在项目中用一个技术，遇到的问题，报错，故障，才是你最有价值的经验，如果遇到自己要做笔记，将各种报错，故障，总结出来 

可以再将源码读一遍，你肯定会有更深层次的一些感悟
 国内的码农，80%的码农是根本没法自己去阅读官方文档的，因为英文基础太差了；19.9%的码农是可以初步慢慢的，吭哧吭哧的，带着查字典的，勉强的自己看看官方文档；0.1%码农，是可以看英文文档跟看中文一样的水平 

如果你真的以后想要在2年后课程结业之后，大家完全依靠自己就可以去学习各种技术，英文水平对你很重要，读，读英文的技术文档 

提升这个自己阅读英文文档的能力，笨笨的方法，每天坚持读一点点英文文档，刚开始读很痛苦，大量的查阅字典，看到单词不会，自己可以搞一个单词本，不会的单词记录下来，不每天都不断的将之前积累的生词本给看一遍，直到如果一个单词你觉得你完全记住了，就从生词本里删除 

每天都读一点官方文档，坚持，坚持个1年能入门，坚持个3年，能小有所成，坚持个5年的话，你可以任意随便的阅读各种英文文档都没问题，就跟你看中文文档一样 

有同学去报名几万块钱的英文班儿，英孚，成人英语，没用，码农没必要学习那种英语班儿，你就坚持5年，每天都读一点英文文档，每天都积累生词簿，或者是常见的一些语法和句式，都积累下来，每天都快速把之前积累的生词簿和语法、句式过一遍，不用去背，每天都看，自然而然就记住了 

RabbitMQ的官网，在我见过的开源项目里，我觉得算是做的非常好的一个官网，我们架构班以后最好也孵化几个开源项目出来，到时候我们可以按照RabbitMQ模板来写我们自己开源项目的文档和官网

### 194_根据官网的指导来带着大家下载和在windows上部署RabbitMQ 

从这一讲开始，我会开始来尽量的教会大家，渔 

我会带着大家来学习，如何自己根据现有的一些技术资源，官方文档，来快速入门一个新技术，以及如何通过官方文档的快速阅读，系统性的学习一个新技术 

很多视频课程，都是照着官方文档讲，讲的还不好，你从他那里学到的还是二手的技术，他可能咀嚼过一次之后教给你的 

最吓人的一点，是你学了一个视频课程，你就只能按照视频课程里的东西来操作和开发，那就完蛋了，你学到的就是一条鱼，不是捕鱼的技巧，想学的都是捕鱼的技巧，在课程里着重的带着大家来学习方法 

先在要本地的windows上部署来RabbitMQ作为我们的开发环境 

在安装rabbitmq之前，先要在windows上安装一个符合要求的erlang（erlang是rabbitmq的开发语言，编程语言，我们如果用java来开发，是不是要在windows上安装JDK，JRE，erlang这种编程语言，你得在windows上安装erlang） 

人家也给你提供了windows操作系统上的一键式安装erlang编程语言的安装程序 

我们就进入了erlang编程语言的官网中去了 

我们应该安装哪个版本的erlang呢？我带着大家通过分析官网，来学习如何入门和使用一个技术，其实大家可以看到整个捕鱼的思路 

我给大家写好一套笔记，大家就按照这套东西来做，你学到了鱼，但是你没学到渔 

有些生僻单词的发音，我会根据我的英文的经验来发，其实很多发音的话呢，我可能是有问题的，facade 

通过分析这个官方文档，我们就可以知道，rabbitmq .37.7这个版本，是用erlang的21.0.x系列的最新的版本就可以，在文件IO读写性能上有提升，而且对TLS的支持是最好的，人家也是建议你用这个版本的 

在erlang的官网，我们可以看到最新的版本就是erlang 21.0.x系列，我们直接找64位的windows安装程序即可，21.0.1版本的windows安装的二进制程序包 

说实话，你如果是入门一个技术的话，严格的按照官网一步一步来是没问题的 

**1、安装erlang** 

http://www.erlang.org/downloads 

点击：[OTP 21.0.1 Windows 64-bit Binary File ](http://erlang.org/download/otp_win64_21.0.1.exe)，进行下载 

右击otp_win64_21.0.1.exe，以管理员权限来运行，必须要这么来运行，选择一个安装目录，完成erlang的安装 

**2、安装和运行rabbitmq** 

http://www.rabbitmq.com/install-windows.html#run-windows 

下载：[rabbitmq-server-3.7.7.exe](https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.7/rabbitmq-server-3.7.7.exe) 

rabbit-server-3.7.7.exe，这个东西一看就是在windows上的一个一键式安装rabbitmq的安装工具，双击他，会显示一个界面，然后下一步下一步下一步，就可以通过这个东西装好rabbitmq 

双击：[rabbitmq-server-3.7.7.exe](https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.7/rabbitmq-server-3.7.7.exe)，直接会完成rabbitmq的安装，将其安装为windows的一个服务，而且会直接以默认的配置来启动rabbitmq 

刚开始使用用默认的环境变量来运行rabbitmq就ok了，但是你也可以定制化修改rabbitmq的环境变量，对于我们来说，刚开始使用，其实不用调整他的环境变量 

**3、rabbitmq服务的管理** 

rabbitmq作为一个windows服务默认就安装完就启动了，后面如果停止、重启rabbitmq服务，直接在我刚才给大家演示的那个windows服务界面就可以操作 

防火墙和其他的一些安全工具可能会阻止rabbitmq绑定到端口上去，所以我的windows默认是把防火墙关闭的，不关闭的话，可能会有问题 

如果要通过命令来停止rabbitmq的broker节点（在MQ消息中间件里，通常将一个消息中间件的节点，称呼为broker），rabbitmq安装目录的sbin目录内的rabbitmqctl.bat，来停止broker或者是检查状态 

rabbitmqctl.bat stop

rabbitmqctl.bat status 

我们找不到rabbitmq的日志文件存放的目录，明显不是在rabbitmq的安装目录中的，此时怎么办呢？渔，我们去找找readme-service.txt，里面肯定会有一些信息的 

C:\Users\lixue\AppData\Roaming\RabbitMQ 

默认情况下，给放到了C盘下面去了，我们会教大家如何来设置这个目录 

默认情况下，rabbitmq的日志，是不断的写入一个日志文件中的，默认是不会新建更多的日志文件的，如果你觉得日志文件内容太多了，要换一个新的日志文件，使用下面的命令 

rabbitmqctl.bat rotate_logs 

**4、通过管理后台来访问rabbitmq** 

15672是默认的http接口的端口，以及管理后台界面的端口 

rabbitmq默认会创建一个用户，guest，密码也是guest，如果是在本机来使用管理后台，可以用guest用户 

rabbimq-plugins.bat enable rabbitmq_management 

http://localhost:15672/

### 195_动手来写RabbitMQ发送与接收消息的第一个HelloWorld 

**1、pom.xml** 

<dependency>

  <groupId>com.rabbitmq</groupId>

  <artifactId>amqp-client</artifactId>

  <version>4.0.2</version>

</dependency>

<dependency>

  <groupId>org.slf4j</groupId>

  <artifactId>slf4j-api</artifactId>

  <version>1.7.21</version>

</dependency>

<dependency>

  <groupId>org.slf4j</groupId>

  <artifactId>slf4j-simple</artifactId>

  <version>1.7.22</version>

  <scope>test</scope>

</dependency> 

**2、消息生产者** 

```
import com.rabbitmq.client.ConnectionFactory;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.Channel; 

public class Send { 
  private final static String QUEUE_NAME = "hello"; 
  public static void main(String[] argv)
               throws java.io.IOException {
        ConnectionFactory factory = new ConnectionFactory();
  factory.setHost("localhost");
  Connection connection = factory.newConnection();
        Channel channel = connection.createChannel(); 
        channel.queueDeclare(QUEUE_NAME, false, false, false, null);
        String message = "Hello World!";
        channel.basicPublish("", QUEUE_NAME, null, message.getBytes());
        System.out.println(" [x] Sent '" + message + "'"); 
        channel.close();
        connection.close();
  } 
}     
```

**3、消息消费者** 

```
import com.rabbitmq.client.ConnectionFactory;
import com.rabbitmq.client.Connection;
import com.rabbitmq.client.Channel;
import com.rabbitmq.client.Consumer;
import com.rabbitmq.client.DefaultConsumer;

public class Recv {
  private final static String QUEUE_NAME = "hello"; 
  public static void main(String[] argv)
               throws java.io.IOException,
                       java.lang.InterruptedException {
 
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        Connection connection = factory.newConnection();
        Channel channel = connection.createChannel();
 
        channel.queueDeclare(QUEUE_NAME, false, false, false, null);
        System.out.println(" [*] Waiting for messages. To exit press CTRL+C");
        
        Consumer consumer = new DefaultConsumer(channel) {
               @Override
               public void handleDelivery(
                               String consumerTag, 
                               Envelope envelope,
                               AMQP.BasicProperties properties, 
                               byte[] body)
                               throws IOException {
                       String message = new String(body, "UTF-8");
                System.out.println(" [x] Received '" + message + "'");
               }
        };
        channel.basicConsume(QUEUE_NAME, true, consumer); 
    }
}
```

###  196_动手来写RabbitMQ发送与接收消息的第二个HelloWorld 

我们参照官网给的一个比较简单的hello world和demo，重写了生产者和消费者，模拟一个效果，就是生产者可以发送消息到队列里面去，然后呢消费者会有多个，多个消费者会从队列中负载均衡的每个消费者获取一定的消息来进行处理 

库存服务，不断的发送消息到rabbitmq中去，调度服务部署两台机器，每台机器都会从队列中消费到一部分的消息来处理，这样的话，就可以用多个消费者来处理消息了 

渔 

为了讲课的效率，可能还是得直接给大家鱼 

库存服务也会部署多台机器，两台机器，每台机器就是一个生产者，多个生产者可以将消息发送到同一个queue中去 

1、NewTask.java 

2、Worker.java 

3、启动两个worker，然后启动一个NewTask发送消息，你会发现两个worker均匀的负载处理了消息，好，我们初步就要这个效果就ok了 

你发送的每条消息，只会被一个worker（consumer）消费到和处理 

consumer有3个，rabbitmq会被一个队列中的消息按照round robin轮询的算法，依次将消息推送给每个consumer，每个consumer都会接受到1/3的消息 

4、ack机制 

如果某个consumer消费一个消息，消费到一半儿就宕机了，这个消息没处理完，就会导致消息的丢失，消息队列中的数据丢失 

在这里停顿一下，看一下面试突击第一季里的消息队列中专门讲解了消息丢失这块的知识 

consumer会自动进行ack，通知rabbitmq自己消费完了一个消息，自动ack的机制，可能什么呢？某个消息还在处理中，consumer就自动通知rabbitmq这个消息处理完了。后面宕机了，消息丢了。 

consumer将自动ack关闭掉，然后自己在处理完一个任务之后，确定处理完毕了，再手动执行ack机制，通知rabbitmq处理完毕。如果处理到一半的时候，宕机了，rabbitmq没有收到这个消息的ack通知，就会将消息分发给其他的consumer再次处理 

5、durability持久化机制 

如果某个消息发送到了rabbitmq中，还没有来得及推送给消费者，此时rabbitmq自己宕机了，也会导致消息会丢失 

消息的持久化机制 

生产者在发送消息的时候，发送到rabbitmq之后，rabbitmq会将消息持久化到磁盘上去，然后才会通知生产者说ok，此次消息发送成功了。如果rabbitmq宕机了，没有将消息持久化到磁盘上去，此时生产者写入会报错，然后你就需要不断的重试写入，直到rabbitmq恢复正常，保证消息不会丢失 

6、消息分发的均匀性 

有的消息处理很耗时，有的消息处理不耗时，此时就会导致有的consumer一直在处理很耗时的消息，接收到的消息比较少；另外的consumer处理的是不耗时的消息，消息是均匀分发的，分发给一个worker，再是下一个worker。如果有的worker一直在处理耗时的任务，就会导致其他的任务处理的可能就是不耗时的任务，一直比较空闲 

除非是一个worker已经处理完了一个message，而且通知了ack以后，才会给这个worker再次分发下一个消息，如果某个worker在处理耗时的任务，还没处理完，rabbitmq就不会将其他的消息分发给这个worker了 

他会直接将其他的耗时的任务分发给其他的worker

### 197_基于Spring Cloud Stream来整合RabbitMQ进行消息驱动开发 

sping cloud其实早就考虑过了，微服务之间可能会通过MQ进行通信，不一定是通过feign进行接口调用的，所以spring cloud有个专门的组件，叫做spring cloud stream，专门帮助各个服务进行消息通信的 

spring cloud stream是个通用的框架，定义了一套自己的模型和概念，封装了底层的具体MQ实现，无论是rabbitmq还是kafka，都能支持，现在主要支持的就是这两块MQ，其实从这里我们也能看到，spring cloud支持了rabbitmq和kafka，所以国内公司在MQ技术选型上，基本就用rabbitmq就可以了 

我们在阶段二对rabbitmq，还是spring cloud streaming整合rabbitmq都是浅浅的看一下，用一下，就跟阶段一的spring boot一样，所以说这块这不是重点，对于一个系统而言，刚开始的时候，你用mq，或者redis，其实都是最简单的使用，都没什么问题 

后面mq，是高并发的重点，深入学习rabbitmq，erlang写的，不太适合读源码；深入阅读rocketmq的源码（阿里开源的，java开发的）；结合mq的深入学习，spring cloud stream的源码也深入阅读一下 

复杂的MQ架构结合复杂的业务系统，结合高并发的架构，在复杂项目里来进行实战 

**1、加入rabbitmq配置** 

在ServiceB和ServiceA中，让他们模拟通过RabbitMQ进行通信，在application.yml中分别加入rabbitmq的配置： 

rabbitmq:

 host: localhost

 port: 5672

 username: guest

 password: guest 

**2、编写生产者** 

在ServiceB中加入以下的一些依赖： 

<dependency>

<groupId>org.springframework.cloud</groupId>

<artifactId>spring-cloud-starter-stream-rabbit</artifactId>

</dependency> 

编写发送消息的接口 

public interface MessageService { 

@Output(“test-queue”)

SubscribableChannel testQueue(); 

} 

在启动类上加入注解 

@EnableBinding(MessageService.class) 

然后在一个Controller中发送消息 

public class ServiceBController { 

@Autowired

private MessageService messageService; 

public String sendMessage() {

Message message = MessageBuilder.withPayload(“hello world”.getBytes()).build();

messageService.testQueue().send(message);

return “SUCCESS”;

} 

} 

**3、编写消费者** 

在application.yml中加入消费组： 

如果你不设置这个消费组，会导致ServiceA启动了两台机器，每条消息都会推送给每台机器。但是如果你将ServiceA所有机器的消费组设置为一个，那么就是在各台机器之间round robin轮询发送消息 

spring:

 application:

  name: ServiceA

 cloud:

  stream:

   bindings:

​    my-test-queue:

​     group: groupA 

在ServiceA中加入以下的一些依赖： 

<dependency>

<groupId>org.springframework.cloud</groupId>

<artifactId>spring-cloud-starter-stream-rabbit</artifactId>

</dependency> 

编写消息接口 

public interface MessageService { 

@Input(“test-queue”)

SubscribableChannel testQueue(); 

} 

在启动类上加入注解 

@EnableBinding(MessageService.class) 

@StreamListener(“test-queue”)

public void receive(byte[] message) {

System.out.println(“接收到的消息是：” + new String(message));

} 

只要消息可以发送，可以接受，多个消费者可以均匀的接受一部分消息，就成功了，ack机制、持久化机制、均匀分发，涉及到一些参数的设置，刚开始用mq，你不搞也无所谓，在你的系统负载很低的时候，故障很少出现的 

服务B发送的消息，服务A只有一台机器会接受到这个消息 

### 198_基于RabbitMQ改造库存服务与调度服务之间的异步通知逻辑 

内存队列，肯定是用的有界队列，队列很有可能会放满，必须走离线存储 

我们现在做的是直接发送消息到rabbitmq里面，用的技术跟纯内存的队列就不一样了，技术方案实际上来说也不一样了，一般来说不会涉及到说rabbitmq满了，拒绝你存放，一般来说不会出现 

我们的那个离线存储的逻辑可以暂时先废除 

离线存储的逻辑什么时候会启用呢，以后很后面，明年，高可用阶段的，rabbitmq如果挂掉了，这个消息发不出去了，怎么办呢？复杂的离线存储的机制 

StockUpdateMessage这个类是可以保留的，封装了一个库存更新的消息，包含了一个UUID，更新库存的操作类型，跟库存更新相关的一份数据 

Observable和Observer，可以废除，之前是做了一块，就是如果异步发送消息之后，库存服务如何知道调度服务已经完成了这个消息的处理呢？观察者模式，在这里的话，我们先废除掉，这块东西的话呢，我们后面可以在做高可用这套机制的时候，做的很复杂 

以后这块会结合zk来做 

queue可以废除掉，因为现在是直接发送消息到rabbitmq中去了

### 199_将分布式电商系统在本地启动来完成集成测试（一） 

分布式系统，单块系统，所有的请求在一个系统里面就可以完成了 

电商系统是由15个系统组成的，一个系统就是一个工程，每个系统（每个工程）是一个服务，微服务架构的角度来解释的话，那么其实这一块就是所谓的很多服务，很多服务组成了一个系统，分布式系统 

一个请求，发送过来之后，可能是需要多个服务协作完成处理这一个请求。如果一件事情需要分布在多台机器上的服务或者子系统来协作完成，每个服务处理这个请求中的一部分事情，如果是这种情况的话，就是所谓的分布式系统 

spring cloud核心几个组件，在一期改造的时候，4个组件，eureka、feign、ribbon、zuul 

集成测试，配置 

权限服务，我们需要在网关中配置这个权限服务的一些路由转发的规则，一个服务配置一个规则就可以了 

如果没有网关的话，前端，android，ios，最大的麻烦在于一系列的请求服务的逻辑，负载均衡，前端（通过js发送ajax请求的），如果没有网关这一层的，权限服务部署了多台机器，现在你的js就需要知道权限服务部署了哪几台机器，自己去针对那些机器做负载均衡，包括实现重试机制，超时机制，限流机制，熔断机制 

网关服务，转发请求，cover服务的路由转发，负载均衡，网络通信，超时，重试 

我们在这里可以不要测的那么细了，因为太耗费时间了，我们呢大概测试一下就可以了，快速的将主流程跑通， 不用每个接口都测一遍 

商品服务那块，特别麻烦的

### 200_将分布式电商系统在本地启动来完成集成测试（二）

分布式系统，跟公司里真实的生产环境，你看着有没有感觉越来越接近，越来越像

权限服务：测试通过
购物车服务：测试通过 

### 201_将分布式电商系统在本地启动来完成集成测试（三） 

评论服务：测试通过  

### 202_将分布式电商系统在本地启动来完成集成测试（四）

### 203_将分布式电商系统在本地启动来完成集成测试（五） 

明明客服服务启动了， eureka注册中心里也是有的，结果zuul网关始终感知不到客服服务，如果在公司里投入生产环境使用的时候，碰到了这个问题怎么办呢？源码级别来解决这个问题。。。   

### 204_通过分析源码来解决zuul网关无法感知到客服服务的问题 

我们其实现在如果要源码级别定位这个问题，首先就是研究一下ribbon和eureka整合的那块代码，zuul是依赖于ribbon，ribbon感知到每个服务的server list，实际上是通过eureka client来的 

ribbon整合eureka获取server list的地方，打上断点来看一下 

spring-cloud-netflix-eureka-client这个工程，里面有eureka和ribbon整合的一个关键类，DomainExtractServerLisrt，这个里面是ribbon和eureka整合的关键代码 

通过源码，我们惊奇的发现，zuul本地的eureka client里面是有完整的注册表的，eshop-customer服务，他对应的机器列表都是有的 

eshop-custommer 

其实是一个很笨蛋的问题，但是正好让我们对这种莫名其妙的报错，通过阅读源码，来排查了一下问题的原因，通过读源码，就在于说我们在postman里发出请求的时候，eshop-customer拼写错误，所以没法获取到这个服务的实例 

eshop-customer 

在postman中我们发现自己拼写的是正确的，但是问题肯定是出在zuul的配置文件里我们肯定是写错了 

在课程的学习过程中，遇到一些莫名其妙的错误，给大家来演示，我们是如何通过阅读源码，站在对源码的理解一步一步来排查报错

### 205_将分布式电商系统在本地启动来完成集成测试（六） 

调度退货入库的时候，客服服务在找调度服务调度退货入库的时候，调度服务会去找wms服务创建退货入库单客服服务调用的时候超过了1秒钟，所以报了一个读超时 

在测试环境中，数据库，网络，本地笔记本电脑，整体系统性能还是挺差的，经常容易出现超时，所以我们在测试环境中，一般在开发的时候，可以适当的将超时时间设置的长一些，在生产环境中再将超时时间给设置小一些 

### 206_针对接口调用超时重试导致脏数据的问题使用唯一索引解决 

1、本来纯本地的系统，系统、机器、内存、cpu、数据库、磁盘，性能都很差，笔记本电脑，所以根本不能跟生产的机器来比

2、系统刚启动，很容易超时，一大堆的东西需要进行初始化

3、在本地我们已经调大了超时的时间，你就再次构造数据，再次请求 

出现了大量的重试，昨天我们也发现了，每次服务间调用的链条一长，尤其在wms这块，很容易会出现重试，一旦重试就很麻烦 

这里就出现了一个问题，这个重试一下子就是15次，这个问题，我们是不是要引起重视 

哪怕是在线上的生产环境中，一旦出现这个调用超时，然后就疯狂的重试，这个肯定是有问题的，重试的次数太多了 

比如说在wms中，你的表肯定得创建唯一索引，避免这种重试机制导致脏数据的产生，一个采购单，本来就应该对应一个采购入库单，结果现在创建了15个采购入库单，最最简单的保证脏数据不产生的方法就是创建唯一索引 

源码层面研究一下，为什么在系统出现超时的时候，会一下子导致15次重试，之所以会出现15个采购入库单，调度服务 -> wms服务的时候，出现了超时，调度服务请求wms服务的createPurchaseInputOrder接口的时候，3秒钟之内没有返回，此时判断超时 

一旦调度服务判断超时之后，就会不停的重试每次重试都无法在3s之内返回，就会导致调度服务不断的重试去请求wms接口，最终就导致了15次的重试 

提醒，如果wms服务假如说宕机了，调度服务最多就是请求2次，宕机机器请求1次，其他机器请求1次。没有测试过，如果wms服务的接口访问超时了，此时重试是如何执行的呢？？？ 

而且我们要研究的是在feign和hystrix没有整合使用的时候，这个是如何对超时的现象进行重试的呢？通过源码来调试一下 

我再构造一个采购单，然后再次执行一次审核，去触发创建采购入库单，让大家来看看，如果不是系统第一次启动和请求，应该不会出现如此严重的超时的 

我们果然，在测试的时候，遇到了很多的问题，wms服务为啥会这么慢呢？因为出现了死锁的问题，deadlock，另外一方面，整个spring cloud环境下的重试也有点问题 

1、接口调用超时重试的时候，导致了大量脏数据的产生【解决，只要给业务表增加唯一索引就可以了，保证其业务上没有脏数据的产生】 

2、在spring cloud环境下，多个服务叠加在一起，会出现超乎寻常的大量的重试，我们要判断一下，这个重试是否正常 

3、wms服务中，出现了很严重的mysql的deadlock死锁的问题，导致wms服务的接口响应的性能很差

### 207_通过增加日志来分析采购入库链路的异常运行流程 

我们要先观察一下异常报错的日志，从异常报错的日志出发来分析一下这个spring cloud环境下重试的问题 

超时的根儿上的原因，肯定是wms服务的deadlock所导致，wms其实就是两个insert语句，插入两行数据，结果你给我来一个死锁，而且如此之慢，这个是根本无法接受的事情 

zuul -> 采购服务 -> 调度服务 -> wms服务 -> 采购服务 

zuul：请求采购服务的接口1个报错

采购服务：请求调度服务的接口3个报错，

调度服务：请求wms服务的接口有8个报错

wms服务：请求采购服务的接口3个报错，每个超时报错有几十次死锁的报错 

我们接下来解决这个问题的下一步，就是在各个接口中加一行日志，打印出来接口调用的情况，我们看看每个接口的调用到底是被重试调用了几次 

采购服务，审核采购单接口：被调用了4次 

zuul网关服务，因为执行请求之后发现超时了，所以在不断的重试，调用审核采购单接口，这4次重试，每次都是间隔了5秒钟 

16:24:13，第一次请求审核采购单接口，网关的接口调用超时时间是5秒钟，5秒之后接口超时，再次重试，每隔5秒调用一次这个接口，一直到最后一次调用是16:24:28 

采购服务：16:24:13

16:24:18

16:24:23

16:24:28 

然后你会看到zuul网关服务在16:24:33秒的时候报错了，zuul在接口超时的情况下，最多是重试4次，如果4次重试都不行，那么在第4次还是失败的时候，就会报错 

调度服务，调度采购入库接口：被调用了13次 

第一次调度采购入库接口被调用，是16:24:14，在审核采购单接口被调用之后，间隔了1秒来调用这里，是差不多ok的 

调度服务：16:24:14

16:24:17

16:24:20

16:24:23

16:24:26 

16:24:26

16:24:29

16:24:32

16:24:35

16:24:38 

16:24:41

16:24:44

16:24:47 

很有规律，每隔3秒钟一次，采购服务调用调度服务的间隔是3秒钟，所以每隔3秒重试一次， 

wms中心的创建采购入库单接口：被调用了大约38次左右 

调度中心被调用了13次，13次每次都会大约会重试wms中心3~4次，所以wms中心被重试了38次，但是wms中心每次被重试都是在一秒内被重试好几次，那几次一般会出现一个严重的死锁的异常，出现死锁了以后就会报错，所以只有调度中心13次请求wms的时候，wms会去请求这个采购服务，采购服务每次都会hang死，进而导致wms中心自己本身也会死锁 

wms服务，16:24:14，第一次被调用

16:24:50，最后一次被调用 

wms中心的接口调用，大致上来说可以看成是每秒钟都会有几次，2次，3次，4次，5次，不等 

通知采购中心“创建采购入库单”事件发生了：13次 

采购服务，第一次，16:24:14

最后一次，16:24:50 

我们可以发现第一个线索，实际上来说，正常情况下，这个东西应该是很快的，zuul网关服务请求采购服务开始，虽然我们本地的机器性能很差，但是几个服务之间的调用链路，差不多是在1s之内完成，也是基本是可以的 

差不多整个调用链路，在1s左右，或者1s多完成是ok的 

理论上来说，按照我们的超时时间的设置，网关是5s，各个服务是3s，应该是不会超时的，1s左右网关就会返回结果给我们的浏览器

采购服务：审核采购单 -> 调度服务：调度采购入库 -> wms服务：创建采购入库单 -> 采购服务：通知采购中心 

我们下一讲继续来分析整个这个调用链路为什么会失败 

我们要先解决一个问题，spring cloud在超时情况下的重试机制，到底是怎么回事儿呢？ 

1、整个调用链条，本身是没什么问题的，问题主要出在wms服务调用采购服务的通知采购中心的接口的时候，卡住了，进而导致后续一连串的超时重试【这个问题是最大的一个问题，wms服务调用采购服务的接口之后为什么没有立即返回呢？】 

2、spring cloud的重试，我们初步排查出来了一定的规律，超时的时候，很明显跟我们之前设置的那个服务宕机情况下的重试是不一样的，超时我们发现会重试3~4次，zuul调用采购服务重构了4次；采购服务被调用4次，每次重试调用了调度服务是3~4次，一共是13次；调度服务被调用了13次，每次重试调用wms服务是3~4次，一共是38次；但是wms服务的38次中只有13次是可以成功去调用采购服务的，所以采购服务的通知采购中心接口也是被调用了13次，wms服务有每秒并发的几个请求会出现严重的死锁 

3、下一讲，我们先来分析一下spring cloud在超时时候的重试机制，通过源码来分析，zuul超时重试，以及普通服务的超时重试 

4、再下一讲，我们先来解决wms服务的接口在一秒之内被调用好几次的时候，为什么会出现mysql的死锁问题？ 

5、再下一讲，我们来增加大量的日志，排查和解决wms服务调用采购服务的那个接口，为什么3秒钟都不返回，最终解决这个调用链条异常的问题 

是分布式系统环境下，第一个比较复杂的问题，当时为什么煞费苦心，阶段一要做如此复杂的一个业务系统了，大家其实就可以遇到很多跟生产环境类似的问题，我们来解决大量的技术问题

### 208_使用源码调试来解决spring cloud超时重试的问题和原理

发现超时的情况下，重试大概是3~4次

我们使用我们当时的demo项目，来试一下，超时情况下，重试的机制

zuul在超时情况下，会重试服务4次，服务B被重试了4次，每次都会重试服务A一共是4次，服务A一共被调用了16次

这个东西就已经可以印证了，在我们的这个超时时间以及重试的配置下，超时的时候，每个服务最多会重试调用其他服务是4次

ribbon:
  ConnectTimeout: 5000
  ReadTimeout: 5000
  OkToRetryOnAllOperations: true
  MaxAutoRetries: 1
  MaxAutoRetriesNextServer: 1

某台机器超时的时候，会对那台机器重试就是4次

某台机器宕机了，此时就会走我们之前测试的那个重试策略，先对那台机器重试一次，不行，就重试其他一台机器

忽略了一点，忘了给大家讲超时重试的机制，源码这块我们忘了讲了

RetrableFeignLoadBalancer里面去，超时的重试是在这个里面控制的，超时的重试机制的控制，不是在我们之前分析的那块源码的，宕机重试的时候，做实验，测试过了，某台机器请求超时的那个重试

在这个配置下，默认情况下，是对某台机器请求超时的时候，是重试4次的

从源码层面我们就已经验证了这一点了 

### 209_添加大量日志来定位wms服务调用采购服务超时的问题

spring cloud重试，所有服务的重试都是正常的，最最关键的一个核心点，在于说wms服务调用采购服务超时了

wms服务在调用采购服务的通知采购中心的接口的时候，执行到更新采购单的状态的时候，卡住了，没执行下去，这块起码卡住了十几秒，这个就是最最根本的一个问题

我跟大家说，是出在这儿了

更新一行数据是会加一个行锁的

第一个事务，占据了表的一行的行锁，还没释放，调用了远程的接口

远程接口触发了第二个事务

第二个事务，尝试去获取表的一行的行锁，结果第一个事务还没释放，所以导致第二个事务更新如此缓慢，一直卡在那儿

我们需要重构一下代码

对本地数据库的操作是纯事务，不应该将本地数据库操作跟远程接口的调用，混在一块儿，可能会出现莫名其妙的事务之间锁争用的问题，通过分布式系统，触发了一个分布式系统场景下的问题

问题解决  

### 210_画图解释分布式系统环境下两个事务竞争MySQL行锁的问题

分布式系统环境下两个事务竞争MySQL行锁的问题

![](C:\Users\zy199005\Desktop\中华石杉\images\java\06\21001.png)     

wms服务那里会报很多的死锁，事务A等待一个锁资源的释放，另外一个事务B也启动了也在等待一个锁资源的释放，事务A在等待事务B释放一个锁资源，事务B又在等待事务A释放一个锁资源 

两个事务就会构成一个死锁

### 211_取消定位问题的日志同时全面排查代码重构问题代码

### 212_将分布式电商系统在本地启动来完成集成测试（七） 

我们主要是测试分布式系统是否能够跑通，所以说很多单库的CRUD的操作，不测试也行，稍微测几个，意思就行了，把电商的几个黄金链路给跑通 

1、采购入库 

（1）手工在数据库中新建采购单

（2）审核采购单通过 -> 调度服务创建采购入库单 -> wms服务插入采购入库单

（3）给采购入库单添加商品上架条目，更新采购入库单的商品到货时间，以及每个采购入库单条目的到货数量

（4）审核采购入库单 -> 更新采购入库单状态为：已入库 -> 更新采购单状态为：已入库 -> 更新wms服务的库存 -> 更新调度服务的库存 -> 财务服务创建采购结算单 -> 更新采购入库单状态为：待结算 -> 更新采购单状态为：待结算

（5）审核采购结算单 -> 更新采购结算单状态为：已完成 -> 更新采购入库单状态为：已完成 -> 更新采购单状态为：已完成 

2、销售出库

3、退货入库 

跑通这个采购入库的流程，是从采购单那儿驱动的

### 213_将分布式电商系统在本地启动来完成集成测试（八） 

销售出库 

（1）提交订单 -> 插入订单 -> 通知库存中心锁定库存 -> 通知调度中心锁定库存 -> 通知wms中心锁定库存 

（2） 

### 214_将分布式电商系统在本地启动来完成集成测试（九） 

销售出库 

（1）提交订单 -> 插入订单 -> 通知库存中心锁定库存 -> 通知调度中心锁定库存 -> 通知wms中心锁定库存 

（2）支付订单 -> 提交完订单，页面一般会显示一个支付二维码，订单中心请求支付服务，从支付宝/微信那儿要一个二维码，同时插入一条支付流水 -> 扫码支付，支付宝/微信确认支付成功之后，会来回调支付服务的一个回调接口 -> 库存中心、调度中心、wms中心扣减库存 -> 调度销售出库，创建销售出库单、发货单、物流单 -> 增加会员积分 

发现一个问题，支付服务，调用订单服务的时候，可能是因为一些问题，所以导致超时了，重试了一共是3次，库存扣减了3次，销售出库单那块没有多，因为唯一索引来约束，会员积分增加了3次 

有一个问题，对于这种插入数据的操作，我们是否要进行超时重试呢，如果要进行超时重试，就有可能会产生脏数据 

那么我们就必须保证这个接口的幂等性 

什么叫做分布式系统接口的幂等性，是什么意思呢？就是说，可能会有人重试你几次，发送一样的请求，你必须保证一样的请求不能重复的更新数据，否则就会导致数据出现脏数据。。。。 

可能是因为刚才我们又是重启订单服务什么的，可能是系统刚重启导致严重的重试，其实我们可以首先恢复数据，我们再试一次，如果没有重试，那么这个问题可以先过，因为在后面分布式系统那块的时候，我们来做分布式系统接口的幂等性 

上次是重试了3次，这次是重试了2次，所以说有点坑爹 

我们可能就得来优化一下系统性能了，否则这个重试实在是太坑了 

支付订单的调用链路，链路很长，性能很差，导致了整个很容易会出现重试，所以说我们下一讲开始，先来分析一下支付订单的调用链路的性能问题

### 215_通过增加日志来分析支付订单接口被重复调用问题在哪里 

莫名其妙，这一次就成功了，没有出现超时重试的情况，但是确实之前我们发现超时重试还是比较常见的，所以比较担忧这个调用链路的性能，但是我们要想，如果上线了以后，线上的机器，大量的服务都部署在本地一台笔记本电脑上的 

每个服务的一个实例，至少部署在一台4核8G的虚拟机上，就一个服务实例 

15个服务实例，部署在本地电脑上，吃cpu，吃内存，吃网卡，所以性能肯定是差，真的上线的话，应该是没这么差的 

再次恢复数据，再来测试一次 

这一块通过日志发现，倒不是性能问题，而是spring cloud的重试机制出了问题，胡乱的莫名其妙的出现了重试 

订单服务莫名其妙的在被调用，导致胡乱的扣减库存和增加会员积分，看起来好像不是支付服务在调用 

支付服务的接口，好像也被调用了 

我们发现调用链路的性能是没问题的，但是就是这个整个接口和调用链路莫名其妙的在被人调用，但是就是不知道是谁在调用 

除非是网关的问题，网关里面可能莫名其妙的在自己调用支付服务的接口，导致这个接口被频繁的重复调用 

这块东西的话呢，我们可以在支付服务和订单服务作为入口，先来开发一个简单的幂等性的保障机制 

现在短时间内也很难找到是谁在重复调用接口，但是说实话，后面讲到分布式系统架构的时候，在分布式系统的接口，幂等性，增删改的操作，如果被重复调用，你如何做幂等性的保障 

临时方案，尽量避免出现重复调用，发现一个规律，一般来说，重复调用，都是在几分钟之内这样子，所以你做一个24小时的操作，基本上问题不大 

正儿八经的保证幂等性的技术方案，要做一个通用的幂等性保证的服务出来，基础框架，基础类库，让各个服务全部加上一个统一的接口幂等性的保证机制 

55分，59分，02分，4分钟，3分钟，每隔几分钟就被调用一次 

搞了半天还是数据的问题，不是我们的问题，是数据的问题，我们有一个定时的调度任务，每隔一段时间会来判断，有没有未支付状态的支付宝方式的订单，原来是我们构造的数据不对，支付接口被回调，微信的支付方式 

结果我们弄成了支付宝的支付方式，所以被后台调度任务一直在处理 

往往会被定时调度任务重试好几次 

解决了构造的数据有问题导致调度任务重复执行接口的问题，解决 

### 216_将分布式电商系统在本地启动来完成集成测试（十）

购物流程

（1）提交订单
（2）支付订单
（3）销售出库
（4）确认收货

退货流程我们之间跑过了，我们下一讲可以再跑一次，退货流程都通了，那么整个系统改造就差不多成功了

### 217_将分布式电商系统在本地启动来完成集成测试（十一） 

退货流程再走一遍 

Illegal character in path at index 57: 

http://eshop-customer/customer/informRefundFinishedEvent/{returnGoodsWorkwheetId}

### 218_调试feign源码来定位和解决客服服务接口调用非法字符异常 

Illegal character in path at index 57: 

http://eshop-customer/customer/informRefundFinishedEvent/{returnGoodsWorkwheetId} 

做详细设计，扣各种spring cloud的细节，站在源码的角度 

Request里的url，在这一步，按理来说，url里的占位符，应该被实际的@PathVariable里的值给替换掉的，结果这里没替换 

http://ServiceA/user/sayHello/1?name=%E5%BC%A0%E4%B8%89&age=20 

builder = /user/informRefundFinishedEvent/

var = returnGoodsWorkwheetId

variables = {returnGoodsWorksheetId=1} 

returnGoodsWorkwheetId

 

 