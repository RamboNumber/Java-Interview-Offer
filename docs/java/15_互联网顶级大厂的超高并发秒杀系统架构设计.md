# 15_互联网顶级大厂的超高并发秒杀系统架构设计

### 001_互联网大厂面试官喜欢问秒杀架构的历史起源

### 002_为什么互联网公司面试官喜欢问经典的秒杀系统架构设计？

### 003_国内的秒杀系统架构设计技术资料的发展过程

### 004_秒杀系统架构设计课程的大纲以及授课思路（1）

### 005_秒杀系统架构设计课程的大纲以及授课思路（2）

### 006_秒杀系统架构设计课程的大纲以及授课思路（3）

### 007_秒杀系统架构设计课程的大纲以及授课思路（4）

### 008_秒杀系统架构设计课程的大纲以及授课思路（5）

2年前，秒杀架构开始火起来了，互联网顶级大厂面试的时候开始喜欢问你，如果让你设计一个电商的秒杀系统架构，你会如何来做？慢慢这个面试题就火了，很多人发现出去面试经常会遇到这个问题，很多面试官也喜欢问这个问题 

2018年，再往前面推，前面几年的话呢，双11/618这个东西，大型大促购物节，顶级互联网电商大厂都会有大型购物节的0点超高并发的抢购，瞬时超高并发，很多人等在0点来抢购 

电商领域和行业里又诞生了一个新的玩法，秒杀活动，各种各样的电商类的APP，搞一批特价的商品，物美价廉，折扣超低，限量，指定在某个时间点，大家可以来抢购，抢到就是赚到，对电商平台来说就是通过活动获取流量 

Java面试中开始问秒杀架构，大概兴起于3年前，2017左右的时候开始兴起，萌芽阶段，只是少数顶级大厂的面试官开始喜欢问这个秒杀架构，2018年开始到2019年，秒杀架构开始变得很流行 

第一个要点，电商秒杀活动是隐藏在我们日常的生活中的，哪怕是一个普通人大致也经历过双11大促，商品秒杀活动，抢购手机，类似这样的事情，普通人大致都经历过，如果你要是说你完全没接触过类似的活动 

第二个要点，就是秒杀系统架构里包含了互联网主流的大部分技术栈，很多典型的互联网技术架构，都包含和运用在了秒杀系统架构里，非常典型的高并发场景，高并发的交易场景，需要运用到大部分的互联网典型架构 

面试官才开始流行对Java工程师的面试，喜欢问这个秒杀系统的架构设计，你到底了解不了解互联网的技术栈，同时对互联网的典型架构到底熟悉不熟悉，能不能灵活的运用，来解决互联网行业的典型业务场景的问题 

不排除很多中小型互联网公司的面试官，是出于跟风的一个考虑，看到别的公司喜欢问，他自己也问，当然说句实话，很多面试官可能问这个问题的时候，自己也未必做过秒杀系统，也未必真的就了解这个东西 

2018年下旬~2019年，网上秒杀系统架构设计的一些技术资料，开始慢慢的就变多了起来，刚开始的时候主要是一些技术还不错的人，自己根据自己的技术积累，有的人是确实在中小型公司里做过秒杀系统，还有的人是没有做过，开始再往上会发表一些博客，写一下自己对秒杀系统架构设计的理解 

有一些技术公众号就会转发一些写的比较不错的一些秒杀系统架构设计的技术博客，有些人在公众号上也能看到一些；有一些IT视频课程网站，就会发布秒杀系统设计的一些视频课程 

如果仅仅是一些技术博客，通常是一篇文章就讲一下秒杀系统 架构设计的整体的思路，仅仅是思路而已，但是的话完全达不到落地的一个水平，这个也是一个很大的问题，并不知道如何落地 

秒杀视频课程，关于秒杀一些核心的技术要点是涉及到了一部分的，但是距离互联网顶级大厂的大规模的秒杀系统，跟这个程度还是有一定的差距的，有很多东西其实那些视频课程里并没有讲到 

1、秒杀促销活动服务 

（1）秒杀促销活动服务的业务流程

（2）秒杀促销活动的Redis数据结构设计 

电商运营人员会在你的这个服务里选择对应的一些热门的商品，给他一个超低折扣，锁定他的一些库存，专门是用于进行秒杀促销活动的售卖，这个服务会把你选择好的商品信息、折扣信息、促销活动信息、库存数据，都放到一个redis里去 

2、秒杀促销活动的前端页面 

（1）秒杀活动页面静态化方案

（2）页面静态化以及CDN缓存方案

（3）静态页面的缓存以及文件服务器存储

（4）秒杀后台接口url隐藏方案

（5）后端与前端时钟同步的秒杀倒计时方案

（6）秒杀场景下的前端验证码方案

（7）秒杀活动页面的限流方案 

3、秒杀场景下的高并发负载均衡架构 

（1）防黑客DDoS攻击的高防IP方案

（2）秒杀场景下的SLB负载均衡架构方案

（3）基于SLB的秒杀场景和普通场景的分流隔离

（4）秒杀场景的Nginx请求分发方案

（5）Nginx环节的请求限流方案

（6）高并发场景下的Nginx内核参数调优 

4、 秒杀场景的高并发抢购架构设计 

（1）秒杀抢购请求的黄金链路流程

（2）高并发场景下的Tomcat内核参数性能调优

（3）秒杀场景下的接口限流方案

（4）重复秒杀请求去重解决方案

（5）秒杀接口的防刷防作弊风控解决方案

（6）秒杀场景的Redis分布式缓存架构方案

（7）基于Redis Lua脚本实现的复杂秒杀业务逻辑

（8）秒杀场景下的库存超卖问题解决方案

（9）秒杀场景下的多线程并发优化实战

（10）高并发场景下的线程池参数调优

（11）秒杀场景下的多线程同步加锁优化实战

（12）秒杀场景下的分布式锁优化实战

（13）秒杀场景下的Disruptor以及内存队列实战 

5、秒杀场景的异步下单技术方案 

（1）秒杀场景下的RocketMQ集群架构方案

（2）秒杀场景下的消息队列技术方案

（3）秒杀场景下的Redis+RocketMQ一致性回滚方案

（4）秒杀场景下的数据库架构方案

（5）秒杀场景的分库分表技术方案

（6）秒杀超高并发场景下的数据库压测

（7）高并发场景下的数据库连接池参数调优

（8）高并发场景下的数据库内核参数调优

（9）秒杀下单服务的核心业务逻辑实现

（10）高并发场景下的数据库锁实战 

6、秒杀成功后的业务逻辑实现 

（1）秒杀成功后的异步通知用户解决方案

（2）秒杀成功后的订单查询方案

（3）秒杀成功后的订单支付以及后续逻辑实现

（4）秒杀成功后长期不支付解决方案 

7、高可用的秒杀系统架构设计 

（1）秒杀系统的全链路中间件高可用架构

（2）秒杀系统的全链路高可用降级方案

（3）Redis缓存崩溃后的秒杀系统自动恢复方案

（4）RocketMQ集群崩溃后的临时本地存储降级方案

（5）数据库集群崩溃的临时本地存储降级方案

（6）秒杀服务崩溃的防服务雪崩降级方案

（7）秒杀系统的全链路漏斗式流量限制方案

（8）秒杀系统的双机房多活部署方案

（9）秒杀系统部分机房故障时的降级方案 

8、秒杀系统的压测、故障演练以及实时大盘 

（1）秒杀系统的全链路压测以及针对性优化

（2）秒杀系统的全链路故障演练以及高可用架构验证

（3）秒杀系统的基于大数据技术的实时数据大盘 

9、秒杀业务中台的架构升级与改造

### 009_关于项目实战过程中穿插讲解所需技术的说明

秒杀系统第一块要做的秒杀活动服务，让运营人员去创建一些秒杀活动，创建了秒杀活动之后，系统就会把这次秒杀活动需要的一些数据填充到redis里去，比如说活动数据，商品数据，库存数据，做一些库存锁定的操作 

秒杀抢购，缓存机制，限流机制，都是要基于redis去做的 

亿级流量商品详情页缓存架构设计，redis的持久化、集群架构、高可用架构、高并发架构，讲解的主要是redis层面的一些架构设计，用redis是比较简单的，主要难的是redis架构如何抗住高并发、高可用 

redis如何来用，如何来用好，如何用redis去实现各种各样复杂的业务需求，我之前从来都没有讲过的1627098499

### 010_案例实战：最普通的基于Redis实现的缓存机制

redis如果说简单了，里面存放的就是一大堆的key-value对，set key value，往里面放一堆数据，然后通过get key，从里面获取一堆数据，往简单了用，基本上来说redis就是这么用的，第一种最经典的场景 

你可以用redis来应对高并发和高性能 

高性能，你有一些很复杂的查询，或者是一些很大的数据资源（比如说图片之类的东西，超长文本之类的东西），可能平时执行的话呢是很慢的，复杂查询，上百行的SQL语句，从数据库里出，可能很慢，或者是一个大html页面，从磁盘上加载 

你就可以把复杂查询以后的结果放入redis里作为缓存，下次再查询的时候就直接从redis缓存里出，不需要再次从数据库里执行上百行的SQL语句去查询了，只要10毫秒就可以做到了 

html页面，可以放在缓存里，每次就直接从缓存里出就可以了，不需要每次都做动态的渲染，比如说基于jsp动态渲染页面，再从磁盘上读取出来返回给浏览器，其实你也可以把一些静态资源放到redis缓存里去 

高并发，假设你每次都查询数据库，数据库最多每秒抗4000个请求，但是你每秒有1万次请求，此时你可以把一些查询结果放在缓存里，后续可能每秒有9500次请求都是走redis缓存的，500次请求走数据库 

同样的机器配置下，数据库每秒也就抗几千次请求，redis每秒可以抗几万次请求到十万次的请求都可以做到1627098499

### 011_案例实战：实现一个最简单的分布式锁

企业级分布式锁，基于redis的分布式锁的企业级实现，在redisson框架里的源码，我们很早就分析过了，基于复杂的lua脚本去实现的，有一整套的实现锁的逻辑在里面的，根本不是给大家讲解分布式锁 

nx 

set key value nx，必须是这个key此时是不存在的，才能设置成功，如果说key要是存在了，此时设置是失败的 

比如说有很多台机器，要竞争一把锁，此时你可以让他们对同一个锁的key，比如lock_key，设置一个value，而且都是要加上nx选项的，此时redis可以通过nx选项，保证说只有一台机器可以设置成功，可以加锁成功，其他机器都是会失败的

### 012_案例实战：博客网站的文章发布与查看

mset，mget，msetnx，m -> multi的意思，mset一下子设置多个key-value对，mget就是一下子获取多个key的value，msetnx就是在多个key都不存在的情况下，一次性设置多个key的value 

mset和mget，相当于是batch批量设置和查询，比如说假设你一次性要往redis里塞入20条数据，假设你是通过for循环加上set，执行20次的set，每一次set操作都要一次完整的网络通信过程 

每一次set就算他是10ms，200ms 

mset，一次性把20个key-value对通过一次网络通信，交给redis去执行，此时就是10ms就可以了，mget也是一个意思，批量查询，msetnx，必须所有key都不存在，然后才可以完成本次的设置 

平时写一篇博客，一方面落入数据库，一方面把数据双写一份到缓存里去162 

### 013_案例实战：博客字数统计与文章预览

strlen，getrange

### 014_案例实战：用户操作日志审计功能

append 

平时我们对系统上的一些用户的一些核心操作，比如更新一些数据的操作，都是会有一个审计日志的功能，其实就是把你用户在系统里的一些操作记录下来，每一次操作都记录成一条日志，审计日志 

后续可以通过一个列表来进行一个查询，分页查询 

我们如果说要基于redis来存储这种操作审计日志，应该如何来做呢？也可以在redis里，搞一个key，每天都有一个key，每天的这个key里就把当天所有的操作日志都串联在一起，查询的时候，都是按天来查询你的操作审计日志就可以了 

key -> value，value取出来，做一下字符串拼接 

redis的append这个api，就是说可以不停的把你的日志追加到指定的key里去

### 015_案例实战：实现一个简单的唯一ID生成器

ID，都是通过数据库的自增主键来实现的，有一些场景下，分库分表的时候，你同一个表里的数据都会分散在多个库的多个表里，这个时候就不能光是靠数据库来生成自增长的主键了，此时就需要生成唯一ID 

snowflake算法，还有一些大厂开源的唯一ID生成组件，他是会解决一些原生的snowflake算的缺陷 

incr1

### 016_案例实战：实现博客点赞次数计数器

incr，decr16

### 017_本周只是开胃菜：后续两周课程内容说明

第二周，我会把redis所有的数据结构以及基于这些数据结构的各种案例实战，都讲完，下周的内容量还是会比较大的，第三周，会讲解redis的高阶功能的使用，事务，lua脚本，pipeline流水线，geohash，hyperloglog，类似于这样的一些东西

### 018_案例实战：社交网站的网址点击追踪机制（1）

### 019_案例实战：社交网站的网址点击追踪机制（2）

### 020_案例实战：社交网站的网址点击追踪机制（3）

开胃菜，初步讲解了一下redis里的一些字符串的用法，最最简单的key-value对，加上一些数字的操作方法，增加/减少；hash，list，set，sorted set，把核心的redis的四大数据结构的各种用法结合案例讲解清楚；下周，hyperloglog、流、事务、lua、流水线、过期时间，高阶的功能，结合案例 

hash数据结构，社交网站的网址点击追踪机制，类似微博 

hset hash field value

hsetnx

hget hash field 

社交网站（微博）一般会把你发表的一些微博里的长连接转换为短连接，这样可以利用短连接进行点击数量追踪，然后再让你进入短连接对应的长连接地址里去，所以可以利用hash数据结构去实现网址点击追踪机制 

http://t.cn/XsGGA9d -> http://redis.com/index.html?dfd=sf& dfd=sf& dfd=sf& dfd=sf& dfd=sf& dfd=sf& 

利用redis的incr自增长，然后10进制转36进制，接着hset存放在hash数据结构里，再提供一个映射转换的hget获取方法，hash数据结构，说白了就是我们的Java里的HashMap，redis里的hash就是一个map，一个map里可以放一些key-value对 

short_url_access_count: {

​    http://t.cn/XsGGA9d: 152,

​    http://t.cn/I93yUUaF: 269

} 

public class Ten2Thirty { 

  private static final String X36 = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ";

  private static final String[] X36_ARRAY = "0,1,2,3,4,5,6,7,8,9,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z".split(","); 

  public static String tenTo36(int num) {

​    StringBuffer sBuffer = new StringBuffer(); 

​    if(num == 0) {

​      sBuffer.append("0");

​    }

​    while(num > 0) {

​      sBuffer.append(X36_ARRAY[num % 36]);

​      num = num / 36;

​    } 

​    return sBuffer.reverse().toString();

  } 

}

### 021_案例实战：博客网站案例代码重构（1）

### 022_案例实战：博客网站案例代码重构（2）

### 023_案例实战：博客网站案例代码重构（3）

博客网站，发布一篇文章，修改一篇文章，查看一篇文章，有人查看了一下这篇文章，就可以维护一下这篇文章他的一个浏览次数，对于这个浏览次数就可以统一都放在一个hash数据结构里，每个key就是一篇文章，value就是文章的浏览次数 

hincrby hash view_count 1

hget hash view_count

### 24_案例实战：基于hash数据结构重构博客网站案例（1）

### 25_案例实战：基于hash数据结构重构博客网站案例（2）

### 26_案例实战：基于hash数据结构重构博客网站案例（3）

redis的hash数据结构，特别适合存放什么呢，就是类似于我们自己Java代码里搞的一些对象，很适合用hash数据结构来存储的，如果说你的一些对象不用hash结构来存储的话，比如你直接用json串把java对象序列化成json，然后用key-value对的字符串形势放在redis里，但是操作起来不是太方便 

hexists判断博客是否存在，不存在可以发表新博客，用hmset一次性设置多个key-value对在hash数据结构里 

查看博客的时候，直接用hgetall hash，一次性把一个hash里全部key-value对拿出来，然后返回就可以了 

更新博客，也是用hmset就可以了 

getrange，setrange，append，key-value的字符串形式，可以用到过期时间，过期时间key-value对在适当的时候是会过期的1

### 27_案例实战：基于令牌的用户登录会话机制（1）

### 28_案例实战：基于令牌的用户登录会话机制（2）

### 29_案例实战：基于令牌的用户登录会话机制（3）

用户平时会访问我们的系统，在处理任何一个请求之前，必须检查一下，这个请求是否带上了一个令牌，如果带了一个令牌，那么此时就必须在redis里检查一下，这个令牌是否有在redis里合法的、有效的一个session会话 

如果有这个session会话，此时就可以允许这个请求被处理，因为说明这个人之前已经登录过我们的系统了，登录过后才会在redis里放一个有效的session会话；如果说没有这个session的话，此时就会导致用户必须强制被迫登录 

如果用户登录通过之后，就会返回给浏览器或者客户端一块令牌，同时在redis里初始化好一个session会话，后续客户端就会在指定时间范围内发送请求的时候带上一块令牌，每次令牌和服务器端的session校验通过就可以执行请求 

过一段时间过后，服务端的redis里的session会话就会过期，过期了之后，又会导致你必须要重新登录，虽然你可能带上了令牌，但是一检查发现这块令牌对应的redis里的session已经过期了 

hset把用户id和令牌存储一下，hset把用户id和过期令牌过期时间存储一下 

每次访问系统都让用户带上令牌，如果令牌不存在就是没登录，hget获取存储的令牌和过期时间，如果令牌过期了也要强制登录，如果令牌校验通过，这次请求就可以通过 

如果令牌要是过期了，就用hdel把存储的令牌和过期时间都删了

### 30_案例实战：秒杀活动下的公平队列抢购机制（1）

### 31_案例实战：秒杀活动下的公平队列抢购机制（2）

通过一些案例，已经给大家把hash数据结构如何来使用，结合案例已经讲解的非常清楚了，接下来的话呢，就是讲解list数据结构，同样的，也是通过各种案例来讲解，讲解list是如何来使用的 

秒杀系统有很多实现方案，其中有一种技术方案，就是对所有涌入系统的秒杀抢购请求，都放入redis的一个list数据结构里去，进行公平队列排队，然后入队之后就等待秒杀结果，专门搞一个消费者从list里按顺序获取抢购请求，按顺序进行库存扣减，扣减成功了就让你抢购成功 

如果说你要是不用公平队列的话，可能就会导致你很多抢购请求进来，大家都在尝试扣减库存，此时可能先涌入进来的请求并没有先对redis执行抢购请求，此时可能后涌入进来的请求先执行了抢购请求，此时就是不公平的 

公平队列，基于redis里的list数据结构，搞一个队列，抢购请求都进队列，先入先出，先出来的人先抢购，此时就是公平的 

list数据结构，你可以把他理解为是Java里的ArrayList，LinkedList，就是一种有序的数据结构，也可以把他作为队列来使用也是可以的 

对于抢购请求入队列，就用lpush list request就可以了，然后对于出队列进行抢购，就用rpop list就可以了，lpush就是左边推入，rpush就是右边推入，lpop就是左边弹出，rpop就是右边弹出 

所以你lpush+rpop，就是做了一个左边推入和右边弹出的先入先出的公平队列 

[第3个请求，第2个请求，第1个请求]16270984

### 32_案例实战：实现博客网站的分页浏览（1）

### 33_案例实战：实现博客网站的分页浏览（2）

博客的发表和修改，查看一个博客的详细内容，浏览次数和点赞次数的统计 

发表博客的时候就把博客数据lpush到一个list里去，分页的时候，就传入page_no和page_size两个参数，然后可以算出来start_index是(page_no - 1) * page_size，end_index就是page_number * page_size – 1 

接着就是用list的lrange list start_index end_index，就可以把那一页的数据取出来了 

除了获取一页数据之外，还要返回list里所有的数据量，就是用llen list就可以

### 34_案例实战：实现OA系统中的待办事项列表管理（1）

### 35_案例实战：实现OA系统中的待办事项列表管理（2）

OA系统，自动化办公系统，说白了就是把企业日常运行的办公的日常事务都在OA系统里来做，请假，审批，开会，项目，任务，待办事项列表 

lindex，lset，linsert，ltrim，lrem 

新增待办事项，lpush list event 

插入待办事项，linsert list index event 

[待办事项3，待办事项2，插入待办事项，待办事项1] 

查询待办事项列表，lrange list 0 -1，所有都查询出来 

完成待办事项，lrem list 0 event，就把这个待办事项给删了，然后lpush done_list event，添加一个已办事项 

批量完成待办事项，ltrim list start_index end_index，然后lpush done_list event1 event2 event3 

修改待办事项，lindex和lset 

查询已办事项列表，lrange done_list 0 -01

### 36_案例实战：网站用户注册时的邮件验证机制

填写完注册信息，然后提交注册，此时后台完成注册校验和用户数据入库，返回一个消息告诉你说注册成功，但是稍后发送了一封验证邮件到你的邮箱，希望你去邮箱里点击一个链接确认你的邮箱

一般来说在注册的后台里，是否在注册的逻辑里直接就发送一封邮件呢？发送邮件是比较耗费时间的，所以通常来说，你后台注册成功之后，都会把一个发送邮件的任务给放到一个队列里去，然后直接返回给你了

接着lpush将发送邮件任务放入list，然后发送邮件的系统就用brpop阻塞式的等待从队列里获取任务，这就可以把list做成阻塞队列的效果了，加一个timeout超时时间即可

### 37_案例实战：网站每日UV数据指标去重统计

set数据结构，跟Java里的set是一样的，就是说放无序的、不重复的数据集合，list一般来说做一个操作，时间复杂度都是O(n)，set一般的操作，时间复杂度是O(1)，无序的，不重复的数据集合，数据结构上的优化，可以做到 

如果说你把重复的数据放到一个set里，他会自动给你进行去重 

网站的uv，有多少用户访问了你的网站，但是一个用户可能会访问多次，此时要对多次访问进行去重，保留完整的不重复的访问过你网站的用户集合，然后算出集合的元素数量，就会知道你当日的uv，sadd把uid放入集合，scard可以拿到uv值，当然这个仅仅是演示，实际实现是不可能这么做的

### 38_案例实战：博客网站的文章标签管理

在博客网站发表一篇文章的时候，都可以自定义给这篇文章打一些标签的，java、大数据、软件架构、心灵鸡汤，对于一篇文章来说，标签其实是不能重复的，就直接可以用set来保存 

sadd是添加标签，srem是删除标签，sismember是判断该文章是否包含某个标签，smembers是返回这个文章的全部标签，scard是这个文章的标签数量

### 39_案例实战：朋友圈点赞功能的实现（1）

### 40_案例实战：朋友圈点赞功能的实现（2）

假设说你发了一条朋友圈，此时可能你的好友会对你的朋友圈进行点赞，还可以取消点赞，你的好友在刷朋友圈的时候可以查看到自己是否对你点赞过，你自己还可以查看到你的朋友圈有哪些人点赞了，有多少人给你点赞了 

sadd给某一条朋友圈添加点赞的一个好友，用户取消点赞的话，那就是srem删除某个好友的点赞，查看你是否对某条朋友圈进行过点赞，sismember，你发出的朋友圈被哪些人点赞了，smembers，你的朋友圈的点赞次数，scard

### 41_案例实战：实现一个网站投票统计程序

sadd把用户添加到某个投票项的投票用户集合里去，sismember可以检查用户是否已经对任何一个投票项发起过投票，scard可以统计每个投票箱的投票人数，smembers可以拿到每个投票项的投票人

### 42_案例实战：实现类似微博的社交关系

关注目标用户，sadd把你加入到人家的关注用户集合里去，sadd把别人加入到你正关注的用户集合里去，取消关注，srem取消两个用户集合的关注，smembers获取你关注的所有人和你被哪些人关注了，scard获取你关注的人数和关注你的人数16

### 43_案例实战：微博的共同关注与推荐关注

sinter set1 set2，取交集，就是共同关注好友；推荐好友关注的人，sdiff获取差集，然后用差集再和你的好友集合sdiff一下，再取差集，就可以得到你没关注的但是你好友关注的人，此时就可以推荐一下； 

sunion，如果+store还可以存储

### 44_案例实战：实现网站上的抽奖程序

srandommember，spop，前者是随机从set里返回几个元素，后者是随机从set里弹出几个元素，sadd可以加入待抽奖的人，smembers返回所有待抽奖人，scard返回参与抽奖的人数，srandmember返回随机抽中奖的人

### 45_案例实战：为商品搜索构建反向索引

为商品添加索引，sadd，给商品添加一个关键词索引集合，sadd把商品添加到每个关键词的商品集合里去，删除商品是一个反向的过程，走srem，获取一个商品所有的关键词，smembers，根据某几个关键词去搜索商品，对每个关键词都smembers一下拿到商品集合，然后走一个sintern对多个集合进行交集 

仅仅是演示，实际上如果你要考虑到分页，那就更加复杂了

### 46_案例实战：实现音乐网站的排行榜程序

sorted set，不能有重复的数据，加入进去的每个数据都可以带一个分数，他里面的数据都会按照分数进行排序，有序的set，他自动按照分数来排序，相当于你可以定制他里面的排序规则了 

zadd，把音乐加入排行榜中，刚开始分数可能就是0；zscore可以获取音乐的分数；zrem可以删除某个音乐；zincrby可以给某个音乐增加分数，这个增加分数可能就是说有人下载了，或者是有人播放了，或者有人分享了， 有人点赞了，此时可以按照规则去加分，那么排序就会移动了；当然也可以减去分数；zrevrank获取音乐在排行榜里的排名；zrevrange set 0 100 withscores，可以获取排名前100的热门歌曲

### 47_案例实战：实现一个新闻推荐机制

我们可以把一个sorted set里的数据倒序排序，选择其中我们指定的分数区间范围内的数据，对这块数据还可以进行分页查询，我们可以维护一个新闻数据集合，里面的分数都是新闻的时间戳 

zadd，把当日最新的新闻加入到一个集合里，zrem是删除某个新闻，zcard是统计当日最新新闻，，zrevrangebyscore max_time min_time start_index count withscores，是说按照他的时间分数进行倒序排序，然后获取指定的分页，zcount 可以获取指定分数范围的数量

### 48_案例实战：购买此商品的顾客也同时购买

zremrangebyscore，zunionstore，zinterstore 

买了一个商品，他会给你推荐一个列表，购买过此商品的顾客也同时购买了其他XX商品，这个也是可以用这个sorted set来实现的 

对每个顾客，每次购买了之后，都会遍历他之前购买过的所有的商品，对每个商品都维护一个同时购买的其他商品的数量，可以用zincrby set 1 其他商品，然后后续如果要看到某个商品购买的人还购买了其他哪些商品，就可以用zrevrange set start_index end_index withscores，

### 49_案例实战：网站搜索框的自动补全功能

zrangebylex、zrevrangebylex、zlexcount、zremrangebylex、zpopmax、zpopmin、bzpopmax、bzpopmin 

对于输入的每个搜索词，都会遍历一下，拼接出来，然后zincrby auto_complete::潜在搜索词 权重 完整搜索词，这样的话，就是每个潜在搜索词都会有一个集合，集合里是各种可能对应的搜索词和权重分数 

然后真正你搜的时候，把你输入的潜在搜索词去执行，zrevrange set 0 9，就是对这个潜在搜索词按照权重分数倒序拿出最近的10个搜索词，做一个自动提示和补全 

string和number 

list、hash、set、sorted set（自己定制他的排序规则，定制搜索浏览的规则）16

### 050_基于HyperLogLog的网站UV统计程序（1）

### 051_基于HyperLogLog的网站UV统计程序（2）

hyperloglog，数据结构+概率算法，组合而成的，去重统计，近似数

1023468个用户来访问过你

1011325，算出来这样的一个近似值，对于很多大数据统计类的程序，精准也没太大的必要，CEO层面，100万的日活

如果基于set来计数，太耗费内存了，基于HyperLogLog算法来计数，是近似计数，有0.8%的误差，但是误差不会太大，可以给出一个相对准确的近似计数，而且就占用12kb的内存，不需要存一个set来统计uv

pfadd key 一大堆item，可以对数据进行计数，如果计算过这个元素，就返回0，没计算过就返回1，他也是可以去重的

pfcount key，可以获取计数结果

这个其实是很有用的，其实一些网站数据分析，什么pv、uv之类的，没必要过于精准，直接就是用redis的hyperloglog计数就挺好的

### 052_网站重复垃圾数据的快速去重和过滤

对于你的一些网站，垃圾数据，多的是一些不良广告，在一些论坛或者别的，有的时候你会看到一些在帖子下面的评论里，是一些广告，都有，可能会遇到有人频繁的提交相同的垃圾信息到你的站点里 

pfadd key content，如果返回的是1，那么说明之前没见过这条数据，如果返回的是0，说明之前见过这条数据了

### 053_周活跃用户数、月活跃用户数、年活跃用户数的统计

pfmerge 多个hyperloglog，可以算出来周、月和年的活跃用户数

### 054_基于位图的网站用户行为记录程序

如果说你要记录一下，在系统 里执行一些特殊的操作，每天执行过某个操作的用户有多少个人，操作日志，审计日志，记录下来每个用户每天做了哪些操作，每个用户每天搞一个set，里面放他做的一些操作日志 

也可以对每种操作都搞一个set，里面放执行过这些操作的用户 

bitmap，位图，二进制里的一位一位的，字符串，int，long，double，二进制，都是对应多少多少位的，一个字节是8位的二进制数，int就是4个字节就是32位，你直接可以在redis里操作二进制的位数据 

可以把网站里每一种操作每天执行过的用户都放在一个位图里，一个用户仅仅对应了一位而已 

setbit key user_id 1

getbit key user_id

bitcount key 

一个位图统计100万用户的行为，也不过一百多kb的内存1

### 055_基于GeoHash的你与商铺距离计算的程序

redis里还有一种特殊的数据结构，叫做Geo，可以让你在里面添加很多的地理位置，每个位置对应一个名称和一个经纬度，然后可以利用这个数据结构计算各种距离的位置，获取方圆半径多少1公里内的店铺 

geoadd key longitude latitude user  116.49428833935545 39.86700462665782

geoadd key longitude latitude shop 116.45961274121092 39.87517301328063

geodist key user shop unit=’km’

### 056_陌生人社交APP里的查找附近的人功能

geoadd key longitude latitude user，记录每个用户的地理位置 

georadiusbymember key user radius unit=’km’，radius设置为1，就是附近1公里的用户都找出来，再从返回的set里删除user自己，就可以了16

### 057_带有自动过期时间的分布式缓存实现

redis，我们之前已经讲解了他的一些数据结构，过期时间、pipeline、事务，过期时间，就是说你可以对一个key指定一个过期时间，到了时间之后，自动就让这个数据就过期了，不再生效了 

set key value

expire key timeout

### 058_支持超时自动释放的分布式锁实现

set key value ex=timeout nx=true

### 059_支持自动过期的用户登录会话实现

set key token ex=timeout

### 060_支持冷数据自动淘汰的自动补全程序

expire key timeout16

### 061_支持身份验证功能的分布式锁释放机制 

pipeline = pipeline() // 流水线里的命令，都是一次性打包发送执行的 

try {

​    pipeline.watch(key)   

// key如果有变化，后续提交事务直接失败，unwatch可以取消监控    

lock_value = pipeline.get(key)    

​    if(lock_value == null) {

​       // 锁已经被释放了

} else if(lock_value == user_id) {

​    // 是自己加的锁，可以释放

​    // 先用multi()打开一个事务，事务里的命令一起成功或者一起失败

​    pipeline.multi()

​    pipeline.delete(key)

​    pipeline.execute() // 提交事务，discard可以放弃事务

} else {

​    // 不是自己加的锁，不能让你去释放

}

} catch() {

​    // 如果有异常说明你提交事务的时候有人更新了key

} finally {

​    // 每个流水线都绑定了一个连接   

// 执行完一个流水线，必须执行reset把连接还给连接池

​    pipeline.reset()

} 

stream，发布/订阅，lua

### 062_看看京东之类的大厂是如何做秒杀活动的？

正式切入秒杀架构的讲解，秒杀活动管理系统的一些任务，只是讲，不会带着大家来做，大秒系统是如何配合起来使用的，一套完全独立的大秒/秒杀大促系统，过后都做完了，会直接整合到我们一个真实的电商平台里去 

花了1年的时间，搞定了一个80万行代码的真实大型电商平台，核心业务可以直接对标京东之类的大型电商商城的业务

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\06201.png)

### 063_秒杀活动管理系统的业务职能分析（1）

### 064_秒杀活动管理系统的业务职能分析（2）

### 065_秒杀活动数据是否应该数据库+缓存进行双写？

### 066_秒杀活动数据在Redis中如何利用数据结构存储？

### 067_有了秒杀活动数据之后，就可以做后续的架构了 

秒杀场次，秒杀商品，特别低的价格，有限的数量 

提供一个系统给运营人员来对秒杀活动进行增删改查，专门负责秒杀活动这块的运营人员，他的职责就是每天想办法去搜刮一些比较好的大品牌的热门的单品，然后跟人家品牌方比如说谈一下，说拿下一个最低的价格和折扣 

也有可能是在公司内部申请一定的补贴，每一件商品补贴多少钱，直接把折扣力度降低为5折，这样可以多吸引一些人过来 

再去规划一些秒杀场次出来，每天搞几场秒杀，每一场秒杀活动是哪些商品，每个商品的折扣和数量是多少 

难道得单独给他秒杀活动做一些开发？单独做一些页面？还得重新让APP发一个版本才能让用户看到秒杀活动？必须得有一个系统化的方法，让运营人员可以管理秒杀活动，接下来通过系统就自动把秒杀活动给展示出去，人家就可以参与秒杀活动 

秒杀活动服务/秒杀活动系统/秒杀活动管理系统/秒杀活动管理工作台，就是专门做出来，让你运营人员，可以在里面新建一个秒杀场次，秒杀场次是有一个时间的，这个秒杀场次持续3天，每天都是12点的时候开抢，在秒杀场次里可以加入进去一些秒杀商品，在给秒杀商品设置好对应的折扣和价格，还有设置好可以秒杀的数量 

也可以让一个秒杀场次仅仅是在明天一天，12点，就搞一次，秒杀场次肯定你是可以自己去配置的 

所以接下来你搞好了一个秒杀场次过后，接下来就必须有一个审批流程，你必须让你的运营主管来审核一个秒杀场次，他审核通过了，秒杀场次接下来就是有效的了，可以按照你配置的时间和商品，在APP里就可以展示出来了

秒杀活动数据应该放在哪里？最最起码的肯定得在数据库里写一份，这个是毋庸置疑的，然后你可以让你的秒杀活动系统，对外提供一个接口，这个接口实际上肯定是可以直接把你配置的一些秒杀场次和秒杀商品以及其他的数据提供出去的

APP，展示出来的时候都是可以直接去从后台查询接口，查询出来我当天的秒杀场次有哪几场，分别对应的是哪个时间段，每个场次里有哪些商品，秒杀价格和秒杀数量是多少，都可以在APP里展示出来了 

对秒杀活动的管理以及查询，都可以做到秒杀活动服务里去，秒杀活动系统 

一旦说你配置好了秒杀活动的数据，之后其实一般来说是不会随便变动的，也就是说秒杀活动的数据通常是不会变的，这份数据在库里几乎就不会怎么改了，完全是APP上大量的用户在查询你的秒杀活动的数据 

频繁的查询你的数据库，有没有这个必要呢？ 

所以此时完全可以采取一个数据库+缓存双写的一个模式，在你配置好了秒杀活动以及通过了审核之后，就把这个秒杀活动的数据直接灌入到Redis缓存里去，接着呢，你对外提供的秒杀活动的查询接口，其实可以直接就从Redis里把数据搂出来 

这样的话，你不怎么变动的秒杀数据就可以直接从缓存里走了，没必要去频繁的查询数据库，那是没有必要的，你当日的几个秒杀场次，每个秒杀场次有几个秒杀商品，每个秒杀商品的秒杀价格和秒杀数量 

在Redis里肯定是不可能通过字符串的key-value对来存放的 

每天有几个秒杀场次，你可以搞一个key，seckill::#{当日的日期}::rounds，在里面可以搞一个list，我们需要的是一个有序的数据集合，在里面可以按照一定的顺序放进去多个秒杀的场次，list里存放的可以是秒杀场次的id主键值 

可以直接就读seckill::#{当日的日期}::rounds这个key，可以拿到当日的秒杀场次的主键id的值，遍历这里的主键id的值，可以再接着去拿每一个秒杀场次里的数据，我们需要知道一个秒杀场次他里面有哪些秒杀商品，还有就是他自己对应的时间点是几点 

seckill::round::#{秒杀场次id}::info，可以跟上一个hash数据结构，可以放一些秒杀场次的基本信息，比如最典型的就是秒杀场次开始的时间点，比如说12:00，10:00，当然还可以跟上其他的一些东西 

对于每个秒杀场次都必须可以拿到他的秒杀商品的id集合 

seckill::round::#{秒杀场次id}::products，跟上一个list，有序的数据结构，在里面可以放入你这个秒杀场次要跟上哪些商品，我拿到这些商品id之后，我如果想要获取商品的更多的数据，可以通过商品id走一个商品系统去查询，商品标题和简要的描述 

seckill::#{秒杀场次id}::#{商品id}::info，里面跟上一个hash数据结构，在hash数据结构里可以去包含所有的秒杀商品的数据，包括了秒杀价格和秒杀数量 

综合的运用list和hash两种数据结构 

但是的话呢，还要再考虑到一点，就是说针对APP里展示出来的秒杀活动的场次和商品列表的数据，量肯定是不大的，其次查询的频率可能很高，所以甚至可以做一份完整的缓存，就是说某一次查询你的秒杀活动场次列表和商品列表的时候，把你完整的一份数据都拼装好，拼装成一个超大的字符串 

放在redis里，也可以让客户端做一个缓存，尽可能先用这份缓存好的完整数据去展示就可以了，比如APP端每隔5分钟做一个过期，在服务端让这份完整拼装好的大缓存数据，包含了当天所有的场次和商品，缓存过期时间可以设置为10分钟，10分钟以后过期，就自动重新基于一大堆的key组装一份数据出来 

保证说我们的用户就可以看到我们每天的秒杀活动了

大秒系统，比如说从前端开始，前端策略，我们是不会自己实现的，你要知道，对于秒杀系统的前端有哪些策略在里面，控制url，时钟同步，一系列的东西，很多的，一大堆的策略，但是我们不会自己去实现 

负载均衡这一层，反向代理这一层，秒杀抢购这块怎么做，异步下单这块怎么做的，下单之后的流程怎么做，高并发、高可用、大数据量，一大堆的架构设计，后面就可以完整的来讲解了 

前奏，序曲，开幕，下一周，我会讲的东西就会比较多，我要把整个秒杀系统在前端层面的各种策略都讲完

### 068_秒杀活动页面的动静数据分离方案

秒杀活动管理系统，配置一些秒杀场次、商品、价格折扣、秒杀库存，前端就可以给他展示出来了，仅仅只是一个列表而已，对于这个列表而言，你可以把里面的数据放在缓存里一份，对于前端来说可以做一个缓存，每隔一段时间去加载和刷新一次就可以了 

点击某个秒杀的商品可以进入到一个秒杀商品的活动页面里去 

亿级流量电商详情页系统的架构设计，伪静态化，实际上并没有静态化，动态渲染，每次一个页面请求过来就直接在Nginx层基于lua从多级缓存里加载页面数据，基于模板技术动态渲染成一个静态页面，再返回回去 

纯静态化，每天都需要对全量几亿甚至几十亿个商品的详情页进行一次静态渲染，把每个详情页渲染成html静态页面，可以去死了，性能损耗，时间开销，存储空间，都是成本上无法接受的 

秒杀商品活动页面，是不太适合这种动态渲染的，每天就那么几个商品，其实商品数量是很少的，页面数量是很少的，而且这种页面的访问频率实际高的，因为很多人都会集中去访问这几个秒杀商品活动页面

### 069_秒杀活动商品详情页的静态化方案

实际上任何一个页面，都不可能完全是静态的，因为肯定会涉及一些别的东西，比如说什么用户自己的个人信息了，还有给用户的一些推荐商品了，用户的位置信息了，类似这样的一些东西，都是动态的，所以说对页面而言，那一定是动态分离，静态的部分做静态化，动态的部分就是动态到后台加载 

亿级流量课程里也有静态化方案，不过那个是动态渲染伪静态化方案，因为当时是考虑到了商品详情页可能经常会改动，所以就用了动态渲染伪静态化方案，避免反复的重新渲染生成 海量页面 

不过秒杀活动商品详情页，那是数量很少的，所以可以直接用纯静态化方案，这个静态化其实也非常的简单，最经典的，用freemarker或者velocity这种模板技术都是可以搞定的，这个技术是6个月小白Java培训的技术水平，不细讲了 

有了静态化的页面之后，就可以做一些静态页面的缓存方案了 

所谓的动静分离，意思就是说在秒杀活动页面里，大部分都是静态，之前都已经通过html页面渲染都已经把内容填充好了，商品标题、价格、秒杀数量、乱七八糟的一些东西、商品描述 

动的一块呢，在页面里肯定会有一些比如说你的用户个人信息、登录状态、位置信息、动态个性化推荐内容，这些是属于动态化的

### 070_一个知识科普：到底什么是CDN缓存？

找DNS解析域名，然后DNS智能解析让你去请求距离最近的CDN加载，其实现请求CDN的负载均衡服务器，然后分发请求给CDN缓存服务器，把一个缓存服务器的地址给你，你再去请求缓存服务器 

CDN厂商，他们的CDN服务器遍布全国各地，对于你自己而言，你不可能说在全国各地搞太多的服务器，你完全可以把你的秒杀活动静态页面以及他依赖的一些js、css和图片，都给推送到CDN厂商的服务器上去，距离用户近一些16270984

### 071_基于CDN的秒杀活动静态页面缓存方案

其实秒杀活动静态页面一般分为两个部分，一个部分是把数据都嵌入html以后的静态htm页面，一个部分是html页面引用的js、css和图片，所以这些静态资源都可以直接推送到CDN里去，所谓CDN可以理解为距离用户所在地最近的一台机器，用户的APP和网页加载的时候就直接走CDN了 

假设我们自己秒杀活动页面的Nginx服务器可能就3台，此时如果说你有100万用户同时要访问你的秒杀活动页面，100万请求都会发送到你的台Nginx服务器上来了，这个是必然的 

但是我们可以选择一些分散在全国各地的CDN服务器，购买他们的服务，把我们的页面上推到CDN厂商的在全国各地的服务器上去，假设有100台服务器，100万用户每个人的请求都会找距离自己比较近的CDN服务器 

每台CDN服务器也不过就在一定时间范围内接受了1万次请求而已 

第一个好处，利用散步在全国各地的CDN服务器去分散我们自己的服务器的压力，利用大量的散步各地的CDN服务器就可以轻松抗下高并发的秒杀活动静态页面的读取的请求，都是没问题的 

第二个好处，假设我们公司的机房在上海，此时云南、贵州、还有很多偏远的地方的用户的请求如果说都要走到我们上海的机房来，那么速度就会可能比较慢，但是如果说秒杀活动页面都分散在各个省份，云南、贵州或者其他的偏远省份都有一些CDN服务器，我们都推送了页面之后 

各个地方的用户就距离自己最近的CDN服务器去加载，加载的速度也会块很多 

如果请求回源到服务端，那么html页面因为是静态的，就可以直接放在nginx服务器上，直接本地读取静态页面返回了，至于说js、css和图片，也可以放在nginx上，如果回源过拉就直接本地读取返回了 

尽可能挑选距离用户最近的CDN

### 072_CDN静态数据缓存方案里的失效与命中问题

假设不小心修改了页面，就需要让CDN快速失效缓存，然后重新推送页面过去； 

至于命中问题，就是要选择推送到哪些CDN节点，如果太分散了，那么成本很高，而且失效的时候成本很高，还可能导致命中率很低，所以最好是推送到距离自己用户量最大的地区，而且保证CDN和公司服务器的网络要好 

1000个CDN服务器上都有我们的页面缓存，此时可能有很大一部分CDN服务器并没有太多的人去访问，但是有少部分的CDN服务器因为距离你大部分用户比较近，可能承载的压力就会比较大一些 

而且是缓存在CDN的二级缓存服务器上，那个缓存服务器容量大，集中，缓存命中率会比较高，而且数量不多，缓存失效速度也快 

CDN服务器数量不要太多，尽量都是选择在距离你大部分用户比较近的地方；万一你要失效缓存，此时CDN服务器数量不多，失效的时候速度就会比较快一些，成本也比较低；但是都是距离用户比较集中，缓存命中率整体也高

### 073_停下脚步：一个思考题和一个课后小作业

还把页面缓存到浏览器里去，让浏览器缓存一些htm页面在用户自己本地笔记本电脑的磁盘上，包括手机APP也可以在手机本地缓存一些页面的数据，其实可以是可以，但是不是太靠谱，尽量建议最好是在html里的返回的HTTP响应里，给一些响应头，尽量去禁用浏览器本地的缓存 

课后小作业：去网上找一个CDN厂商，去研究一下人家提供的CDN的产品和服务，包括他们的一些文档，了解一下CDN这个技术的一些细节以及各家厂商提供的CDN是一个什么样的东西

### 074_基于定时授时的前后端时钟同步方案

用户拿到静态页面之后，接着就要等待秒杀活动开始，但是这个是要基于时间来判断的，而且需要进行时钟倒计时，可能页面/客户端和服务端的时间是不同步的，这个完全是有可能的，就比如说有的人把自己的电脑和手机的时钟调整过，所以一般需要前端定时访问后端的一个授时服务接口，进行时钟同步和倒计时

### 075_秒杀抢购接口URL地址的动态隐藏方案

技术高手，自己写一个程序，直接咔嚓一下去访问你的秒杀接口URL地址，他不就可以提前发起秒杀抢购了吗，就避免到了时间之后跟很多人一起去抢购了 

不能直接在前端界面里写死一个url地址去访问后台的秒杀抢购接口，那样别人会提现发现了胡乱访问的，合理的应该是把秒杀抢购接口做成动态URL，一直到秒杀开始前1分钟，才让秒杀前端发送一个请求到后台一个接口，去获取动态的秒杀抢购接口URL地址 

这个秒杀抢购接口URL地址自己本身其实也可以暴露，但是这个接口要求必须带上一个随机字符串的md5加密值，否则一律不允许访问

### 076_基于动态验证码解决机器刷单行为以及前端限流

秒杀系统架构图

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\07601.png)

 

为了避免说秒杀一开始，所有人都瞬间发起请求，几秒钟之内发起了几百万，上千万次请求过来，那是相当的这个恐怖的，一方面为了避免机器刷单，一方面是为了进行前端限流和错流，必须在用户点击秒杀抢购按钮的时候，弹出一个验证码 

弹出一幅拼图，让你拉动拼图拼好，这个都有很多商用验证码解决方案 

一旦每个人都要进行一个拼图验证码的过程，假设100万人平均用30秒去拼好一幅拼图，瞬间就会让你的流量拆分到30秒里去了，每秒3万多请求，此时对我们后台的Nginx服务器的压力一下子就减小了 

https://007.qq.com/product.html 

其实很简单，控制页面上的抢购按钮，秒杀开始前都是灰的，秒杀开始后就让点击一次，立马按钮变灰，不允许反复点击，在30秒内不允许重复抢购，避免有人疯狂的点击按钮，不停的发送请求过来

### 077_负载均衡和、SLB和LVS这些专业名词到底指的是什么？

就是咱们这一次要发布的课程会把负载均衡的架构给大家讲解清楚，不会带着大家来实战，真正在公司里，负载均衡这块的架构部署和运维，都是运维工程师去干的，LVS+KeepAlived+Nginx这样的一套架构，Java工程师是不碰这套架构的

各个互联网公司的主流负载均衡的架构，其实基本上来说都是这样的一套架构 

后续会直接把秒杀系统课程做的架构放到我们的真实几十万行代码的电商平台以及生产环境去跑，但是前端、负载均衡，这一块不会开放给大家真实生产环境的实战权限，因为也没有必要，我们会直接把前端->负载均衡这一套东西给你做好 

SLB -> Server Load Balancer -> 负载均衡 

就是假设我们有一个系统，是一个单块系统，部署很多台机器，此时你的请求每秒有100个请求，一共部署了2台机器，此时你必须让每秒100个请求先到你的负载均衡的设备上去，然后让负载均衡的设备100个请求均匀分发给2台机器，每台机器50个请求 

LVS，就是一种负载均衡的技术，部署在linux服务器上，LVS服务器获取到100个请求，均匀分发给2台部署在Linux服务器上的Tomcat，Web服务器，每台Web服务器拿到2个请求就可以了

### 078_LVS集群的整体架构设计介绍

用户在自己的笔记本电脑上的浏览器里，或者是自己的手机上的APP里，各种点击发送请求到后台系统上去，此时发送请求的时候必定会指定一个域名，或者起码是ip地址，否则的话根本无法连接到后天系统 

域名，此时可以用这个域名到DNS服务器上做一个解析，解析之后指向一个IP地址，可以是LVS服务器的IP地址 

LVS服务器，作为负载均衡调度器，请求到来了以后，直接基于linux内核内部的一些底层机制进行请求转发，转发给后端的Web服务器池子里的一台服务器，本质上就是在进行负载均衡，而且因为LVS主要是基于linxu内核级的一些底层机制做请求转发，所以说效率极高，单机抗每秒几十万甚至上百万吞吐量绝对不是问题 

但是Web服务器都是Tomcat之类的，一般4核8G的服务器每秒1000以内的请求就差不多了，所以说假设发现后端服务器池子承载的请求容量到极限了，此时就可以进行Web服务器的集群扩容 

为什么叫做LVS呢？Linux Virtual Server，就是linux虚拟服务器，也就是说把你一组Web服务器统一成一台虚拟机服务器对外提供服务，外面的人访问你，都是访问你的LVS，看起来就跟只有一台服务器一样

### 079_异地多机房多活LVS集群部署架构介绍

对于小系统一般都是单机房部署就可以了，那么架构其实就是如上面那样说的，但是如果是大系统或者大网站，甚至是国际化的大网站，比如说一个网站要在中国提供服务，也要在美国和欧洲提供服务

在中国很多人要访问他，在美国和欧洲也有很多人要访问他

我们的系统的机房是在哪里呢？假设你的机房在国内的上海或者北京，在美国或者欧洲的人访问你的站点，通过DNS进行域名解析之后拿到的IP地址可能是在你的中国的机房里的一台服务器

此时他的网络请求可能需要从国外走各种网络转发，一路转发到你在国内的机房里的一台服务器，大白话讲网络的课程，这样可能会导致这个请求的速度很慢，就是如果一个网站仅仅在国内部署了机器，那么当你一旦到了国外去访问国内的网站，通常都是比较慢的，如果访问国外的网站，就会比较快了

那么一般都会做异地多机房多活的LVS集群部署，所谓的异地多机房多活，我们仅仅在这里就是先介绍一个概念，以后会逐步深入介绍，甚至是给大家提供真实的阿里云的多机房多活环境来实战

假设是一个国际化的站点，那么实际上就是可以在国内部署一个机房，美国部署一个机房，德国部署一个机房，这异地多机房之间都部署了同样的数据存储，互相之间需要进行数据交换和同步，也就是说所有核心数据都必须在不同地区的机房之间进行同步，才能保证不同地区的用户看到一样的内容

然后网站共享一个虚拟ip地址就可以了，实际上在访问的时候，中国的用户解析虚拟ip地址后会把请求路由到国内的机房，然后国外的用户就会路由到美国或者德国的机房去，这样可以就近去访问距离自己比较近的机房

### 080_基于NAT技术实现的LVS请求转发原理（1）

### 081_基于NAT技术实现的LVS请求转发原理（2）

### 082_基于NAT技术实现的LVS请求转发原理（3）

### 083_基于NAT技术实现的LVS请求转发原理（4）

一般LVS服务器对外提供的是一个virtual ip address，就是虚拟服务器的ip地址，所有客户端/前端都访问这一个虚拟ip地址，其实就是LVS服务器的地址，接着LVS服务器收到请求了之后，会基于NAT技术进行地址改写 

如果运行在用户本地笔记本电脑上的浏览器想要发送请求给我们的后端系统，此时要把请求发送给LVS服务器，最最起码也得基于TCP/IP协议，去跟LVS服务器在TCP/IP这个层面去进行一个TCP三次握手，建立好TCP连接 

接着就是在TCP连接基础之上，去发送HTTP请求报文，把HTTP请求报文发送到LVS上去，LVS再把这个HTTP请求报文转发给我们的Web服务器，也就是Tomcat服务器，Tomcat获取到一个完整的HTTP请求报文，接着就可以进行请求处理 

其实，假设你认为LVS是收到HTTP请求报文然后转发给Web服务器这样的一个原理，那你就完全大错特错了！对LVS的原理的理解就彻底错误了！LVS是工作在四层网络协议上的负载均衡的技术，Nginx他们是工作在七层网络协议上的负载均衡的技术 

第四层的网络协议就是TCP/IP协议，第七层的网络协议就是HTTP协议 

NAT就是Network Address Translation，也就是网络地址转换 

假设此时运行在笔记本电脑上的一个浏览器，此时要发送请求给LVS服务器，先要跟LVS服务器建立一个TCP连接，建立TCP连接大家应该还记得吧？TCP三次握手，互相之间要来回发送几个报文，SYN报文，ACK报文 

LVS负载均衡系统核心代码是挂载在linux内核层面的，所以linux服务器拿到一个建立TCP连接过程中的初始SYN报文之后，这个SYN报文就会由linux内核转交给LVS的核心代码模块，此时LVS核心代码模块会根据负载均衡算法从后端Web服务器池子里挑选出来一台机器，就会把SYN报文之类的其他报文，直接报文转发给Web服务器 

然后就会用NAT技术改写报文里面的目标地址和端口为Web服务器的地址和端口，把报文转发过去 

而且此时客户端（笔记本上的浏览器或者是手机上的APP）一定是想要跟LVS服务器建立一个TCP Socket网络连接的，此时LVS会把跟你这个客户端的Socket连接记录到一个hash表里去，以及这个连接的请求转发到哪个Web服务器了，下次同一个Socket连接再发送后续的报文来的时候，LVS就会把这个连接的报文同样再次转发给上次转发的那台Web服务器去 

LVS看到的以及处理的，包括针对的，都没有一个所谓的HTTP概念，他仅仅是有一个TCP/IP的概念，他是运行在四层网络协议上的一个负载均衡的技术，他根本就不去care你的HTTP协议或者请求 

HTTP请求实际上在底层也是很多的网络包/报文组成的 

假设Web服务器上运行的是一个Tomcat，Tomcat会通过linux内核跟LVS服务器也是建立好TCP连接，然后通过TCP连接，Tomcat会读取到一个一个的完整的HTTP请求，Tomcat理解的是一个完整的HTTP请求 

Tomcat本身是针对七层网络协议里的应用层的HTTP协议，他自己是不跟linux内核打交道，都是Tomcat基于一些API跟Linux内核层面有一个交互，Linux内核层面跟LVS服务器建立好TCP连接之后，然后收取你指定的一份数据，这份数据拿到以后交给Tomcat，Tomcat针对一个完整的HTTP请求对应的数据进行处理 

LVS会对每个连接进行监听，在不同的TCP状态下有不同的超时时间，如果超时没拿到请求，就把这个连接从hash表了删除就可以了084_

### 084_基于IP隧道技术的LVS请求与响应分离原理（1）

### 085_基于IP隧道技术的LVS请求与响应分离原理（2）

### 086_基于IP隧道技术的LVS请求与响应分离原理（3）

### 087_基于IP隧道技术的LVS请求与响应分离原理（4）

Tomcat不断的通过从底层的linux内核层面读取接收到的网络报文里面的数据，直到这些数据可以组成一个HTTP请求，Tomcat会把HTTP请求转交给我们的Servlet，如果说我们用的是Web层的框架，框架的Servlet就会把请求转交给我们自己写的代码 

接下来我们自己写的代码就会处理这个请求，做一系列的事情，比如说更新数据库，查询数据库，更新缓存、es、nosql，都有可能，搞了一通之后，接下来会返回一个响应回去给浏览器/手机APP 

这个所谓的响应返回回去的时候，也是Tomcat通过JDK提供的API，JDK API是通过底层linux提供的一些API，去进行网络上的IO操作，其实linux内核会把你的响应数据的一个一个的报文，返回给LVS服务器 

对于LVS服务器会把你的响应报文返回给你对应的那个客户端去 

对于浏览器或者是手机APP而言，也是不停的通过底层的OS层面（Windows、Android）不停的收取网络数据包，响应报文，假设大家都是基于HTTP协议进行通信和交互的，假设收取数据到一定的程度，感觉收到了一个完整的HTTP响应报文 

接下来可以按照HTTP协议解析HTTP响应，可能是把HTTP响应里封装的一个HTML页面的数据在浏览器里渲染展示出来，也可能仅仅是获取到一个JSON字符串，然后给用户进行一些提示 

LVS最早在实现的时候经过压测，如果说所有的响应报文都依然从LVS来走，如果Web服务器达到20台以上的时候，往往整体的每秒QPS都达到了几万了，此时LVS调度服务器的吞吐量会达到极限，成为一个瓶颈，主要是LVS服务器得处理请求，还得获取响应返回，最起码是他的网络带宽可能就吃不消了 

但是事实上一般来说，大部分的请求都是请求里的数据很少，但是响应的数据很多，所以说可以采取一种技术让LVS服务器处理请求，但是让Web服务器自己就直接把响应返回给用户笔记本上的前端或者手机上的APP 

IP隧道技术，是IP tunneling，也叫做IP封装技术，IP encapsulation 

大家只要理解一下就好了，因为涉及到比较专业的网络知识，一般都是运维工程师玩儿的，还涉及到一些网络硬件设备的知识，所以我们只要大致理解这么个思路就ok了，大致来说，就是每一台Web服务器都把自己的vip配置在ip隧道设备上 

然后LVS拿到请求之后，把目标地址为Web服务器的vip的请求报文封装到另外一个IP报文里，接着转发IP报文给一台Web服务器，Web服务器拿到IP报文以后解析一下，拿到里面的请求报文，发现目标地址就是一个vip，结果这个vip就配置在自己的IP隧道设备上，此时就可以处理这个请求 

Web服务器想要去发送响应，响应发送给谁呢？已经不是LVS了，他此时会根据你的报文里的一些信息，然后Web服务器直接根据自己的路由表，找到需要返回响应的客户端，把响应报文返回给指定的客户端就可以了 

他这个里面涉及到的大量的概念根本不在Java工程师的知识体系里，除非说大家有一个非常完整的网络相关的整套知识体系，大白话讲网络的课程，远远不够，大学是计算机科班出身的程序员，计算机网络 

还有一种直接路由技术，也跟这个IP隧道技术是类似的，大家肯定有很多疑问，但是不要管他了，只要知道有这么一两种方法，LVS服务器可以把请求转发给Web服务器，然后Web服务器的响应直接返回给用户就可以了 

这样LVS的吞吐量会大幅度提升，一台调度服务器支撑后端Web服务器池子几十台都没问题的1627098499

### 088_LVS的多种负载均衡算法讲解和介绍

接下来讲讲都有哪些负载均衡算法，实际上这个问题一般也是互联网公司的里常见面试题之一，以前确实没系统的讲解过，大家可以好好听听 

最简单的就是round robin了，就是每个请求不停的转发给下一个服务器，如果服务器挂了就摘除，就这么简单，但是如果不同的服务器处理速度和吞吐量不一样，必然会导致性能较差的服务器最终就扛不住了，请求一直阻塞 

如果做了weighted加权处理之后，就可以给不同性能的服务器设置不同的权重，权重较大的可以获得更多的请求，权重较低的可以获取较少的请求，这样可以保证各个服务器相对较为均衡一些 

但是这样也有一点问题，就是你设置的权重如果不太准确的话，也未必合理 

本质上LVS服务器转发请求给Web服务器，互相之间也是建立底层的Socket TCP连接，所以说LVS可以记录下来每一台Web服务器目前建立了多少个连接了，每次有新的请求来就把新的连接给连接数量最少的服务器就可以了 

但是这么做其实也有一样的问题，如果不同服务器的处理速度不一样，那每台服务器都处理一样数量的连接，绝对是有问题的 

所以可以做加权处理，设置不同服务器的权重，这样在建立连接的时候会让服务器的连接数量的比例和各个服务器的权重比例都是成正比的 

假设把Web服务器改成缓存服务器，每次请求目标ip都是针对某台缓存服务器的，尽量让针对同一个目标ip的请求都到一台缓存服务器上去，这样的话呢，就可以让你的缓存命中率比较高一些 

这个是比较适合缓存集群的，也就是说假设请求是直接转发给缓存集群的话，一般尽量是让一个目标IP的请求路由到上次发送过去的那个缓存服务器上去，这样可以提高缓存命中率，但是前提是那台服务器的连接数量不能太高 

而且他有一个优化，就是一个目标IP可以映射到一组服务器，假设一台服务器超载了，那就路由到其他服务器，此时连接映射到这一组服务器上去，如果一段时间没请求了，就可以把连接里映射的部分服务器摘除 

这个就是之前讲过的，对于连接里的源地址或者是目标地址，可以把他路由到的服务器记录到hash表里去，下次还是继续路由到那台服务器上去 

动态反馈负载均衡算法 

这个也很好理解，就是收集和监控各个服务器的请求处理时间和负载情况，综合下来判定一下每台服务器大致每秒可以处理多少请求的吞吐量，当前负载情况如何，来决定把请求抓饭给谁 

round robin + 加权权重，随机 + 加权权重，hash负载均衡（根据请求参数里的某个值，把这个固定的值算一个hash，就固定路由到那台服务器上去），动态反馈负载均衡算法

### 089_LVS的Linux内核级实现原理（1）

LVS实际上是在linux内核里修改了TCP/IP协议栈，这样可以对收到的请求直接在linux内核层面进行地址改写和转发，所以因为他运行在内核层面，这样才能让他的性能和吞吐量都极为的好

LVS有一个IPVS模块挂载在了内核的LOCAL_IN链和IP_FORWARD链两个地方，一个IP报文到达的时候，如果目标地址是virtual ip address，就会转交给LOCAL_IN链，会被挂载在LOCAL_IN链上的IPVS模块处理

IPVS模块正常情况下会根据负载均衡算法选择一个后端服务器，把报文进行改写和转发，接着会在hash表了记录这个连接和转发的后端服务器地址，下次如果这个连接的报文再到达的时候，就直接根据hash表里的连接对应的服务器地址，直接转发

然后NAT方式改写和转发过去的报文响应回来的时候，会被挂载在IP_FORWARD链上的IPVS模块捕获，接着进行改写响应报文的地址，返回给用户

hash表里的一个连接数据只要128字节，所以一般服务器可以调度几百万个连接都没问题

### 090_LVS的Linux内核级实现原理（2） 

如果说用linux内核提供的定时器来做 

Hash表里的连接会设置一个定时器，连接超时的时候就会回收这个连接，但是如果有几百万个连接，那么可能会导致一个很严重的问题，就是有几百万个超时器，这么多超时器一起运行会导致很大的负载 

LVS比较早期的版本就是用linux内核的定时器去做的，如果说你的连接数量较大了以后，此时linux服务器的cpu负载就会非常的高 

实际的实现思路，是用了kafka里的时间轮机制，大家可以具体去看看kafka里的时间轮机制，在大数据的kafka的课程里已经讲解过如果在内存里要进行数万甚至是数十万的任务的超时监控，最好别每个任务都设置一个定时器，那样其实对cpu负载和内存的消耗，都是极大的，是不靠谱的 

kafka就是设计了几个时间轮，不同的时间轮的时间周期是不同的，然后把不停的超时时间的连接放在不停的时间轮的格子里，让一个或者多个指针不停的旋转，每隔一秒让指针转一下，就可以让不同的时间格里的连接超时失效 

把大量的超时任务放在时间轮里，你就几个指针不停的旋转，首先不是每个任务都有定时器，减少了内存消耗，其次没有那么多的定时器一起转，那么cpu负载就不会太高了，那么就可以接受了 

LVS而言，他的Hash表里的大量的连接的超时监控，也是通过多个时间轮来实现的，原理说实话跟Kafka可以说是几乎一模一样 

Hash表的分段锁，把Hash表拆成了很多个小分段，不同的分段一把锁，这样可以降低锁的粒度，减少高并发过程中的锁的频繁冲突，跟ConcurrentHashMap是一个原理，对不对，不同的技术搞到最后，都是想通的1627098499

### 091_基于七层网络协议的负载均衡技术是如何运作的？

LVS绝对是运作在四层网络协议上的负载均衡的技术，对他来说，根本就没有HTTP这样的一个概念，他仅仅只是关注最最底层的一些网络报文而已 

假设，如果我们要是想做七层网络协议的负载均衡的技术，也就是说基于HTTP请求去进行负载均衡和请求转发 

最大的问题在于，需要先多次报文过后建立好一个TCP连接，接着拿到通过TCP连接发送过来的完整HTTP协议的请求，然后把这个请求从内核空间切换到用户空间，交给用户空间运行的一个负载均衡技术去进行处理，根据请求里的一些内容进行转发给后端服务器，再次切换到内核空间，跟后端服务器建立TCP连接，把请求发送过去 

拿到的响应先是从内核空间转交给用户空间的负载均衡技术，接着再把响应通过内核发送回去 

这会导致大量的内核空间和用户空间的切换 

所以一般一旦涉及到了用户空间的系统运行，单机也就是抗每秒最多1000或者几千个请求，或者高配置服务器下是每秒几万请求，但是并发和吞吐远远低于LVS，LVS一般抗个每秒几万到几十万的请求都不是问题，甚至是百万并发都有可能实现 

但是好处在于可以根据HTTP请求进行路由转发

### 092_为什么通常把四层协议的LVS和七层协议的Nginx一起使用？

### 093_KeepAlived+LVS高可用架构以及Nginx集群高可用

LVS绝对是运作在四层网络协议上的负载均衡的技术，对他来说，根本就没有HTTP这样的一个概念，他仅仅只是关注最最底层的一些网络报文而已 

假设，如果我们要是想做七层网络协议的负载均衡的技术，也就是说基于HTTP请求去进行负载均衡和请求转发 

最大的问题在于，需要先多次报文过后建立好一个TCP连接，接着拿到通过TCP连接发送过来的完整HTTP协议的请求，然后把这个请求从内核空间切换到用户空间，交给用户空间运行的一个负载均衡技术去进行处理，根据请求里的一些内容进行转发给后端服务器，再次切换到内核空间，跟后端服务器建立TCP连接，把请求发送过去 

拿到的响应先是从内核空间转交给用户空间的负载均衡技术，接着再把响应通过内核发送回去 

这会导致大量的内核空间和用户空间的切换 

所以一般一旦涉及到了用户空间的系统运行，单机也就是抗每秒最多1000或者几千个请求，或者高配置服务器下是每秒几万请求，但是并发和吞吐远远低于LVS，LVS一般抗个每秒几万到几十万的请求都不是问题，甚至是百万并发都有可能实现 

但是好处在于可以根据HTTP请求进行路由转发 

四层协议的LVS和七层协议的Nginx（本身就是专门处理HTTP请求），为什么通常在负载均衡这一层会一起来使用呢？走七层协议进行负载均衡性能远远不如LVS，如果没有LVS，仅仅就是用多台Nginx服务器组成的集群去接收所有的请求，去转发请求给后端的Web服务器 

在负载均衡这一层，在最外侧，通常都是部署一个LVS作为核心的负载均衡的设备，通过大连的优化，可以很轻松的做到单机百万级的并发量，缺点，没有办法根据HTTP请求的内容去进行一些高阶的功能和转发 

Nginx可以拿到完整的HTTP请求，可以做很多高阶的负载均衡的功能，在功能上，Nginx可以做很多很多的事情，甚至可以在Nginx里嵌入lua脚本，在Nginx本地处理请求，读取缓存，亿级流量的课程里，就有这种应用

### 094_国内主流互联网公司的负载均衡架构总结

LVS架构原理

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\09401.png)

### 095_Nginx到底在互联网架构中是用来干什么的？

LVS，一般来说不用Java工程师了解LVS的存在，运维全部帮你搞定了，如果你需要让一个域名指向你的系统，很多Java工程师根本都没有域名的概念，对外提供的就是一个系统或者是一个服务，人家来调用你 

高级/资深Java工程师，对LVS不了解也都没关系，大家都是做系统的，不是搞运维的，IT运维，域名，DNS，HTTPS+SSL证书，服务器硬件采购，操作系统，linux，网络，监控，容器，docker/k8s，都是他们搞的

 负载均衡，Nginx，邮件系统部署，都是他们干的 

Java架构师，LVS和Nginx有一定的了解，甚至如果你是在初创型的就几个人的小公司里，没有职位的区分，运维、DBA、基础架构、Java系统开发、大数据、机器管理、容器，所有东西都得你这个所谓的小公司里的Java工程师来，如果是现在阿里云、腾讯云，采购一整套的基础的一些东西

LVS是什么，干什么的，为什么要用他，怎么用，什么情况下用，核心架构原理是什么，如何工作的，Nginx是什么，干什么的，为什么要用，怎么用，核心原理是什么，一般用他来干什么

哪怕你是Java架构师，都不需要你具备LVS和Nginx的实操能力，真的轮不到你去实操，一定是运维去干的，如果是几个人的创业型小公司，一般都是直接采购云平台的东西了，就算你要用，也仅仅是通过云平台上的LVS和Nginx做一点简单的配置就可以了，做点简单的运维，人家都提供界面给你了 

两周学习Nginx足以，学这些东西仅仅两周足以1627098499

### 096_Nginx的配置文件里都包含哪几块内容？

亿级流量电商详情页系统架构实战，Nginx+Lua做很多复杂的操作，有没有可能是你自己作为一个Java开发人员直接摸到Nginx部署的服务器上去，直接自己操作在Nginx里部署Lua脚本，万一你要是一个不留神，直接Nginx挂掉 

即使是用Nginx+Lua去进行一个开发，通常来说Nginx都是运维去管理的，他是理解Lua脚本如何使用和部署的，如果说你在自己本地部署一个Nginx，自己可以在里面放一个Lua脚本，自己调试一下 

把这个脚本提供给你的运维，运维可以代替你去进行比较稳妥的Lua脚本的部署，如果运维 团队比较给力，甚至可以把基于Nginx部署Lua脚本这个事情做成一个Web界面平台，你可以通过一个Web界面去部署你的Lua脚本 

作为Java工程师，你有这个精力去深入阅读Nginx内核源码吗？你有这个技术实例敢拍着胸脯说，可以搞定Nginx吗？也是需要非常有经验，有多年Nginx运维和维护经验，看过内核源码的一些资深运维工程师去维护他 

Nginx编译打包，部署，启动停止，目录结构，配置文件的编写，根本没有必要带着大家动手一步一步的去玩儿Nginx的。Nginx一般怎么用，其实就是修改Nginx配置文件的一个过程，通过给他加各种各样的配置，就可以去用Nginx的功能，用他来干什么 

\# worker进程

worker_processes 3; 

\# 这里配置的是nginx和用户之间的网络连接的一些东西

events {  

} 

http {

​    include  mime.types;

​    default_type application/octet-stream;

​    sendfile on;

​    keepalive_timeout 65;

​    \# server是虚拟主机的概念

​    server {

​       listen   80;

​       server_name localhost;

​       \# 每一个location就代表一种请求如何处理

​       location / {

​           root html;

​           index   index.html;

}

error_page  500 502 503 504 /50x.html

location = /50x.html {

​    root    html;

}

}

}

### 097_基于Nginx部署的一个简单Web站点配置示意（1）

### 098_基于Nginx部署的一个简单Web站点配置示意（2）

user nobody nobody;  # 意思就是所有linux用户和用户组都可以启动nginx

worker_processes 3;   # 这就是nginx的worker进程数量，他自己也有一个主进程

error_log logs/error.log;   # nginx服务器运行的错误日志存放路径

pid nginx.pid;   # nginx服务器运行时的pid文件存放路径 

\# 这里配置的是nginx和用户之间的网络连接的一些东西

events { 

​    user epoll;   # 配置网络通信的事件驱动模型，这里用epoll

​    worker_connections 1024; # 最大有多少个客户端跟nginx可以建立连接

} 

http {

​    \# 下面两个定义了能识别的多媒体类型

​    include  mime.types;

​    default_type application/octet-stream;

​    \# 配置使用senfile方式进行网络数据传输

​    sendfile on;

​    \# 这是配置连接的超时时间

​    keepalive_timeout 65;

​    \# 配置请求日志的格式

​    log_format  access.log   ‘$remote_addr-[$time_local]-“$request”-“$http_user_agent”’; 

\# server是虚拟主机的概念

​    server {

​       listen   80; # 这个虚拟机主键监听的端口号和主机名

​       server_name localhost;

​       \# 下面是配置针对这个虚拟主机的请求日志的存放地址

​       access_log   /user/local/nginx/log/access.log

​       \# 配置请求错误的界面

error_page  404 /404.html    # 这个目录会找到location root目录/404.html

​       \# 每一个location就代表一种请求如何处理

​       location /server1 { # 收到/server1/user.html请求

​           \# 会到/server-html目录下，找你请求的/server1/user.html页面

​           root /server-html;

​           \# 站点默认页面，请求/server1，就到/server-html/server1下，找index.html

​           index   index.html;

} 

location xxx { 

}

} 

}

### 99_Nginx核心功能以及在互联网架构里的位置是什么？ 

Nginx自己的性能、并发和吞吐量，对他的网络、压缩以及各种高阶的内核级的参数去进行一些配置和调优，下周会稍微涉及到一些，优化Nginx并发能力的一些内核参数如何调优；如果你仅仅是用Nginx去实现一些功能，功能性的配置 

server+location，监听的端口号，location主要是配置对收到的http请求如何进行处理，还不止是location，如果是对于你收到的http请求，目前看到的是仅仅是读取本地的html文件去进行返回 

下周会讲解，nginx如何做反向代理和其他的高阶的功能，收到了一个http请求之后，转发给后端的一些Tomcat服务器，做一个负载均衡，Tomcat服务器去读写数据库，甚至Tomcat本身就是一个Servlet+JSP的容器，JSP是模板技术，里面可以嵌入进去一些html静态代码，在html静态代码里还可以嵌入动态执行的java代码 

如果你Tomcat是用JSP来运行，可能是执行一些JSP里嵌入的java代码，提取一些数据，做一些crud的操作，然后把数据填充到JSP里的静态html代码里去，把一个最终完整的html页面返回给nginx，nginx再把html页面返回给浏览器 

同步和异步是通信模式，常见的描述是RPC或者网络通信里的说法，比如同步RPC或者异步RPC，同步就是调用方一直等待响应，异步就是调用方法出请求后就返回，直接干别的事儿了，之前的请求结果等待回调 

那阻塞和非阻塞一般是说进程的IO调用，更多地也是说基于Socket的IO操作，阻塞IO和非阻塞IO，比如阻塞IO，就是线程挂起，等待IO结果；非阻塞IO就是说，线程不会挂起等待一个IO操作的结果 

同步阻塞：同步，说的是客户端发出请求，一直同步等待响应；然后服务端收到请求之后要执行IO操作，比如磁盘IO，网络IO，数据库IO，等等，此时针对所有的IO操作，都会阻塞等待IO结果，无论是网络IO调用其他服务，还是磁盘IO读写本地文件，或者网络IO读写数据库 

同步非阻塞：同步是说，客户端发出请求一直等待响应，服务端收到请求，发现IO操作没法直接完成，直接去干别的了，但是此时不返回响应给客户端，等非阻塞IO完成了，再把结果返回给客户端 

异步阻塞：客户端发出请求不等待响应，服务端收到请求后阻塞式IO，完了再通知客户端，一般这种不会有 

异步非阻塞：客户端发出请求不等待响应，服务端收到请求后非阻塞IO，IO不能立马完成就去干别的，等IO搞定了再通知客户端

### 100_同步异步以及阻塞非阻塞的区别和关系（1）

### 101_同步异步以及阻塞非阻塞的区别和关系（2）

同步和异步是通信模式，常见的描述是RPC或者网络通信里的说法，比如同步RPC或者异步RPC，同步就是调用方一直等待响应，异步就是调用方法出请求后就返回，直接干别的事儿了，之前的请求结果等待回调 

那阻塞和非阻塞一般是说进程的IO调用，更多地也是说基于Socket的IO操作，阻塞IO和非阻塞IO，比如阻塞IO，就是线程挂起，等待IO结果；非阻塞IO就是说，线程不会挂起等待一个IO操作的结果 

同步阻塞：同步，说的是客户端发出请求，一直同步等待响应；然后服务端收到请求之后要执行IO操作，比如磁盘IO，网络IO，数据库IO，等等，此时针对所有的IO操作，都会阻塞等待IO结果，无论是网络IO调用其他服务，还是磁盘IO读写本地文件，或者网络IO读写数据库 

同步非阻塞：同步是说，客户端发出请求一直等待响应，服务端收到请求，发现IO操作没法直接完成，直接去干别的了，但是此时不返回响应给客户端，等非阻塞IO完成了，再把结果返回给客户端 

异步阻塞：客户端发出请求不等待响应，服务端收到请求后阻塞式IO，完了再通知客户端，一般这种不会有 

异步非阻塞：客户端发出请求不等待响应，服务端收到请求后非阻塞IO，IO不能立马完成就去干别的，等IO搞定了再通知客户端

### 102_抗下互联网高并发的服务器架构模式

搞一个主进程，收到一个请求就交给一个子进程处理，预先生成一大波子进程，然后处理完请求不要回收子进程，而是继续进行池化管理，Apache服务器就是这种多进程模式，但是现在是不太流行了 

nginx其实就是多进程模式，如果你是高配置的物理机，其实可以开辟很多的进程出来，大量的进程可以高性能高并发的处理瞬间来的大量的高并发的请求 

这个其实IIS服务器，或者是Tomcat轻量级Servlet容器服务器，都是这种多线程模式，我们平时常用的主要就是Tomcat，就说说Tomcat好了，他就是多线程抗高并发，一般会基于NIO来做，有限线程资源和数量，抗下大量的高并发 

比如说，你要用tomat部署一个系统，一般来说，除非你是用的spring boot，都是你需要自己手动在线上的服务器里先启动一个Tomcat，起码得先部署一个tomcat，接着每次你要部署你的系统，你需要把你的系统打成符合tomat要求的war包 

放到tomcat指定目录下去，接着启动/重启/tomcat热加载war包，tomcat启动之后，会拿到你的war包里的代码，类以及你依赖的第三方的jar包里的类，全部都会被tomcat加载，tomcat自己就是用java写的一个java系统 

tomcat自己的java代码，就是用nio之类的JDK底层的网络包，监听指定的端口号，跟nginx建立网络连接，然后接收http请求，把每个请求比如交给一个工作线程去处理，他的工作线程会调用我们部署的系统的代码，比如说servlet、filter之类的东西，jsp 

我们写的代码做各种各样的事情就可以了 

tomcat本身启动就是一个jvm进程，他就是一个单进程的web服务器系统，只不过会执行我们自己写的系统的war包，他属于用多个工作线程并发处理多个请求，每一个线程同一时间只能处理一个请求 

### 103_Nginx的通信IO模型、模块化架构和并发架构总结

Nginx本身虽然是处理HTTP请求的，也就是跑在七层网络协议上的，但是他也是可以做到高并发的，他用的网络通信架构师异步非阻塞，并发架构师多进程机制，他在启动之后是一个主进程和多个子进程，子进程负责处理客户端请求，用的是异步非阻塞模式 

主进程负责建立、绑定和关闭socket网络连接 

一个worker进程收到请求之后，客户端可以直接去干别的，本身这可以是异步模式的，接着worker进程判断如果IO（比如读取本地磁盘的html，或者是请求后端的Tomcat服务器）不能立马处理，那么就可以去处理别的请求，等IO执行完毕之后，linux内核通知worker进程，worker进程再通知客户端 

异步非阻塞，保证了他一个worker进程就可以实现高并发的网络通信架构，处理高并发请求，同时多个worker的多进程机制，也可以让他实现针对高并发的并发模型，只要你的物理机配置足够高，可以多加一些worker进程 

nginx把自己内部的大量的功能做成了很多的模块，别人要对nginx内核做一个扩展，很容易的，直接按照他的规范开发一个模块，嵌入到nginx里去执行就可以了，nginx处理请求的时候，会先调用很多的模块对请求做一个内存里的处理

要做IO的时候，就提交给内核去执行，非阻塞的IO 

多进程架构 + 模块化架构 + 异步非阻塞架构，就是nginx最核心的几大架构1627098499

### 104_Nginx实现高并发高吞吐的核心技术是什么？ 

如果在4核8G的服务器上部署一个Tomcat，里面有一些线程并发运行处理请求，Tomat服务器上部署的系统一般每秒就处理几百请求到1000请求就撑死了，但是为什么Redis、Nginx、LVS每秒也能轻松上万请求 

你每个线程拿到一个请求，全部得后续执行阻塞式IO，访问数据库也属于通过网络IO跟数据库通信，这也是阻塞IO，你访问Redis或者ES，都是在走网络IO跟他们通信，都是属于阻塞式IO 

一个线程每秒才处理5~10个请求，100个线程，500个请求~1000个请求，4核8G的服务器的CPU负载就满了，没法处理更高的并发请求了 

nginx是非阻塞的IO，把IO交给内核，自己就去处理别的请求了，等linux处理完请求了再主动通知nginx进程，这个linux内核工作的过程就叫做事务驱动模型，他并不是让nginx自己不停的轮询IO操作进度，而是linux内核主动通知他 

linux内核提供的select、poll、epoll、kqueue都是做非阻塞IO的，进程可以快速处理请求，快速提交IO请求给linux内核，自己不用阻塞等待IO请求结果，所以进程是绝对可以高并发处理的，不会卡在一个IO上 

这里的IO指的一般是请求后端的Tomcat服务器 

select，他是说，针对关注的事件创建描述符集合，包括read、write和exception三种事件描述符集合，然后调用linux内核的select()函数，如果内核返回这个集合的时候，自己轮询三个集合里的事件描述符，不停的看事件是否发生 

poll，也是先创建事件描述符集合，然后去轮询事件描述符集合，select要创建三个集合，poll只要创建一个集合 

epoll，上面两个都需要进程自己轮询和管理描述符集合，epoll是完全交给内核去做，调用内核接口创建一个有N个描述符的事件列表，给描述符设置自己关注的事件，接着完全是内核自己轮询和管理，有事件发生就回调通知进程 

一般都是用epoll

### 105_Nginx使用什么样的IO事件驱动模型？

Nginx架构原理

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\10501.png)

如果在4核8G的服务器上部署一个Tomcat，里面有一些线程并发运行处理请求，Tomat服务器上部署的系统一般每秒就处理几百请求到1000请求就撑死了，但是为什么Redis、Nginx、LVS每秒也能轻松上万请求 

你每个线程拿到一个请求，全部得后续执行阻塞式IO，访问数据库也属于通过网络IO跟数据库通信，这也是阻塞IO，你访问Redis或者ES，都是在走网络IO跟他们通信，都是属于阻塞式IO 

一个线程每秒才处理5~10个请求，100个线程，500个请求~1000个请求，4核8G的服务器的CPU负载就满了，没法处理更高的并发请求了 

nginx是非阻塞的IO，把IO交给内核，自己就去处理别的请求了，等linux处理完请求了再主动通知nginx进程，这个linux内核工作的过程就叫做事务驱动模型，他并不是让nginx自己不停的轮询IO操作进度，而是linux内核主动通知他 

linux内核提供的select、poll、epoll、kqueue都是做非阻塞IO的，进程可以快速处理请求，快速提交IO请求给linux内核，自己不用阻塞等待IO请求结果，所以进程是绝对可以高并发处理的，不会卡在一个IO上 

这里的IO指的一般是请求后端的Tomcat服务器 

select，他是说，针对关注的事件创建描述符集合，包括read、write和exception三种事件描述符集合，然后调用linux内核的select()函数，如果内核返回这个集合的时候，自己轮询三个集合里的事件描述符，不停的看事件是否发生 

poll，也是先创建事件描述符集合，然后去轮询事件描述符集合，select要创建三个集合，poll只要创建一个集合 

epoll，上面两个都需要进程自己轮询和管理描述符集合，epoll是完全交给内核去做，调用内核接口创建一个有N个描述符的事件列表，给描述符设置自己关注的事件，接着完全是内核自己轮询和管理，有事件发生就回调通知进程 

一般都是用epoll 

模块化，通过各种不同的模块，可以实现很多的高阶功能，正向代理，反向代理，请求rewrite，负载均衡1627098499

### 106_Nginx读取本地静态HTML文件和商品详情页架构的关系

nginx直接读取本地的静态html文件返回给浏览器，理论结合实践，亿级流量商品详情页的课程里，如果做商品详情页这一块的系统架构，小网站，每次如果你在商品管理系统里更新和维护了商品的数据，此时直接静态化生成商品的详情页的html，推送到nginx服务器上去了

nginx服务器里的配置文件，配置一些指定格式的url，/product/iphone5.html，每个商品详情页都是一个html，按照一定的名字推送到nginx服务器的指定目录下去，用户浏览一个一个的商品详情页的时候，直接请求到html这里，本地就直接读取html了

sever {
	listen 80;
	server_name	view.website.zhss;
	rewrite	^/ http://www.zhss.com/;
}

请求http://view.webbsit.zhss/，域名解析到nginx上，nginx判断是自己这个server的，直接做一个域名跳转，最终请求是http://www.zhss.com/去反馈的

server {
	listen 80;
	server_name	view.website.zhss	view.website.info;
	if($host ~ website\.info) {
		rewrite ^(.*) http://view.website.zhss$1 parameter;
}
}

请求http://view.website.info/order，会跳转给http://view.website.zhss/order

server {
	listen 80;
	server_name view1.website.zhss view2.website.zhss;
	if ($http_host ~* ^(.*)\.website\.zhss$) {
		rewrite ^(.*) http://view.webite.zhss$1;
		break;
}
}

请求http://view1.website.zhss/order，http://view2.website.zhss/order会跳转给http://view.website.zhss/order

### 107_Nginx本地为商品详情页HTML提供JS和CSS等静态资源

Nginx除了读取本地的静态html资源之外，还可以在里面放图片、css、js，都可以的，视频文件也是可以的，比如说你有一个iphone.html，在里面肯定会引用很多的图片、css和js，甚至视频，引用的地址也是一个url地址，浏览器对每个图片、css和js、视频，都会按照他的url地址，去发送一个http请求 

打开你的浏览器的开发调试工作台，然后任意请求一个html页面，此时你会发现他一定会按照html页面里的标签代码，去发送多个请求，加载图片、css、js，这些都是通过http请求去加载过来的 

在nginx本地还可以在指定目录下，去放一些网站依赖的静态资源文件，比如说，/css/zhss_website.css，直接配置location，如果匹配上/css的请求直接在nginx本地xx目录下找一个zhss_website.css文件就可以了，js、image、vedio 

尽量避免把一些静态的资源，html、js、css、image、vedio放在tomcat部署的web服务器上，tomcat，无非就是类似/css/zhss_website.css的请求到了tomcat，tomcat本身也是可以在本地磁盘上，读取你的项目指定目录里的某个css文件返回回去就可以了 

tomcat本身定位应该是做java web的servlet/jsp的容器，跑java web的系统，最好是一些动态请求，crud了再到tomcat里去执行 

sever {

​    listen 80;

​    server_name view.website.zhss;

​    rewrite  ^/ http://www.zhss.com/;

} 

请求http://view.webbsit.zhss/，域名解析到nginx上，nginx判断是自己这个server的，直接做一个域名跳转，最终请求是http://www.zhss.com/去反馈的 

server {

​    listen 80;

​    server_name view.website.zhss view.website.info;

​    if($host ~ website\.info) {

​       rewrite ^(.*) http://view.website.zhss$1 parameter;

}

} 

请求http://view.website.info/order，会跳转给http://view.website.zhss/order 

server {

​    listen 80;

​    server_name view1.website.zhss view2.website.zhss;

​    if ($http_host ~* ^(.*)\.website\.zhss$) {

​       rewrite ^(.*) http://view.webite.zhss$1;

​       break;

}

} 

请求http://view1.website.zhss/order，http://view2.website.zhss/order会跳转给http://view.website.zhss/order 

server {

​    listen 80;

​    server_name mirror1.webiste.zhss;

​    rewrite ^(.*) http://view1.website.zhss$1 last;

} 

server {

​    listen 81;

​    server_name mirror2.website.zhss;

​    rewrite ^(.*) http://view2.website.zhss$1 last;

} 

域名镜像，多个站点互为主备

### 108_基于Nginx的Rewrite功能实现域名跳转和域名镜像 

nginx读取本地静态的html、js、css、image、vedio这些资源文件，他的意义在哪儿，一般来说对于这些静态资源，他往往是放在nginx本地是没问题的，尤其是对一些中小型站点，也没必要专门搭建图片服务器，图片服务器，CDN缓存推送 

对于这个view.website.zhss域名，你可以在DNS里对这个域名做一个解析，解析的ip地址可以直接到nginx这里来，nginx如果说收到了这个请求之后，会发现请求里的域名是view.website.zhss 

ngxin就会跟自己配置文件里的各个server做一个匹配，nginx里配置的每一个server都是一个独立的域名，但是不同的域名其实都可以解析到nginx这里来，只不过说nginx这里收到针对他的不同域名的请求之后，可以去匹配不同的serve 

sever {

​    listen 80;

​    server_name view.website.zhss;

​    rewrite  ^/ http://www.zhss.com/;

} 

请求http://view.webbsit.zhss/，域名解析到nginx上，nginx判断是自己这个server的，直接做一个域名跳转，最终请求是http://www.zhss.com/去反馈的 

server {

​    listen 80;

​    server_name view.website.zhss view.website.info;

​    if($host ~ website\.info) {

​       rewrite ^(.*) http://view.website.zhss$1 parameter;

}

} 

请求http://view.website.info/order，会跳转给http://view.website.zhss/order 

server {

​    listen 80;

​    server_name view1.website.zhss view2.website.zhss;

​    if ($http_host ~* ^(.*)\.website\.zhss$) {

​       rewrite ^(.*) http://view.webite.zhss$1;

​       break;

}

} 

请求http://view1.website.zhss/order，http://view2.website.zhss/order会跳转给http://view.website.zhss/order 

server {

​    listen 80;

​    server_name mirror1.webiste.zhss;

​    rewrite ^(.*) http://view1.website.zhss$1 last;

} 

server {

​    listen 81;

​    server_name mirror2.website.zhss;

​    rewrite ^(.*) http://view2.website.zhss$1 last;

} 

域名镜像，多个站点互为主备

### 109_基于Nginx的Rewrite功能实现独立域名和防盗链

server {

​    listen 80;

​    server_name view1.website.zhss view2.website.zhss;

​    if ($http_host ~* ^(.*)\.website\.zhss$) {

​       rewrite ^(.*) http://view.webite.zhss$1;

​       break;

}

} 

请求http://view1.website.zhss/order，http://view2.website.zhss/order会跳转给http://view.website.zhss/order 

server {

​    listen 80;

​    server_name mirror1.webiste.zhss;

​    rewrite ^(.*) http://view1.website.zhss$1 last;

} 

server {

​    listen 81;

​    server_name mirror2.website.zhss;

​    rewrite ^(.*) http://view2.website.zhss$1 last;

} 

域名镜像，多个站点互为主备 

server {

​    listen 80;

​    server_name forum.website.zhss;

​    rewrite ^(.*) http://www.zhss.com/forum$1 last;

} 

一般来说一个公司，他会有一个顶级域名，zhss.com，官网可以是[www.zhss.com](http://www.zhss.com)，你可能会有多个二级域名绑定到你的不同的系统上去，电商网站，[www.zhss.com](http://www.zhss.com)，第三方卖家，seller.zhss.com，把这个二级域名都跳转到商家系统，http://192.168.3.101:8080/seller$1 last，就可以把针对二级域名的请求都做一个跳转 

一个公司可能会提供多个二级域名，多个二级域名都可以配置在nginx里，对应多个server，配置的跳转就是不同的二级域名的请求转发给不同的系统的服务器所在地址就可以了 

server {

​    listen 80;

​    server_name [www.zhss.com](http://www.zhss.com);

​    location /file/ {

​       root /server/file;

​       valid_referers none blocked server_names *.zhss.com;

​       if($invalid_referer) {

​           rewrite ^/ http://www.zhss.com/images/forbidden.png;

}

}

}

### 110_正向代理和反向代理到底指的是什么？

nginx两块应用：读取本地静态资源文件；域名跳转，但是有可能也会有，对老站点老域名跳转到新站点新域名去，多公司的多个二级域名可以把请求转发到公司内部的不同的系统上去；反向代理+负载均衡 

正向代理，是局域网内的服务器访问外网的资源，一般很少用，不怎么用；nginx平时做的最多的，其实是反向代理，是服务器对外网提供服务，外网请求进来代理到局域网内的服务器

### 111_Nginx用upstream指令配置后端服务器组

upstream backend_servers {

​    server backend1.zhss.com weight=5;

​    server backend2.zhss.com fail_timeout=30s;

​    server backend3.zhss.com;

} 

server {

​    listen 80;

​    server_name [www.zhss.com](http://www.zhss.com);

​    location / {

​       proxy_pass  backend_servers;

}

} 

weight是权重，max_fails默认是1，在fail_timeout时间范围内，请求失败次数超过max_fails就判定为down，同时down状态维持fail_timetou时间范围，默认就是round robin轮询算法去请求后端服务器

### 112_基于Nginx实现不同系统的负载均衡

在公司里会有很多的不同的系统，假设你的公司里就一个系统，这个系统就是一个单块系统，只不过多部署了几台机器，很多小公司，核心产品就是一个XX系统，很多这样的，就一个系统部署了几台服务器 

后端其实是会有多个系统，X1系统，X2系统，商品系统，订单系统，用户系统，营销系统，物流系统，仓储系统，库存系统，拆分成很多系统，每个系统都会部署几台服务器，每个系统他的url模式都不太一样 

http://www.zhss.com/product/xxxx

http://www.zhss.com/order/xxxx 

upstream order_servers {

​    server order1.zhss.com:8080;

​    server order2.zhss.com:8080;

​    server order3.zhss.com:8080;

} 

upstream product_servers {

​    server product1.zhss.com:8080;

​    server product2.zhss.com:8080;

​    server product3.zhss.com:8080;

} 

server {

​    listen 80;

​    server_name [www.zhss.com](http://www.zhss.com);

​    index index.html;

​    location /order/ {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

location /product/ {

​    proxy_pass http://product_servers;

​    proxy_set_header Host $host;

}

}

### 113_基于Nginx实现多个二级域名的负载均衡 

商城系统来举例，会有多个二级域名，[www.zhss-mall.com](http://www.zhss-mall.com)，都是给用户来使用的，直接会进入你的商城界面，用户在里面可以查看商品，优惠信息，订单，购物车，退款，物流，看库存数据，一大堆事情可以做 

得有三个端，买家端、卖家端、平台端，第三方卖家入驻你的平台，此时他肯定要使用你的卖家系统，seller.zhss-mall.com，这是一个二级域名；对你公司自己而言，你是一个平台，你要管理所有的用户和所有的卖家，platform.zhss-mall.com 

假设你还有其他的端，开放平台，暴露你商城的一些API出去，给你的第三方商家把你的系统和他们自己的一些系统集成在一起，open.zhss-mall.com 

upstream order_servers {

​    server order1.zhss.com:8080;

​    server order2.zhss.com:8080;

​    server order3.zhss.com:8080;

} 

upstream product_servers {

​    server product1.zhss.com:8080;

​    server product2.zhss.com:8080;

​    server product3.zhss.com:8080;

} 

server {

​    listen 80;

​    server_name [www.zhss-mall.com](http://www.zhss-mall.com);

​    index index.html;

​    location /order {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

location /product {

​       proxy_pass http://product_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [seller.zhss.com](http://www.zhss.com);

​    index index.html;

​    location /卖家端系统1 {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

​    location /卖家端系统2 {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [platform.zhss.com](http://www.zhss.com);

​    index index.html;

​    location / {

​       proxy_pass http://product_servers;

​       proxy_set_header Host $host;

}

}

### 114_思考一下为什么Nginx之后还要接入一个网关？

大公司大厂，是比较复杂的，但是，也是万变不离其宗的，但是大致也是这些东西，对外提供的无非都是一些二级域名，在nginx里，不同的二级域名可以配置成不同的server，里面可以有不同的location，反向代理到不同的upstream服务器组做负载均衡 

小型公司而言，就几个大系统，不同的大系统可能拆分为几个小系统，可能有几个二级域名，其实按照nginx下面的配置去做就足够了，即使说你新上线了一个系统，或者是某个系统加了几台机器 

此时都是简单的修改一下nginx的配置重启一下就够了 

如果说你的公司规模但凡稍微大了一些之后，可能你的技术团队上百人，几百人，中型公司而已，每天都有新增的服务，微服务架构，把系统拆分的更加细，一个订单系统就要拆分为好多个小服务，每个服务都要部署好几台服务器 

那你不能每天都频繁修改nginx配置吧？就要在nginx里直接去配置一个新的upstream组？配置一个server或者加一个location？系统接收对外请求都是走http，让nginx走负载均衡去转发请求就可以了 

系统内部之间调用呢？dubbo架构，spring cloud alibaba架构；spring cloud netflix架构，来做系统之间的分布式调用，也都没有问题 

upstream order_servers {

​    server order1.zhss.com:8080;

​    server order2.zhss.com:8080;

​    server order3.zhss.com:8080;

} 

upstream product_servers {

​    server product1.zhss.com:8080;

​    server product2.zhss.com:8080;

​    server product3.zhss.com:8080;

} 

server {

​    listen 80;

​    server_name [www.zhss-mall.com](http://www.zhss-mall.com);

​    index index.html;

​    location /order {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

location /product {

​       proxy_pass http://product_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [seller.zhss.com](http://www.zhss.com);

​    index index.html;

​    location /卖家端系统1 {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

​    location /卖家端系统2 {

​       proxy_pass http://order_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [platform.zhss.com](http://www.zhss.com);

​    index index.html;

​    location / {

​       proxy_pass http://product_servers;

​       proxy_set_header Host $host;

}

}  

### 115_中大型互联网公司是如何使用Nginx的？

nginx讲的很细，讲各种配置，各种用法，真的不适合我们Java方向的工程师或者是架构师，所以我讲的两周，全部是对Java工程师的干货，都是很偏向实战方向的，Nginx要理解他的哪些东西，以及他在整个公司IT架构里的位置 

大公司大厂，是比较复杂的，但是，也是万变不离其宗的，但是大致也是这些东西，对外提供的无非都是一些二级域名，在nginx里，不同的二级域名可以配置成不同的server，里面可以有不同的location，反向代理到不同的upstream服务器组做负载均衡 

小型公司而言，就几个大系统，不同的大系统可能拆分为几个小系统，可能有几个二级域名，其实按照nginx下面的配置去做就足够了，即使说你新上线了一个系统，或者是某个系统加了几台机器 

此时都是简单的修改一下nginx的配置重启一下就够了 

如果说你的公司规模但凡稍微大了一些之后，可能你的技术团队上百人，几百人，中型公司而已，每天都有新增的服务，微服务架构，把系统拆分的更加细，一个订单系统就要拆分为好多个小服务，每个服务都要部署好几台服务器 

那你不能每天都频繁修改nginx配置吧？就要在nginx里直接去配置一个新的upstream组？配置一个server或者加一个location？系统接收对外请求都是走http，让nginx走负载均衡去转发请求就可以了 

系统内部之间调用呢？dubbo架构，spring cloud alibaba架构；spring cloud netflix架构，来做系统之间的分布式调用，也都没有问题 

假设你商城，有三个端，买家端，卖家端，平台端，三个大系统，每个大系统里都有很多的小系统和无数的服务 

upstream buyer_gateway_servers {

​    server order1.zhss.com:8080;

​    server order2.zhss.com:8080;

​    server order3.zhss.com:8080;

} 

upstream seller_gateway_ servers {

​    server product1.zhss.com:8080;

​    server product2.zhss.com:8080;

​    server product3.zhss.com:8080;

} 

upstream platform_gateway_servers {

​    server order1.zhss.com:8080;

​    server order2.zhss.com:8080;

​    server order3.zhss.com:8080;

} 

server {

​    listen 80;

​    server_name [www.zhss-mall.com](http://www.zhss-mall.com);

​    index index.html;

​    location / {

​       proxy_pass buyer_gateway_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [seller.zhss.com](http://www.zhss.com);

​    index index.html;

​    location / {

​       proxy_pass seller_gateway_servers;

​       proxy_set_header Host $host;

}

} 

server {

​    listen 80;

​    server_name [platform.zhss.com](http://www.zhss.com);

​    index index.html;

​    location / {

​       proxy_pass platform_gateway_servers;

​       proxy_set_header Host $host;

}

} 

面试突击第二季，动态网关，买家端的网关，可以在他的web界面里，动态的配置，或者是只要你有买家端的新服务上线，或者已有服务的机器上下线的变动，让买家端的网关立马可以自动感知到，甚至不需要手动的配置 

买家端的网关立马感知到有一个新服务，新服务在哪几台服务器上，其他服务机器增减，网关也可以立马感知到，买家端网关收到的所有请求都根据请求url模式匹配和转发就可以了，[www.zhss-mall.com/order/create?xx](http://www.zhss-mall.com/order/create?xx)一堆参数，直接把请求转发给订单系统就可以了，假设你的买家端多了一个新的系统或者是服务，x服务 

买家端必须自动感知这个新系统的上线，[www.zhss-mall.com/x/create?xxx](http://www.zhss-mall.com/x/create?xxx)，买家端网关立马感知到模式，路由到x系统去就可以了

### 116_使用独立二级域名让秒杀系统与电商系统隔离部署

### 117_使用独立二级域名让秒杀系统与电商系统隔离部署（续）

### 118_使用独立二级域名让秒杀系统与电商系统隔离部署（3）

LVS、Nginx，这些都是一些负载均衡的技术，在秒杀系统里，在负载均衡这一层大概都做一些什么事情，一般来说，后台系统，他跟域名之间的关系是什么，[www.zhss.com](http://www.zhss.com)，假设你是一个C端公司，都是直接是你的电商网站，或者是你的APP的官网；假设你是一个B端公司，公司门户官网 

对外提供一些系统，可能会提供不同的系统，还有一些系统是对内部工作人员提供的一些系统，platform.zhss.com，seller.zhss.com，supplier.zhss.com，假设我们是一个电商公司，mall.zhss.com，指向了我们的电商网站 

用户正常的如果来浏览商品，下订单，或者支付，或者是别的请求，都走这个电商网站，mall.zhss.com，秒杀系统如果说也是走这个域名访问后台系统，甚至把秒杀系统和电商系统部署在一起，都在一批机器上 

瞬时流量是超高的，导致电商网站正常的一些请求，就被堵死了 

mall.zhss.com/order，mall.zhss.com/seckill，mall.zhss.com域名解析到了LVS服务器上去，后面都对应着同样一批nginx，搞秒杀活动，瞬时每秒数万请求都解析mall.zhss.com域名，都涌入了LVS+Nginx 

可能一下子万一你的LVS和Nginx的配置不是太高，可能会导致一下子让你的负载均衡层的设备的资源被打满，此时你商城正常的一些用户浏览商品，下订单之类的请求，可能就不行了，他们正常的请求，也是mall.zhss.com，也是走到一批LVS+NGINX去 

速度突然变得很慢 

商城域名：mall.zhss.com

秒杀系统：seckill.zhss.com 

对二级域名进行解析的时候，不要解析到mall.zhss.com域名对应的LVS上去，最好是seckill.zhss.com单独解析到一个独立的LVS+Nginx服务器上去，nginx之前给大家讲解了直接处理请求从本地磁盘读取静态html文件 

秒杀抢购的URL地址必须是独立域名下的地址：mall.zhss.com/seckill，seckill.zhss.com/order?xx=xx&xx=xx 

浏览秒杀活动的商品详情页，秒杀商品都是极为热门的商品，详情页浏览量会非常的大，如果说你把商品详情页放在你的Nginx服务器上，所有人都直接从你的Nginx服务器上去读取秒杀商品详情页 

网络带宽撑不住，一个普通的商品详情页，无论是网站上，还是手机APP，你以为他最大的内容是什么？就是高清图片，商品详情页里 一般有大量的图片，甚至是嵌入大量的视频，所以说网站仅仅浏览一些html，几kb大小，js和css都没多大，最大的就是那些高清图片，可能一个详情页的所有图片+视频的静态资源，5mb 

你的Nginx服务器的带宽可以给多少？每秒钟可以支撑多少流量经过？300mb，同时也就只能让60个人去加载你的详情页，每个人都要加载5mb，差不多就打满了，所以说如果你的详情页都放在Nginx去加载，最大的问题在于他的带宽 

大带宽的Nginx服务器，搞很多台，要不然的话，带宽撑不住 

大带宽服务器，很贵的，网络带宽如果是在服务器上弄大带宽，价格很高的，如果仅仅是一个1mb的带宽的普通服务器，一年可能小几千，300mb的大带宽服务器，一年可能要上十万，几十万 

CDN的计费是走流量的，比如说加载一次商品详情页需要5mb，你在CDN上购买的都是流量资源包，CDN购买一年的1TB的流量资源包，可能就几百块钱，非常便宜，100w mb，加载一次详情页是5mb流量，支撑你每年20w次的详情页，每年加载2000w次详情页，这个成本就很低很低了 

你需要把你的秒杀商品详情页指定一个域名，seckill-product.zhss.com/xxx，这个域名必须是解析到CDN产品上去的，直接就会找距离自己最近的CDN服务器去加载图片、视频之类的资源就可以了，css、js也可以的 

在这个之前，你需要配置CDN产品的源站站点的IP地址，就是你的秒杀活动商品详情页的Nginx服务器的地址，你可以在CDN的界面上选择一个预热，就是CDN的各个节点直接从你的Nginx服务器上拉取各种静态资源，按照你指定的URL地址去拉取，缓存在CDN的各个节点上

### 119_如何使用云厂商的DDoS高防产品防止黑客攻击？

### 120_如何使用云厂商的DDoS高防产品防止黑客攻击？（续）

防止黑客的DDoS攻击，黄牛刷单、黑客其他攻击、非法作弊请求 

儒猿技术窝的《面试突击第三季》，里面讲解了安全性的一些基础知识 

比如说你购买了CDN产品的10TB资源流量包，然后正常情况下，支撑上百万次请求都没问题，结果有黑客对你的seckill-detail.zhss.com域名进行DDoS攻击，利用很多台服务器，疯狂的请求你的这个域名 

他疯狂的请求，短时间内，半个小时内就请求了上百万次，请求你的CDN上百万次，就把你的10TB的资源流量包全部刷光 

如果说CDN的流量被刷爆了之后，再次请求你的域名，CDN就不能提供服务了，直接会让所有请求回源到Nginx服务器上去，假设他是100mb带宽的机器，此时被疯狂攻击，直接会挂掉，无法再提供任何正常请求了 

你的Nginx服务器可以设置一个防火墙，防火墙可以自动过滤掉大量的请求 

如果说，你的seckill.zhss.com这个域名被黑客DDoS攻击之后，可能导致大量的黑客构造出来的请求直接到你的秒杀抢购的后台服务器上去了，从LVS到Nginx，到Tomcat服务器上去 

某一个秒杀活动刚刚开始，结果黑客疯狂的攻击你，黑客的请求把你的LVS、Nginx、Tomcat服务器的资源全部打满，甚至打挂，此时导致正常用户的秒杀请求根本没有办法来执行了，全部失效了 

DDoS攻击，一定要用云厂商的DDoS高防商业产品 

独立二级域名解析到DDoS高防产品去，DDoS高防产品的源站地址解析到SLB上去，所以请求先经过DDoS高防，如果被攻击，流量直接被清晰过滤，合法流量才会到SLB去，SLB此时再进行转发 

一般来说是一年几十万的费用，整个就可以把你的读链路和写链路都加入DDoS高防产品保护起来1627098499

### 121_如何基于云厂商验证码机制拦截黄牛和黑客请求？

黄牛，或者是刷单，黑客非法请求，典型的特征，他们都不是人去点击秒杀抢购的请求，他们可能都是在秒杀开始之后想办法截取秒杀抢购后台真是的url地址，他们可能没有发起DDoS攻击，就是普通的一些请求，但是这些请求都是他们写的程序构造出来的，直接发送到DDoS高防产品 

比如说，正常来说一般用户可能就一个账号去抢购一个秒杀的产品，但是黄牛党，直接搞了一个程序，利用5台电脑，或者是10台电脑，20台电脑，他有多个账号同时发起请求过来，普通人都是在页面上或者APP上来进行点击的，所以手速一般比较慢 

但是黄牛党搞多个电脑多个账号，他直接是程序发送，就是可以在程序里写一个for循环，不停的发送请求，万一他的请求速度比较快，可能会导致一些黄牛党的程序运行的速度远远高于普通用户的手速 

黄牛抢了一堆秒杀低价产品，然后再转手倒卖出去，一般会来一些QQ群，微信群 

黑客自己吃饱了撑的，也是自己写程序发送请求到后台去 

验证码机制，前端滑动提交一大堆的行为： 

o  浏览器名称、版本

o  操作系统

o  屏幕长宽

o  鼠标点击、移动

o  键盘敲击

o  屏幕滑动轨迹

o  URL

o  是否安装Flash 

验证码后端基于大数据和AI验证是不是合法的行为，如果是，就判定验证码滑动通过，然后请求发到后端，后端调用验证码后端接口检查本次验证码滑动是否通过，如果通过才能方形，否则如果是黄牛，刷单，黑客，接口绝对不放行 

直接在nginx+lua层就可以把这个事儿给做了

### 122_如何自己开发一套完整的反作弊机制

DDoS肯定可以防止了，黄牛/黑客自己写程序构造请求可以过滤，比如说有一个黄牛的团队，他们也不是自己写程序，就是一个团队，专门靠堆人肉进行秒杀商品的代抢服务，人肉刷票服务，每个人都搞几个手机，几台电脑，专门刷秒杀产品 

可能有一些用户他就是比较正常的状态下频繁的提交抢购请求，其实这种往往也是不允许的，一个用户在一个抢购场次内，不允许对同一个商品重复提交抢购请求，也只允许对指定商品数量发起指定次数的抢购请求 

每一次请求过来，都要调用一下反作弊服务，记录每个用户对每个场次每个商品的抢购请求标志位，只要你已经对这个场次里的商品抢购过了，此时一定不会让你作弊再次抢购了，pass掉重试机制导致的频繁重试 

还有可能是有的用户一下子搞多个僵尸账号，然后同一时间一起发送请求，就专门进行刷单，此时可以对历史请求日志进行分析，抽取出哪些IP地址是喜欢每个秒杀场次都发请求的，而且每次都超速超标发送过多请求，直接就封杀掉 

还有就是可以对一些账号的日常行为进行数据分析，如果发现有些账号平时都不怎么浏览，几乎从不登录，每次就是到秒杀场次的时候才来拼命抢，也可以判定为是刷单账号，直接封禁掉就可以了 

反作弊机制，往往都是要依赖大数据分析手段的，对账号过往日常行为进行分析，然后封禁账号，或者是让nginx调用反作弊接口，判定请求是否合理，这就可以过滤掉一大批作弊请求了1627098499

### 123_如何基于限流算法对秒杀系统进行整体限流？

DDoS、刷单/非法请求、人肉刷单作弊，放行的流量其实一般都是正常用户自己来参与的秒杀请求，之前整体对秒杀系统的后台已经做过完善的压测了，每秒的TPS在5万，就已经很高了，如果超出5万，那么可能明显发现后台很多机器的负载压力就很大了 

正常的抢购请求一下子发送过来每秒有5.5万请求了 

秒杀商品太火爆了 

一秒内5.5万个请求如果说一旦放行到了后台服务器去，可能你的秒杀系统的压力会极大，可能会扛不住 

一定要在Nginx环节，基于Lua脚本做一个秒杀系统整体性的限流，一共有2台Nginx服务器，秒杀系统整体后台Tomcat服务器最多可以抗5万TPS，每台Nginx服务器每秒最多可以方形2.5万请求1627098499

### 124_限流算法介绍：令牌桶算法和漏洞算法

令牌桶：假设一秒就限速100，那么专门搞一个后台线程按照10ms一个令牌的速率往一个桶里放令牌，1s刚好放100个令牌，对每一秒都做一个令牌桶，请求过来从当前这一秒的令牌桶里拿令牌，拿到就放行，否则就拒绝 

通过这种方式，可以确保，每秒钟可以拿到令牌进行处理的请求，最多就能是100个请求 

漏桶：做一个漏桶，请求就是水滴，水滴流入漏桶的速度任意，无论有多少请求都直接流入漏桶，用漏桶装起来，然后漏桶按固定速度出水滴，比如一秒限速100，那么就10ms出一个水滴，如果桶满了，有新的请求要作为水滴入桶，此时就拒绝 

用一些队列这种数据结构做令牌桶或者是漏桶，可以实现上述的思路1627098499

### 125_如何基于Nginx+Lua实现一套业务限流机制

秒杀系统架构图 (1)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\12501.png)

DDoS、非法请求、作弊刷单、整体限流、业务限流 

是跟业务有关系的，比如说，秒杀场次，几个商品，商品A的限购数量是1000，商品B限购数量是2万件，商品C限购数量是1000件，在这种情况下，你觉得你放行5万个请求到秒杀抢购后台服务器的意义在哪儿？ 

分段限流，Nginx+Lua这一层，整体限流做完了，确保每一秒放过去的流量都是在后台最大压力承载范围之内，接着用Lua脚本做业务限流，你必须基于Redis记录下来，一个秒杀场次里的每个商品的限购数量是多少，你必须得知道，其实在秒杀开始之前，就可以提前Lua脚本加载进内存里了 

每个秒杀场次的每个商品的抢购请求的数量，可以基于Redis做一个原子incrby累加计数，然后，如果说商品A发现放行到Tomat服务器的抢购请求的数量已经超过1000了，此时就可以直接对商品A后续抢购请求直接返回一个响应，说抢购完毕 

商品B，如果发现放行到后台Tomcat服务器的抢购请求数量超过2w了，也直接可以拒绝后续的抢购请求了 

假设有10万用户同时参加秒杀抢购，瞬时过来的流量很高，有DDoS（几十万请求，直接被过滤掉），非法请求（几千个非法请求，直接过滤掉），人肉作弊请求（几百个请求，也直接过滤掉），正常的10万（整体限流，过滤掉5万请求，保留5万请求），业务限流（2.2万件限购数量，2.8万请求被过滤，仅仅放行2.2万请求） 

最终放行的2.2万请求全部会抢购到自己心仪的商品，7.8万人被整体限流和业务限流都给干掉了，如果有5万人是被整体限流pass掉了（系统繁忙，秒杀失败了），2.8万人是被业务限流（抱歉，商品库存被秒杀一空拉！） 

很公平，FIFO，手速越快，请求先到，就给你先放过去，请求后到，就会发现你被业务限流了 

对于你的基于Tomcat部署的秒杀系统而言，最后他看到的也就是最多一秒过来2.2万个请求这样子，假设10万人一起抢，到LVS+Nginx那一层的每秒并发也就上万而已，Tomcat部署的那一堆服务器最多每秒也就几千个请求而已了

### 126_秒杀抢购可以基于数据库来实现吗？

你的这一场秒杀活动，所有商品加起来一共有多少可售卖的库存，就会有多少请求放过来，每个秒杀场次的每个商品，最多就放可售库存一样数量的抢购请求过来，就直接利用数据库的行锁来做抢购不就可以了么 

update xx set 库存=库存-1，数据库里是有行锁的，可以保证你多线程并发更新一行数据，都是串行起来的，假设你某个商品可以秒杀的数量有1万件，然后瞬间涌入1万请求，一下子就把你的数据库给击垮了 

8核16G的机器以及16核32G的机器，部署的MySQL数据库，1万请求，瞬间cpu、内存、磁盘IO、网络IO瞬间会飙升到最高值，服务器接近于崩溃的边缘 

秒杀架构，通常来说，都会用Redis或者是其他的NoSQL来做秒杀抢购库存存放的地方，你的每个商品不是要参与秒杀吗，其实一般会把这些参与秒杀的商品的库存，提前加载到Redis缓存里来 

无非就是扣减Redis里的库存，一旦库存扣减完毕，直接就宣告这个秒杀商品售罄 

可销售库存，锁定库存，已销售库存，还是三个库存，在秒杀管理平台，可以看到这三个指标的实时监控数据1627098499

### 127_基于Redis缓存集群的秒杀抢购方案介绍

可销售库存，锁定库存，已销售库存，还是三个库存，在秒杀管理平台，可以看到这三个指标的实时监控数据1627098499

### 128_秒杀商品库存提前写入Redis的方案分析

seckill::product::153 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

冻结库存，freezed_stock_amount，是在库存系统的表里 

可销售库存，锁定库存，已销售库存，还是三个库存，在秒杀管理平台，可以看到这三个指标的实时监控数据1

### 129_基于Redis缓存集群实现秒杀抢购的细节分析

秒杀前端界面，大致就是用户他想买哪个东西，就会在秒杀开始之前等在这个界面里，看着秒杀抢购按钮的倒计时，倒计时结束之后，秒杀抢购的按钮会立马变成可以点击的状态，在那之前是一个置灰的状态 

用户立马会点击这个秒杀抢购的按钮，这就是这个用户针对某个商品发起了一个秒杀抢购的请求，如果说你的秒杀活动的人很多的话，此时可能瞬间会涌入大量的请求，但是真正放到秒杀抢购系统的请求，不会特别的多，跟可以秒杀的商品库存数量是差不多的 

只不过瞬时几秒钟内，可能涌入的请求超多 

假设秒杀抢购系统做了充足的Tomcat性能调优，每台4核8G的服务器可以抗每秒1000个请求，部署 10台机器就可以了 

得先做一个判断，先判断一下sale_stock_amount，可售库存是否充足，是否可以支持我还可以抢一件，别这个数字都是0了，我还去进行hdecrby，肯定不行 

seckill::product::153 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

hincrby，hdecrby，redis的一些命令，就可以对这个商品的库存数据做一个操作，hmset 

读取可销售库存，判断是否可以购买，如果是就扣减可销售库存，增加锁定库存；后续会走一个异步下单流程；等用户对订单实际完成支付之后，就会扣减redis里的锁定库存，增加已销售库存 

缺陷？这还用说，并发场景下，绝对数据混乱，也就是所谓的库存超卖1627098499

### 130_秒杀抢购过程中的库存超卖问题分析

第一个是检查库存是否足够，足够就执行锁库存 

seckill::product::153 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

很简单，加分布式锁啊，各位同学，秒杀抢购的时候加锁，避免超卖，支付的加锁，避免数据错乱，一直不支付自动释放库存，也加锁，缺点就是，分布式锁的性能并不是特别好，因为他是依托停顿+轮询去检查锁是否释放的，不管怎么说都会产生大量的网络通信，这就会导致并发能力下降了

### 131_秒杀抢购库存超卖问题的解决方案

### 132_秒杀抢购库存超卖问题的解决方案（2）

seckill::product::153 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

方案一，可以用我们之前讲解过的分布式锁，先获取一个锁，seckill::product::153::lock，Redisson去做，绝对不会在你获得锁的期间有别人去查询和更新库存数据，此时你可以放心的查询，如果可以抢，就扣减库存，释放锁，别人获取锁，再重复 

对于高并发来说是不太合适的，Redisson分布式锁的实现源码，Lua脚本+watch dog，锁等待，每个人都得轮询不停的去重新尝试加锁，中间都有一个等待的过程，会产生很多的网络通信的开销 

获取锁的人释放了锁，其他的人还都处于锁等待的过程中，此时过了几十ms才有下一个人获取了锁，分布式锁的整体并发的能力不是不太好 

方案二，FIFO内存队列，秒杀抢购系统里，对所有进来的请求全部进入一个内存队列排队，按照先后顺序出队再去Redis里执行秒杀抢购逻辑，把针对某个商品的所有抢购请求都hash路由到一台服务器上去，进入一个内存队列，保证对这个商品的抢购都是有序的 

亿级流量，缓存和数据库的一致性的方案，就讲了这个内存队列的方案，严格保证顺序，就一定是一致的，内存队列万一你的系统重启或者宕机，可能会导致崩溃，内存队列里的数据就会丢失，万一请求特别多，可能会导致你的内存爆掉，频繁gc 

方案三，乐观锁 

抢购前先检查是否有可售库存 

乐观锁方案，每个库存数据都绑定一个版本号，每次更新的时候用乐观锁判断，是否没变过，如果没变，才可以更新，redis可以做到的，有一个watch机制，而且watch机制可以和pipeline结合起来用 

watch一些数据是否有变化，然后通过pipeline一次性提交多个操作，如果有变化提交就失败，否则就成功，提交完之后最后要获取到库存的值，如果发现是负数，那么秒杀就失败了 

乐观锁方案会很耗费CPU，他可能会频繁的去更新结果是失败的，因为版本已经别别人变更了，此时他要重新再查询最新的版本额，再次更新，可能很多服务器的很多请求都会处于轮询的状态 

电商大厂，做秒杀抢购的库存超卖的解决，都是直接写lua脚本，一下子提交到redis去执行就可以了，redis可以保证lua脚本按照顺序来执行，每个lua脚本的执行都是原子的，就是没人会来影响你 

会把查询库存数量是否可以抢购 + 更新一堆库存字段 + 是否抢购成功的返回值，这些逻辑封装在一个lua脚本里，然后提交lua脚本可以Redis去执行，一秒钟上万个请求过来了，其实就是上万个lua脚本提交到Redis内存里去执行 

seckill::product::153::01 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

seckill::product::153::02 = {

​    sale_stock_amount: 3000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

seckill::product::153::03 = {

​    sale_stock_amount: 4000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

seckill::product::152 = {

​    sale_stock_amount: 1000,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

seckill::product::151 = {

​    sale_stock_amount: 100,

​    locked_stock_amount: 0,

​    saled_stock_amount: 0

} 

毕竟redis是单线程的，还是可以用分段锁策略优化并发能力的，比如你redis集群部署三个master，那每个商品库存可以拆三个分段，压力均匀在三台机器上，然后可以做转移扣减，就ok了 

对于支付或者释放来说，也是乐观锁watch，然后pipeline提交，就可以了 

提前把商品的库存，有1万个库存，就把1万个库存数据压入redis里的一个队列里去，抢购的时候，就是不停的从队列里出队，如果出完了，就说明抢购结束

### 133_秒杀抢购黄金链路的下单与支付环节

秒杀抢购进行中，请耐心等待，页面/APP得不断的发送请求，比如说每隔1s发送一个请求到后台去查询，这个秒杀抢购的请求对应的订单是否异步的生成了，页面上显示的就是，不好意思，商品已经售罄

### 134_秒杀抢购系统还需要限流、防刷和防重吗？

### 135_秒杀系统需要了解Tomcat架构原理和优化手段

秒杀系统架构图 (2)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\13501.png)

### 136_为什么必须要用MQ来进行秒杀下单削峰？

### 137_秒杀异步下单的流量控制技术方案

面试突击第一季，几个MQ连环炮，基于MQ的削峰场景 

订单系统平时支撑日常的下单请求，高峰期可能每秒也就几百个请求，订单系统的部署承载的最大的TPS，每秒可以支撑1000个订单同时下达，部署两三台机器，每秒最多1000个订单下来 

20台机器，30台机器，抗每秒上万个订单同时下达 

做一个流控，每秒调用订单系统500次，每秒最多下500个订单，1万个订单，20秒就可以搞定这1万个订单了 

这周+下周，直接会把秒杀系统的架构全部讲解完毕，方案都粗略的设计完毕，下下周，我们开始落地实战开发 

下单流控这块，直接基于线程池来做就可以了，消费出来一个抢购成功的通知，推到线程池里去，线程池，250个线程，秒杀下单系统，可以准备2台服务器，一起消费MQ，每台服务器比如就是125个线程

### 138_前端与客户端如何确定秒杀下单成功了？

用户大致分为两种情况，就是在ngxin+lua这一层就直接请求被干掉了，看到的就直接是，不好意思，抢购失败，商品已经没有库存了；在抢购系统的时候，抢购失败了，此时也会看到上述提醒；抢购成功了，看到的文本，可以是类似于，抢购进行中，或者抢购请求处理中，请耐心等待 

阿里云，企业用的，购买很多的服务器，也是一种电商，也要支付和下订单，即使你支付成功了，他显示给你的是一段文本，正在支付处理中，请稍等，几秒钟过后，才会有一个页面的跳转，此时已经处理完毕 

如果此时他要是退出了这个界面，那就看不到具体的处理情况了，正常情况下，你应该提醒他，请不要离开这个界面，前端/客户端就可以不停的轮询发送请求到后台，检查这个秒杀抢购的订单是否创建成功了 

可以页面离开，跳转到支付界面，让你进行订单的支付，后续的全流程，都是电商的核心系统在处理了，跟你秒杀系统就没关系了

### 139_下单成功之后的订单支付逻辑与未支付逻辑

在一个界面里等待，如果说发现订单创建好了，就可以正常的支付；不停的刷新订单列表，可以查到这个秒杀的订单，此时点击支付也是可以的；会跑定时调度任务，或者是直接用MQ延迟消息，做订单的自动取消 

创建成功了一个普通的订单，可以在MQ的延迟队列里发送一个30分钟的延迟消息，30分钟后拿到这个延迟消息，检查订单的支付状态，还没支付就直接关闭掉；创建的是一个秒杀订单，可以在MQ的延迟队列里发送一个10分钟的延迟消息，秒杀订单如果10分钟没支付，直接就取消订单

### 140_消息中间件技术方案在秒杀架构里的用武之地

### 141_MQ消息零丢失方案与秒杀下单的关系

### 142_MQ消息重复解决方案与秒杀下单的关系

### 143_MQ消息积压解决方案与秒杀订单延迟问题（1）

### 144_MQ消息积压解决方案与秒杀订单延迟问题（2）

### 145_MQ消息积压解决方案与秒杀订单延迟问题（3）

秒杀系统架构图 (3)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\14501.png)

MQ里的消息0丢失，消息不会重复，消息积压问题，高可用架构，等等 

如何保证，你的秒杀抢购成功的消息绝对不会丢失，消息绝对不能重复 

儒猿技术窝，《从0开始带你成为消息中间件实战高手》，68块钱，两顿饭钱，一天饭钱，讲了很多的MQ的基础原理知识和初级实战知识，围绕RocketMQ详细分析了一下如何基于RocketMQ的事务消息的机制，结合别的机制去实现消息零丢失的方案 

秒杀场次id + 商品id + 用户id + 订单类型（秒杀类型），组合成一个唯一索引，此时才能保证用户在一个秒杀场次里，对一个商品最多只能下一个秒杀类型的订单 

就是可以写一个key，秒杀场次id+商品id+用户id，如果发现redis里已经有了，此时就不能重复进行下单了 

可能会导致大量的用户，已经无法抢购了，很多人已经抢购成功了，但是也没人可以快速的查到订单去进行支付，大家 会很疑惑，一直等待，可能要几十分钟之后才可以查询到订单去进行支付  

此时可以先不用把订单写入订单系统去，直接把订单写入到redis里去，然后让用户直接基于redis来查询订单，再进行支付 

拿出来一个通知的时候可以做一个检查，发现这个通知拿到的时间跟这个通知写入到MQ的时间相比，已经超过了2分钟了，20s左右就可以从写入MQ到下订单成功了，此时可以认为MQ那边出现了严重的几十万甚至上百万消息的积压问题 

立马启动一个针对消息积压的临时解决方案 

对于前端而言，他可以稍微给一个buffer，他定好一个时间，如果超过3分钟还没拉取到订单下成功的通知，此时就直接跳转一个界面，告诉用户说，不好意思，秒杀抢购失败，请重新进行秒杀下单 

比如通过快速的释放秒杀库存，一秒钟去调用秒杀库存服务1000次，两台服务器每秒可以调用秒杀库存服务2000次，部署2台服务器，底层基于redis，所以速率可以是很快的，60s * 2000 = 120000万次，积压了50万条消息，4分钟就可以把库存都释放完毕了 

不要完全去释放库存，可以把一部分通知继续正常去下订单，同时还在不停的释放库存

### 146_所谓的高可用架构到底指的是什么？

秒杀系统的代码实现环节，秒杀系统是一个大项目，架构设计环节，编码实现环节，平台实战环节，高可用，高可用，指的是你的一个复杂或者简单系统，构成这个系统的任何一个环节如果说故障了，此时你的系统能否正常运转 

如果说你的系统出现了一些故障，但是此时你有一些预备的预案，此时系统还可以正常运转，也许功能上有点打折扣，但是基本还能正常跑，这个时候就可以说你的系统是高可用的，可用性，99%，99.9%，99.99%，99.999% 

365 * 24 * 60 * 60，比如说你的系统曾经出现过多少分钟，多少小时的不可用，不可用的时间被扣减掉了之后，剩余的就是可用的时间，可用的时间在一年总时间里的占比就是你的可用性，99.9%已经很不错了，99.99%就很好了，99.999%金融级的可用性了 

越复杂的系统要实现高可用越难，包含的环节太多了，任何一个环节出了故障都可能导致秒杀会失败，秒杀系统不可用，针对每个环节都设计高可用的一些方案，高可用架构设计，故障降级方案 

没有高可用架构，此时可能你一年里就有那么几十分钟，因为某个环节故障了，导致系统不可用，99%，99.9%

### 147_秒杀系统的全链路高可用架构设计

LVS高可用：LVS双机器部署 + keepalived

Nginx高可用：多机器冗余部署，几台服务器，LVS会做负载均衡

秒杀抢购服务：多机器冗余部署，Nginx会做负载均衡

Redis Cluster：master-slave架构，主从架构，自动故障切换

RocketMQ：集群部署+多副本冗余

秒杀下单服务：多机器冗余部署

秒杀库存服务：多机器冗余部署

### 148_Redis集群崩溃时的秒杀系统自动恢复方案 

用我们上一讲说的那套方案，任何一个环节里的任何一台机器宕机了，不会影响系统正常的运行的，怕的是有的环节直接全面崩溃 

一旦Redis崩溃，就没法进行秒杀抢购了，此时如果说把所有的请求全部打回去就说是抢购失败，那也不太好，可能会导致这次秒杀活动就彻底失败了，其实此时比较好的方案是把秒杀抢购流水日志顺序写入本地磁盘，进入os cache，返回的状态告诉用户说，正在抢购中，让他耐心等待 

其实此时，他要不然就是停留在这个界面等待，要不然就是进入到之前抢购过的一个秒杀商品详情页里去，此时页面里查询他的抢购状态就是发现在redis里可以查到他对这个商品发起过抢购，状态是正在执行中 

以此类推，包括所有的用户对每个商品的抢购状态都最好存一下，包括抢购失败，抢购进行中，抢购成功，完成创建订单，完成支付订单，等等，这样子其实在秒杀商品详情页里是可以获取到每个用户对每个商品的抢购状态的 

以此可以类推他是否可以再次下单抢购，比如之前说过的MQ积压方案，快速释放掉了他的抢购，此时他再次进入详情页，是可以重新进行抢购的 

所以这里也是一样的，此时是完全可以把抢购流水日志顺序写入磁盘文件，进入os cache，其实性能也还可以 

此时秒杀就直接结束了，因为大部分流量被拦截了，少部分流量放进来都进磁盘了作为流水日志了，接着Redis集群可以紧急进行修复，比如说重启之类的，然后让秒杀系统后台线程自动探查Redis是否恢复，如果恢复了，则顺序读取本地磁盘的秒杀流水日志，进行流量重放去执行抢购，后续流程一切正常 

### 149_Redis主节点崩溃时没及时同步导致的库存超卖问题

如果基于redis来做秒杀抢购，redis cluster，master-slave主从同步架构，而且是异步复制的，此时如果说redis master节点宕机了，还没来得及把库存抢购扣减的操作同步到slave去，此时slave切换为master，导致了库存超卖怎么办

库存只有100件，结果有101个人抢购成功了，这个问题就是redis异步复制导致的库存超卖的问题，除非你改造redis源码，把异步复制做成同步复制，任何一个抢购请求在master执行完毕之后，必须从master复制到slave之后，才能认为是成功 

首先，这个问题是小概率的问题，redis master宕机本身是小概率问题，如果你对这个问题不处理，很多时候公司线上系统也不会对这个问题进行处理，出故障是个小概率的事件，可能redis集群跑几年都不会出现这样的问题 

两个方案：redis主从同步取消，只有master，把redis cluster，就用最普通的redis部署几台机器，然后可以用twemproxy中间件或者是codis对几天机器上的redis做分布式数据存储和路由，取消slave复制这个概念 

一点任何一台机器宕机，此时对那台机器的抢购操作转化为流水日志写入服务器本地磁盘文件，等到那台redis机器恢复了，再重放流水去执行就可以了，库存超卖问题，还能实现redis崩溃的高可用机制 

第二个方案，依靠订单系统去检查的方案，不是太好

### 150_MQ集群崩溃时的服务直连降级方案

MQ崩溃，只不过是无法下订单罢了，但是抢购是可以正常执行的，所以抢购成功与否，结果直接返回就行了，然后此时可以把秒杀抢购下单通知写入内存，内存中进入本地磁盘文件，接着后台开个线程，逐步的慢慢读取这些数据然后直接调用下单服务就行了，他的缺点就在于说，可能会丢失一点数据罢了 

对我们而言，在这里，完全没必要去等待MQ恢复，你可以把秒杀抢购成功的通知写入内存缓冲，然后刷入磁盘，后台开一个线程直接做服务直连，直接绕过MQ，以一个很慢，速度很低的节奏，把抢购成功通知慢慢的发送给秒杀下单服务的接口

### 151_订单系统异常时的MQ重试与死信方案

订单系统如果异常，此时需要基于MQ的重试队列进行重试，重试几次都不行，需要进入死信队列，专门有处理死信的线程，间隔比如1小时后再进行反复重试

### 152_秒杀抢购与下单系统的高可用保障方案

秒杀抢购系统多机器部署，一般是没有问题的，何况redis和mq崩溃的情况他都已经考虑到了，因此问题不大，即使个别机器宕机，也没太大关系，下单系统宕机，最多就是MQ消息积压过久，此时方案已经说过了，快速释放库存，让用户分散重新下单就行了

### 153_秒杀抢购与下单的异构存储分布式事务方案

一定要做一个事务方案，redis本身就是支持事务机制的，开启一个redis事务，执行秒杀抢购，更新redis里的状态，发送通知给MQ，如果都成功了，就提交redis事务，返回状态给用户就可以了 

如果发送通知给MQ失败了，此时走降级方案，写内存以及保存到本地磁盘，执行成功了，这个事务也算是成功了 

redis事务+MQ事务，异构存储混合事务

### 154_多机房多活部署以及单机房故障降级

讲一下多机房多活方案，但是不会讲解在秒杀系统里的落地实践，那一定是整个公司完整的系统全部做成双机房多活，不只是秒杀系统一个人的问题

### 155_秒杀系统的全链路压测以及高可用演练

先基于虚拟机环境各种部署和开发代码，演练，做一个模拟的本地全链路压测，每秒1000请求，每秒500请求，做 一些高可用演练，真实平台去做一个集成，会做一些代码改造，跟真实电商平台集成在一起

### 156_梳理一下秒杀系统的整体实现过程和思路（1）

### 157_梳理一下秒杀系统的整体实现过程和思路（2）

http接口，调用这个接口，可以传入配置好的秒杀场次和每个场次的商品，以及每个商品的秒杀价格和库存数量，最后大家会把自己做好的部分代码，跟我们真实的电商平台的代码、中间件，做一个适配，代码改造 

部分代码，秒杀抢购服务和秒杀下单服务，部署在真实的服务器上，会跟我们写好的其他的服务进行一个交互，包括秒杀运营系统，我们都是做好的，有界面的，到时候可以自己去我们开发好的界面上做一个配置，启动一个模拟秒杀流量导入 

每秒上万的并发请求就涌入你的秒杀抢购服务，基于我们提供好的一套redis环境去做秒杀抢购的事情，基于我们提供的MQ推送抢购成功通知，秒杀下单服务就会去调用我们的电商平台的订单系统去进行秒杀下单 

第一阶段：架构设计和方案设计

第二阶段：秒杀系统的编码实现

第三阶段：秒杀核心服务与真实电商平台适配和改造，部署生产环境实战

### 158_在本地笔记本电脑上部署一个单机版RocketMQ（1）

### 159_在本地笔记本电脑上部署一个单机版RocketMQ（2）

rabbitmq和rocketmq，kafka数据传输 

MQ使用RocketMQ，对于RocketMQ的基础知识，建议大家直接看儒猿技术窝的《从0开始带你成为消息中间件实战高手》，我们这里不再重复讲这些基础内容和知识了，后续架构班会直接讲RocketMQ的源码 

https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.7.1/rocketmq-all-4.7.1-bin-release.zip 

下载，接着解压缩，先启动NameServer 

nohup sh bin/mqnamesrv &

tail -f ~/Logs/rocketmqLogs/namesrv.log 

接着启动一个Broker： 

nohup sh bin/mqbroker -n localhost:9876 &

tail -f ~/Logs/rocketmqLogs/broker.log 

export NAMESRV_ADDR=localhost:9876

sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer

sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 

sh bin/mqshutdown broker

sh bin/mqshutdown namesrv 

最好是装一台虚拟机，单机版的redis，单机版的rocketmq，mysql数据库装在自己本地就可以了，nginx也可以装在那台虚拟机上，把核心代码都写一下，环境和流程可以跑通，就可以了

### 160_在Intellij IDEA搭建秒杀系统的工程结构（1）

### 161_在Intellij IDEA搭建秒杀系统的工程结构（2）

### 162_在Intellij IDEA搭建秒杀系统的工程结构（3）

### 163_在Intellij IDEA搭建秒杀系统的工程结构（4）

### 164_在Intellij IDEA搭建秒杀系统的工程结构（5）

### 165_在Intellij IDEA搭建秒杀系统的工程结构（6）

### 166_关于秒杀系统使用Spring Cloud Netflix老版本的问题说明

spring cloud alibaba 

儒猿技术窝，spring cloud alibaba技术栈的微服务专栏，9.9 

nacos+dubbo整合起来，nacos三节点集群部署

### 167_在秒杀运营服务中开发配置秒杀活动接口（1）

### 168_在秒杀运营服务中开发配置秒杀活动接口（2）

### 169_在秒杀运营服务中开发配置秒杀活动接口（3）

### 170_在秒杀运营服务中开发配置秒杀活动接口（4）

### 171_在秒杀运营服务中开发配置秒杀活动接口（5）

### 172_在秒杀运营服务中开发配置秒杀活动接口（6）

### 173_在秒杀运营服务中开发配置秒杀活动接口（7）

### 174_本机启动独立Redis节点组成的集群（1）

### 175_本机启动独立Redis节点组成的集群（2）

### 176_创建独立的通用工程存放Jedis客户端封装（1）

### 177_创建独立的通用工程存放Jedis客户端封装（2）

### 178_创建独立的通用工程存放Jedis客户端封装（3）

### 179_让秒杀运营服务实现场次和商品的缓存双写（1）

### 180_让秒杀运营服务实现场次和商品的缓存双写（2）

### 181_让秒杀运营服务实现场次和商品的缓存双写（3）

### 182_已有代码回顾以及本周目标任务说明

### 183_本地启动单机版RocketMQ以及整合代码（1）

### 184_本地启动单机版RocketMQ以及整合代码（2）

### 185_实现秒杀商品新增通知发送到MQ中（1）

### 186_实现秒杀商品新增通知发送到MQ中（2）

### 187_实现秒杀商品新增通知发送到MQ中（3）

### 188_实现秒杀商品新增通知发送到MQ中（4）

### 189_实现秒杀商品新增通知发送到MQ中（5）

### 190_实现秒杀商品页面渲染服务的代码逻辑

### 191_测试秒杀商品通知发送MQ以及渲染HTML页面 

如果说你希望能够实现一天当中的秒杀场次的排序，sorted set，每个场次的时间作为他的一个分数，放进去，他自动就会按照场次时间进行排序，如果场次时间有修改和变动，他会自动调整顺序的 

如果说你要调整一个场次里各个商品的顺序，sorted set，给场次挂商品的时候，可以设置一个顺序序号，把序号作为sorted set里面的分数，此时他就会自动按照序号进行排序了，如果要调整顺序，把调整后的序号修改为soreted set里面的元素的分数，顺序就自动会进行调整了 

redis数据结构，会用到 

zxxx API，给他加入一个分数的概念，都是ok的

### 192_回顾已经写好的秒杀系统代码逻辑

### 193_实现秒杀商品库存服务的代码逻辑（1）

### 194_实现秒杀商品库存服务的代码逻辑（2）

### 196_实现秒杀商品库存服务的代码逻辑（3）

### 196_实现秒杀商品库存服务的代码逻辑（4）

### 197_实现秒杀商品库存服务的代码逻辑（5）

### 198_实现秒杀商品库存服务的代码逻辑（6）

### 199_实现秒杀商品库存服务的代码逻辑（7）

### 200_实现秒杀商品库存服务的代码逻辑（8）

### 201_关于课程延迟发布以及电商云平台的收费说明（1）

### 202_关于课程延迟发布以及电商云平台的收费说明（2）

### 203_关于课程延迟发布以及电商云平台的收费说明（3）

### 204_秒杀课程带给大家最大的价值是什么？

### 205_回顾一下已经写好的秒杀系统的代码流程

### 206_回顾一下秒杀系统的高并发读架构

### 207_回顾一下秒杀开始后的流程（1）

### 208_回顾一下秒杀开始后的流程（2）

### 209_回顾一下秒杀开始后的流程（3）

1、 整体的业务架构+技术架构

2、 超高并发的架构设计

3、 超高并发场景下的高可用架构设计

4、 （可选）真实电商业务流程下的秒杀系统完善

5、 （可选）真实每秒1w+并发下的秒杀系统的生产实践

### 210_基于Lua开发OpenResty中内嵌的限流机制（1）

### 211_基于Lua开发OpenResty中内嵌的限流机制（2）

### 212_基于Lua开发OpenResty中内嵌的限流机制（3）

### 213_基于Lua开发OpenResty中内嵌的限流机制（4）

### 214_基于Lua开发OpenResty中内嵌的限流机制（5）

### 215_基于Lua开发OpenResty中内嵌的限流机制（6）

### 216_基于Lua开发OpenResty中内嵌的限流机制（7）

### 217_基于Lua开发OpenResty中内嵌的限流机制（8）

OpenResty，Redis 

Lua本身是一种脚本语言，Perl、Python之类的是差不多的，我也不会刻意去记他的语法，语法也很多，其实都差不多

### 218_从Redisson源码中学习Redis Lua脚本的使用（1）

### 219_从Redisson源码中学习Redis Lua脚本的使用（2）

### 220_探索一下Java客户端执行Lua脚本的用法

### 221_用hash数据结构来重构秒杀库存数据

分布式架构实战里，有分布式锁这块的讲解，Redisson框架，基于Redis的分布式锁

### 222_看看Lua语法中如何进行字符串和数字的类型转换

### 223_实现秒杀抢购流程的核心代码逻辑（1）

### 224_实现秒杀抢购流程的核心代码逻辑（2）

### 225_实现秒杀抢购流程的核心代码逻辑（3）

### 226_实现秒杀抢购流程的核心代码逻辑（4）

### 227_实现秒杀抢购流程的核心代码逻辑（5）

### 228_实现秒杀抢购流程的核心代码逻辑（6）

### 229_实现秒杀抢购流程的核心代码逻辑（7）

### 230_实现秒杀抢购流程的核心代码逻辑（8）

### 231_实现秒杀抢购流程的核心代码逻辑（9）

### 232_实现秒杀抢购流程的核心代码逻辑（10）

### 233_实现秒杀抢购流程的核心代码逻辑（11）

### 234_对秒杀系统的完整流程进行测试（1）

### 235_对秒杀系统的完整流程进行测试（2）

### 236_对秒杀系统的完整流程进行测试（3）

### 237_对秒杀系统的完整流程进行测试（4）

### 238_对秒杀系统的完整流程进行测试（5）

### 239_对秒杀系统的完整流程进行测试（6）

### 240_对秒杀系统的完整流程进行测试（7）

### 241_对秒杀系统的完整流程进行测试（8）

nohup sh bin/mqnamesrv &

tail -f ~/Logs/rocketmqLogs/namesrv.log 

nohup sh bin/mqbroker -n localhost:9876 &

tail -f ~/Logs/rocketmqLogs/broker.log

技术积累、架构设计、代码落地、平台实战（额外收费，200块） 

LVS + keepalived高可用架构，如何部署，网上一大堆

### 242_秒杀系统高可用架构落地代码讲解（1）

### 243_秒杀系统高可用架构落地代码讲解（2）

### 244_秒杀系统高可用架构落地代码讲解（3）

### 245_秒杀系统高可用架构落地代码讲解（4）

### 246_秒杀系统高可用架构落地代码讲解（5） 

### 247_秒杀系统高可用架构落地代码讲解（6）

1、 Redis高可用方案

2、 MQ高可用方案

3、 MQ消息零丢失方案+幂等方案

4、 订单系统故障降级方案

5、 MQ消息积压fail-fast方案

6、 秒杀下单服务和秒杀抢购服务的高可用

### 248_等待后续的秒杀项目云平台实战内容

技术积累、架构设计、代码落地、平台实战（额外收费，200块） 

课程讲到这个级别就算是很不错的一个项目实战的课程了，技术很浅薄，demo，项目纯demo极为的简单，从技术学习的角度来说，还差了最后一公里，从业务和技术两个角度来说，拿出去自己去面试，是没问题的 

你可能需要自己去看很多的电商APP真实的秒杀的功能，做深度的思考， 从业务层面去思考看我们的代码还需要做哪些业务的完善；另外从技术的层面来说，你需要去思考，在真实的秒杀流量下，你需要多少机器，能抗多少并发，机器的负载是什么 

才能出去面试，作为项目来讲 

发虚，毕竟没真正的搞过，玩儿过，所以感觉上还是发虚，一公里 

电商云平台就是解决这个最后一公里的，完全对标真实的商城，所有界面和功能都是真实的，平台入驻第三方商家，每个商家可以管理自己的店铺的功能，用户可以在商城里选购商品这样子 

基于这个平台去看他真实的业务流程，完善我们真实的业务代码细节，从业务上力求真实；可以给大家提供真实的自动化测试的环境，模拟把你的代码跑一遍测试，可以提供20~30台的服务器，真正的模拟出每秒上万的并发，你去部署跑一下流量，看看你的机器到底能抗多少并发，机器的负载到底是多少 

每个人就能拿着自己独一无二的生产经验和生产数据，出去面试了，你就会觉得很有底，你发现你真的跑过，玩儿过，很有底气  

LVS + keepalived高可用架构，如何部署，网上一大堆  

1、 Redis高可用方案

2、 MQ高可用方案

3、 MQ消息零丢失方案+幂等方案

4、 订单系统故障降级方案

5、 MQ消息积压fail-fast方案

6、 秒杀下单服务和秒杀抢购服务的高可用

秒杀系统架构图

![](C:\Users\zy199005\Desktop\中华石杉\images\java\15\24801.png)