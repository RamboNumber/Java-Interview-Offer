## 12_Netty核心功能精讲以及核心源码剖析

### 001_课程计划说明、未来年薪展望以及职业发展规划 

电商系统 -> 分布式架构 -> 分布式事务、分布式锁、分布式服务框架 

集合、并发、IO、网络 -> 两个自研的中间件项目，分布式微服务注册中心（50%），分布式海量小文件存储系统（80%） 

Netty -> 带出来一大堆的项目 

---> 你有能力做基础架构的架构师，任何你公司里需要的各种基础架构的系统，或者是底层的系统，或者是需要自研的中间件系统，你都可以带团队，来做架构设计，基于底层技术来进行研发 

---> 你有足够的底层技术和功底，来研究后续技术的源码，Kafka（大数据架构课）、RocketMQ、MyCat、Tomcat 

很多很多的系统，中国没有那么多的高并发系统，大部分的系统，微服务+分布式，一般来说就够了，微服务架构，配置中心、日志中心、监控中心、服务治理、API治理 

缓存、消息、搜索、海量数据，技术上的难度没有那么的高 

----> 小公司的架构师的水平 + 大公司的资深工程师的水平 -> 50万~70万左右

----> 小公司的首席架构/CTO的水平 + 大公司的P7+的水平（P8）-> 七八十万 ~ 100万左右 

Kafka的源码怎么看？并发、集合、IO、网络

RocketMQ的源码怎么看？并发、集合、IO、网络 

亿级流量的电商详情页的缓存系统 -> Redis的使用 + 常规性的缓存架构的设计方案

### 002_未来课程计划：Netty核心功能精讲以及核心源码剖析 

集合、并发、IO、网络、NIO，都搞定了 

Netty -> 基于NIO的网络通信的框架，高性能、高并发，很多时候都会基于Netty去研发一些底层的基础系统 

Netty核心功能精讲，做几个demo，剖析他的核心源码，分析他的架构设计之道，高性能、高可靠、高并发 

基于Netty开发很多互联网公司里的应对海量用户、大量客户端的一些基础系统的项目

### 003_未来课程计划：三个基于Netty的真实大用户量项目实战 

一周多的时间，课程设计，不比课程录制更简单，慢慢梳理，技术先讲，什么技术后讲，各种技术怎么串联起来，怎么去配合对应的项目 

1、基于Netty以及自研DFS的仿钉钉云盘项目实战 

互联网公司里不同的应用场景，仿钉钉的云盘项目，钉钉，但凡你是用办公通讯工具，有一个场景非常的场景，基于云盘的文件上传和分享。比如说你现在要上传一个文件，后续你根据自己的需求，要把文件分享给不同的同事 

你跟一个同事在聊天，直接就把一个文件传输给他，文件会上传到云盘里去占据你的个人空加你，但是直接这个同事可以看到这个文件的分享链接/图标，然后他就可以去下载 

基于Netty去开发客户端和服务端，文件从用户那里过来的一个上传，实际上这个文件最终会进入到分布式海量小文件存储系统里去，人家下载文件，就从分布式海量小文件存储系统里去读取 

大量的客户端，百万客户端，都连接到服务器，进行文件的上传，天然和真实的一个Netty使用场景，维护一个连接，做耗时的网络通信和数据传输 

2、基于Netty的百万日活APP即时通信系统实战 

互联网公司里非常常见的一个应用，体育运动APP，在线教育APP，社交APP，在线问诊APP，他都会有APP内部的一个即时通讯系统，APP内部，你可以给不同的用户发送消息，去聊天 ，一群人可以在一起聊天 

点对点，一个人跟一个人去聊天 

非常适合用Netty来开发，大量的客户端跟服务器维持长连接，如果有一个人要跟另外一个人说话，发送一条消息过去，直接会通过长连接推送到服务器，服务器通过长连接推送到别人的客户端APP上去 

3、基于Netty的支撑1亿客户端的消息推送系统实战 

手机经常会收到一些APP的push消息，突然会弹出来一些框，在手机顶部，就是各种APP推送的消息，你的服务器需要向上亿客户端push消息，也得基于Netty来开发 

很多同学如果学电商的系统实战，但是你自己手头做的可能就是一些很low的内部系统，OA，工厂管理，财务系统，电商？另外一个，做大量的基础系统，分布式海量小文件系统，分布式微服务注册中心，仿钉钉的云盘系统，即时通信系统，消息PUSH系统，直接可以说自己是基础架构系统的架构师，或者资深工程师，都可以

###  004_未来课程计划：从Dubbo源码剖析到Netty在RPC框架中的实战

带大家手写RPC框架，类似Dubbo，之前Dubbo非常的不活跃了，很多公司其实都会封装自己的版本，但是现在阿里重启Dubbo了，手写RPC框架就没意义了，Netty除了中间件系统、基础系统的研发，用在RPC框架里，是分布式系统通信框架，网络通信，RPC框架都是用的Netty来做网络通信

直接来读Dubbo的源码，然后剖析类似Netty的网络通信框架在RPC框架里的运用和实战

### 005_未来课程计划：完成两个工业级的中间件项目的架构重构 

netty的三个基础系统，dubbo rpc框架源码，两个中间件项目给搞完，50%的微服务注册中心项目，架构做大量的重构，补充进去基于netty的网络通信框架；80%的分布式海量小文件存储系统，精讲zookeeper技术，引入这个系统，做元数据节点改造成分布式和高可用的，两个项目完成 

底层技术极为扎实，并发源码，netty源码，大量的基础系统、中间件系统的项目实战，积累大量的底层技术的实战经验 

技术功底极为深厚，基础架构经验，2020年开春，或者2019年年底出去找工作，今年的一大堆的基础架构的项目，都可以派上用场了，哪怕你做的项目很low，但是你可以用已经讲解过的亿级流量的缓存架构，分布式架构，基础架构，拿出去说事儿 

中小公司的架构师，就没问题了，年薪五六十万，去面大公司的资深工程师，妥妥的，阿里P6+，年薪五六十万 

明年，高并发、高性能、高可用的业务系统架构，对应的源码都搞完，1年多的时间 

3年多的课，100%吃透 -> 有我指导跳槽两次左右，经历一些知名点的公司 ->  

中小公司的首席架构，独角兽（估值几十亿美金的公司），知名创业公司（估值几亿美金），高级架构，资深架构，七八十万起步；大公司，P7，学历、履历，七八十万起步的 

### 006_使用Java原生NIO进行网络编程有哪些缺陷？

NIO网络编程技术，大量的进行了项目实战，自己封装和开发了复杂的网络通信的程序

连接异常、网络闪断、半包读写、网络拥塞、异常码流，一大堆生产问题需要自己进行大量的编程来实现

如果要直接使用原生NIO进行开发，比如大名鼎鼎的Kafka就是自行基于NIO封装的网络通信框架，那么需要对上面的问题进行处理，比如我们之前的项目，就必须考虑到网络中断，连接异常，粘包拆包，请求排队，等等，一大堆的问题

Kafka可以这么做，我们可以这么做，但是不见得所有普通人都可以这么做，别看我轻松把代码写了出来，但是其实对大部分人来说，这是很困难的一件事情

如果你技术很牛，能hold住，其实可以自己基于Java NIO来封装和定制开发；但是如果说对网络通信这块不是很了解的话，或者说没有那么的精通，建议采用NIO框架来进行编程，框架就是把常见的各种生产问题都考虑到了，都做了优化

### 007_大名鼎鼎的Netty到底有哪些优点让我们去使用它？ 

NIO框架：Netty，封装了底层很多复杂的网络通信细节，让你开发程序非常的简单，它提供了很多的高阶的功能，可以让你基于他开发出来非常复杂的网络通信程序 

NIO API，有点复杂，Selector、Channel、SelectionKey、Buffer 

Netty简化了网络编程的API 

数据传输，直接基于Buffer封装成二进制字节流的数据格式，网络通信的时候，需要支持不同的协议，而且可以对自定义的数据结构进行编码和解码，Netty都支持了 

Netty还提供了很灵活的扩展的功能 

Netty：高性能、高并发/高吞吐、高可靠 

Netty：大量的商业项目都使用了Netty，所以经过了复杂生产环境的验证，基本上来说作为一个开源项目非常的成熟

### 008_搭建一个Netty入门程序的工程以及配置好其Maven依赖 

NIO，必须先写一些Hello World程序

### 009_一步一步动手开发Netty入门程序的服务端

### 010_一步一步动手开发Netty入门程序的服务端（2）

初次学习Netty的新手，学习Netty的陡峭程度可能比Java NIO还要高一些

但是Java NIO是原生的，其实你会发现，Java NIO的那套模型因为省略了很多复杂的概念，所以去芜存菁，简洁，学习的难度曲线很平顺，很好理解，很好上手

Netty学习陡峭曲线是很陡峭的，学习挺难理解的，好用，容易用，功能强大，开发生产级的功能和程序会简答很多，理解他比较难，理解他需要结合他的源码去理解

直接配合源码去理解Netty他的架构原理，底层细节，彻底搞透Netty内部到底是怎么回事，接下来再来基于Netty去开发一些基础系统，做一些项目 

### 011_一步一步动手开发Netty入门程序的客户端

### 012_一步一步动手开发Netty入门程序的客户端（2）

### 013_运行我们的Netty第一个入门程序调试一下看看效果

### 014_把Netty的源码包下载追加到Eclipse中方便我们调试源码

### 015_先来看看让人懵逼的EventLoopGroup是什么？ 

EventLoopGroup，本质在底层就是一个线程池的这么一个东西，可以让你从里面获取新的线程，以及他会负责管理这些线程的生命周期 

他的线程池里的默认的线程数量，实际上就是你的机器可用的cpu核的数量 * 2，比如说你是4核8G的机器，那么默认线程池的线程数量就是8个，最小最小也得是1，起码线程池里得有1个线程 

ThreadFactory是null，肯定是会用默认的机制去创建新的线程出来 

ThreadPerTaskExecutor + 默认的ThreadFactory 

你有多少个线程，就会对应一个EventExecutor数组，每个线程就对应一个EventExecutor

### 016_Netty的EventLoopGroup线程池是如何进行初始化的？ 

每个线程就对应了一个NioEventLoop 

很多的线程，每个线程叫做NioEventLoop，每个线程都会负责一部分的客户端连接的SocketChannel，对这些SocketChannel都会注册在线程自己的Selector中，每个线程通过自己的Selector去轮询（Loop）他负责的这一批客户端连接的网络请求事件 

NioEventLoop，负责轮询Nio事件的线程，轮询多个客户端连接的Nio事件 

线程池的初始化，NioEventLoopGroup，cpu核 * 2 = 线程数量，每个线程就对应一个NioEventLoop，有一个自己的Selector，每个线程就通过Selector负责一批SocketChannel（客户端连接）的Nio网络事件的轮询

### 017_那么ServerBootstrap又是个什么东西呢？

### 018_作业：对比NIO程序自己去探索Netty Server的启动过程 

为了让大家开始真正的学会“渔”的能力，不要总是只是知道跟着我来学，每周都给大家留一些作业，让大家课后自己去做，有问题可以QQ找我来提问，也可以在学员群里跟大家去交流，都可以 

对比一下NIO Server程序启动的过程，你可以自己顺着Netty的源码先去预习，先尝试自己去分析，Netty Server启动的过程，是涉及到了哪些东西和环节，用processon画画图，自己先预习和思考 

下周我会把Netty Server启动初始化的源码，Netty Client初始化的源码，互相发送请求，处理请求，返回响应的这个过程，源码，给大家快速的梳理一遍 

### 019_将Netty版本调整回4.x，避免使用废弃的5.x版本

### 020_对照原生NIO代码看看Netty如何创建ServerSocketChannel

### 021_Netty是如何对ServerSocketChannel进行初始化的？

### 022_如何将ServerSocketChannel注册到Selector由独立线程轮询？

### 023_用一张图来展示出ServerSocketChannel初始化的过程

01_Netty源码架构

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02301.png)    

### 024_补充一个细节：ServierSocketChannel如何绑定监听端口？ 

netty官方已经废弃掉了最新的5.x版本，一直在更新的还是4.x版本 

event loop，轮询网络事件，线程 

这里面的东西肯定是负责通过一个Selector去轮询多个网络连接的事件 

ServerSocketChannel -> 端口号，然后通过一个独立的线程（Acceptor）去使用Selector去轮询这个ServerSocketChannel是否有连接的事件接入，如果有的话，跟客户端建立连接，然后将客户端的连接分发给Processor线程 

每个Processor线程应该是负责处理一部分客户端的连接，使用自己的Selector不断的轮询各个客户端连接的网络事件，收到了请求解析出来 

请求应该是交给Handler线程去进行处理，处理请求，设置响应 

Processor把响应发送给对应的客户端就可以了 

Broker这块的代码，写的过于复杂了，而且设计的不太好，scala来写服务端代码，没看出来任何优势，玩儿玩儿scala的语法糖，client -> Producer + 网络通信，代码写的是相当的棒，非常的有章法 

Netty有点恶心，核心架构、核心机制、底层设计，还是不错的，高并发、高可靠、高性能，做的是不错的，代码写的一点都不好

### 025_Netty的线程是如何轮询ServerSocketChannel的网络连接事件的？

01_Netty源码架构 (1)

 ![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02501.png)

select方法，大体上可以认为在一个无限循环的方法里，不停的去等待是否有新的网络事件的发生，如果有就返回

### 026_如果发现了客户端发起的连接事件是如何进行处理的？

01_Netty源码架构 (2)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02601.png)

### 027_Server端对建立好的客户端连接是交给谁来轮询网络请求的？

01_Netty源码架构 (3)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02701.png)

 

就是ServerSocketChannel的pipeline有一个初始化的过程，他在里面加入了一个自己的Handler，这个Handler就是专门负责处理连接的，他就会在这里来进行处理

### 028_对于客户端发送过来的消息是如何读取以及处理的？

01_Netty源码架构 (4)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02801.png)

### 029_系统处理完请求后又是如何将响应消息发送客户端的？

01_Netty源码架构 (5)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\02901.png)

### 030_作业：大家根据Server端源码分析思路，自行分析Client端源码

### 031_Netty Client是如何尝试跟Server建立连接的？

### 032_具体看看Netty Client对Server发起connect请求的源码

### 033_Netty Client的线程是如何轮询建立好连接的网络事件的？

### 034_Netty Client是如何将我们准备好的请求发送出去的？

### 035_从Netty Server端返回的响应是如何接收的？

01_Netty源码架构 (6)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\03501.png)

### 036_再看Netty线程模型：如果Server端单线程会有什么问题？ 

Server端纯用单线程来处理，可以不可以？ 

但是性能实在是太差了，单线程，就算是把CPU给跑满，也难以应对大量的客户端高并发的连接和请求，CPU打满，大量的客户端发送过来的请求来不及处理，直接进行超时处理，还需要进行请求重试 

很有可能会导致大量的连接频繁的发送请求超时，最后连接断开，非常的不稳定 

单线程是来不及处理的，会导致性能很差，连接非常的不稳定

### 037_线程模型的进化：单个Acceptor线程 + 多个Processor线程

### 038_支撑百万连接的线程模型：Acceptor线程池 

现在的连接都是最最简单的TCP三次握手的这种连接而已 

很多生产环境的连接会更加的复杂，Acceptor线程跟每个客户端发起的连接请求不光是TCP三次握手，还要基于授权认证的协议跟连接进行复杂的权限确认，确保说这个客户单是有权限接入进来的 

百万连接，单个高配置物理机，32核128G的高配机器，单机支撑百万客户端连接的话，单个Acceptor线程肯定是来不及处理这么多的连接请求的 

Acceptor线程进行池化，由很多个Acceptor角色的线程负责处理跟客户端连接的请求，每个Acceptor线程可以负责跟一批客户端建立连接，建立好的连接再转交给 Processor线程池，100个Acceptor线程 

百万 连接，每个Acceptor线程负责处理1万个客户端的连接，那这样的话，压力就会小很多

### 039_Netty中的两个线程池就足够了吗？不，还不够！

### 040_再看Netty Server端读取请求的网络IO核心源码流程

### 041_仔细阅读Unsafe内部网络IO的数据缓冲机制源码

说一下他的这个组件的RecvByteBufAllocator，动态的根据你上一次请求获取到的数据大小，动态的预估这次请求的数据大小大致是会有多少，根据预估的结果创建出来一个比较符合预估大小的一个缓冲区出来 

他应该是负责去分配ByteBuf数据缓冲区的一个组件 

他每次到底是分配多大的一个ByteBuf数据缓冲区呢？分布式海量小文件存储系统的时候，自定义了一套协议，kafka，每次请求过来，请求头都必修带着本次请求数据的大小，我们是根据请求头来分配ByteBuffer的 

netty而言，他不能指望每次请求都有一个请求头，通用框架，根据每次请求的大小动态的预估下一次请求的大小，动态根据预估的大小创建对应的ByteBuf

### 042_细看一次网络请求数据到底是如何读取出来的？ 

根据预估分配一个ByteBuf 

根据预估分配的ByteBuf的大小创建一个原生的NIO ByteBuffer，从原生SocketChannel中读取数据放入原生ByteBuffer里，再把数据放入Netty ByteBuf里，完成一次请求数据的读取，就搞定了

### 043_原生Java NIO中的ByteBuffer使用时有哪些缺陷？  

1、原生ByteBuffer是固定长度的，一旦创建无法动态的调整大小，创建大了，会导致内存浪费；创建小了，就不过放的 

2、ByteBuffer非常的不好用，因为他里面需要read，channel读取数据放入ByteBuffer中，此时要使用这个ByteBuffer里的数据，还得记得调用flip，重置position，然后才可以从ByteBuffer里读取数据  

Netty为什么要封装自己的ByteBuf呢？第一，他允许ByteBuf大小动态调整，第二，他封装的读写API更加的简单，不需要去care flip 

### 044_Netty为请求处理提供的良好扩展：自定义业务逻辑链条

01_Netty源码架构 (7)

![](C:\Users\zy199005\Desktop\中华石杉\images\java\12\04401.png)

可以加入自己的业务处理链条，一个请求过来，同步处理，同步返回响应；异步处理，提交请求数据到一个异步线程池，线程池是你自己做的，异步读写磁盘，干一些网络通信的事情，直接返回响应即可 

同步+异步一起干，有些东西是需要同步的，有些东西可以是异步的

### 045_先回顾一下传统BIO网络通信为什么性能那么差 

Netty架构设计，做一个总结和收尾，高并发、高性能、高可靠 

BIO，一个请求一个线程，高并发下线程资源不够，机器cpu负载100%，机器容易宕机，或者系统性能急剧下降 

你呢基本上来说，是来一个请求用一个线程来处理，或者一个客户端就用一个线程来进行处理，几千个客户端要来发起请求，难道你用几千个线程来抗吗？来一个请求创建一个线程进行处理 

线程资源是很宝贵的

### 046_Netty底层基于NIO的非阻塞模型为什么可以支撑高并发 

高并发 

大量的客户端，成千上万，十万级，百万级；大量的请求过来，每秒几千几万，每秒十万

### 047_JDK 1.4采用select、poll模式实现的NIO是什么原理 

线程遍历所有的文件（linux里一切皆文件，比如说网络连接也可以理解为文件），如果某个网络连接没有就绪（没有网络事件发生），就对那个网络连接插入一个wait_queue节点，然后继续遍历别的文件 

如果有某个文件有就绪状态（比如有网络数据包到达），就把这些文件的就绪状态复制给用户进程，如果没有一个文件是就绪状态 

那么就阻塞等待唤醒，如果某个网络连接有事件（比如网络数据包到达），就遍历自己的wait_queue等待队列，然后回调函数，唤醒在阻塞等待的线程 

线程被唤醒之后，再次遍历所有文件的就绪状态，如果有就绪就返回给用户进程 

实现单个线程的多路复用，select/poll模式几乎是一致的 

### 048_JDK 1.5之后对NIO优化为采用epoll模式为何提高了性能 

select/poll模式下，如果线程被唤醒通知有某个文件是就绪状态，此时线程需要重新遍历所有的文件去看到底是谁就绪了 

Selector注册了100个客户端，其中只有一个客户端有网络事件，这个时候Selector必须重新遍历一遍100个客户端，收集出来一个客户端的网络事件交给你的线程来进行处理，性能有问题 

epoll不一样，如果有某个文件有就绪状态，直接回调epoll回调函数，把就绪的文件放入epoll的一个数据结果中，然后epoll直接就知道哪些文件是就绪的，不需要有一个重新遍历的过程，所以效率更高  

### 049_如何优化Netty的线程模型支撑百万连接接入？

一般采用多个线程监听ServerSocketChannel的连接请求，然后多个线程负责IO读写，这是常见的模式，在高配置物理机上，甚至可以单机支撑百万连接，比如64核128G的高配物理机，Acceptor线程开启100个，负责百万客户端的接入，然后IO线程开启1000个甚至更多，每个IO线程仅仅负责1000左右的客户端读写请求 

这就可以单机实现百万连接了 

### 050_Netty的无锁串行化设计思想是如何保证高性能的 

一个IO线程负责监听多个客户端的请求，并且处理请求，然后发送响应 

这就是所谓的无锁串行化设计思想，假如说一个IO线程接收了多个请求分发给多个不同的Worker线程去处理，其实就是之前我们做的那种思路，可能会导致线程与线程之间进行共享资源的争抢 

一旦发生了锁的问题，那么会导致并发能力更差 

所以Netty默认就是一个IO线程无锁串行化处理各个客户端的请求，处理完一个返回了响应，再处理下一个请求 

如果我们要是认为再加一层Handler线程池，争抢处理请求，而且不会导致资源争用，那么其实自己再加一层线程池也没什么 

高并发架构设计里去，你自己开启很多线程，这些线程处理共享资源的时候频繁加锁，争用锁，导致性能的问题 

### 051_Netty是如何尽一切可能去优化内部的多线程并发的 

高并发 

对Netty框架内部而言，主要的性能优化就是在于并发的优化，尽量避免使用synchronized等重量级锁，而是采用volatile、CAS、并发安全集合、读写锁，来尽最大可能优化多线程并发的锁争用问题 

NioEventLoop，内部就对很多成员变量都用了volatile，尽量对一个线程写，其他线程读的场景，采用volatile保证可见性即可，避免使用重量级锁 

ChannelOutputBuffer中，很多统计数据，都是采用Atomic类实现的，多线程并发读写，都没问题 

NioEventLoop，newTaskQueue，ConcurrentLinkedQueue，之前给大家分析过这些源码，其实他们底层大量基于volatile、CAS，降低锁粒度，各种方式来写的，优化了并发性能 

HashedWheelTimer，里面用了读写锁来优化 

大量的高级并发技术的运用，大幅度优化了Netty内部并发锁冲突的性能问题，大家以后注意，在并发编程的时候尽量要提升性能 

作业：希望大家可以把netty的源码导入intellij idea中去，在那个里面可以去搜索搜索，netty中框架内部，为了优化多线程并发能力，减少锁争用，volatile、CAS、并发集合类、读写锁，在哪里使用了，读一读细节

### 052_网络通信中的序列化环节应该如何优化性能？ 

比如说你有一个自定义的对象，序列化成二进制字节流，把字节流发送到服务端；服务端反序列化，二进制字节流还原成一个对象，处理，再次把响应序列化成二进制字节流，发送到客户端去，客户端再次反序列化成响应对象 

序列化，就是把你的数据搞成二进制字节流，或者二进制数组；反序列化，就是一个反过程 

Java序列化机制，会把序列化以后的字节流搞的太大了，而且序列化的过程也较慢，所以不是太好，Netty默认支持Protobuf，序列化里性能几乎是最好的，而且序列化以后的字节流也不大 

所以序列化这块性能更好 

Protobuf，序列化成二进制字节流，小，速度快，反序列化速度也会比较快 

### 053_通过Direct Buffer如何大幅度提升网络读写性能？ 

ByteBuffer，有一种特殊的Buffer，Direct模式的Buffer，创建ByteBuffer的时候可以指定是创建Direct模式的Buffer  

不用Direct模式，性能比较差 

ByteBuffer，direct模式来创建的 

### 054_如何通过内存池实现ByteBuf分配与回收的性能优化 

内存块的申请和销毁都是比较耗时的，对性能也有影响 

内存池，ByteBuf池子的概念，就可以在池子里维护一定的ByteBuf，需要的时候直接从池子里获取ByteBuf使用即可，使用完毕之后就重新放回到池子里去

### 055_对核心的网络参数如何优化可以保证高性能？ 

SO_RCVBUF和SO_SNDBUF，128kb或者256kb 

SO_TCPNODELAY，关闭这个算法，避免自动打包发送，避免高延时 

### 056_Netty的高可靠性：自动识别连接断开和释放资源 

CONNECT_TIMEOUT_MILLIS，可以设置连接超时时间

### 057_链路不可用的自动监测：Netty的空闲检测机制

### 058_在NioEventLoop线程中处理IO异常避免线程中断

### 059_NIO epoll空轮询bug：Netty自动识别与处理 

NIO底层epoll，JDK版本的bug，轮询遍历所有的网络连接，epoll空轮询，明明没有任何的事件，他不停的疯狂的轮询底层的所有的网络连接，导致CPU 100%，NIO，Netty在底层直接解决掉了 

直接把注册在这个Selector上面的SocketChannel注册到新创建的Selector上面去，把当前这个有问题的Selector给关闭，释放资源，避免他空轮询导致cpu 100% 

### 060_自动对缓冲池中的缓冲块进行释放避免内存泄露

### 061_Netty架构设计总结：高并发、高性能、高可靠、易于扩展

高并发架构设计：两层线程模型、NIO多路复用非阻塞、无锁串行化、并发优化

高性能架构设计：Protobuf序列化协议、direct buffer、bytebuf内存池、网络参数优化

高可靠架构设计：网络连接断开、网络链路探查、NioEventLoop线程容错、JDK epoll bug处理、内存块自动释放

可扩展架构设计：handler链条你可以自己扩展、序列化协议、定制网络通信协议

### 062_作业：对于提到的架构设计的细节自己在源码里找出来 

光看视频，只能达到50%的效果，认认真真完成我布置的作业，有问题可以来问我，达到剩余50%的效果，100%的效果 

高并发架构设计：两层线程模型、NIO多路复用非阻塞、无锁串行化、并发优化

高性能架构设计：Protobuf序列化协议、direct buffer、bytebuf内存池、网络参数优化

高可靠架构设计：网络连接断开、网络链路探查、NioEventLoop线程容错、JDK epoll bug处理、内存块自动释放

可扩展架构设计：handler链条你可以自己扩展、序列化协议、定制网络通信协议