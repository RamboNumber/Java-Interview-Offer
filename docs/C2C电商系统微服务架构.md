# C2C电商系统微服务架构

### 01_我们为什么要使用复杂的微服务架构？

单块应用：在intellij idea/eclipse建一个工程，spring mvc+spring+mybatis整合，里面写一堆controller、service、dao、mapper、sql，加一大堆的配置文件，有可能还会引入一些redis、elasticsearch、mq之类的一些依赖，可能会操作一些类似的东西

maven插件，把工程里的所有代码和依赖打包成一个jar包/war包，配置文件

公司提供给你一台linux服务器，虚拟机/物理机，在机器上你自己先部署了一个tomcat，然后把你写好的系统的jar包/war包放到tomcat指定目录下去，然后重启一下tomcat，tomcat一旦启动就会监听类似于8080的端口号

然后你针对8080 端口号发起http请求，请求会由tomcat直接转交给你的springmvc，一层一层调用你写的代码

单块应用的问题，10个人以上维护一个系统，频繁的代码分支进行合并，冲突很多，因为人多，导致很混乱，谁也不能保证10个人以上还能每个人就一定只会更新自己负责的一部分代码

每次光是解决大量的代码冲突，就会耗费好几天的时间，解决完了冲突，还得进行一下测试，保证代码正常运行

完全可能会你拉一个分支修改了代码之后，别人已经在你上线之前就更新了别的分支的代码，合并到master分支还发布上线了，所以你上线之前一般需要把master分支最新代码合并到一个测试分支上，把你的功能分支再合并上去，然后进行测试

每次测试，可能都会因为别人频繁改动代码，导致必须把系统所有相关功能都测试一遍，而且很可能因为别人胡乱改过你负责的部分的代码，导致合并之后，测试的时候运行就有问题，这是完全可能的

所以这是一种严重耦合的情况，会导致每次测试和上线，都需要大量的成本解决代码冲突因为别人可能胡乱修改你的代码，导致测试需要全量回归所有功能，导致上线很慢很麻烦，而且更麻烦的是不同版本之间的上线协同

可能你拉的一个代码分支，修改的仅仅是针对5个功能，但是另外一个人之前修改了20个功能，你们的代码一旦合并了 ， 可能导致大量的地方都有变动，可能会导致另外几十个依赖你们的代码的功能也会有一定的潜在的风险

在这种单块系统的情况下，代码一旦合并，上线之前，必须在测试环境对上百个完整的功能都做一下回归测试，这个耗费的时间就很长了。

可能有的人要在某一天上线，你就必须等待他后一天再上线，每个人都必须做上线前做代码合并，然后全量回归，最后再上线，下一个人继续合并最新代码，全量回归，再上线，这个过程很麻烦，一旦出了差错，可能导致你直接把代码合并到master部署，上线可能会失败的。

（1）很多人维护一个单块应用，频繁的进行代码合并，频繁的解决代码冲突，解决冲突的时间和成本很高的，导致开发效率低下

（2）每次上线都要跟最新代码进行合并，重新进行全量功能的回归测试，很多代码都可能给有变动，必须全量回归测试，耗费时间很多，开发效率低下

（3）多人频繁上线，你等我，我等你，互相协调困难，而且可能会出现别人多次先上线你多次重复的合并代码，解决冲突，全量回归测试，做很多次重复的事情，导致效率低下

（4）测试服务器都很少，就一台测试服务器，你开发好了代码，你连测试都不能测，必须等待别人的分支先测试完毕上线，你才能去合并代码，解决冲突，等到一个测试环境可以用，回归测试

（5）10个人以 内维护一个单块应用，基本这些成本还不算太 大，但是一旦10人以上维护一个单块应用，上述的成本会极大，导致系统每个需求的测试和上线，都非常缓慢，要耗费大量时间做全量回归测试，上线日期还得互相配合互相等待互相协调一个疏忽，就可能导致没侧测试完全的代码上线出线上事故

（6）如果你想要升级技术架构，不能随意升级，因为可能影响别人，你得让所有人都学习这个新技术架构才行；如果你想要升级已有技术的版本，不能随意升级，因为可能新版本对你的代码没影响，但是对别人的代码有影响

10人以 内基本都问题不大，但是10人以上，往往 维护一个单块应用就比较 麻烦了，通常建议的比较 合理的是5人以 内的小团队 负责维护一个系统，这对这个问题，就需要进行微服务架构，把大系统拆分为很多小系统，几个人负责一个服务

这样每个服务独立 的开发、测试和上线，代码冲突少了，每次上线就回归测试自己的一个服务 即可，测试速度快 了，上线是独立 的，只要向后兼容 接口就行了，不需要跟别人等待和协调，技术架构和技术版本的升级，几个人ok就行，成本降低，更加灵活了

### 02_国内BAT互联网大厂的微服务架构演进路线

几乎所有技术组件都是自研，国内最早的微服务架构几乎就是一些互联网大厂自研了一大堆的组件，来支撑拆分成N多服务的大型系统的运行和多人协作开发，包括系统的监控和维护，等等 

注册中心、RPC框架、多环境隔离、自动化部署、分布式事务、限流/熔断/降级、配置中心、监控中心、链路监控、日志中心、API网关、安全认证、服务治理 

后来在三五年之前，阿里开源的Dubbo比较流行，在国内基本上把系统拆分为微服务的一些大大小小的公司，用的都是阿里开源的Dubbo，注册中心用ZooKeeper的居多，当时dubbo+zookeeper基本就是一个最原始的微服务技术架构的雏形 

至于其他东西，不同的公司可能会找不同的开源项目，但是都没太统一的标准，而且很多公司干脆压根儿就不用其他组件

### 03_海外硅谷互联网大厂的微服务架构演进路线

国外互联网公司，其实也都是几个大公司自己自研，后来逐渐的有一个叫做netflix公司的微服务技术架构开源出来，在国外有很大的影响力，然后接着就被整合到了spring社区，变成了spring cloud项目，里面整合的是netflix等国外公司的微服务相关组件 

早期的spring cloud微服务体系的组件，是以eureka、feign+ribbon、zuul、hystrix，用zipkin和sleuth做链路监控，stream做消息中间件集成，contract做契约测试支持，当然gateway也可以做网关，consul也是一种注册中心，还有跟spring security配合的安全认证，跟k8s配合的容器支持 

这些都是国外公司为主的开源项目，spring cloud打包集成在一起，在国外比较有市场，两三年前在国内也火了，大量公司都开始拥抱spring cloud，尤其是中小型公司，几乎都是用spring cloud 

因此呈现的一个状态，就是大厂几乎都是自研，部分大厂是以阿里的dubbo为核心自研的，部分中小型公司还是以dubbo为核心，加上自己找一些开源项目，然后更大比重的中小型公司，就是spring cloud那套技术架构

### 04_目前国内公司的主流微服务技术栈介绍

两三年前，因为阿里开源的dubbo曾经不怎么维护，然后加上spring cloud完善的技术栈冲击进来，所以大部分中小型公司都开始拥抱spring cloud，dubbo的使用比例下降很多，所以最近两三年，国内微服务这块，其实大公司是以纯自研/dubbo+自研为主的，中小公司是以全面拥抱spring cloud netflix技术栈为主的 

但是最近一年多，情况产生了变化，因为阿里的dubbo重启活跃维护，同时阿里把自己的微服务技术栈融入进了spring cloud体系和标准，形成了一个spring cloud alibaba微服务技术组件，也就是以nacos、dubbo、seata、sentinal、rocketmq等技术为核心的一套技术体系 

注册中心：nacos -> eureka

RPC框架：dubbo -> feign+ribbon

分布式事务：seata -> 无

限流/熔断/降级：sentinel -> hystrix

API网关：无 -> zuul 

spring cloud netflix微服务技术组件，开始更新的非常不活跃，netflix公司公开宣布他之前开源的一些微服务组件未来就不会怎么更新了，这就导致spring cloud netflix微服务技术组件的未来有点不太光明 

spring cloud alibaba微服务技术组件，活跃的更新，社区也重启，做的很好，宣讲，演讲，采访，开始比较活跃起来 

所以最近一年其实很多公司也开始尝试用spring cloud alibaba的技术组件，再加上一些其他的开源组件，同时其他的开源组件里，其实国内前天互联网公司也开源了不少优秀的项目，比如说携程开源的apollo（spring cloud config），大众点评开源的CAT（zipkin、slueth），加上其他国外的优秀开源项目，比如Prometheus，ELK，Spring Cloud Gateway，等等，可以组成一套全新的以国内开源技术为核心的微服务体系 

中心公司开始进行分化，有部分公司还是spring cloud netflix为主的一套技术栈，有少部分公司开始尝试推行spring cloud alibaba技术栈+国内开源的组件（apollo、CAT）+ Prometheus + ELK + Spring Cloud Gateway（Nginx+lua、Kong、Zuul、API网关自研） 

我个人倾向于以及比较认可的，是这套技术体系，也认为会是未来国内的主流，因为netflix很多组件维护的都不够活跃了，所以衰退是必然的，加上国内的开源项目，都是中文文档，中文社区，交流也方便，也很活跃，所以我们的课程主要是以这套国内为主的微服务技术体系为主的，也是面向未来的一套技术体系

### 05_C2C电商社会化治理平台项目介绍

接下来会以一个真实的项目案例来讲解整个微服务技术架构的落地实践，会以一个C2C二手电商平台的社会化治理系统来讲解，因为虽然微服务技术栈跟具体的业务没关系，其实无论什么业务系统，都可以用一套微服务技术体系，但是如果单纯的讲解技术理论，通过一些没任何含义的demo代码来演示，可能很多人没法很好的理解技术和自己项目的关系 

所谓的C2C二手电商平台，就是用户可以在上面作为卖家发布自己的二手商品，然后等待买家来谈，来购买，平台就是作为中间服务方提供一些列的平台功能支持，其实国内大的二手交易平台还是有几个的，同时还有好一些专注于垂直领域的二手电商交易平台，比如说二手奢侈品电商，二手电子产品电商 

C2C二手交易电商平台 

留一个小作业和小任务：希望大家可以自己去体验一下各种二手电商平台的界面，看看他里面都有什么，怎么玩儿的，体验一下 

用户是买家也是卖家，但是会有一个问题，可能卖家会上传违规商品，买家和卖家进行留言互动可能会进行一些人身攻击之类的违规行为，所以本质上这个电商平台从卖家到买家，都是用户，那怎么进行社会化的治理呢？ 

如果是平台方建立规模庞大的审核团队，那么可能会导致每一件商品以及发表的评论都需要进行预审，预审通过了平台才会给你显示出来，但是要知道一个问题，大型的C2C二手电商平台，可能里面日活用户达到数百万，甚至上千万，每天发表的评论可能也多达上千万，如果全部由自建的预审团队一条一条审核，那这个人力成本是无法接受的 

这就需要有一个专门的社会化治理平台 

通过一个平台，以技术的手段，将有人举报违规的内容推送给部分用户，让用户参与到平台治理中来，用户投票决定某个商品或者评论等内容是否违规，这样平台仅仅作为一个桥梁，让用户进行社会化自治

### 06_C2C电商社会化治理平台面向的痛点介绍

接下来会以一个真实的项目案例来讲解整个微服务技术架构的落地实践，会以一个C2C二手电商平台的社会化治理系统来讲解，因为虽然微服务技术栈跟具体的业务没关系，其实无论什么业务系统，都可以用一套微服务技术体系，但是如果单纯的讲解技术理论，通过一些没任何含义的demo代码来演示，可能很多人没法很好的理解技术和自己项目的关系

所谓的C2C二手电商平台，就是用户可以在上面作为卖家发布自己的二手商品，然后等待买家来谈，来购买，平台就是作为中间服务方提供一些列的平台功能支持，其实国内大的二手交易平台还是有几个的，同时还有好一些专注于垂直领域的二手电商交易平台，比如说二手奢侈品电商，二手电子产品电商

C2C二手交易电商平台

留一个小作业和小任务：希望大家可以自己去体验一下各种二手电商平台的界面，看看他里面都有什么，怎么玩儿的，体验一下

用户是买家也是卖家，但是会有一个问题，可能卖家会上传违规商品（侵犯版权的盗版商品、假冒伪劣商品、诈骗团伙上传的虚假商品、违法的商品），买家和卖家进行留言互动（通过IM系统进行私聊，对商品还可以进行评论）可能会进行一些人身攻击之类的违规行为，所以本质上这个电商平台从卖家到买家，都是用户，那怎么进行社会化的治理呢？

如果是平台方建立规模庞大的审核团队，那么可能会导致每一件商品以及发表的评论都需要进行预审，预审通过了平台才会给你显示出来，但是要知道一个问题，大型的C2C二手电商平台，可能里面日活用户达到数百万，甚至上千万，每天发表的评论可能也多达上千万，如果全部由自建的预审团队一条一条审核，那这个人力成本是无法接受的

这就需要有一个专门的社会化治理平台

通过一个平台，以技术的手段，将有人举报违规的内容推送给部分用户，让用户参与到平台治理中来，用户投票决定某个商品或者评论等内容是否违规，这样平台仅仅作为一个桥梁，让用户进行社会化自治

### 07_C2C电商社会化治理平台的解决方案介绍

### 08_C2C电商社会化治理平台整体架构设计

### 09_C2C电商社会化治理平台的微服务拆分设计 

核心思路，开发一个系统，很多非法举报进行社会化治理，所谓社会化治理，就是把每个举报都圈定一部分用户作为评审员，让他们进行投票，如果过半数判定举报成立，就成立，同时为了激励用户参与进行评审，可以给他们一些奖励，比如说奖励一些虚拟货币，后续可以在专门的积分兑换商城里兑换一些奖品 

在C2C二手电商平台里，一般举报有如下的一些场景： 

（1）   通常这类平台会做一些用于社交的社区/论坛之类的，可以发帖之类的，所以这类帖子内容如果有一些不良言论，可能会被举报

（2）   有的人挂出的二手商品本身就可能是非法商品，可能会被举报

（3）   一般有人挂出自己的二手商品之后，别人可以进行留言提问，然后留言交互过程中可能会涉及到侮辱性的语言，此时留言可能会被举报

（4）   一般这类平台都会提供买卖双方进行私聊的IM功能，这个私聊对话可能涉及不良言论，可能会被举报

（5）   还有时候商品不光是涉及留言和私聊，对这个卖家售出的历史商品，买家可能是可以进行评论的，这个评论是卖家个人积累的一个信誉评价，但是可能出现不良言论，此时可能会被举报 

如果说我们把社会化治理平台做成一个单块系统，这里的逻辑也是比较多的，可能这个系统就需要5个人以上来同时维护，而且不同的人负责维护里面不同的代码模块，中间还会有一些大家共同维护的一些代码 

频繁的代码冲突、你先修改了一部分代码部署上线了一次、我必须合并代码再次回归测试、另外一个人又修改了一部分代码部署上线了一次、我必须再次合并代码回归测试、等待别人使用测试环境、等待别人部署上线、多人协作开发效率还是很低的、如果说你真的体验过，一个大的系统拆分几个服务，每个人就负责维护自己的服务 

习惯了一个人维护一个服务，根本就不愿意再跟别人一起维护一个服务了 

针对上述场景，全部都可以在有人进行举报的时候，对应的功能模块（比如说商品、论坛、IM、评论，等等），就可以接入和调用社会化治理平台，我们对外提供的应该首先是一个举报服务，作为一个入口 

这个举报服务涵盖了：投票制度管理（可以针对不同的举报类型，定义不同的投票制度，比如5人3胜，3人2胜，最大等待时长，替补评审员机制，等等），提交举报接口，调用评审员服务圈定评审员，PUSH管理，举报查询接口，投票生命周期管理（发起投票、过程监控、超时等待、候补评审员管理、投票结果），调用奖励服务 

还有一个评审员服务，包含了：评审员管理（根据用户画像的标签，由运营去圈定一波人做评审员，其实核心在于圈定那些每周都至少会来逛一次的活跃用户，这个规则可以自行配置），评审员圈选，评审员状态管理，评审员过滤，疲劳度控制，评审员自动调整，候补评审员选择，评审结果接收 

最后是一个奖励服务，包含了：奖励规则配置，奖励发放，奖励兑换

### 10_为什么微服务化的系统架构必须要有注册中心？

理解起来，偏向于真实实战场景一些，讲起来也不会那么的纯理论，大家听起来也没那么的枯燥，http接口，更多的情况下，可以对外暴露的仅仅是一个MQ的入口，其实这种系统在开发的真实过程中，一般来说架构技术选型

如果为了要让社会化治理的系统，跟其他的系统进行解耦

开的口子最好是MQ，其他系统只能看到这个MQ，提交举报消息到MQ去，举报服务可以消费MQ里的人家提交的举报

对MQ技术稍微有一点了解的话，kafka，rocketmq，rabbitmq，消费的时候，都是可以让举报服务部署多台服务器来消费的，topic -> partition，多个partition分区，就可以让举报服务部署多台服务器并行的消费举报任务

分布式系统的spring cloud技术没有基本的了解，互联网Java工程师面试突击第二季，分布式篇，我对spring cloud技术体系有一个较为深入的讲解，spring cloud netflix技术栈，rpc框架是feign，是基于http协议

发送过去的就是一个http请求，人家返回给你的也是一个http响应

假设你要是基于dubbo，13.56.2.308: 7890，直接基于TCP跟那台服务器的指定端口建立一个TCP长连接，短连接，可以基于一定自定义协议去制定数据如何组装，基于序列化的协议，把你的请求数据序列化成二进制数据流发送过去，就是一个请求

### 11_ZooKeeper、Eureka、Consul、Nacos的选型对比

![服务注册中心选型对比](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/c2c/images/11/%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%E9%80%89%E5%9E%8B%E5%AF%B9%E6%AF%94.png)

zookeeper、eureka、consul、nacos

zookeeper和eureka，consul用的没那么多，nacos现在用的越来越多，以后也会是一个大的趋势，但是现在可能还没那么的普及

zookeeper的基础知识以及基本的架构原理，互联网Java工程师面试突击第三季，里面是有讲过zk的一些架构原理，如果大家不了解的话，可以去看一下

zookeeper的原理，leader+follower，leader写，同步到follower，follower可以读，保证顺序一致性，就是基本尽量保证到数据一致的，主动推送，典型的CP，leader崩溃的时候，为了保证数据一致性，尽量不要读到不一致的数据，此时要重新选举leader以及做数据同步，此时集群会短暂的不可用，CP

服务注册中心选型对比的时候，其他的分布式系统选型的时候，CP，AP

P简单来说就是任何分布式系统一般都会满足，他就是分区容错性；CP，C，一致性，尽可能的去保证你读取到的数据是一致的，牺牲掉一个A，可用性，一旦leader崩溃，zk可能会有一个短时间内，几十s有可能，集群不可用，此时需要重新选举一个leader，然后再做数据同步，保证数据一致性之后再开放让你来读取

consistency，availability

关于eureka的一些架构原理，21天互联网Java工程师面试训练营（分布式篇），儒猿技术窝，重点讲解了eureka的一些架构原理

eureka的原理，peer-to-peer，大家都能写也都能读，每个节点都要同步给其他节点，但是是异步复制的，所以随时读任何一个节点，可能读到的数据都不一样，任何一个节点宕机，其他节点正常工作，可用性超高，但是数据一致性不行，AP

Consul也是基于raft算法的CP模型

Nacos也是基于raft算法的CP模型，同时也支持配置成类似eureka的AP

其实CP或者AP也都行，CP就是偶尔可能短时间不可用，AP就是可能数据不一致，两个都有问题，但是在生产环境下，无论CP还是AP其实都用的很多

其实说白了，zk作为注册中心是早期dubbo时代的标配；后续spring cloud进入国内市场，大家就都用eureka了，但是spring cloud也推荐了consul，所以consul也有不少人在用，zk、eureka、consul，其实都有人用

但是未来还是建议大家用nacos，因为nacos的功能最为完善，包括了雪崩保护、自动注销实例、监听支持、多数据中心、跨注册中心同步、spring cloud集成、dubbo集成、k8s集成，这些都支持，其他的几个技术基本都支持部分罢了

### 12_SpringCloudAlibaba之Nacos注册中心架构原理

![Nacos架构原理](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/c2c/images/12/Nacos%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png)

偏向于底层的一些网络知识的话，基础知识，互联网Java工程师面试突击第三季，主要就是讲解一些基础知识，网络的东西，TCP，HTTP，长连接和短连接，这些基础概念在里面都是讲过的

服务通过nacos server内部的open api进行服务注册，nacos server内部有一个sevice服务的概念，里面有多个instance实例的概念，同时对不同的service服务可以划归到不同的namespace命名空间下去

namespace可以是一个技术团队，比如说一个技术团队，业务A的技术团队所有的服务都放在一个namespace命名空间下面，业务B的技术团队所有的服务都放在另外一个namespace命名空间

其实说白了，注册的时候就是在注册表里维护好每个服务的每个实例的服务器地址，包括ip地址和端口号，这是最为关键的

而且一旦注册成功之后，服务就会跟nacos server进行定时的心跳，保持心跳是很关键的，nacos server会定时检查服务各个实例的心跳，如果一定时间没心跳，就认为这个服务实例宕机了，就从注册表里摘除了

其他服务会从nacos server通过open api查询要调用的服务实例列表，而且nacos客户端会启动一个定时任务，每隔10s就重新拉取一次服务实例列表，这样如果调用的服务有上线或者下线，就能很快感知到了

此外还可以对要调用的服务进行监听，如果有异常变动会由nacos server反向通知他

### 13_深入Nacos服务注册中心的内核原理

nacos本身的话，其实是完全可以脱离spring cloud自己独立运作的，但是他目前是集成到spring cloud alibaba里去的，也就是在spring cloud的标准之下实现了一些东西，spring cloud自己是有一个接口，叫做ServiceRegistry，也就是服务注册中心的概念

他是一个接口，nacos是实现了一个实现类的，也就是NacosServiceRegistry，实现了register、deregister、close、setStatus、getStatus之类的方法

自动装配是一个spring boot的一个概念，如果大家不理解的话，可以自行搜索一些资料去查阅，用最最简单的话来说，自动装配的意思，其实就是说系统启动的时候，自动装配机制会运行，实现一些系统的初始化、自动做一些事儿

比如说spring cloud alibaba，假设用dubbo开发服务，本质上是有一个自动装配类的，这个自动装配类会监听spring的ApplicationStartedEvent这个事件，其实简单理解就是服务启动的时候通过spring的一些动作，监听到某个事件就自动运行了

自动运行，就是去调用NacosServiceRegistry的register方法去进行服务注册

而且除了注册之外，还会通过schedule线程池去提交一个定时调度任务，源码如下：

this.exeutorService.schedule(new BeatReactor.BeatTask(beatInfo), beatInfo.getPeriod(), TimeUnit.MILLISECONDS)，这就是一个心跳机制，定时发送心跳给nacos server

接着会进行注册，注册的话是访问nacos server的open api，其实就是http接口，他有一个接口：http://31.208.59.24:8848/nacos/v1/ns/instance?serviceName=xx&ip=xx&port=xx，这么一个东西，也没什么特别的，这里就是访问注册接口罢了

nacos server那里是基于一个ConcurrentHashMap作为注册表来放服务信息的，直接会构造一个Service放到map里，然后对Service去addInstance添加一个实例，本质里面就是在维护信息，同时还会建立定时检查实例心跳的机制

最后还会基于一致性协议，比如说raft协议，去把注册同步给其他节点

服务发现的本质其实也是一个http接口，就是：http://31.208.59.24:8848/nacos/v1/ns/instance/list?serviceName=xx，就这么一个接口，其实也没特别的东西，然后就会启动定时任务，每隔10s拉取一次最新的实例列表，然后服务端还会监听他监听服务的状态，有异常就会基于UDP协议反向通知客户端这次服务异常变动

### 14_基于Nacos实现高可用服务注册中心部署

编译nacos：下载`nacos-server-1.4`版本的源码，接着进行编译

windows的git客户端，有一个软件的，装一下，也是一个命令行，就可以在本地执行git之类的命令都是可以的，半小时~1小时，甚至几小时都有可能，看你自己的网速吧，github上下载还是挺慢的

```
git clone https://github.com/alibaba/nacos.git
cd nacos/
# 半小时
mvn -Prelease-nacos -Dmaven.test.skip=true clean install -U

ls -al distribution/target/
// change the $version to your actual path
cd distribution/target/nacos-server-$version/nacos/bin
```

然后上传到三台服务器上去，并且分别进行解压缩，需要自己下载一个WinScp一个软件，就可以界面里把你win宿主机上的文件直接拖拉上传到虚拟机的指定目录下去

重命名cluster.conf.example，去掉example，配置三台机器的地址和端口号，默认端口号是8848，使用默认的derby数据库就行了，但是生产环境下，可以部署一个MySQL数据库，执行nacos-mysql.sql初始化数据库里的表之类的东西，然后分别修改他们的application.properties文件，里面修改数据库配置

```
spring.datasource.platform=mysql
db.num=1
db.url.0=xxx
db.user=xx
db.password=xxx
```

分别进入三台机器的bin目录，执行startup.sh，检查logs目录下的start.out启动日志，如果启动成功就可以了，访问任何一个节点的8848端口的/nacos地址，进入nacos控制台，可以看到nacos集群的具体情况

#### 虚拟机搭建

Nacos原理已经讲完了，如果说后续我们要做一个C2C二手电商平台的社会化治理系统的项目案例，以及落地后续的一系列的微服务架构，首先我们就知道，必须得有一个服务注册中心，技术选型和对比，以及nacos原理和讲解，已经都说完了

做一个nacos高可用集群的部署，不会做简单的单机部署，生产环境肯定是高可用的集群部署的，我们就按照这套思路来做，需要3台虚拟机，centos 7.x去部署，我用的虚拟机软件是virtualbox

如何创建虚拟机，以及在虚拟机里安装centos 7.x操作系统，准备好3台centos机器，我之前在其他的课程里有讲过，所以视频直接抽了几个出来，如何安装和部署centos虚拟机，会看到4个视频，按照4个视频步骤，以及使用我提供的资料里的网盘链接里面有，你需要的centos和jdk

安装3台虚拟机以及centos操作系统，jdk需要安装好

参考视频以及以下内容去创建一个CentOS 7.x的虚拟机

\###（1）使用课程提供的CentOS 7镜像即可，CentOS-7-x86_64-Minimal-1611.iso

链接：https://pan.baidu.com/s/1UuBLvuljNmScqtjuXDqsOA 提取码：3zoq

\###（2）创建虚拟机：打开Virtual Box，点击“新建”按钮，点击“下一步”，输入虚拟机名称为hbase-standalone，选择操作系统为Linux，选择版本为Red Hat-64bit，分配2048 MB内存，后面的选项全部用默认，在Virtual Disk File location and size中，一定要自己选择一个目录来存放虚拟机文件，最后点击“create”按钮，开始创建虚拟机。

\###（3）设置虚拟机网卡：选择创建好的虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择“Bridged Adapter”。虚拟机必须有一块网卡，这块网卡是通过什么方式去宿主机里的网卡进行通讯，桥接网卡，就可以让你的虚拟机里的网卡跟你宿主机的一块网卡进行通讯

\###（4）安装虚拟机中的CentOS 7操作系统：选择创建好的虚拟机，点击“开始”按钮，选择安装介质（即本地的CentOS 7镜像文件），按照课程选择后自动安装即可

\###（5）安装完以后，CentOS会提醒你要重启一下，就是reboot，你就reboot就可以了。

\###（6）配置网络

> vi /etc/sysconfig/network-scripts/ifcfg-enp0s3

先让它动态分配一个ip地址

```
ONBOOT=yes
```

> service network restart

> ip addr

再设置静态ip地址

```
BOOTPROTO=static
IPADDR=192.168.31.250
NETMASK=255.255.255.0 
GATEWAY=192.168.31.1 
```

> service network restart

> ip addr

#### 配置DNS

检查NetManager的状态：

> systemctl status NetworkManager.service

检查NetManager管理的网络接口：

> nmcli dev status

检查NetManager管理的网络连接：

> nmcli connection show

设置dns：

> nmcli con mod enp0s3 ipv4.dns "114.114.114.114 8.8.8.8"

让dns配置生效：

> nmcli con up enp0s3

\###（7）配置hosts

> vi /etc/hosts 配置本机的hostname到ip地址的映射

\###（8）配置SecureCRT

此时就可以使用SecureCRT从本机连接到虚拟机进行操作了

一般来说，虚拟机管理软件，virtual box，可以用来创建和管理虚拟机，但是一般不会直接在virtualbox里面去操作，因为比较麻烦，没有办法复制粘贴

SecureCRT，在windows宿主机中，去连接virtual box中的虚拟机

收费的，我这里有完美破解版，跟着课程一起给大家，破解

\###（9）关闭防火墙

> systemctl stop firewalld.service systemctl disable firewalld.service

关闭windows的防火墙

后面要搭建集群，有的大数据技术的集群之间，在本地你给了防火墙的话，可能会没有办法互相连接，会导致搭建失败

\###（10）配置yum

> yum clean all yum makecache yum install wget

\###（11）安装JDK

##### 1、将jdk-8u131-linux-x64.rpm通过WinSCP上传到虚拟机中

##### 2、安装JDK：rpm -ivh jdk-8u131-linux-x64.rpm

##### 3、配置jdk相关的环境变量

```
vi .bashrc
export JAVA_HOME=/usr/java/latest
export PATH=$PATH:$JAVA_HOME/bin
source .bashrc
```

##### 4、测试jdk安装是否成功：java -version

\###（12）在另外2个虚拟机中安装CentOS集群

按照上述步骤，再安装2台一模一样环境的linux机器

安装好之后，在每台机器的hosts文件里面，配置好所有的机器的ip地址到hostname的映射关系

比如说，的hosts里面

192.168.31.xxx hadoop01 192.168.31.xxx hadoop02 192.168.31.xxx hadoop03

\###（13）配置3台CentOS为ssh免密码互相通信

首先在三台机器上分别各自执行下面的命令生成自己的文件：ssh-keygen -t rsa

生成本机的公钥，过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在/root/.ssh目录下

在三台机器上分别各自进入目录，拷贝自己的公钥文件为authroized_keys文件，让三台机器先各自对自己免密码ssh可以登录

> cd /root/.ssh cp id_rsa.pub authorized_keys

将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了

接着配置三台机器互相之间的ssh免密码登录，在每台机器上执行下面的命令 使用`ssh-copy-id -i hostname`命令将本机的公钥拷贝到指定机器的`authorized_keys`文件中

### 15_为什么微服务化的系统必须通过RPC框架进行通信？

服务注册中心这个东西初步的捋顺了，你微服务架构里每个服务在任何一台机器上部署，都必须往服务注册中心去做一个注册，服务注册中心就可以维护你的每个服务部署的实例地址列表

HTTP、RPC

假设如果你没有RPC的框架，此时假如说你的每个服务对外暴露的接口，都是一些HTTP接口，http://21.38.254.306:8080/xx/xx?xx=xx，其他服务如果说要调用你的这个服务，就必须使用apache的http的组件，或者是JDK自带的http组件，构建出来一个HTTP请求，包括请求路径，请求头，请求体（JSON串过去）

好不容易构建出来一个完整的HTTP请求，通过http组件发送这个HTTP请求过去，在底层也是得跟那个服务的指定机器的指定端口号，也都是建立TCP连接，在TCP连接之上，发送HTTP协议组装出来的请求过去

接收人家给你的HTTP响应，解析HTTP响应，响应头，状态码（404，500），响应体（JSON串），极为麻烦

划时代的RPC这种东西，比如说你发布一个服务，主要就是定义了一些接口

```
public interface ServcieA {

	public String hello(String name);

}
```

如果我要调用你的接口，此时我在代码里，不需要care什么HTTP之类的东西

```
public void service() {
	String result = serviceA.hello(“zhagnsan”);
}
```

### 16_Feign+Ribbon、Dubbo、gRPC的选型对比（上）

### 17_Feign+Ribbon、Dubbo、gRPC的选型对比（下）

![几种RPC框架技术对比](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/c2c/images/16/%E5%87%A0%E7%A7%8DRPC%E6%A1%86%E6%9E%B6%E6%8A%80%E6%9C%AF%E5%AF%B9%E6%AF%94.png)

手动写一些构造HTTP请求的代码去发送给评审员服务的服务器，去调用他的http接口，总不能这样，RPC框架，举报服务在代码里就调用一个interface的接口，底层直接让RPC框架发送请求到对应的服务器上去

儒猿技术窝，《21天互联网Java工程师面试训练营（分布式篇）》，视频专栏，spring cloud的一些技术都一些讲解

feign+ribbon，spring cloud netflix技术栈，RPC调用，用的就是feign框架+ribbon做负载均衡，暴露出来的服务接口，就是最最稀松平常的基于spring mvc写的controller暴露出来的一些http接口，定义一个http的url地址

通过feign框架进行RPC调用：String result = serviceA.hello(name)，会按照http协议来组装你的请求数据，数据格式都是按照http协议里的请求来做的，http请求还还必须做一个序列化，序列化成二进制的字节流，通过底层的tcp连接发送过去

本质上服务A的部署是基于tomcat去进行部署的，tomcat会监听你指定的端口号，当别人要发送http请求给你的时候，首先必须跟tomcat建立tcp网络连接，发送http请求给tomcat，tomcat收到之后，解析出来这个http请求，交给你的spring mvc写的controller来进行处理

dubbo自己使用的一套协议，自定义协议，也可以是别的协议，肯定不是http协议，去组装请求数据，然后做一个序列化，二进制字节数组或者是字节流，都可以，通过底层的网络连接把请求数据发送过去就可以了

ServiceA这个类，调用他里面的hello()这个方法，传入name这个参数，获取result这个返回值，然后通过网络连接把响应数据按照自己的协议封装，序列化，通过网络连接发送给服务B就可以了

spring cloud feign、dubbo、gRPC，分别百度一个hello world级别的demo，自己写一下，感受和体验一下就知道了

### 18_Dubbo RPC框架集成Nacos注册中心

### 19_基于Dubbo开发C2C电商社会化治理平台人群服务（上）

### 19_基于Dubbo开发C2C电商社会化治理平台人群服务（下）

### 20_基于Dubbo开发C2C电商社会化治理平台任务服务

### 21_基于Dubbo开发C2C电商社会化治理平台权益服务（上）

### 21_基于Dubbo开发C2C电商社会化治理平台权益服务（下）

### 22_01、基于Dubbo开发C2C电商社会化治理平台基础服务（1）

### 22_02、基于Dubbo开发C2C电商社会化治理平台基础服务（2）

### 22_03、基于Dubbo开发C2C电商社会化治理平台基础服务（3）

### 22_04、基于Dubbo开发C2C电商社会化治理平台基础服务（4）

### 22_05、基于Dubbo开发C2C电商社会化治理平台基础服务（5）

nacos+dubbo整合起来的一个demo，nacos和dubbo基本的原理，都有了一个讲解，在demo里我集成了mybatis和druid数据源，去基于mysql做一些增删改查，每个服务在执行业务逻辑的时候还是得对数据库做一下操作的，seata分布式事务的方案，每个服务都必须有一个自己本地的数据库的事务

社会化治理：social govern

举报：report

评审员：reviewer

奖励：reward

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（1）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（2）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（3）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（4）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（5）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（6）

### 23_微服务化的C2C电商社会化治理平台如何进行全链路测试？（7）

nacos + dubbo整合，原理，以及案例代码的开发，大家都已经有了一定的了解了，手把手带着大家去做了，spring cloud alibaba去开发自己公司的项目的话，整合框架，背后的原理，设计、开发、测试、上线

（1）系统设计（概要设计、详细设计 / 简化设计）

概要设计：业务架构、技术架构、业务流程、技术方案、部署方案，都需要做一个大致上的设计，架构师

详细设计：接口定义、数据库表结构、核心类建模、各个接口被请求时的系统运行时序图、技术方案细化

简化设计：画一些系统运行流程图、技术方案、接口、表、时序图

设计完毕之后，会有一个设计评审的过程，会找相关的其他同学过来评审，比如说给人家确认一下你设计的接口，是否满足你的调用方的需求

（2）开发代码

每个人可能都是维护自己负责的子系统、服务，微服务框架，spring cloud alibaba里面的nacos + dubbo，用dubbo定义各种你需要对外提供的接口，按照你自己的设计文档以及技术方案去进行代码的开发

如果仅仅是一些crud的话，此时基于数据库就可以搞定了

但是如果说涉及到一些复杂的技术方案，使用中间件系统，es、redis、mongodb、hbase、rocketmq，等等

（3）本地自测

服务本地跑起来自己测试各个功能，直接通过dubbo服务调用，浏览器的http请求，直接请求你的接口，测试一下自己的各个功能，你自己一个人维护一个java web系统，不依赖别人的接口

也可能跑不起来，那就单元测试，单元测试其实是一个很专业的领域，跑本地单元测试的时候，需要把你的spring容器跑起来，然后对各种bean的注入可能需要打桩，接着再测试各个接口，这里不详细展开了

说句题外话，国内很规范做单元测试的公司也不多，大多公司的单元测试做的第一不规范，第二不完善，第三形同虚设，所以基本可以忽略，如果要把单测做好了，写单测的代码的时间跟你写系统代码的时间可能甚至是1:1，1:2

更多的还是写完代码，自己本地跑起来，想办法简单测试一下罢了

有的时候跑起来需要有一些其他人负责的服务的配合，这个时候有一些方案可以做到，比如说本地可以跑起来一个服务注册中心，然后其他人的服务你手头没有，那团队可以做一个统一的本地服务模拟工程，工程跑起来就自动往本地注册中心去注册，接口的返回结果都是mock的

然后你的服务跑起来，就可以跑通了，包括数据库，缓存，MQ这些东西，都可以在本地部署，有一个本地的maven profile，放这些配置

小项目，协作方不是太多

或者是在公司内网环境里，提供几台机器，作为dev环境，部署了数据库、缓存、MQ等中间件，服务可以部署，一台机器可以多部署几个服务，然后当你笔记本电脑在公司内网的时候，就可以访问到那几台机器，那么你本地启动，就可以直接访问到测试环境里的其他服务了

maven profile，spring boot profile，百度搜一下，非常的简单，都是很对不同的环境可以去放一套不同的配置资源文件

（4）持续集成：可选

很多同学可能都听说过持续集成，但是不太清楚到底是什么

有的公司会做持续集成，意思是你每天开发好的代码和功能，都必须有对应的单元测试可以进行自动化的测试，然后你本地单元测试跑通了，就可以提交代码到git仓库以及进行代码合并，直接触发jenkins这种集成工具，他会拉取你的最新代码，然后跑你所有的单元测试，确保你的代码在所有测试下可以正常运行

甚至可能是多个人维护一个系统，每个人每天/隔天，都要提交自己的代码+单测到git仓库进行代码合并，集成的概念，单人维护一个独立的工程/服务，每天不停的提交最新代码到你的git仓库里，你在不停的把自己最新写好的代码集成到已有的完整的代码里去

持续集成，提代码

多人维护一个系统，你本地写好的代码，本地跑单测是ok的，但是你提到git仓库合并进去，此时可能别人也会提代码合并进去，此时你们俩的代码集成在一起了，此时到底集成好的代码能不能正常工作呢？

jenkins持续集成的工具，如果发现你有提交代码以及合并的操作，此时jenkins会触发一个任务，拉取你的代码到本地，自动运行所有的单元测试，用你的单元测试自动运行和检查，来确保你现在最新的集成后的代码都是可以运行的

有的时候还会专门写特定的自动化集成测试代码，就是说你代码提交之后，然后可能会把你完整代码跑起来，此时所有代码是一个集成在一起的状态，接着就是运行集成测试的代码，把你集成在一起的代码都跑一下，检查是否正常

但是这个比较麻烦，搞持续集成，在工具上要求git、jenkins之类的整合，你要做很多配置，同时要求你每天的代码都有对应的自动化测试代码，所以真的把持续集成做好，做的很标准的公司，其实不多

（5）联调测试/功能测试

一个人维护一个java web系统，对其他人没有依赖，太low了

比较正常的，就是你写好了代码，自己简单自测完毕了，然后部署到一个联调测试/功能测试的环境里去，这个环境，是可能团队内部各个服务之间联调，或者甚至和其他团队的系统进行联调的地方

这个环境最好是独立的一套环境，部署好之后，QA会进行大量的手工测试，各种点击系统，也可能会有自动化测试，不过说实话，能玩儿自动化测试的公司不多，最后在这个环境，会有一个PM功能验收测试的过程

这个环境重在联调，把各个系统和服务跑通，确保功能运行没问题，一般机器都是低配置的，而且一个服务就一台机器，甚至是一台机器几个服务都有可能

（6）预发布测试

接着一般是预发布环境的测试，这个环境一般是模拟线上环境，可能会在这里做压力测试、全链路压测、性能测试、可用性测试、稳定性测试，都可能会在这里做，一般机器配置跟线上一样，每个服务都是独立机器，甚至每个服务给多台机器

比如说线上高峰QPS是1w+，线上机器是4核8G的，部署20台，那么预发布环境可能就是模拟每秒QPS是1000+，每个服务部署2台机器，做一个低压力测试，把全链路都压一下，测试性能，QPS，机器负载

有时候也可能会跑一些可用性测试，比如设计一些特殊故障之类的

在这个环境，通常流量是从线上获取回放过来的，有一个线上流量回放的过程，很多公司其实没这个环节，此时可能也就是走个过场，但是正经来说，是要做流量回放的，不是靠人力来测试，而是回放线上流量，看系统的功能是否正常，压力是否ok，性能是否还行，机器负载如何，全链路表现如何

有时候也会在这个环境让QA做一个全量功能回归测试，这可能是大版本变动的时候要做的

如果一切正常，那么就可以上线了

（7）线上部署

生产环境必须是一套独立的机器，直接进行部署即可，上线之后要通过各个机器的重要日志、请求是否正常、机器负载等是否正常、然后PM线上验收，一切正常，上线成功

### 24_如何基于Nacos实现多测试环境与生产环境的隔离？

### 25_C2C电商社会化治理平台的多环境部署隔离

![开发测试流程](https://gitee.com/th_520/Java-Interview-Advanced/raw/master/docs/c2c/images/25/%E5%BC%80%E5%8F%91%E6%B5%8B%E8%AF%95%E6%B5%81%E7%A8%8B.png)

### 26_互联网公司部署方案：蓝绿部署，灰度发布以及滚动发布

设计、开发、测试、部署，流程都讲过了，微服务技术栈，服务注册中心，nacos，RPC框架，dubbo，设计就要把各个服务拆分完毕，包括你的业务逻辑，需求，接口，数据库，类，功能的时序图 

每个人就负责开发自己的服务就可以了，nacos+dubbo 

用dubbo开发一些接口，只要定义一些接口和dubbo注解，更多的还是写java代码 

不同的环境之下，你的服务注册的namespace必须是不同的 

（1）滚动发布 

这是最常见的部署模式，一般就是说你一个服务/系统都会部署在多台机器上，部署的时候，要不然是手动依次部署，最low的比如就是每台服务器上放一个tomcat，每台机器依次停机tomcat，然后把新的代码放进去，再重新启动tomcat，各个机器逐渐重启，这就是最low的滚动发布 

中小型公司现在稍微好点的话，都会做自动化部署，自动化部署用的比较多的是jenkins，因为jenkins是支持持续集成和持续交付的，之前说过持续集成，那么持续交付就是比持续集成更进一步，简单来说，就是你每天都提交代码，他每天都自动跑测试确保代码集成没问题，然后可能每隔几天，就把一个生产可用的小版本交付到线上 

这个时候，就需要一个自动化部署，jenkins可以自动在多台机器上部署你的服务/系统，过程其实也是类似的，只不过把手动改成自动罢了，你自己部署tomcat/基于spring boot内嵌容器，其实都行 

中大型公司，一般发布系统都是自己研发的，你在上面指定对一个服务，指定一个git仓库的代码分支，然后指定一个环境，指定一批机器，发布系统自动到git仓库拉取代码到本地，编译打包，然后在你指定环境的机器上，依次停止当前运行的进程，然后依次重启你新代码的服务进程 

这都是典型的滚动发布 

但凡发布，都要考虑两个问题，一个是验证，一个是回滚 

验证就是说，你怎么确定你这次部署成功了？一般来说，要观察每台机器启动后处理请求时的日志，日志是否正常，是否有报错，一般日志正常、没有报错，那么就算是启动成功了，有时候也会让QA/PM做一个线上验证 

那么万一发布失败了呢？此时就得回滚，因为不同的上线是不一样的，有时候你仅仅是对代码做一些微调，大多数时候是针对新需求有上线，加了新的代码/接口，有时候是架构重构，实现机制和技术架构都变了 

所以回滚的话，也不太一样，比如你如果是加了一些新的接口，结果上线失败了，此时心接口没人访问，直接代码回滚到旧版本重新部署就行了；如果你是做技术架构升级，此时失败了，可能很多请求已经处理失败，数据丢失，严重的时候会导致公司丢失订单，或者是数据写入了但是都错了 

此时可能会采用回滚代码，或者清洗错乱数据的方式来回滚，总之，针对你的发布，你要考虑到失败之后的回滚方案，回滚代码，就得用旧版本的代码，然后重新在各个机器依次部署，就算是一次回滚了，至于丢失了数据没有，要不要清洗数据，这个看情况来定 

滚动发布的话，风险还是比较大的，因为一旦你用了自动化的滚动发布，那么发布系统会自动把你的所有机器都部署新版本的代码，这个时候中间很有可能会出现问题，导致大规模的异常和损失 

所以现在一般中大型公司，都不会贸然用滚动发布模式了  

（2）灰度发布 

灰度发布，指的就是说，不要上线就滚动全部发布到所有机器，一般就是会部署在比如1台机器上，采用新版本，然后切比如10%的流量过去，观察那10%的流量在1台机器上运行一段时间，比如运行个几天时间，观察日志、异常、数据，是否一切正常，如果验证发现全部正常，那么此时就可以全量发布了 

全量发布的时候，就是采用滚动发布那种模式 

这个好处就是说，你先用10%以内的小流量放到灰度新版本的那台机器上验证一段时间，感觉没问题了，才会全量部署，这么操作，即使有问题，也就10%以内的请求出现问题，损失不会太大的，如果你公司体量特别大，灰度也可以是1%，甚至0.1%的流量 

因为体量太大的公司，1%的流量就很大了 

如果灰度的时候有问题，那么立刻把10%以内的小流量切去请求老版本代码部署的机器，灰度版本的机器立马就没流量请求了，这个回滚速度是极快的 

通常灰度验证过后，全量发布，都不会有太大的问题，基本上再出问题概率就很小了，所以现在中大型互联网公司，一般都是灰度发布模式的 

（3）蓝绿部署 

蓝绿部署的意思是说，你得同时准备两个集群，一个集群放新版本代码，一个集群放老版本代码，然后新版本代码的集群准备好了过后，直接线上流量切到新版本集群上去，跑一段时间来验证，如果发现有问题，回滚就是立马把流量切回老版本集群，回滚是很快速的 

如果新版本集群运行一段时间感觉没问题了，此时就可以把老版本集群给下线了 

那么为什么有灰度发布了还要用蓝绿部署呢？ 

是这样的，灰度发布过后，还是要全量部署的，但是有时候，如果涉及到一些新的架构方案，或者是新的接口，10%以内的小流量可能没办法暴露出线上的高并发问题，所以灰度验证没问题，结果全量部署还是有一个小概率会失败 

此时全量发布用滚动发布的方式，逐步部署过去，很快会引发大规模的失败，此时回滚，是很慢的，因为要一台一台逐步回滚 

所以说，一般针对那种改动不太大的小版本，比如加个接口，修改一些代码，修复几个bug，类似这种整体变动不大的情况，建议用灰度发布，因为这种一般灰度验证没问题，全量部署也不会有问题 

但是如果涉及到那种很大规模的架构重构或者架构升级，比如数据存储架构升级，或者是技术架构整体改造，或者是代码大规模重构，类似这种场景，最好是用蓝绿部署，也就是说，完全部署一个新的集群，然后把较大的流量切过去，比如先切10%，再切50%，最后切到100%，让新集群承载100%的流量跑一段时间 

过程中一旦有问题，立马流量全部切回老集群，这个回滚速度就比灰度发布的全量部署回滚要快多了，因为仅仅是切流量而已，不需要重新部署 

### 27_Nacos如何支持蓝绿部署、灰度发布以及滚动发布？

非常遗憾的告诉大家，现在并没有一个开源项目能够完美的支持灰度发布和蓝绿部署，因为这并不是一个nacos就可以做到的，大家都知道，nacos支持namespace等高级特性，还可以带服务元数据，所以如果要基于nacos来搞灰度发布和蓝绿部署是可以的

比如说，你灰度的时候，负责灰度的版本上线，就注册到一个prod-grey namespace下去，或者是你得带上一些服务标签，类似元数据的概念，然后你的网关流量入口，就得能够识别出灰度机器，按照你的配置分发流量

然后你的类似dubbo的RPC框架的负载均衡模块，也得识别出灰度机器，按照配置分发流量过去

就是这个事儿要做成，必须是nacos + dubbo + 网关，都得做很多定制和改造，才能搞定的，其实做是可以做的，因为网关的灰度，很多开源网关都支持，dubbo的负载均衡机制也是可以扩展自定义的

但是这都有工作量，你得自己去做，这不是现成的

蓝绿部署其实也是同理，你新集群的服务都得带标签，或者干脆就去别的namespace，然后你的流量分发这块要能够控制好，包括回滚时的流量秒级切换

所以这块正常来说，并没有办法说针对中小型公司，直接开箱即用，就可以灰度发布或者是蓝绿部署，因为流量控制这块有点棘手

### 28_C2C电商社会化治理平台的蓝绿部署，灰色发布以及滚动发布

对大多数中小公司而言，可能用jenkins之类的工具做一个自动化的滚动部署就差不多了，此时教大家一些经验，就是说，灰度发布，其实我们可以玩儿成伪灰度发布，什么意思呢？比如你一个服务部署了3台机器

那么当你有一个新版本的时候，你先部署1台机器，这不就变相的相当于是灰度了？默认会有30%+的流量进入这台机器，你先观察一下这30%的流量表现如何，如果感觉不错，再把另外2台机器部署了

2台机器，你有新版本，先部署1台机器，50%流量过来，先观察一下再说

说白了，就是人为的去控制这个部署的节奏和过程，流量分发这块不要动，还是均匀分发，搞出一个伪灰度的效果

蓝绿部署也是同样的，一般注册中心都是支持主动调用它的API进行服务下线的，此时你可以让你每个服务都实现一个下线的接口，里面调用eureka/nacos之类的API可以主动下线你的服务实例

此时你可以部署一个新版本的集群，部署好之后，他们都会注册过去，然后你依次手动调用老版本集群各个服务实例的下线接口，一般请求nacos/eureka的http接口，就可以把服务下线了，此时一旦老版本服务实例下线，那么流量就会全部给新版本的集群了

但是你老版本集群别停机，他仅仅是服务下线了而已，没请求了

如果一旦发现新版本集群有问题，再调用你的服务的注册接口，直接让各个服务主动发送http之类的请求到服务注册中心，把自己这个服务实例注册上去，把新版本集群全部手动下线一下

这么搞也是可以的，就是有开发成本

还有一种，就是说你新版本的集群直接部署，然后从存储到缓存到所有中间件，到数据库，到机器，全部是独立的一套，全新，全独立，如果需要以前的数据，可以半夜凌晨做一个停机维护，比如凌晨1点到5点，停机4个小时，把老系统所有数据都复制一份到新版本系统的数据存储里去

然后5点过后，简单控制一下流量分发入口，让每一个请求到老版本集群一份，也到新版本集群一份，这个其实好做，网关那块可以控制一下，然后新老集群一起跑，观察新集群是否稳定，如果稳定，再把老集群下线了，全部采用新系统对外提供服务

### 29_为啥微服务化的系统需要分布式事务方案——事务基础筑基

#### 事务的基础知识筑基（一）：ACID以及几种隔离级别

1、面试题 

事务的几个特点是什么？数据库事务有哪些隔离级别？MySQL的默认隔离级别是？ 

2、面试官心里分析 

用mysql开发的三个基本面：存储引擎、索引，然后就是事务，你必须得用事务。因为一个业务系统里，肯定要加事务保证一堆关联操作，要么一起成功要么一起失败，对不对？所以这是聊数据库必问的一个问题。 

最最最最最最基本的用mysql来开发，就3点：存储引擎（了解），索引（能建索引，写的SQL都用上索引），事务（了解事务的隔离级别，基于spring的事务支持在代码里加事务），咱们项目阶段一里的在mysql相关的也就这么几点。 

存储引擎 -> innodb，索引，基本按照你的SQL的需求都建了索引（可能漏了部分索引忘了建），事务（@Transactional注解，对service层统一加了事务） 

3、面试题剖析 

**3.1** **事务的ACID** 

这个先说一下ACID，必须得知道： 

（1）Atomic：原子性，就是一堆SQL，要么一起成功，要么都别执行，不允许某个SQL成功了，某个SQL失败了，这就是扯淡，不是原子性。 

（2）Consistency：一致性，这个是针对数据一致性来说的，就是一组SQL执行之前，数据必须是准确的，执行之后，数据也必须是准确的。别搞了半天，执行完了SQL，结果SQL对应的数据修改没给你执行，那不是坑爹么。 

（3）Isolation：隔离性，这个就是说多个事务在跑的时候不能互相干扰，别事务A操作个数据，弄到一半儿还没弄好呢，结果事务B来改了这个数据，导致事务A的操作出错了，那不就搞笑了。 

（4）Durability：持久性，事务成功了，就必须永久对数据的修改是有效的，别过了一会儿数据自己没了，不见了，那就好玩儿了。 

**3.2** **事务隔离级别** 

总之，面试问你事务，先聊一下ACID，然后聊聊隔离级别 

（1）读未提交，Read Uncommitted：这个很坑爹，就是说某个事务还没提交的时候，修改的数据，就让别的事务给读到了，这就恶心了，很容易导致出错的。这个也叫做脏读。 

（2）读已提交，Read Committed（不可重复读）：这个比上面那个稍微好一点，但是一样比较尴尬，就是说事务A在跑的时候， 先查询了一个数据是值1，然后过了段时间，事务B把那个数据给修改了一下还提交了，此时事务A再次查询这个数据就成了值2了，这是读了人家事务提交的数据啊，所以是读已提交。这个也叫做不可重复读，就是所谓的一个事务内对一个数据两次读，可能会读到不一样的值。 

（3）可重复读，Read Repeatable：这个就是比上面那个再好点儿，就是说事务A在执行过程中，对某个数据的值，无论读多少次都是1；哪怕这个过程中事务B修改了数据的值还提交了，但是事务A读到的还是自己事务开始时这个数据的值。 

（4）串行化：幻读，不可重复读和可重复读都是针对两个事务同时对某条数据在修改，但是幻读针对的是插入，比如某个事务把所有行的某个字段都修改为了2，结果另外一个事务插入了一条数据，那个字段的值是1，然后就尴尬了。第一个事务会突然发现多出来一条数据，那个数据的字段是1。如果要解决幻读，就需要使用串行化级别的隔离级别，所有事务都串行起来，不允许多个事务并行操作。 

MySQL的默认隔离级别是Read Repeatable，就是可重复读，就是说每个事务都会开启一个自己要操作的某个数据的快照，事务期间，读到的都是这个数据的快照罢了，对一个数据的多次读都是一样的。 

但是另外几个隔离级别都是提供的。 

我们聊下MySQL是如何实现Read Repeatable的吧，因为一般我们都不修改这个隔离级别，但是你得清楚是怎么回事儿，MySQL是通过MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。 

innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。 

事务id，在mysql内部是全局唯一递增的，事务id=1，事务id=2，事务id=3 

id       name        创建事务id       删除事务id 

1         张三         120              122

2        李四         119              空

2        小李四       122              空 

事务id=121的事务，查询id=1的这一行的时候，一定会找到创建事务id <= 当前事务id的那一行，select * from table where id=1，就可以查到上面那一行 

事务id=122的事务，将id=1的这一行给删除了，此时就会将id=1的行的删除事务id设置成122 

事务id=121的事务，再次查询id=1的那一行，能查到吗？能查到，要求创建事务id <= 当前事务id，当前事务id < 删除事务id 

事务id=121的事务，查询id=2的那一行，查到name=李四 

事务id=122的事务，将id=2的那一行的name修改成name=小李四 

事务id=121的事务，查询id=2的那一行，答案是：李四，创建事务id <= 当前事务id，当前事务id < 删除事务id 

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。 

那么如果某个事务执行期间，别的事务更新了一条数据呢？这个很关键的一个实现，其实就是在innodb中，是插入了一行记录，然后将新插入的记录的创建时间设置为新的事务的id，同时将这条记录之前的那个版本的删除时间设置为新的事务的id。 

现在get到这个点了吧？这样的话，你的这个事务其实对某行记录的查询，始终都是查找的之前的那个快照，因为之前的那个快照的创建时间小于等于自己事务id，然后删除时间的事务id比自己事务id大，所以这个事务运行期间，会一直读取到这条数据的同一个版本。 

记住，聊到事务隔离级别，必须把这套东西给喷出来，尤其是mvcc，说实话，市面上相当大比重的java程序员，对mvcc是不了解的，这个东西很简单，结构大家居然不知道，真是相当大的差异！

#### 事务的基础知识筑基（二）：Spring的事务支持以及传播特性

1、面试题

spring的事务支持（注解事务、声明事务、编程事务、事务的传播机制）？执行某个操作，前50次成功，第51次失败。a 全部回滚；b 前50次提交，第51次抛异常。ab场景分别如何设置spring事务。 

2、面试官心里分析 

聊完上面那个问题，面试官估计心里对你感觉相当不错了，但是呢，事儿没玩，还得聊聊实际项目中，你的java系统里的事务咋玩儿的啊？这就涉及到了spring对事务的支持，然后重要的事务传播机制！ 

3、面试题剖析 

这个，你一般就聊下，spring支持编程式事务，和声明式事务。编程式事务就是用个事务类TransactionTemplate来管理事务，这个一般现在没人傻到干这个事儿了；声明式事务分成在xml里配置个AOP来声明个切面加事务，一般现在也没人傻到干这个了；大部分情况下，都是用@Transactional注解。 

这个@Transactional注解呢，根据阿里编码规范，一般建议加在方法级别，就是要事务的方法就加事务，不要事务的方法就别加事务，否则很多一个数据库操作的方法，你还加事务，你这不是。。。？可能有同学注意到我们项目阶段一，都是傻乎乎的在类上加了事务对吧？ 

对，其实不是我傻，是我装傻，我当时为了快速开发，所以都给扔类上了，类里所有方法都开启了事务，但是后续我们会慢慢优化这些细节的。 

另外这个注解一般要加rollbackFor，就是指定哪些异常类型才要回滚事务 

还有比较重要的，就是有个isolation属性，你可以自己手动调整事务的隔离级别，但是这个一般不调整，记住，别乱调整事务隔离级别，一般可重复读+mysql mvcc机制跑的很好，你别瞎折腾。 

另外一个重要的事务属性，就是propagation，事务的传播行为，我们就重点先来聊一下事务的传播行为，这个在项目里可能确实是要用到的。其实说白了，这个事务的传播机制，就是说，一个加了@Transactional的事务方法，和嵌套了另外一个@Transactional的事务方法的时候，包括再次嵌入@Transactional事务方法的时候，这个事务怎么玩儿？ 

我们的项目阶段一里是不是有大量这种场景？呵呵，所以啊，真实复杂业务系统的好处就在这里，真好，后面我会慢慢优化这些细节，包括事务的传播机制等等，让你各种技术都在复杂业务系统里看看怎么玩儿的。    

```
public class ServiceA {
	@Autowired
	private ServiceB b;
	@Transactional
	public void methodA() {
		// 一坨数据库操作
		for(int i = 0; i < 51; i++) {
			try {
				b.methodB();
			} catch(Exception e) {
				// 打印异常日志
			}
		}
		// 一坨数据库操作
	} 
}
public class ServiceB {
	@Transactional(propagation = PROPAGATION_REQUIRES_NEW)
	public void methodB() throws Exception {
		// 一坨数据库操作
	} 
}
```

项目阶段一里面，是不是有好多OrderService调用了MembershipService调用了WmsService 

一共有7种事务传播行为： 

（1）PROPAGATION_REQUIRED：这个是最常见的，就是说，如果ServiceA.method调用了ServiceB.method，如果ServiceA.method开启了事务，然后ServiceB.method也声明了事务，那么ServiceB.method不会开启独立事务，而是将自己的操作放在ServiceA.method的事务中来执行，ServiceA和ServiceB任何一个报错都会导致整个事务回滚。这就是默认的行为，其实一般我们都是需要这样子的。 

（2）PROPAGATION_SUPPORTS：如果ServiceA.method开了事务，那么ServiceB就将自己加入ServiceA中来运行，如果ServiceA.method没有开事务，那么ServiceB自己也不开事务 

（3）PROPAGATION_MANDATORY：必须被一个开启了事务的方法来调用自己，否则报错 

（4）PROPAGATION_REQUIRES_NEW：ServiceB.method强制性自己开启一个新的事务，然后ServiceA.method的事务会卡住，等ServiceB事务完了自己再继续。这就是影响的回滚了，如果ServiceA报错了，ServiceB是不会受到影响的，ServiceB报错了，ServiceA也可以选择性的回滚或者是提交。 

（5）PROPAGATION_NOT_SUPPORTED：就是ServiceB.method不支持事务，ServiceA的事务执行到ServiceB那儿，就挂起来了，ServiceB用非事务方式运行结束，ServiceA事务再继续运行。这个好处就是ServiceB代码报错不会让ServiceA回滚。 

（6）PROPAGATION_NEVER：不能被一个事务来调用，ServiceA.method开事务了，但是调用了ServiceB会报错 

（7）PROPAGATION_NESTED：开启嵌套事务，ServiceB开启一个子事务，如果回滚的话，那么ServiceB就回滚到开启子事务的这个save point。 

大家回头想想那个面试题，其实就是ServiceA里循环51调用ServiceB，第51次调用ServiceB失败了。第一个选项，就是两个事务都设置为PROPAGATION_REQUIRED就好了，ServiceB的所有操作都加入了ServiceA启动的一个大事务里去，任何一次失败都会导致整个事务的回滚；第二个选项，就是将ServiceB设置为PROPAGATION_REQUIRES_NEW，这样ServiceB的每次调用都在一个独立的事务里执行，这样的话，即使第51次报错，但是仅仅只是回滚第51次的操作，前面50次都在独立的事务里成功了，是不会回滚的。 

其实一般也就PROPAGATION_REQUIRES_NEW比较常用，要的效果就是嵌套的那个事务是独立的事务，自己提交或者回滚，不影响外面的大事务，外面的大事务可以获取抛出的异常，自己决定是继续提交大事务还是回滚大事务。 

一般在单块系统开发，多人协作的时候比较常见，就是小A调用小B的模块，小A不管小B是成功还是不成功，自己都要提交，这个时候可以这么弄，就是说小B的操作不是构成小A的事务的重要组成部分，就是个分支。

### 30_1、业内分布式事务方案介绍—最基础的分布式事务：XA规范及2PC分布式事务理论介绍

#### 1、XA规范

上一讲演示的过程中，如果一个系统操作多个数据库，肯定是有跨多个库的分布式事务的一个问题，在很多年之前全世界，老美早就已经发现这个问题了，很早以前就定义了一整套的解决方案来处理分布式事务的问题

有个叫做X/Open的组织定义了分布式事务的模型，这里面有几个角色，就是AP（Application，应用程序），TM（Transaction Manager，事务管理器），RM（Resource Manager，资源管理器），CRM（Communication Resource Manager，通信资源管理器）

这么说有点儿抽象，其实Application说白了就是我们的系统，TM的话就是一个在系统里嵌入的一个专门管理横跨多个数据库的事务的一个组件，RM的话说白了就是数据库（比如MySQL），CRM可以是消息中间件（但是也可以不用这个东西）

然后这里定义了一个很重要的概念，就是全局事务，这个玩意儿说白了就是一个横跨多个数据库的事务，就是一个事务里，涉及了多个数据库的操作，然后要保证多个数据库中，任何一个操作失败了，其他所有库的操作全部回滚，这就是所谓的分布式事务

上面这套东西就是所谓的X/Open组织搞的一个分布式事务的模型，那么XA是啥呢？说白了，就是定义好的那个TM与RM之间的接口规范，就是管理分布式事务的那个组件跟各个数据库之间通信的一个接口，说白了就是这个意思

完了比如管理分布式事务的组件，TM就会根据XA定义的接口规范，刷刷刷跟各个数据库通信和交互，告诉大家说，各位数据库同学一起来回滚一下，或者是一起来提交个事务把，大概这个意思

这个XA仅仅是个规范，具体的实现是数据库产商来提供的，比如说MySQL就会提供XA规范的接口函数和类库实现，等等

#### 2、2PC理论

X/Open组织定义的一套分布式事务的模型，还是比较虚的，还没办法落地，而且XA接口规范也是一个比较务虚的一个东西，光靠我说的这些东西还是没法落地的

基本上来说，你搞明白了XA也就明白了2PC了，2PC说白了就是基于XA规范搞的一套分布式事务的理论，也可以叫做一套规范，或者是协议，都ok。Two-Phase-Commitment-Protocol，两阶段提交协议

2PC，其实就是基于XA规范，来让分布式事务可以落地，定义了很多实现分布式事务过程中的一些细节

（1）准备阶段

画个图来玩玩儿，用咱们的那个流量充值的例子来举个例子好了，简单来说，就是TM先发送个prepare消息给各个数据库，让各个库先把分布式事务里要执行的各种操作，先准备执行，其实此时各个库会差不多先执行好，就是不提交罢了

如果你硬是要理解一下的话，也可以认为是prepare消息一发，各个库先在本地开个事务，然后执行好SQL，万事俱备只欠东风了，而且注意这里各个数据库会准备好随时可以提交或者是回滚，有对应的日志记录的

然后各个数据库都返回一个响应消息给事务管理器，如果成功了就发送一个成功的消息，如果失败了就发送一个失败的消息

（2）提交阶段

第一种情况，要是TM哥儿们发现某个数据库告诉他说，不好意思啊，我这儿失败了，那就尴尬了。或者是TM等了半天，某个数据库楞是死活不返回消息，跟失踪了一样，不知道在干嘛，也就麻烦了

这个时候TM直接判定这个分布式事务失败，毕竟某个数据库那里报了个错么，对不对，然后TM通知所有的数据库，全部回滚回滚回滚，赶紧的，做了啥操作都回滚，其实这里你可以认为是通知每个数据库，把自己本地的那个事务回滚不就得了，然后各个库都回滚好了以后就通知TM，TM就认为整个分布式事务都回滚了

但是呢，要是TM接收到所有的数据库返回的消息都是成功，那就happy了，直接发送个消息通知各个数据库说提交，兄弟们，然后各个数据库都在自己本地提交事务呗，就这么回事儿，提交好了通知下TM，TM要是发现所有数据库的事务都提交成功了，就认为整个分布式事务成功了

### 30_2、业内分布式事务方案介绍 — 画图剖析2PC分布式事务方案的缺陷以及问题

1、同步阻塞：在阶段一里执行prepare操作会占用资源，一直到整个分布式事务完成，才会释放资源，这个过程中，如果有其他人要访问这个资源，就会被阻塞住 

2、单点故障：TM是个单点，一旦挂掉就完蛋了 

3、事务状态丢失：即使把TM做成一个双机热备的，一个TM挂了自动选举其他的TM出来，但是如果TM挂掉的同时，接收到commit消息的某个库也挂了，此时即使重新选举了其他的TM，压根儿不知道这个分布式事务当前的状态，因为不知道哪个库接收过commit消息，那个接收过commit消息的库也挂了，兄弟 

4、脑裂问题：在阶段二中，如果发生了脑裂问题，那么就会导致某些数据库没有接收到commit消息，那就完蛋了，有些库收到了commit消息，结果有些库没有收到，这咋整呢，那肯定完蛋了

### 30_3、业内分布式事务方案介绍—针对2PC的问题引入3PC分布式事务方案的理论知识讲解

3PC，说白了，就是three-phase-commitment，三阶段提交协议，这个是针对2PC做的一个改进，主要就是为了解决2PC协议的一些问题 

3PC的话改成了下面的过程： 

（1）CanCommit阶段：这个就是TM发送一个CanCommit消息给各个数据库，然后各个库返回个结果，注意一下，这里的话呢，是不会执行实际的SQL语句的，其实说白了，就是各个库看看自己网络环境啊，各方面是否ready 

（2）PreCommit阶段：如果各个库对CanCommit消息返回的都是成功，那么就进入PreCommit阶段，TM发送PreCommit消息给各个库，这个时候就相当于2PC里的阶段一，其实就会执行各个SQL语句，只是不提交罢了；如果有个库对CanCommit消息返回了失败，那么就尴尬了，TM发送abort消息给各个库，大家别玩儿了，结束这个分布式事务 

（3）DoCommit阶段：如果各个库对PreCommit阶段都返回了成功，那么发送DoCommit消息给各个库，就说提交事务吧，兄弟们，各个库如果都返回提交成功给TM，那么分布式事务成功；如果有个库对PreCommit返回的是失败，或者超时一直没返回，那么TM认为分布式事务失败，直接发abort消息给各个库，说兄弟们回滚吧，各个库回滚成功之后通知TM，分布式事务回滚 

说白了大概就是这样子，但是这里的话，跟2PC相比，主要做了下面两个改进点： 

（1）引入了CanCommit阶段 

（2）在DoCommit阶段，各个库自己也有超时机制，也就是说，如果一个库收到了PreCommit自己还返回成功了，等了一会儿，如果超时时间到了，还没收到TM发送的DoCommit消息或者是abort消息，直接判定为TM可能出故障了，人家库自己颠儿颠儿的就执行DoCommit操作，提交事务了。 

因为这里就是说，如果这个库接收到了PreCommit消息，说明第一阶段各个库对CanCommit都返回成功了啊，这样TM才会发送PreCommit来，那么就默认为基本上各个库的PreCommit都会成功，所以大家没接收到DoCommit，直接自己执行提交操作了 

所以这个超时的机制是基于CanCommit的引入来实现的，有了一个CanCommit多了一个阶段，大家才能自己执行超时commit机制，这不就解决了TM挂掉的单点问题么，大家想想是不是这样子 

另外资源阻塞问题也能减轻一下，因为一个库如果一直接收不到DoCommit消息，不会一直锁着资源，人家自己会提交释放资源的，所有能减轻资源阻塞问题，比2PC稍微好一些吧而已 

3PC的缺陷： 

但是其实这种的话，也不是完全就一定好的，因为还是可能有问题啊，如果人家TM在DoCommit阶段发送了abort消息给各个库，结果因为脑裂问题，某个库没接收到abort消息，自己还颠儿颠儿的执行了commit操作，不是也不对么 

所以啊，其实2PC也好，3PC也好，都没法完全保证分布式事务的ok的，要明白这一点，总有一些特殊情况下会出问题的

### 30_4、业内分布式事务方案介绍—分布式事务业内常见解决方案初步介绍及基础知识筑基

1、面试题 

分布式事务了解吗？你们如何解决分布式事务问题的？ 

2、面试官心里分析 

只要聊到你做了分布式系统，必问分布式事务，你对分布式事务一无所知的话，确实会很坑，你起码得知道有哪些方案，一般怎么来做，每个方案的优缺点是什么。 

现在面试，分布式系统成了标配，而分布式系统带来的分布式事务也成了标配了。因为你做系统肯定要用事务吧，那你用事务的话，分布式系统之后肯定要用分布式事务吧。。。呵呵。。。先不说你搞过没有，起码你得明白有哪几种方案，每种方案可能有啥坑？比如TCC方案的网络问题、XA方案的一致性问题 

3、面试题剖析 

（1）两阶段提交方案/XA方案 

也叫做两阶段提交事务方案，这个举个例子，比如说咱们公司里经常tb是吧（就是团建），然后一般会有个tb主席（就是负责组织团建的那个人）。 

tb，team building，团建 

第一个阶段，一般tb主席会提前一周问一下团队里的每个人，说，大家伙，下周六我们去滑雪+烧烤，去吗？这个时候tb主席开始等待每个人的回答，如果所有人都说ok，那么就可以决定一起去这次tb。如果这个阶段里，任何一个人回答说，我有事不去了，那么tb主席就会取消这次活动。 

第二个阶段，那下周六大家就一起去滑雪+烧烤了 

所以这个就是所谓的XA事务，两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复ok，那么就正式提交事务，在各个数据库上执行操作；如果任何一个数据库回答不ok，那么就回滚事务。 

这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于spring + JTA就可以搞定，自己随便搜个demo看看就知道了。 

这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。我可以给大家介绍一下， 现在微服务，一个大的系统分成几百个服务，几十个服务。一般来说，我们的规定和规范，是要求说每个服务只能操作自己对应的一个数据库。 

如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范，你随便交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，经常数据被别人改错，自己的库被别人写挂。 

如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许你交叉访问别人的数据库！ 

（2）TCC方案 

TCC的全程是：Try、Confirm、Cancel。 

这个其实是用到了补偿的概念，分为了三个阶段： 

1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留

2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作

3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作 

给大家举个例子吧，比如说跨银行转账的时候，要涉及到两个银行的分布式事务，如果用TCC方案来实现，思路是这样的： 

1）Try阶段：先把两个银行账户中的资金给它冻结住就不让操作了

2）Confirm阶段：执行实际的转账操作，A银行账户的资金扣减，B银行账户的资金增加

3）Cancel阶段：如果任何一个银行的操作执行失败，那么就需要回滚进行补偿，就是比如A银行账户如果已经扣减了，但是B银行账户资金增加失败了，那么就得把A银行账户资金给加回去

这种方案说实话几乎很少用人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。 

比如说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用TCC，严格严格保证分布式事务要么全部成功，要么全部自动回滚，严格保证资金的正确性，在资金上出现问题 

比较适合的场景：这个就是除非你是真的一致性要求太高，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码。 

而且最好是你的各个业务执行的时间都比较短。 

但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者是补偿逻辑，实在太恶心了，那个业务代码很难维护。 

（3）本地消息表 

国外的ebay搞出来的这么一套思想 

这个大概意思是这样的 

1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表

2）接着A系统将这个消息发送到MQ中去

3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息

4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态

5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理

6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止 

这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的？？？这个会导致如果是高并发场景咋办呢？咋扩展呢？所以一般确实很少用 

（4）可靠消息最终一致性方案 

这个的意思，就是干脆不要用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。 

大概的意思就是：

1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了

2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息

3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务

4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。

5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿 

这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用RocketMQ支持的，要不你就自己基于类似ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的 

（5）最大努力通知方案 

这个方案的大致意思就是： 

1）系统A本地事务执行完之后，发送个消息到MQ

2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口

3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃 

（6）你们公司是如何处理分布式事务的？ 

这个，说真的，确实我们这个课程没法带着大家来实战，因为定位不是这个。但是如果你真的被问到，你可以这么说，我们某某特别严格的场景，用的是TCC来保证强一致性；然后其他的一些场景基于了阿里的RocketMQ来实现了分布式事务。 

你找一个严格资金要求绝对不能错的场景，你可以说你是用的TCC方案；如果是一般的分布式事务场景，订单插入之后要调用库存服务更新库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案 

友情提示一下，rocketmq 3.2.6之前的版本，是可以按照上面的思路来的，但是之后接口做了一些改变，我这里不再赘述了。 

当然如果你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比如基于rabbitmq来玩儿。 

4、昨天学员给我提的一个问题 

老师，我们现在想保证我们的某个系统非常的可靠，任何一个数据都不能错，我们用的是微服务架构，几十个服务。结果我们一盘点，发现，如果到处都要搞的话，一个系统要做几十个分布式事务出来。 

我们的经验，我带几十人的team，最大的一个项目，起码几百个服务，复杂的分布式大型系统，里面其实也没几个分布式事务。 

你其实用任何一个分布式事务的这么一个方案，都会导致你那块儿代码会复杂10倍。很多情况下，系统A调用系统B、系统C、系统D，我们可能根本就不做分布式事务。如果调用报错会打印异常日志。 

每个月也就那么几个bug，很多bug是功能性的，体验性的，真的是涉及到数据层面的一些bug，一个月就几个，两三个？如果你为了确保系统自动保证数据100%不能错，上了几十个分布式事务，代码太复杂；性能太差，系统吞吐量、性能大幅度下跌。 

99%的分布式接口调用，不要做分布式事务，直接就是监控（发邮件、发短信）、记录日志（一旦出错，完整的日志）、事后快速的定位、排查和出解决方案、修复数据。

每个月，每隔几个月，都会对少量的因为代码bug，导致出错的数据，进行人工的修复数据，自己临时动手写个程序，可能要补一些数据，可能要删除一些数据，可能要修改一些字段的值。 

比你做50个分布式事务，成本要来的低上百倍，低几十倍 

trade off，权衡，要用分布式事务的时候，一定是有成本，代码会很复杂，开发很长时间，性能和吞吐量下跌，系统更加复杂更加脆弱反而更加容易出bug；好处，如果做好了，TCC、可靠消息最终一致性方案，一定可以100%保证你那快数据不会出错。 

1%，0.1%，0.01%的业务，资金、交易、订单，我们会用分布式事务方案来保证，会员积分、优惠券、商品信息，其实不要这么搞了

### 30_5_1、业内分布式事务方案介绍：分布式相关核心理论之CAP与BASE的基础知识筑基

### 30_5_2、业内分布式事务方案介绍：分布式相关核心理论之CAP与BASE的基础知识筑基

CAP、BASE跟后面要看的分布式事务有直接的关系，但是这两个分布式的理论对我们研究分布式系统里面的一些技术和方案都是作为基础的知识需要掌握的 

这个CAP这个东西啊，也是个在研究分布式相关的问题中，比较经典的这么一个理论，大家在学习下面的知识之前，最好是先有相关知识的一个积累，这样下面学习起来才会比较轻松一些 

CAP，就是Consistency、Availability、Partition Tolerence的简称，简单来说，就是一致性、可用性、分区容忍性，所以这个CAP理论讲的就是这么个东西，但是这里的话呢，其实大家觉得很虚，虚的不行，简直是虚头巴脑啊 

所以网上很多类似的什么CAP理论的文章和博客，都是这么讲解的，大家看了就觉得心里凉凉的，不知道是啥玩意儿 

（1）一致性 

先说说C，就是一致性吧，这个其实很好理解，就是说一个分布式系统中，一旦你做了一个数据的修改，那么这个操作成功的时候，就必须是分布式系统的各个节点都是一样的， 

能说，客户端发起一个数据修改的请求，然后服务器告诉他成功了，结果去查的时候，从某个节点上查询数据，发现这个数据不对啊，这样的话就成了数据不一致了，就是分布式系统的各个节点上的数据是不一样的，就是不一致 

这个所谓一致性还分成几种： 

啥叫**强一致性**呢，就是说上面讲的那种就是强一致性；**弱一致性**呢，就是你更新个数据，鬼知道能不能让各个节点都更新成功；**最终一致性**，就是可能更新过后，一段时间内，数据不一致，最后过了一段时间成功了 

最终一致性，应该是分布式系统中非常常见的这么一个东西，redis主从同步，你可以做成主从异步同步的，主节点同步数据到从节点上去的时候，异步，最终一致性的体现。 

你的一个客户端往redis主节点里面写入了一条数据，在一段时间内，你客户端如果从redis从节点去查询数据，此时可能是查不到的，但是redis主从机制给你保证的是，过了一段时间之后，你再查，一定是可以从redis从节点里查到的 

（2）可用性

这个A，就是可用性，其实也很好理解，就是你的分布式系统必须是可用的啊，说句不好听的，要是一会儿访问你是成功，一会儿访问你失败，那失败的时候就是不可用，有不可用的情况存在，就导致可用性降低了 

什么叫做可用？客户端往分布式系统的各个节点发送请求，都是可以获取到响应的，要不是可以写入成功，要不是可以查询成功；什么叫做不可用呢？客户端往分布式系统中的各个节点发送请求的时候，获取不到响应结果，这个时候，系统就是不可用了，写入失败，人家不让你写入，不接受你的请求 

可用性分成好多级别，比如99%，99.9%，99.99%，99.999% 

99%，一年中只能有80小时左右是可以允许访问失败的

99.9%，一年中大概有8小时左右是可以访问失败

99.99%，一年中有大概不到1小时是可以访问失败的

99.999%，一年中有大概不到5分钟是可以访问失败的

99.9999%，一年中只能有大概不到1分钟可以访问失败 

那一般来说，就我个人观察，很多行业大部分的系统，其实99%可用性都没到，或者可能大概就在99%是一个很正常的水平，每年总得故障几次。能做到99.9%的系统就算是比较牛的了，也算很不错了，毕竟一年内就几个小时不可用 

一般做到99.99%，也就是所谓的4个9，那就是比较高的水平了。而至于说99.999%，五个9，那是行业内的顶尖水平 

（3）分区容忍性 

分区，partition，network partition，网络分区 => 分布式系统之间的网络环境出了故障，分布式系统的各个节点之间现在已经无法进行通信了 

分区容忍性，你的分布式系统可以容忍网络分区的故障，出现上面说的那种网络分区的故障之后，分布式系统的各个节点之间无法进行通信，无所谓，整套分布式系统各个节点，各自为战，该干嘛干嘛，只不过互相之间无法通信而已 

分布式系统还是在运转着，你分别给各个节点发送请求，人家还是可以给你一些响应结果的，这个就是实现了分区容忍性 

这玩意儿搞的稀奇古怪的，啥东西啊，其实说白了，就是一个分布式的系统，如果遇到了网络分区的故障，也就是说，分布式系统互相之间无法联通了，这个时候咋整呢，有点儿恶心啊，这里要求的是，遇到网络分区故障，也类似于传说中的脑裂吧，然后系统还是可以正常对外提供服务的 

如果不具备分区容忍性，那会怎么样呢？那就是说一旦网络故障，整套系统崩溃，你哪怕给各个节点发送消息，全部失败，清一色失败，整套系统甚至会宕机，不再运转了 

（4）CAP => CP or AP 

不可能CAP三者兼得的，CAP理论里面，最最重要的一点，就是说，不可能一个分布式系统同时兼备一致性、可用性、分区容忍性，要么几句是CP（一致性 + 分区容忍性），要么就是AP（可用性 + 分区容忍性） 

基于这套理论，redis、mongodb、hbase什么什么的分布式系统，都是参照着CAP理论来设计的，有些系统是CP，有些系统是AP 

（4）CP 

一般来说，CAP要么同时满足AP，要么同时满足CP，不可能同时满足CAP的，啥意思呢 

如果实现CP的时候，为什么就无法同时满足AP了？为什么有了一致性，就不能有可用性了？CAP里面，为什么要们是CP，要么是AP？为什么一定要有P？分区容忍性，分布式系统，如果一旦出现了一些网络分区的故障之后，保证整套系统继续运转是非常重要的一点，所以很多分布式系统es，都设计了防止脑裂的机制 

P是一定要有，CP，AP，CA（不存在的） 

CP，为什么就没有A了呢？ 

假设，出现了网络分区的故障，但是因为有P，所以分布式系统继续运转，但是此时分布式系统的节点之间无法进行通信，也就无法同步数据了 

此时客户端要来查询数据，也就是那个key1的数据了，此时系统实际上是处于一个不一致的状态，因为各个节点之间的数据是不一样的，如果客户端来查询key1这条数据，你要是要保证CP的话，就得返回一个特殊的结果（异常）给客户端 

任何一个节点此时不接收任何查询的请求，返回一个异常（系统当前处于不一致的状态，无法查询），这样的话呢，客户端是看不到不一致的数据的 

此时对客户端而言，要么查到的是一致性的数据，要么如果数据不一致什么都查不到，不让你看到不一致的数据，这就保证了CAP里的C，一致性，分布式系统本身处于不一致的时候，让你看不到不一致的数据，就保证了一致性，保证了CP 

但是此时的话，就牺牲掉了A，可用性，因为此时不让你看到不一致的数据，所以你发送请求过来是返回异常的，请求失败了，此时分布式系统就暂时处于不可用的状态下，也就是保证了CP，就没有了A 

弄个分布式系统给大家演示一下，就俩节点，假设现在发生了网络分区故障，好了，那么P起码要保证吧，就是网络分区的时候，系统还是要正常可以运行的，所以P先保证了，对吧，然后呢，因为网络分区，导致俩节点互相不能通信了 

现在呢，你写入一条数据到其中一个节点，好了，结果这个节点没法同步数据到其他的节点上去啊，咋整呢，尴尬啊尴尬，俩节点上数据不一致了 

所以这个时候，如果你要满足C，也就是一致性，你觉得应该怎么办，你要是继续让所有人访问两个节点，那数据100%不一致，一会儿数据这样，一会儿数据那样，这个时候，你就只能牺牲掉A了 

也就是说，在这种情况下，你的系统直接对外不再提供服务，人家查询直接返回异常，不让查到不一致的数据，不就可以保证一致性了，呵呵，但是你就牺牲了可用性了，因为这个时候你的系统是不可用的 

经典的就是一些分布式存储，比如说zookeeper、mongodb、hbase等等，跟他们都是CP的，也就是说数据100%一致，但是有可能有些时候你请求是失败的，不让你请求到不一致的数据，这就是CP 

如果要保证CP的话，C，保证说你在任何情况下写入一条数据，接着从任何一个节点去查都可以看到一致的数据，不可能让你一会儿看到旧数据，一会儿看到的是新数据，这样就保证了一致性 

有些特殊的情况下，确实数据就是没法同步，没法一致性，此时可能就得牺牲A了，可能短暂的情况下，你发送请求过去人家返回异常给你，此时就是短暂不可用的，让你过段时间在重试查询 

（5）AP 

如果网络故障，数据没同步，数据处于不一致的状态下，要保证A，可用性，你两个节点都要允许任何客户端来查询，都可以查到，这样的话呢，整个系统就处于可用的状态下，但是此时就牺牲掉了C 

一会儿可以查到key1的数据，一会儿从另外一个节点去查又查不到了，这就是对客户端而言，看到了不一致的数据 

在各种分布式系统里面，CAP不可能同时兼得，指的主要是什么呢，就是发生网络故障的时候，可能一些数据没有同步一致性，此时要么就是CP，要么就是AP 

那如果要保证AP呢，也就是可用性必须保证，人家过来查必须给人查，那就牺牲掉一致性咯，随便查，要怎么查怎么查，但是查到的数据不一致，那我不管了，反正就这么回事儿了，哈哈哈。。。起码我可用性保证了，一致性就没了 

对于12306、电商系统，这种业务类系统，一般都是AP，也就是说，你可能看到的商品库存或者火车票的库存，是错的，有可能是旧的啊，那么数据很可能看到的都是不一致的，但是呢，你买东西或者买票的时候，一定会检查库存，就可以了 

但是保证了可用性就ok，任何时候都要响应结果，不能动不动就失败 

12306买票，AP，C其实是没保证的。很多人同时在订票，每次订票之后这个车票的库存就会扣减，但是车票库存扣减之后，可能不能及时的被你的12306网站展示出来，可能你查询的车票的库存，是从另外一个库里去查的，最新的库存数据还没同步过来，此时数据是不一致的 

所以你看到的是不一致的数据，C，但是AP，可用性是保证的，时时刻刻都让你可以看到数据，可以买票，可以查询，但是呢可能你看到的车票还剩5张，但是你发起订票的时候，人家一检查最新的库存，判断已经是0张了，就不让你买了呗 

（6）BASE理论 

所谓的BASE，Basicly Available、Soft State、Eventual Consistency，也就是基本可用、软状态、最终一致性 

BASE希望的是，CAP里面基本都可以同时实现，但是不要求同时全部100%完美的实现，CAP三者同时基本实现，BASE，基本可用、最终一致性 

此时要保证基本可用性，应该怎么办呢？两个节点都可以查询的，但是这个时候你会发现说有的节点可以返回数据，有的节点无法返回数据，会看到不一致的状态，这个不一致的状态，就是指的是BASE中的S，soft state，软状态 

基本可用，降级，正常情况下，是查询可以负载均衡到各个节点去查的，也就是可以多节点抗高并发查询，但是此时如果你要降级的话，可以降级为，所有客户端强制查询主节点，这样看到的数据暂时而言都是一样的，都是从主节点去查 

但是因为客户端访问量太大了，同时用一个主节点来支撑很坑，扛不住，怎么办呢，主节点做限流降级，也就是说如果流量太大了，直接返回一个空，让你稍后再来查询 

如果你这样子来降级了，保证的就是所谓的基本可用，降级的措施在里面了，跟正常的可用是不一样的，比正常的可用要差一些，但是还是基本可以用的 

最终一致性，一旦故障或者延迟解决了，数据过了一段时间最终一定是可以同步到其他节点的，数据最终一定是可以处于一致性的 

这个基本可用的意思，就是说可以适当进行降级，比如说某些系统是可以进行降级的，在故障的时候，直接引导到降级的一些功能里去，举个例子吧，本来商品详情页可以是个极度华丽的页面，但是如果降级的话，那么就变成一个比较简陋的页面，里面包含少量数据 

软状态意思就是说，可以存在中间的数据状态，就是比如多个节点在同步数据，在一段时间内，可能每个节点数据不一致，正在同步过程中，这个就是软状态 

最终一致性，就是说，虽然存在软状态，但是最终还是会变成一致的 

所以说，CAP和BASE是俩理论，是俩基础理论，你在设计分布式系统的话，可以用CAP中的CP或者AP，也可以采用BASE理论，有一些不一样，也有一些关系

### 30_6、业内分布式事务方案介绍—各种分布式事务技术方案如何结合起来运用在流量充值中心内

分布式事务常见的几种方案： 

（1）XA分布式事务，一般用于单系统多库的场景，当然要是多系统多库，也可以，但是就很麻烦了，不适用于这个方案了

（2）TCC方案，try-confirm-cancel方案

（3）可靠消息最终一致性方案，都不能叫做分布式事务的方案，事务，分布式一致性的方案

（4）最大努力通知方案

（5）适合长事务（分布式）的sagas方案，之前在分布式事务解决方案的筑基里面，没有提到sagas，不会放在流量充值中心系统里面来实战，会放到后面我们的实际的大电商项目里去实战 

在我们这里而言，非常简单 

（1）TCC方案，适合于，你的多个服务的操作都比较快 

TCC相当于是一堆同步服务调用的操作，包裹在一个事务里面，同步，关键词，人家给你发起一个请求，触发了一个复杂的TCC事务，人家要等你这个事务完成结束了，然后才能接续往下走的 

假如你的TCC事务里面涉及了10来个服务的调用，要10来秒才能结束，太不靠谱了 

TCC方案应对的其实是大量的同步服务调用的复杂的事务场景，如果要用TCC来保证分布式事务的执行，一般来说尽量确保每个服务的调用都比较快，一般来说确保一个TCC分布式事务的执行，大概需要总共1秒以内的时间 

资金转账、创建订单、抽奖机会、积分、流量券相关的服务调用的逻辑，包裹在一个分布式事务内，用TCC来控制这个分布式事务，因为这里的一些操作基本都是在流量充值中心内部的一些服务，都比较快 

TCC来控制，try他们一把，锁定一些资源；confirm一把，执行各个服务的业务逻辑；如果任何一个服务出现报错和失败；那么tcc就去cancel掉各个服务的逻辑，各个服务通过补偿来的方法逻辑，去回滚之前做出的数据变动 

（2）可靠消息最终一致性的方案 

这个方案，适合于那那种比较耗时的操作，通过这个消息中间件做成异步调用，发送一个消息出去，人家服务消费消息来执行业务逻辑，CAP理论，C（最终一致性），也就是说包裹在一个事务中的多个操作，其中有些操作可能在一定时间内是没执行的 

可能要等过一段时间之后，然后才能去执行，最终一定会执行的，最终一致性的方案，通过MQ消息中间件保证消息的可靠性，最终来实现最终一致性的方案 

调用起来很耗时的操作，比如说流量充值内，调用第三方运营商的系统接口完成流量充值，坑爹了，很可能会出问题，网络调用超时，人家系统代码写的太烂，一个流量充值要耗费个10秒钟才能完成 

你就很不适合包裹在TCC里面了，因为这个东西调用第三方的系统接口，如果一旦超时了，很容易影响系统本地其他服务的操作 

而且的话呢，一般来说，如果你充值话费，或者是充值流量，肯定不是说你刚付钱充值完毕，人家会通知你充值成功了，发你一个短信，告诉你说，具体是否充值到账，请以运营商那边的信息为准 

你付钱之后，其实流量还没充值好，在一段时间内是没充值的，最终过一段时间，几分钟之后，人家一定会保证给你把流量充值到位 

调用第三方运营商系统接口的操作，很适合用可靠消息最终一致性的方案 

（3）最大努力通知方案 

跟可靠消息最终一致性方案是类似的，可靠消息最终一致性方案，会保证最终必须要让那个执行成功的，但是最大努力通知方案，不一定保证最终一定会成功，可能会失败，但是他会尽力给你去给你通知那个服务的执行 

比较适合那种不太核心一些服务调用的操作，比如说消息服务，充值好了以后发送短信，一般来说肯定是要发出去短信的，但是如果真的不小心发送失败了，发送短信失败了也无所谓的。。。 

可以一共最大努力通知方案

### 30_7、业内分布式事务方案介绍—画图说明TCC分布式事务具体方案及几种变种方案原理

**1、通用性TCC技术方案** 

其实所谓的TCC思想，画一张图来说，大致说起来就很简单，我们就用自己的流量充值中心来举个例子好了，你要是不考虑具体的技术实现的话，大概来说呢，相当于就是下面这样的思路： 

（1）主业务服务：相当于流量充值中心的服务，他就是TCC事务的主控服务，主要控制的服务，负责整个分布式事务的编排和管理，执行，回滚，都是他来控制

（2）从业务服务：相当于我们的资金服务、订单服务、积分服务、抽奖服务、流量券服务，主要就是提供了3个接口，try-confirm-cancel，try接口里是锁定资源，confirm是业务逻辑，cancel是回滚逻辑

（3）业务活动管理器：管理具体的分布式事务的状态，分布式事务中各个服务对应的子事务的状态，包括就是他会负责去触发各个从业务服务的confirm和cancel接口的执行和调用。。。 

（1）try阶段，资源的锁定，先冻结掉用户的账户资金，将一部分资金转出到冻结资金字段里去；可以创建一个充值订单，但是状态是“交易中”

（2）confirm阶段，就是将用户的冻结资金口减掉，转移到商户的账户里去；同时将充值订单的状态修改为“交易成功”；完成抽奖机会、积分、流量券的新增

（3）cancel阶段，try阶段任何一个服务有问题的话，那么就cancel掉，相当于是将冻结的资金还回去，将订单状态修改为“交易失败”；如果confirm阶段任何一个服务有问题的话，也是cancel掉，相当于是将商户账户里的资金还到用户账户里去，同时将订单的状态修改为“交易失败”

有一张比较经典的图，就是主业务服务->数据库，然后几个从业务服务->数据库，接着主业务服务会访问业务活动管理器（有活动日志），主业务服务发起执行try，然后主业务服务通知业务活动管理器，业务活动管理器再通知各个从业务发起confirm或者是cancel操作，可以把这张图给体现一下 

这里主业务服务其实就是总控整套逻辑的，然后从业务服务就是干活儿的，业务活动管理器主要是记录整个分布式事务活动状态的，这个还是挺有必要的吧，这样保存分布式事务进行过程中的各种状态才可以啊，兄弟！ 

不然分布式事务临时终端了，你系统重启，谁知道你之前跑到哪一步了啊，哥儿们！ 

所以他会记录整个分布式事务的状态，分布式事务里各个服务代表的子事务的状态，而且他是负责在提交分布式事务的时候，调用各个从业务服务的confirm接口的，如果出问题的话也是他调用各个从业务服务的cancel接口的 

所以说这里的一个执行流程和步骤大概是这样子的： 

（1）主业务服务会先在本地开启一个本地事务（这个本地事务说白了，就是你的主业务服务是不是也可能会干点儿什么事儿）

（2）主业务服务向业务活动管理器申请启动一个分布式事务活动，主业务服务向业务活动管理器注册各个从业务活动

（3）接着主业务服务负责调用各个从业务服务的try接口

（4）如果所有从业务服务的try接口都调用成功的话，那么主业务服务就提交本地事务，然后通知业务活动管理器调用各个从业务服务的confirm接口

（5）如果有某个服务的try接口调用失败的话，那么主业务服务回滚本地事务，然后通知业务活动管理器调用各个从业务服务的cancel接口

（6）如果主业务服务触发了confirm操作，但是如果confirm过程中有失败，那么也会让业务活动管理器通知各个从业务服务cancel

（7）最后分布式事务结束 

**2、异步确保型TCC技术方案**

如果要接入到一个TCC分布式事务中来，从业务服务必须改造自己的接口，本来就是一个接口，现在要新增两个接口，try接口，cancel接口。改造起来比较麻烦 

这个大概来说就是把之前的通用型TCC方案给改造了一下，就是在主业务服务和从业务服务之间加了一个可靠消息服务，但是这个可靠消息服务可不是在请求什么MQ之类的东西，而是将消息放在数据库里的 

大致来说呢，就是主业务服务的try、confirm和canel操作都调用可靠消息服务，然后可靠消息服务在try阶段插入一条消息到本地数据库；接着主业务服务执行confirm操作，可靠消息服务就是根据之前的消息，调用从业务服务实际的业务接口；如果要是这个调用失败的话，那么主业务服务发起cancel，可靠消息服务删除自己本地的消息即可 

这种方案大家可以看到，其实说白了最大的优点，就是不需要从业务服务配合改造，提供try、confirm和cancel三个接口了，本来人家可能就一个接口，现在你楞是要求人家提供三个接口，真尴尬 

那要是用了这种方案，就可以用可靠消息服务替代各个从业务服务提供TCC三个接口了 

**3、补偿性TCC解决方案**

这个其实是跟通用型的TCC方案类似的，只不过从业务服务就提供俩接口就ok了，Do和Compensate，就是执行接口和补偿接口，这种方案的好处就是折中一下了，不需要从业务服务改造出来一个T接口，就是锁定资源的接口，只需要加一个补偿接口，如果业务逻辑执行失败之后，进行补偿 

这样就可以少做一个接口了，但是因为没有做资源的一个锁定，那么大家需要自己注意类似资金转账的余额检查之类的事儿了，还有就是补偿的时候，因为你没做资源锁定，所以要注意一下补偿机制是否一定会成功 

其实说实话，这个补偿性的TCC方案还是蛮不错挺有吸引力的 

Do接口，Compensate接口，不要try接口，不要锁定资源，直接执行业务逻辑，如果有失败就调用Compensate接口，补偿接口，回滚刚才的操作 

### 30_8、业内分布式事务方案介绍：分析TCC分布式事务技术方案落地在项目中的一些细节

其实这个所谓TCC方案里有很多细节要考量一下啊！！！ 

**1、接口拆分问题** 

首先就是，从业务服务的每个接口都要拆分为三个接口，一个是try接口，一个是confirm接口，一个是cancel接口，也就是说要提供分布式事务实现的业务接口，自己就要考虑好这个，要提供3个接口 

虽然真是够麻烦的，不过也没办法 

try接口里，一般就是预留资源，比如说经典的资金转账，卡掉一些锁定资金，你要是不这么干，万一别的分布式事务给你干掉了一些资金，那你实际执行confirm的时候一旦检查资金余额就会发现转账失败，余额不足了 

有些接口，没有资源锁定的操作，try接口就留空 

confirm就是原来的业务方法，该干嘛干嘛 

cnacel接口，要提供回滚的方法，就是把try或者confirm里的操作给他回滚了 

就比如说，如果是try阶段，资金服务的try成功了，资金被冻结了24块钱，结果订单服务的try失败了，主业务服务就会通知回滚，调用资金服务的cancel接口，就要检查一下lock_amount字段里的值，将里面的24块钱转回到原来的amount字段里面去 

confirm阶段，资金服务，都把24块钱从id=1的账号里转移到id=2的账号里去了，lock_amount也扣减掉了24块钱。结果积分服务的confirm失败了，整个分布式事务回滚，调用各个接口的cancel接口 

资金服务，就变成了需要将id=2的账号的amount字段扣减掉24块钱，给id=1的账户增加24块钱 

**2、接口的几种特殊情况** 

（1）空回滚：那要是try阶段，比如网络问题，人家压根儿没调通你的try接口，结果就认定失败，直接调用你的cancel接口，咋办？所以你这个时候啥都不能干 

（2）try回滚以及confirm回滚：try阶段如果执行了，但是其他服务try失败了，那么会调用cancel来回滚，你要可以回滚掉try阶段的操作；confirm阶段要是你执行了，但是有别的服务失败了，此时你就要回滚掉confirm阶段的操作 

（3）倒置请求：比如说人家调用try接口，中间网络超时了，结果认定失败，直接调用cancel空回滚了；结果过了几秒钟try接口请求到来，此时咋整呢？尴尬了吧，你要在这个时候不允许执行try接口操作；同理啊，confirm请求超时了，结果都cancel掉了，但是过了几秒请求来了，让你confirm，你能干这事儿吗？ 

**3、接口的幂等性保证** 

你有没有考虑过一个问题，就是try、confirm和cancel都可能被多次调用，所以无论怎么样，你都得保证这几个接口的幂等性，分布式接口幂等性那必须依赖第三方的中间件来实现，可以考虑使用经典的zk，zk非常适用于分布式系统的协调类操作 

所以一个接口对同一个参数调用，只能调用一次，保证幂等操作 

**4、tcc分布式事务框架** 

我们的主业务服务那块，那必须得用tcc事务框架，不然各种接口调用，还有就是业务活动管理器，难不成都大家自己来写代码搞？？？？那就废掉了啊！所以必须要选用一种tcc分布式事务框架，来实现主业务服务的各种try confirm concel控制逻辑，同时实现业务活动的管理 

**5、总结** 

玩儿tcc初步来讲主要就是上述那些问题，其实说白了，一个就是从业务服务那块的接口的问题，还有一个其实就是主业务服务那块的业务活动管理器的控制，以及整个分布式事务的控制

### 31_国内工程师开源的分布式事务框架以及Seata选型对比

开放了一些我在架构班里讲解分布式事务时候的一点点视频，就跟我们的小案例，C2C二手电商平台的社会化治理系统，进行一个结合，为什么我们需要一个分布式事务，在具体的技术层面，我们如何来进行选型，有哪些东西可以选择 

除了阿里开源的Seata之外，根本就没什么太成熟的分布式事务的框架，TCC方案 -> ByteTCC，写的还不错，LCN，写的也还不错，这些框架都是小范围的流传和使用，没有大规模的很多中大型公司都落地的案例，生产可能都有一些坑 

ByteTCC、LCN，一定要确保先把里面的源码先吃透，搞定，源码每个细节都能搞定，再投入自己生产去用，有坑，有问题，自己绝对可以hold住，在架构班的课程里，Atomikos的源码，ByteTCC分布式事务框架的源码，自己写类似于RocketMQ分布式事务方案的可靠消息的服务中间系统 

中小型公司，如果要上分布式事务的方案，最好用成熟大厂开源的方案，阿里开源的Seata，可靠性还是有保障的，Seata的基本的原理，和使用，跟案例的整合

### 32_SpringCloudAlibaba之Seata分布式事务方案（1）

### 32_SpringCloudAlibaba之Seata分布式事务方案（2）

### 33_SpringCloudAlibaba之Seata分布式事务原理剖析（1）

### 33_SpringCloudAlibaba之Seata分布式事务原理剖析（2） 

TCC框架，Saga框架，Seata支持好几种分布式事务方案，全面、完善的分布式事务解决方案，成熟框架 

TCC方案，ByteTCC，搜索一下，维护这个框架的，QQ群 

你的核心接口，全部要写三套，每个接口都要拆分为3个接口，Try、Commit、Cancel，一个接口，chooseCandidate接口，tryChooseCandidate，commitChooseCandidate，cancelChooseCandidate，每个接口要写3套逻辑 

try里面，可以在数据库添加一批评审员，状态都设置为INVALID，无效的；commit，就可以把他们的状态设置为VALID；cancel，就可以把插入的那批评审员给删除了 

分布式事务这个技术以及思想，过去在国内一直没有受到重视的，大厂还是小公司，只要你不是金融级跟钱直接相关的系统，普通的互联网系统，哪怕是订单系统，都不会上分布式事务，TCC思想，太麻烦了 

Atomikos框架，但是在国内也是很少很少用的 

Seata，支持多种分布式事务方案：TCC、XA、AT、Saga 

TCC而言，你不需要写TCC三个接口的，你的业务代码，就跟以前是一样的，就是一个接口，你的接口以前是什么样子的，现在也可以是什么样的，接口都没有任何的变化的话，分布式事务到底是怎么做的 

Saga，在国内很少很少用，长事务，N多个服务串联在一起执行，补偿 

TC、TM、RM 

TC，Seata自己独立部署的一个server，他用于全面的管理每个分布式事务；TM，用于对单个分布式事务进行管理和注册；RM，是对一个分布式事务内的每个服务本地分支事务进行管理的 

分布式事务框架的角度来思考，TM这个东西他是怎么运作起来的 

TM这个东西，完全是可以基于一个注解来进行驱动的，Spring，AOP机制，切面机制，Spring技术体系里，还有一些其他的拦截机制，监听机制，回调机制，完全可以针对TM设计一个注解，@ShishanTransaction，加在你的举报服务的Service组件的方法上，依托Spring的机制去对方法调用做一个拦截，如果你发现这个方法加了你指定的分布式事务的注解 

提取一下本次请求里带的一些请求头或者是请求附加的内置参数，有没有一个全局事务id，xid，这个带着@ShishanTransaction注解的方法，他其实是一个分布式事务的起始方法，TM这样的一个组件的业务逻辑就可以开始运作起来了 

引入依赖，还会在spring boot配置文件里配置一下分布式事务的一些配置，分布式事务server端的地址，暴露出来的都是RESTful API接口，基于HTTP请求就可以了 

TM如果要找TC注册一个全局事务，此时就可以通过HTTP通信组件，发送HTTP请求到指定地址的TC server的接口就可以了，TC server可以注册一个全局事务，生成一个唯一的txid，返回给你的TM 

RM如何可以拦截你的本地数据库的操作呢？ 

代理你的数据源，操作数据库，必须要有一个数据库连接，JDBC接口规范里就是一个Connection对象，数据库连接池，Druid、C3P0、DBCP，维护一个数据库连接池，一定会从数据库连接池里获取一个数据库连接 

依托这个数据库连接去对数据库执行增删改的操作 

你可以针对你的数据库连接去做一个代理，也就是说，业务系统拿到的数据库连接是被你代理过的，他基于你的代理数据库连接执行增删改操作，代码会先执行到你手上，此时你就可以做一些操作了 

增删改的语句，执行一些查询，DELETE语句，UPDATE，生成一个逆向的UPDATE语句，想要把一个字段改成1100，900，生成一个INSERT语句 

在一个本地事务里，让他执行增删改，把你生成的undo log插入到数据库的undo_log表里去，发送HTTP请求到TC去注册一个分支事务，提交本地事务，把增删改操作和undo log插入都放一个本地事务里，他们会一起成功或者失败 

### 34_对分布式事务方案SeataServer进行部署

http://seata.io/zh-cn/blog/download.html 

在seata官网下载，下载1.0.0版本的安装 包就行了，就那个binary的, seata-server-1.0.0.zip，然后scp上传到机器上去，yum -y install unzip，然后unzip命令解压缩 

seata server要存储很多数据，两种模式，一个是file，一个是db，建议用file，因为db的性能有点差，默认就是file模式的：sh seata-server.sh -p 8901 -h 192.168.31.155 -m file，如果要用db模式，参考官网，还得先建一大堆的表 

这边给大家讲一下registry.conf是什么，就是seata server可以集成注册中心，可以让你seata server注册到比如nacos去，作为一个服务，然后你各个服务就不用手动配置seata server地址了，直接发现就可以了，不过不用这个手动配置其实也行 

file.conf，是seata server的配置信息，撸一遍就行了，他还可以支持集成nacos配置中心，把配置放到nacos里去，然后seata server自动加载配置，一般你不用也可以 

但是正常启动，会要求1GB内存，那肯定是不够的，需要修改启动文件里的内存分配，调整好了就可以启动了 

nohup sh seata-server.sh -p 8901 -h 192.168.31.155 -m file > /dev/null 2>&1 &

### 35_对DubboRPC框架与Seata分布式事务集成

### 36_C2C电商社会化治理平台核心链路的分布式事务

讲完了seata的一些原理之后，就要讲他的代码集成了，跟Dubbo，Spring Boot，Spring Cloud Alibaba等框架进行集成，教会大家一个办法，如果你要把某个技术集成到你自己的项目里去，最好的参考示例工程是什么呢？ 

官网文档里提供的示例程序，把他的示例程序给跑起来，就可以参考示例程序把技术集成到自己的项目里去就可以了 

在集成的过程中，可能那个技术，比如说seata并没有考虑到跟一些框架集成时的版本兼容的问题，所以经常会出现把某个技术集成到你自己的项目去的时候出现了一些版本兼容的问题 

http://seata.io/ 

dubbo + nacos，只不过是用spring cloud alibaba提供的一些便捷方法来进行搭建，本质nacos作为服务注册中心，dubbo作为rpc框架，现在如果说你要把seata引入进去，此时你完完全全就可以参考这个示例工程 

ORM框架，用的是jdbcTemplate

### 37_微服务化的系统是如何发生服务雪崩的？

雪崩、熔断、限流、降级 

Tomcat收到的每一个HTTP请求，都会交给一个独立的工作线程去处理，独立的工作线程一般可以配置为100~500之间的数量，看的是你的CPU有几个核，支持的线程数量就越多，4核，线程数量在200~300之间 

spring mvc、struts2这样的mvc框架，框架一般会把自己的servlet/filter配置在tomcat里面，tomcat就会把http请求交给mvc框架的servlet/filter，mvc框架就开始执行，处理请求 

mvc框架往往会扫描到你加了@Controller注解的类，扫描你里面的@Mapping注解，看看这个请求的URL路径格式和@Mapping注解里的指定的URL格式来做一个匹配，会把请求交给你的类里面的方法来处理 

微服务之间调用的一个线程模型了解清楚，任何请求在分布式系统里执行，每个服务里都有一个线程会去处理这个请求

### 38_为什么微服务化的系统需要进行限流和熔断？

#### 为什么微服务化的系统需要一个日志中心？ 

线上运行的系统，跑着跑着，突然有用户找客服反馈你的系统有个什么什么bug！bug可能有对应的异常报错，也可能没有报错，就是纯粹的是你的代码级别的bug，客服会找你们技术这边 

快速的定位bug发生的原因，定位 -> 修复 

通过查阅日志，分析bug发生时，系统是如何运行的，是否运行中有问题导致bug

如果你单个服务/系统部署的机器数量都超过5台了，此时必须要上日志中心

### 39_Sentinel、Hystrix等技术的选型对比

### 40_SpringCloudAlibaba之Sentinel限流熔断框架

### 41_SpringCloudAlibaba之Sentinel框架原理(1)

### 42_C2C电商社会化治理平台基于Sentinel实现限流(1)

### 43_C2C电商社会化治理平台基于Sentinel实现资源隔离

### 44_C2C电商社会化治理平台基于Sentinel实现熔断保护

授人以渔 

看一个新的技术，首先是先了解他的一个基本的介绍，包括他的定位、功能、特性、问题；了解一下他是怎么用起来的，看一下他最最基本的一些demo，引入依赖、注解/代码/配置、功能 

看看官网的技术文档，了解一下他基本的工作原理，看看这个技术假设在一个最基本的demo的指引下，集成和整合到你的项目里去以后，用起来了，用了他的一些功能了，解决了一些问题了，这背后基本的原理是什么 

对阻塞线程数的控制 + 熔断 + 降级，解决了服务雪崩的问题 

Spring Cloud Netflix，Hystrix这个项目，功能层面还是挺好用的，但是他的源码写的特别的烂，运用了特别奇怪的代码方式，源码，架构班，sprin cloud netflix源码剖析的课程里面，我深入分析过hystrix的源码的 

Spring Cloud Alibaba，Sentinel这个项目 

https://github.com/alibaba/Sentinel/wiki/%E4%BB%8B%E7%BB%8D 

雪崩、隔离、熔断、降级、限流 

Tomcat收到的每一个HTTP请求，都会交给一个独立的工作线程去处理，独立的工作线程一般可以配置为100~500之间的数量，看的是你的CPU有几个核，支持的线程数量就越多，4核，线程数量在200~300之间 

spring mvc、struts2这样的mvc框架，框架一般会把自己的servlet/filter配置在tomcat里面，tomcat就会把http请求交给mvc框架的servlet/filter，mvc框架就开始执行，处理请求 

mvc框架往往会扫描到你加了@Controller注解的类，扫描你里面的@Mapping注解，看看这个请求的URL路径格式和@Mapping注解里的指定的URL格式来做一个匹配，会把请求交给你的类里面的方法来处理 

微服务之间调用的一个线程模型了解清楚，任何请求在分布式系统里执行，每个服务里都有一个线程会去处理这个请求

### 45_为什么微服务化的系统需要一个配置中心？

### 47_携程开源的Apollo配置中心的原理介绍

### 48_Apollo配置中心的内核原理深度解析

### 49_对Apollo配置中心进行分布式架构部署

### 50_在C2C电商社会化治理平台中引入Apollo配置中心

### 51_如何让Apollo配置中心实现多环境配置隔离？

spring cloud netflix 跟 spring cloud alibaba 

服务注册中心，就很多种技术选型；分布式事务，也是很多种框架；sentinel和hystrix，能干的事情类似；配置中心领域，携程开源了一款apollo，spring cloud config，nacos本身是一个服务注册中心但是也带了配置中心的功能 

进行技术选型，首先当然应该是站在一个细化的角度，从各个方面去对比技术，sentinel和hystrix对比，apollo nacos spring cloud config选型对比，马上就出来博客，他们都可以用表格给你展示的很清晰 

哪个技术最热门，用的人最多，那么你就选用那个技术；都很热门，用的人都多，怎么选呢？分场景，每种技术适合什么场景；分公司，每种技术适合小公司还是大公司；RabbitMQ、RocketMQ、Kafka如何选型对比 

apollo，架构是比较复杂，比较完善的，功能上也很完善，活跃，并不一定中小型公司去使用apollo，spring cloud alibaba，nacos，nacos作为一个服务注册中心本身就包含了配置中心的功能，没必要花很多时间再去部署一套apollo 

可以考虑用apollo，把配置这块的功能和架构都抽取出来了 

spring cloud config，如果你用的不是spring cloud alibaba，用的是spring cloud netflix，那么你可以配合那个技术栈，直接用spring cloud提供的config项目作为配置中心就可以了，因为这是属于spring cloud原生技术栈里提供的 

nacos完全可以满足很多中小型公司的配置中心的需求，哪怕是大公司也可以用的，apollo确实用的公司很多，中大型公司都会去用apollo，而且他的功能很完善的，讲，还是讲apollo，nacos 

https://github.com/ctripcorp/apollo

### 52_为什么微服务化的系统要需要一个监控中心？

### 53_Zabbix、Falcon、Prometheus的选型对比

2015~2016年左右以及之前，国内比较多的用的是Zabbix这个国外开源的监控系统，直接部署做一些配置，就可以让他去监控你的各个服务器以及部署的服务实例了；Facon，做的还是比较好的；2018~2019再往后推，基本上跟着微服务体系，一般监控中心国外和国内，最火的，用的比较多的就是Prometheus 

很多系统平时上线了以后，似乎就是咔咔的跑着，好像你也没有去关注他，如果没人反馈bug给你，你似乎根本就不去care他 

线上系统，可能产生的三个层面的问题： 

1、 机器资源的层面：cpu、内存、网络、磁盘、io，出现了负载过高的问题

2、 JVM进程的层面：jvm内部各个区域的内存使用以及gc频率

3、 代码层面：代码逻辑的内部，抛异常，出现一些不希望发生的系统异常 

但凡是线上的系统，配置中心、分布式事务、sentinel dashboard都可以不用，主要把服务注册中心和rpc框架搞好，一套服务之间RPC调用组成的一套系统能上线就可以了，可以跑起来了，但是监控中心是必须要有的

### 54_Prometheus的整体架构设计以及原理介绍

在你要监控的服务器上部署exporters，比如node_exporter就是基于linux内核写的，专门收集机器的cpu、内存、网络、io、磁盘的资源使用情况，然后prometheus server就可以从exporters拉取metrics过来存储和展示，以及进行报警 

除此之外，对什么mysql，redis之类的中间件，都有对应的exporter，你自己也可以写exporter，按照他的标准写就行了 

还有一种，就是有一个pushgateway，可以让你直接推送metrics给他，比如你系统的一些业务指标监控，就可以走这种方式 

然后prometheus server可以把拉取到的metrics存储到本地磁盘去，基于TSDB进行时序数据的存储，也会自动的清理旧数据，保留最新数据，TSDB是时间序列数据库，尤为适合这种监控指标的存储，按时间来存储 

定时查询配置好的报警规则，如果发现metric满足规则，就把alert发送到alertmanager去，此时会通过钉钉了、email了、短信之类的方式去对你进行告警 

如果要能够可视化的查看metric监控报表，一般是基于Grafana可视化系统来进行的，Grafana天然支持对接prometheus，专门是做指标可视化的，很方便使用，需要独立进行部署，不然他自己也有Prometheus Web UI，可以基于他的PromQL查询语句去查询

### 55_基于Prometheus实现系统的指标监控以及告警

### 56_C2C电商社会化治理平台的监控体系设计 

具体部署不用演示了，因为比较简单，给一下步骤就行 

```
prometheus-2.4.0.linux-amd64.tar.gz 

tar -zxvf prometheus-2.4.0.linux-amd64.tar.gz -C /data
cd /data
chown -R root:root prometheus-2.4.0.linux-amd64
ln -sv prometheus-2.4.0.linux-amd64 prometheus 

cd /data/prometheus
./prometheus 
```

直接访问本机的9090端口号，就能看到prometheus的web ui 

接着就是对要监控的机器去部署node exporter，是基于go语言写的，可以拿到cpu、内存、磁盘空间、磁盘io、网络带宽、系统负载、主板温度等一系列的机器资源监控指标，这个是最基本要做的监控 

在https://prometheus.io/download/里面找到node exporter的下载地址，下载一个最新的版本，接着进行解压缩，直接./node_exporter运行起来，就O了，默认的监听端口是9100，然后再把这个node exporter跟prometheus集成起来 

编辑prometheus的配置文件，有一个prometheus.yml，里面需要加入job去跟node_exporter进行集成 

```
scrape_configs:
-  job_name: ‘prometheus’
	static_configs:
	-targets: [‘192.168.xx.xx:9090’]
- job_name: ‘node_exporter’
   static_configs:
   -targets: [‘192.168.xx.xx:9100’] 
```

重启prometheus server就可以了，直接进入web ui就可以在targets里找到你要监控的机器，然后里面各项资源监控报表都可以看到了 

让spring boot业务系统接入prometheus也很简单，首先是加入一些依赖 

就是io.prometheus.simpleclient相关的一些依赖，这个其实大家可以自行搜索，网上很多文章讲这个，我们就是说思路，然后在Application类上加入@EnablePrometheusEndpoint注解就可以了 

此时你访问http://localhost:8080/prometheus，就可以看到jvm的监控指标了 

如果还要接入自定义的指标，需要加入一个拦截器，然后代码里用prometheus client提供的Counter类去进行指标计数就可以了，除此之外，还有gauge、Histogram之类的指标收集API，都可以用来统计业务指标，然后就跟之前一样，接入prometheus即可 

业务指标：需要采集三个，异常指标，QPS，接口时延，TP99，TP95，TP90 

其实说实话，这些操作步骤，搜索就是很多，所以不带着大家做了，但是希望大家脑子里应该有这么个思路 

### 57_为什么微服务化的系统需要一个日志中心？

### 58_Elasticsearch的整体架构原理介绍

线上运行的系统，跑着跑着，突然有用户找客服反馈你的系统有个什么什么bug！bug可能有对应的异常报错，也可能没有报错，就是纯粹的是你的代码级别的bug，客服会找你们技术这边 

快速的定位bug发生的原因，定位 -> 修复 

通过查阅日志，分析bug发生时，系统是如何运行的，是否运行中有问题导致bug 

如果你单个服务/系统部署的机器数量都超过5台了，此时必须要上日志中心

### 59_基于Elasticsearch设计日志中心的原理分析

### 60_C2C电商社会化治理平台的日志体系设计

### 61_C2C电商社会化治理平台接入日志中心

线上运行的系统，跑着跑着，突然有用户找客服反馈你的系统有个什么什么bug！bug可能有对应的异常报错，也可能没有报错，就是纯粹的是你的代码级别的bug，客服会找你们技术这边

快速的定位bug发生的原因，定位 -> 修复

通过查阅日志，分析bug发生时，系统是如何运行的，是否运行中有问题导致bug 

如果你单个服务/系统部署的机器数量都超过5台了，此时必须要上日志中心 

《互联网Java工程师面试突击第一季》，儒猿技术窝，免费，不要钱，ES的架构原理，我都讲过，大家直接去看一下，倒排索引，正排索引，shard数据分片，replica副本，写入原理，分布式搜索原理 

业务、项目、服务、时间戳（long）、不同的日志有自己的特殊的业务id（用户id、举报渠道）、日志内容（举报的具体内容数据，json字符串来存储） 

日志中心没有开源的，可能有少数的不知名的，但是连我都没听说过 

一般都是公司自己基于ES来做研发的，ES+HBase，黄金组合，HBase适合的是海量数据进行存储，ES，内置生成ES的document id，id作为rowkey把日志内容放到HBase里去，程序员，可以根据业务、项目、服务、时间戳（long）、不同的日志有自己的特殊的业务id（用户id、举报渠道）来搜索日志

![](C:\Users\zy199005\Desktop\中华石杉\images\社会化治理平台架构设计.png)

### 62_为什么微服务化的系统需要进行链路追踪？

### 63_Sleuth、Zipin、CAT的选型对比

### 64_CAT实现分布式链路追踪的架构原理

监控：机器资源、JVM进程、系统（QPS、延时、异常）、请求链路 

日志：异常日志、请求日志 

链路 

请求链路

请求1次数据库  耗时多少ms

请求1次缓存   耗时多少ms

请求2次评审员服务 耗时多少ms

请求1次奖励服务   异常    耗时1s

### 66_为什么微服务化的系统需要一个API网关 

网关的核心功能 

（1）动态路由：新开发某个服务，动态把请求路径和服务的映射关系热加载到网关里去；服务增减机器，网关自动热感知

（2）灰度发布

（3）授权认证

（4）性能监控：每个API接口的耗时、成功率、QPS

（5）系统日志

（6）数据缓存

（7）限流熔断

### 67_Zuul Srping Cloud Gateway的选型对比

### 68_Zuul 作为网关的架构原理介绍

设计模式，其实在各种开源项目里，到处都是设计模式 

pre过滤器 

-3：ServletDetectionFilter

-2：Servlet30WrapperFilter

-1：FromBodyWrapperFilter

1：DebugFilter

5：PreDecorationFilter 

routing过滤器 

10：RibbonRoutingFilter

100：SimpleHostRoutingFilter

500：SendForwardFilter 

post过滤器 

1000：SendResponseFilter 

error过滤器 

0：SendErrorFilter 

zuul的源码，其实非常简单的，一点都不复杂，zuul的源码，有可能是spring cloud几个核心组件里面，最简单的，ribbon差不多一个级别的简单

### 69_基于网关实现服务动态路由和灰度发布