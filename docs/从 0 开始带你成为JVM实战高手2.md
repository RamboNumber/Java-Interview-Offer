### 050、动手实验：使用 jstat 摸清线上系统的JVM运行状况

**1、前文回顾**

上周我们已经通过带着大家分析GC日志的方式，给大家重新回顾了一遍JVM的整体运行原理，包括对象优先在Eden区分配，Young GC的触发时机和执行过程，对象进入老年代的时机，Full GC的触发时机和执行过程，相信大家通过GC日志可以把JVM整体运行原理理解的更加的深入而且透彻。

本周我们就要带着大家开始用一个工具来分析运行中的系统，他的对象增长的速率，Young GC的触发频率，Young GC的耗时，每次Young GC后有多少对象是存活下来的，每次Young GC过后有多少对象进入了老年代，老年代对象增长的速率，Full GC的触发频率，Full GC的耗时。

**2、功能强大的jstat**

平时我们对运行中的系统，如果要检查他的JVM的整体运行情况，比较实用的工具之一，就是jstat

他可以轻易的让你看到当前运行中的系统，他的JVM内的Eden、Survivor、老年代的内存使用情况，还有Young GC和Full gC的执行次数以及耗时。

通过这些指标，我们可以轻松的分析出当前系统的运行情况，判断当前系统的内存使用压力以及GC压力，还有就是内存分配是否合理。下面我们就一点点来看看这个jstat工具的使用。

**3、jstat -gc PID**

首先第一个命令，就是在你们的生产机器linux上，找出你们的Java进程的PID，这个大家自行百度一下即可，用jps命令就可以看到。

接着就针对我们的Java进程执行：jstat -gc PID。这就可以看到这个Java进程（其实本质就是一个JVM）的内存和GC情况了。

运行这个命令之后会看到如下列，给大家解释一下：

1. S0C：这是From Survivor区的大小
2. S1C：这是To Survivor区的大小
3. S0U：这是From Survivor区当前使用的内存大小
4. S1U：这是To Survivor区当前使用的内存大小
5. EC：这是Eden区的大小
6. EU：这是Eden区当前使用的内存大小
7. OC：这是老年代的大小
8. OU：这是老年代当前使用的内存大小
9. MC：这是方法区（永久代、元数据区）的大小
10. MU：这是方法区（永久代、元数据区）的当前使用的内存大小
11. YGC：这是系统运行迄今为止的Young GC次数
12. YGCT：这是Young GC的耗时
13. FGC：这是系统运行迄今为止的Full GC次数
14. FGCT：这是Full GC的耗时
15. GCT：这是所有GC的总耗时

不知道大家发现什么没有，其实这些指标都是非常实用的jvm gc分析指标，接下来我们一步一步告诉大家该怎么使用这个工具。

另外给大家说句题外话，接下来我们有两篇文章会给大家一段模拟出生产案例的程序，然后在windows本地电脑上运行，然后我们会带着大家用jstat工具去分析他的jvm运行情况。

但是jstat工具本身如果要在windows上运行需要使用专门针对windows的版本，所以后面的文章我们会教会大家怎么在windows上使用jstat工具的。

**4、其他的jstat命令**

除了上面的jstat -gc命令是最常用的以外，他还有一些命令可以看到更多详细的信息，如下所示：

1. jstat -gccapacity PID：堆内存分析
2. jstat -gcnew PID：年轻代GC分析，这里的TT和MTT可以看到对象在年轻代存活的年龄和存活的最大年龄
3. jstat -gcnewcapacity PID：年轻代内存分析
4. jstat -gcold PID：老年代GC分析
5. jstat -gcoldcapacity PID：老年代内存分析
6. jstat -gcmetacapacity PID：元数据区内存分析

大家可以后面自己有机会尝试尝试这些命令，多看看，还是挺好玩儿的，但是其实最完整、最常用、最实用的还是jstat -gc命令，基本足够我们日常分析jvm的运行情况了。

**5、到底该如何使用jstat工具？**

接着教教大家一些jstat工具使用的小技巧，先明确一下，我们分析线上的JVM进程，最想要知道的信息有哪些？

包括如下：新生代对象增长的速率，Young GC的触发频率，Young GC的耗时，每次Young GC后有多少对象是存活下来的，每次Young GC过后有多少对象进入了老年代，老年代对象增长的速率，Full GC的触发频率，Full GC的耗时。

只要知道了这些信息，其实我们就可以结合之前几周的文章分析过的JVMGC优化的方法，合理分配内存空间，尽可能让对象留在年轻代不进入老年代，避免发生频繁的Full GC。这就是对JVM最好的性能优化了！

因此我们一点点分析，通过jstat工具如何得到上述信息。

**6、新生代对象增长的速率**

如果认真看过之前几周文章的同学，应该都知道，我们平时对jvm第一个要了解的事儿，就是随着系统运行，每秒钟会在年轻代的Eden区分配多少对象。

要分析这东西，你只要在线上linux机器上运行如下命令：jstat -gc PID 1000 10

这行命令，他的意思就是每隔1秒钟更新出来最新的一行jstat统计信息，一共执行10次jstat统计

通过这个命令，你可以非常灵活的对线上机器通过固定频率输出统计信息，观察每隔一段时间的jvm中的Eden区对象占用变化。

比如给大家举个例子，执行这个命令之后，第一秒先显示出来Eden区使用了200MB内存，第二秒显示出来的那行统计信息里，发信Eden区使用了205MB内存，第三秒显示出来的那行统计信息里，发现Eden区使用了209MB内存，以此类推。

此时你可以轻易的推断出来，这个系统大概每秒钟会新增5MB左右的对象。

而且这里大家可以根据自己系统的情况灵活多变的使用，比如你们系统负载很低，不一定每秒都有请求，那么可以把上面的1秒钟调整为1分钟，甚至10分钟，去看你们系统每隔1分钟或者10分钟大概增长多少对象。

还有就是一般系统都有高峰和日常两种状态，比如系统高峰期用的人很多，此时你就应该在系统高峰期去用上述命令看看高峰期的对象增长速率。然后你再得在非高峰的日常时间段内看看对象的增长速率。

按照上述思路，基本上你可以对线上系统的高峰和日常两个时间段内的对象增长速率有很清晰的了解。

**7、Young GC的触发频率和每次耗时**

接着下一步我们就想知道大概多久会触发一次Young GC，以及每次Young GC的耗时了。

其实多久触发一次Young GC就很容易推测出来了，因为系统高峰和日常时候的对象增长速率你都知道了，那么非常简单就可以推测出来高峰期多久发生一次Young GC，日常期多久发生一次Young GC。

比如你Eden区有800MB内存，那么发现高峰期每秒新增5MB对象，大概高峰期就是3分钟会触发一次Young GC。日常期每秒新增0.5MB对象，那么日常期大概需要半个小时才会触发一次Young GC。

那么每次Young GC的平均耗时呢？

简单，之前给大家说过，jstat会告诉你迄今为止系统已经发生了多少次Young GC以及这些Young GC的总耗时。

比如系统运行24小时后共发生了260次Young GC，总耗时为20s。那么平均下来每次Young GC大概就耗时几十毫秒的时间。

你大概就知道每次Young GC的时候会导致系统停顿几十毫秒。

**8、每次Young GC后有多少对象是存活和进入老年代**

接着我们想要知道，每次Young GC后有多少对象会存活下来，以及有多少对象会进入老年代。

其实每次Young GC过后有多少对象会存活下来，这个没法直接看出来，但是有办法可以大致推测出来。

之前我们已经推算出来高峰期的时候多久发生一次Young GC，比如3分钟会有一次Young GC

那么此时我们可以执行下述jstat命令：jstat -gc PID 180000 10。这就相当于是让他每隔三分钟执行一次统计，连续执行10次。

此时大家可以观察一下，每隔三分钟之后发生了一次Young GC，此时Eden、Survivor、老年代的对象变化。

正常来说，Eden区肯定会在几乎放满之后重新变得里面对象很少，比如800MB的空间就使用了几十MB。Survivor区肯定会放入一些存活对象，老年代可能会增长一些对象占用。所以这里的关键，就是观察老年代的对象增长速率。

从一个正常的角度来看，老年代的对象是不太可能不停的快速增长的，因为普通的系统其实没那么多长期存活的对象。如果你发现比如每次Young GC过后，老年代对象都要增长几十MB，那很有可能就是你一次Young GC过后存活对象太多了。

存活对象太多，可能导致放入Survivor区域之后触发了动态年龄判定规则进入老年代，也可能是Survivor区域放不下了，所以大部分存活对象进入老年代。

最常见的就是这种情况。如果你的老年代每次在Young GC过后就新增几百KB，或者几MB的对象，这个还算情有可缘，但是如果老年代对象快速增长，那一定是不正常的。

所以通过上述观察策略，你就可以知道每次Young GC过后多少对象是存活的，实际上Survivor区域里的和进入老年代的对象，都是存活的。

你也可以知道老年代对象的增长速率，比如每隔3分钟一次Young GC，每次会有50MB对象进入老年代，这就是年代对象的增长速率，每隔3分钟增长50MB。

**9、Full GC的触发时机和耗时**

只要知道了老年代对象的增长速率，那么Full GC的触发时机就很清晰了，比如老年代总共有800MB的内存，每隔3分钟新增50MB对象，那么大概每小时就会触发一次Full GC。

然后可以看到jstat打印出来的系统运行起劲为止的Full GC次数以及总耗时，比如一共执行了10次Full GC，共耗时30s，每次Full GC大概就是需要耗费3s左右。

**10、本文总结**

通过本文对jstat命令的介绍，以及结合我们之前学习过的jvm运行原理，我们教给了大家这套分析线上系统jvm运行情况的技巧

大家完全可以灵活运行jstat这个实用的工具，轻而易举的掌控到线上jvm运行的详细情况，然后针对jvm的具体运行情况去进行有针对性的优化。

另外，很多同学会问了：老师，其实有很多其他的工具也特别好用啊，比如JConsole、VisualVM等可视化的监控工具，还有其他一些开源的监控系统，都是可视化的。

针对这个问题，其实我也没说不可以用那些可视化工具，下篇文章我们就要给大家介绍更多的可视化监控JVM的工具。

但是有一点要告诉大家，一个优秀、合格的工程师，他一定是可以非常灵活的运用各种命令行工具，在命令行就搞定一切的。

所以jstat作为一个最简单易用、高效实用的命令行jvm监控工具，其实绝对是值得大家首先掌握他的。因为每个人的公司情况不一样，万一你公司不支持你用各种可视化工具呢？那你就必须从最“low”最原始的命令行工具开始，快速上手实用，定位问题。

而且其实你理解了本文的思想之后，你用其他任何工具，都能轻松的把线上jvm的运行情况通过工具提供的数据分析清楚。

**11、今日思考题**

今天交给大家一个练习题，就是在自己线上负责的系统使用jstat命令，按照上述我们介绍的思路，把以下jvm运行情况全部摸出来：

- 新生代对象增长的速率
- Young GC的触发频率
- Young GC的耗时
- 每次Young GC后有多少对象是存活下来的
- 每次Young GC过后有多少对象进入了老年代
- 老年代对象增长的速率
- Full GC的触发频率
- Full GC的耗时

如果有分析心得的，可以发评论区里，跟其他同学一起分享，如果在落地过程中遇到任何问题，也欢迎大家在评论区踊跃发言提问。

**End**

### 051、动手实验：使用jmap和jhat摸清线上系统的对象分布

**1、前文总结**

上一篇文章我们给大家介绍了一个平时工作中非常实用的工具，jstat。

用jstat就可以非常轻松便捷的了解到线上系统的运行状况，从新对象增速、Young GC触发频率以及耗时，再到对象进入老年代的增速以及Full GC触发频率以及耗时，可以完全摸清楚线上系统的JVM运行情况，为可能要做的优化做准备。

本文我们继续给大家介绍两个平时工作里非常实用的工具，jmap和jhat。

这两个工具可以帮助我们观察线上JVM中的对象分布，了解到你的系统平时运行过程中，到底哪些对象占据了主角位置，他们占据了多少内存空间，让你对你的系统运行有更加细致的了解。

**2、使用jmap了解系统运行时的内存区域**

其实如果单单只是要了解JVM的运行状况，然后去进行JVM GC优化，通常来说jstat就完全够用了

但是有的时候可能我们会发现JVM新增对象的速度很快，然后就想要去看看，**到底什么对象占据了那么多的内存。**

如果发现有的对象在代码中可以优化一下创建的时机，避免那种对象对内存占用过大，那么也许甚至可以去反过来优化一下代码。

当然，其实如果不是出现OOM那种极端情况，也并没有那么大的必要去着急优化代码。

但是这篇文章我们来学习一下如何了解线上系统jvm中的对象分布，也是有好处的，比如之前我们在上周的案例中就发现年轻代里总是有500kb左右的未知对象，大家是不是会很好奇？如果可以看到jvm中这500kb的对象到底是什么就好了，所以学习一下这个技巧是有用的。

先看一个命令：**jmap -heap PID**

这个命令可以打印出来一系列的信息，我们就不长篇大论的粘贴出来具体的信息了，因为内容篇幅太大了，其实也没太大意义，因为里面的东西大家自己看字面意思都能看懂的。我们就简单给大家说一下这里会打印出来什么东西。

大致来说，这个信息会打印出来堆内存相关的一些参数设置，然后就是当前堆内存里的一些基本各个区域的情况

比如Eden区总容量、已经使用的容量、剩余的空间容量，两个Survivor区的总容量、已经使用的容量和剩余的空间容量，老年代的总容量、已经使用的容量和剩余的容量。

但是这些信息大家会想了，其实jstat已经有了啊！对的，所以一般不会用jmap去看这些信息，毕竟他信息还没jstat全呢，因为没有gc相关的统计。

**3、使用jmap了解系统运行时的对象分布**

其实jmap命令比较有用的一个使用方式，是如下的：

**jmap -histo PID**

这个命令会打印出来类似下面的信息：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78916800_1578303728.png)

这个就很有意思了，各位同学看上述打印出来的东西，他会按照各种对象占用内存空间的大小降序排列，把占用内存最多的对象放在最上面。

所以如果你只是想要简单的了解一下当前jvm中的对象对内存占用的情况，只要直接用jmap -histo命令即可，非常好用

你可以快速了解到当前内存里到底是哪个对象占用了大量的内存空间。

**4、使用jmap生成堆内存转储快照**

但是如果你仅仅只是看一个大概，感觉就只是看看上述那些对象占用内存的情况，感觉还不够，想要来点深入而且仔细点的

那就可以用jmap命令生成一个堆内存快照放到一个文件里去，用如下的命令即可：

**jmap -dump:live,format=b,file=dump.hprof PID**

这个命令会在当前目录下生成一个dump.hrpof文件，这里是二进制的格式，你不能直接打开看的，他把这一时刻JVM堆内存里所有对象的快照放到文件里去了，供你后续去分析。

**5、使用jhat在浏览器中分析堆转出快照**

接着就可以使用jhat去分析堆快照了，jhat内置了web服务器，他会支持你通过浏览器来以图形化的方式分析堆转储快照

使用如下命令即可启动jhat服务器，还可以指定自己想要的http端口号，默认是7000端口号：

**jhat dump.hprof -port 7000**

接着你就在浏览器上访问当前这台机器的7000端口号，就可以通过图形化的方式去分析堆内存里的对象分布情况了。

这里我们先简单介绍一下，后面马上有两个案例，jstat、jmap、jhat我们都会带着大家动手来玩一下的。

**6、今日思考题**

今天留给大家的作业，就是自己去线上系统的机器上，用jmap -histo看看对象大致分布情况，然后用jmap生成一个堆转储快照，再用jhat分析一下堆转储快照，看看当前系统运行的时候，各种对象的分布情况。

大家可以到评论区里说说自己看堆转储快照的感受，是不是发现自己越来越了解JVM是如何运行的了呢？

**End**

### 052、从测试到上线：如何分析JVM运行状况及合理优化？

**1、前文回顾**

前面两篇文章，已经给大家介绍了jstat、jmap、jhat等工具，可以非常轻松的分析出系统运行时的JVM状况，包括内存使用压力还有GC压力，包括内存中的对象分布情况。

这篇文章，我们结合之前介绍过的两个工具，给大家做一个实际开发、测试到上线的一个整体JVM优化的梳理。

**2、开发好系统之后的预估性优化**

大家平时如果在开发一个新系统的时候，完成开发之后，是不是就要经历测试以及上线的过程？

此时在系统开发完毕之后，实际上各位同学就应该参照之前我们多个案例中介绍的思路，对系统进行预估性的优化。

那什么叫做预估性的优化呢？

就是跟之前案例中说的一样，自行估算系统每秒大概多少请求，每个请求会创建多少对象，占用多少内存，机器应该选用什么样的配置，年轻代应该给多少内存，Young GC触发的频率，对象进入老年代的速率，老年代应该给多少内存，Full GC触发的频率。

这些东西其实是可以根据你自己写的代码，大致合理的预估一下的。

在预估完成之后，就可以采用之前多个案例介绍的优化思路，先给自己的系统设置一些初始性的JVM参数

比如堆内存大小，年轻代大小，Eden和Survivor的比例，老年代的大小，大对象的阈值，大龄对象进入老年代的阈值，等等。

优化思路其实简单来说就一句话：**尽量让每次Young GC后的存活对象小于Survivor区域的50%，都留存在年轻代里。尽量别让对象进入老年代。尽量减少Full GC的频率，避免频繁Full GC对JVM性能的影响。**

这个之前几周内容都在围绕这个核心在讲述，相信大家现在都理解的很清楚了。实际上这个过程应该是在新系统开发完毕之后必须有的一个环节。

**3、系统压测时的JVM优化**

通常一个新系统开发完毕之后，就会经过一连串的测试

从本地的单元测试，到系统集成测试，再到测试环境的功能测试，预发布环境的压力测试，要保证系统的功能全部正常

而且在一定压力下性能、稳定性和并发能力都正常，最后才会部署到生产环境运行。

这里非常关键的一个环节就是预发布环境的压力测试，通常在这个环节，会使用一些压力测试工具模拟比如1000个用户同时访问系统，造成每秒500个请求的压力，然后看系统能否支撑住每秒500请求的压力。同时看系统各个接口的响应延时是否在比如200ms之内，也就是接口性能不能太慢，或者是在数据库中模拟出来百万级单表数据，然后看系统是否还能稳定运行。

具体如何进行系统压测，不是我们这里要讲述的内容，大家自行百度一下“Java压力测试”，就会看到很多开源的工具，可以轻松模拟出N个用户同时访问你系统的场景，还能给你一份压力测试报告，告诉你系统可以支撑每秒多少请求，包括系统接口的响应延时。

在这个环节，通常压测工具会对系统发起持续不断的请求，持续很长时间，比如几个小时，甚至几天时间。

所以此时，大家完全就可以在这个环节，对测试机器运行的系统，采用jstat工具来分析在模拟真实环境的压力下，JVM的整体运行状态。

具体如何使用jstat来进行分析，之前都讲的很详细了，包括如何借助jstat的各种功能分析出来以下JVM的关键运行指标：新生代对象增长的速率，Young GC的触发频率，Young GC的耗时，每次Young GC后有多少对象是存活下来的，每次Young GC过后有多少对象进入了老年代，老年代对象增长的速率，Full GC的触发频率，Full GC的耗时。

然后根据压测环境中的JVM运行状况，如果发现对象过快进入老年代，可能是因为年轻代太小导致频繁Young GC，然后Young GC的时候很多对象还是存活的，结果Survivor也太小，导致很多对象频繁进入老年代。当然也可能是别的什么原因。

此时就需要采用之前介绍的优化思路，合理调整新生代、老年代、Eden、Survivor各个区域的内存大小，保证对象尽量留在年轻代，不要过快进入老年代中。

之前很多人网上会胡乱搜索JMV优化的博客，看到里面人家怎么优化，你就怎么优化

比如很多博客说年轻代和老年代的占比一般是3:8，其实完全是片面的。每个系统都是不一样的，特点不同，复杂度不同。

大家记住一点：真正的优化，必须是你根据自己的系统，实际观察之后，然后合理调整内存分布，根本没什么固定的JVM优化模板。

当你对压测环境下的系统优化好JVM参数之后，观察Young GC和Full GC频率都很低，此时就可以部署系统上线了。

**4、对线上系统进行JVM监控**

当你的系统上线之后，你就需要对线上系统的JVM进行监控，这个监控通常来说有两种办法。

**第一种方法**会“low”一些，其实就是每天在高峰期和低峰期都用jstat、jmap、jhat等工具去看看线上系统的JVM运行是否正常，有没有频繁Full GC的问题。

如果有就优化，没有的话，平时每天都定时去看看，或者每周都去看看即可。

**第二种方法**在中大型公司里会多一些，大家都知道，很多中大型公司都会部署专门的监控系统，比较常见的有Zabbix、OpenFalcon、Ganglia，等等。

然后你部署的系统都可以把JVM统计项发送到这些监控系统里去。

此时你就可以在这些监控系统可视化的界面里，看到你需要的所有指标，包括你的各个内存区域的对象占用变化曲线，直接可以看到Eden区的对象增速，还会告诉你Young GC发生的频率以及耗时，包括老年代的对象增速以及Full GC的频率和耗时。

而且这些工具还允许你设置监控。也就是说，你可以指定一个监控规则，比如线上系统的JVM，如果10分钟之内发生5次以上Full GC，就需要发送报警给你。比如发送到你的邮箱、短信里，这样你就不用自己每天去看着了。

但是这些监控工具的使用不在我们专栏范畴里，因为这些内容并不一定每个公司都一样，也不一定每个公司都有

大家如果有兴趣，完全可以自行百度学习，比如“OpenFalcon监控JVM”，会看到很多资料。

对于我们而言，主要会带大家使用的就是JDK自身提供的命令行工具，包括jstat、jmap和jhat

其实把这些命令行用好了，基本线上系统的JVM监控和优化都能搞定了。而且我本人而言，还是非常推崇工程师平时除了要会用图形化工具，还必须得熟练使用命令行的工具，这才像一个“工程师”应该有的样子。

简单一句话总结：对线上运行的系统，要不然用命令行工具手动监控，发现问题就优化，要不然就是依托公司的监控系统进行自动监控，可视化查看日常系统的运行状态。

**5、今日思考题**

你们公司的系统开发流程里有压测环节吗？

如果有，你能否在自己的工作流程中加入一项，在开发之后先进行预估性JVM优化，然后再在压测环境进行测试性JVM优化。

此外，你们线上生产环境有没有JVM监控方案？

如果没有，建议在工作内容中引入一项，每天日常工作可以在固定时间段去线上机器里用命令行工具观察一下JVM运行状态，作为日常线上系统巡查的一个工作内容。

如果你们公司有类似Zabbix、OpenFalcon之类的监控系统，是否对线上系统都指定了JVM GC监控？如果JVM发生频繁Full GC能否及时通知到你？

专栏学习到这里，大家完全可以对自己负责的生产系统“**下手**”了。引入规范化、流程化的JVM监控和优化的工作内容，保障自己负责的系统的JVM性能是绝对良好的。

**End**

### 053、案例实战：每秒10万并发的BI系统，如何定位和解决频繁Young GC问题？

**1、前文回顾**

不知道大家还记得之前我们给大家分析过的一个案例，就是一个BI系统因为负载很高所以触发了非常频繁的Young GC

**我们把这个案例的原文放在下面，因为隔了一段时间了，有的同学可能有些遗忘，咱们一起再来回顾一遍这个案例**

回顾之后，我们接着用代码模拟出这个案例的场景，然后用jstat来分析一下jvm的运行状态，带大家来实战一下jstat命令的使用。

**2、服务于百万级商家的BI系统是什么？**

先说一下我们线上一个真实的生产系统，是一个服务于百万级商家的BI系统。所谓BI系统，很多开发业务系统的同学可能没接触过，简单介绍一下他的背景。

简单来说，比如一个平台有数十万甚至上百万的商家在你的平台上做生意，会使用你的这个平台系统，此时一定会产生大量的数据

然后基于这些数据我们需要为商家提供一些数据报表，比如：每个商家每天有多少访客？有多少交易？付费转化率是多少？当然实际情况会比这个简单几句话复杂很多，我们这里就简单说个概念而已。

所以此时就需要一套BI系统，所谓BI，英文全称是“Business Intelligence”，也就是“商业智能”，听起来是不是特别的高大上？

其实也别想的太高大上了，说白了，就是把一些商家平时日常经营的数据收集起来进行分析，然后把各种数据报表展示给商家的一套系统。

所谓“商业智能”，指的就是给你看一些数据报表，然后让你平时能够更好的了解自己的经营状况，然后让老板“智能”的去调整经营策略，提升业绩。

所以类似这样的一个BI系统，大致的运行逻辑如下所示，首先从我们提供给商家日常使用的一个平台上会采集出来很多商家日常经营的数据，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94522300_1578303735.cn/txdocpic/0/64a85974a95f162917f56c27734292f9/0)

接着就可以对这些经营数据依托各种大数据计算平台，比如Hadoop、Spark、Flink等技术进行海量数据的计算，计算出来各种各样的数据报表，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6850000_1578303736.cn/txdocpic/0/78d0d3d671ffe7167277b36188df951d/0)

然后我们需要将计算好的各种数据分析报表都放入一些存储中，比如说MySQL、Elastcisearch、HBase都可以存放类似的数据，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17766700_1578303736.cn/txdocpic/0/8df26d89ab5ec10da32a3d79c6134151/0)

最后一步，就是基于MySQL、HBase、Elasticsearch中存储的数据报表，基于Java开发出来一个BI系统，通过这个系统把各种存储好的数据暴露给前端，允许前端基于各种条件对存储好的数据进行复杂的筛选和分析，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27759900_1578303736.cn/txdocpic/0/94a2fd799b14896a8f9e908b5e407edb/0)

**3、刚开始上线系统时候的部署架构**

我们在这里**重点作为案例分析**的就是上述场景中的“BI系统，其他环节都跟大数据相关的技术是有关联的，暂时先不用care，未来有机会可以给大家出更多的课程来阐述那些技术。

刚开始的时候这个BI系统使用的商家是不多的，因为大家要知道，即使在一个庞大的互联网大厂里，虽然大厂本身积累了大量商家，但是要针对他们上线一个付费产品，刚开始未必所有人都买账，所以一开始系统上线大概就少数商家在使用，比如就几千个商家。

刚开始系统部署的非常简单，就是用几台机器来部署了上述的BI系统，机器都是普通的4核8G的配置

在这个配置之下，一般来说给堆内存中的新生代分配的内存都在1.5G左右，Eden区大概也就1G左右的空间，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/39659900_1578303736.cn/txdocpic/0/ccb37c6e35533b8dff1c47ce882a0863/0)

**4、技术痛点：实时自动刷新报表 + 大数据量报表**

其实刚开始，在少数商家的量级之下，这个系统是没多大问题的，运行的非常良好

但是问题恰恰就出在突然使用系统的商家数量开始暴涨的时候，突然使用系统的商家开始越来越多，给大家举个例子，当商家的数量级达到几万的时候。

此时要给大家说明一个此类BI系统的特点，就是在BI系统中有一种数据报表，他是支持前端页面有一个JS脚本，自动每隔几秒钟就发送请求到后台刷新一下数据的，**这种报表称之为“实时数据报表”**，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/48693600_1578303736.cn/txdocpic/0/60af57a050f134d2e1b508c55571a182/0)

那么大家可以设想一下，假设仅仅就几万商家作为你的系统用户，很可能同一时间打开那个实时报表的商家就有几千个

然后每个商家打开实时报表之后，前端页面都会每隔几秒钟发送请求到后台来加载最新数据

基本上会出现你BI系统部署的每台机器每秒的请求会达到几百个，这里我们假设就是每秒500个请求吧。

然后每个请求会加载出来一张报表需要的大量数据，因为BI系统可能还需要针对那些数据进行内存中的现场计算加工一下，才能返回给前端页面展示。

根据我们之前的测算，每个请求大概需要加载出来100kb的数据进行计算，因此每秒500个请求，就需要加载出来50MB的数据到内存中进行计算，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55843700_1578303736.cn/txdocpic/0/fa9d08c6b4b3a1cf29c17b390511869a/0)

**5、没什么大影响的频繁Young GC**

其实大家都已经发现上述系统的问题了，在上述系统运行模型下，基本上每秒会加载50MB的数据到Eden区中

只要区区20s，就会迅速填满Eden区，然后触发一次Young GC对新生代进行垃圾回收。

当然1G左右的Eden进行Young GC其实速度相对是比较快的，可能也就几十ms的时间就可以搞定了

所以之前也分析过，其实对系统性能影响并不大，而且上述BI系统场景下，基本上每次Young GC后存活对象可能就几十MB，甚至是几MB。

所以如果仅仅只是这样的话，那么大家可能会看到如下场景，BI系统运行20s过后，就会突然卡顿个10ms，但是对终端用户和系统性能几乎是没有影响的，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/73054800_1578303736.cn/txdocpic/0/5b40474a48e743fbc7a8bd7781ea4149/0)

**6、模拟代码的JVM参数设置**

接着我们会用一段程序来模拟出上述BI系统那种频繁Young GC的一个场景，此时JVM参数如下所示：

-XX:NewSize=104857600 -XX:MaxNewSize=104857600 -XX:InitialHeapSize=209715200 -XX:MaxHeapSize=209715200 -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=3145728 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log

大家只要注意一下上述我们把堆内存设置为了200MB，把年轻代设置为了100MB，然后Eden区是80MB，每块Survivor区是10MB，老年代也是100MB。

我们把案例中的内存大小适当缩小了一些，这样方便大家在本地windows电脑来运行试验。

**7、示例程序**

下面是我们的示例程序：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/82756600_1578303736.png)

针对这段示例程序给大家做一点说明。

首先看第一行代码：Thread.sleep(30000); 

为什么刚开始先休眠30s？

因为一会儿会告诉大家，程序刚启动，必须得先让我们找到这个程序的PID，也就是进程ID，然后再执行jstat命令来观察程序运行时JVM的状态。

接着看loadData()方法内的代码，其实非常简单，他会循环50次，模拟每秒50个请求

然后每次请求会分配一个100KB的数组，模拟每次请求会从数据存储中加载出来100KB的数据。接着会休眠1秒钟，模拟这一切都是发生在1秒内的。

其实这些对象都是短生存周期的对象，所以方法运行结束直接对象都是垃圾，随时可以回收的。

然后在main()方法里有一个while(true)循环，模拟系统按照每秒钟50个请求，每个请求加载100KB数据的方式不停的运行，除非我们手动终止程序，否则永不停歇。

**8、如何在windows上执行命令？**

这里交给大家一个windows上做实验特别好用的工具，就是Git for Windows

大家可能疑惑，这跟Git有什么关系？他不是一个版本管理工具吗？

没错，但是这个Git for Windows，你主要安装之后，就可以在windows上启动一个Git Bash的窗口，然后你可以随意执行各种命令，非常的好用。

所以推荐给大家这个工具的官网：https://gitforwindows.org/

大家自己到官网里下载最新版本即可，安装和使用非常的简单。

在你安装完毕之后，在windows桌面上右击的时候，会看到一个“Git Bash Here”的选项，此时选择他，就可以直接打开一个命令行窗口，里面可以随意执行命令。

当然，其实如果你不想那么麻烦，也可以直接打开windows自己的命令行窗口，在里面也可以执行jps、jstat等命令，这也是没问题的。只不过我习惯于通过Git for Windows来执行一些命令。

**9、通过jstat观察程序的运行状态**

接着我们使用预订的JVM参数启动程序，此时程序会先进入一个30秒的休眠状态，此时尽快执行jps命令，查看一下我们启动程序的进程ID，如下图所示：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94600100_1578303736.cn/txdocpic/0/9d634f210850bb925bbcfcfc8d99406d/0)

此时会发现我们运行的Demo1这个程序的JVM进程ID是51464。

然后尽快执行下述jstat命令：**jstat -gc 51464 1000 1000**

他的意思就是针对51464这个进程统计JVM运行状态，同时每隔1秒钟打印一次统计信息，连续打印1000次。

然后我们就让jstat开始统计运行，每隔一秒他都会打印一行新的统计信息，过了几十秒后可以看到如下图所示的统计信息：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/9918100_1578303737.cn/txdocpic/0/7b3993b060b1a6ad3c8d7fc85d21881b/0)

接着我们一点点来分析这个图。首先我们先看如下图所示的一段信息：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17922100_1578303737.cn/txdocpic/0/352b172f3e2708f8d11f211ea15f1ed8/0)

这个EU大家应该还记得，就是之前我们所说的Eden区被使用的容量，可以发现他刚开始是3MB左右的内存使用量

接着从我们程序开始运行，会发现每秒钟都会有对象增长，从3MB左右到7MB左右，接着是12MB，17MB，22MB，每秒都会新增5MB左右的对象。

这个跟我们写的代码是完全吻合的，我们就是每秒钟会增加5mB左右的对象。

然后当Eden区使用量达到70多MB的时候，再要分配5MB的对象就失败了，此时就会触发一次Young GC，然后大家继续看下面的图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29709700_1578303737.cn/txdocpic/0/19ed1ca1630e1a5de6a00a56473ca830/0)

**注意看上面红圈里的内容**，大家会发现，Eden区的使用量从70多MB降低为了1MB多，这就是因为一次Young GC直接回收掉了大部分对象。

所以我们现在就知道了，针对这个代码示例，可以清晰的从jstat中看出来，对象增速大致为每秒5MB左右，大致在十几秒左右会触发一次Young GC

这个就是Young GC的触发频率，以及每次Young GC的耗时，大家看下图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38520600_1578303737.cn/txdocpic/0/faaced3f84e7df0bc252a85264323545/0)

上图清晰告诉你了，一次Young GC回收70多MB对象，大概就1毫秒，所以大家想想，Young GC其实是很快的，即使回收800MB的对象，也就10毫秒那样。

所以你想如果是线上系统，他Eden区800MB的话，每秒新增对象50MB，十多秒一次Young GC，也就10毫秒左右，系统卡顿10毫秒，几乎没什么大影响的。

所以我们继续推论，在这个示例中，80MB的Eden区，每秒新增对象5MB，大概十多秒触发一次Young GC，每次Young GC耗时在1毫秒左右。

那么每次Young GC过后存活的对象呢？

简单看上上图，S1U就是Survivor中被使用的内存，之前一直是0，在一次Young GC过后变成了675KB，所以一次Young GC后也就存活675KB的对象而已，轻松放入10MB的Survivor中。

而且大家注意上上图中的OU，那是老年代被使用的内存量，在Young GC前后都是0

这说明这个系统运行良好，Young GC都不会导致对象进入老年代，这就几乎不需要什么优化了。因为几乎可以默认老年代对象增速为0，Full  GC发生频率趋向于0，对系统无影响。

所以大家回顾一下，通过一个示例程序的运行，是不是可以通过jstat分析出来以下信息：

- 新生代对象增长的速率
- Young GC的触发频率
- Young GC的耗时
- 每次Young GC后有多少对象是存活下来的
- 每次Young GC过后有多少对象进入了老年代
- 老年代对象增长的速率
- Full GC的触发频率
- Full GC的耗时

**10、今日思考题**

大家可以让这个程序多运行一段时间，反复观察其每次Young GC之后的Eden、Survivor、Old区的内存变化

然后多思考思考，对一个运行中的系统，到底要观察些什么东西？

如果在这个过程中有什么疑问或者心得体会，大家一定记得在评论区踊跃留言提问交流！

**End**

### 054、案例实战：每日百亿数据量的实时分析引擎，如何定位和解决频繁Full GC问题？

**1、前文回顾**

大家应该还记得之前我们有一篇文章，分析了一个实时计算系统因为负载过高导致了非常频繁的Full GC

这里先在下面贴出来之前的内容，大家先看一下回顾回顾，接着我们会给出示例代码，运行起来之后通过jstat来观察其运行中的问题。

然后我还会优化一下JVM参数配置，再次运行系统，通过jstat来观察JVM优化以后的效果。

**2、一个日处理上亿数据的计算系统**

先给大家说一下这个系统的案例背景，当时我们团队里自己研发的一个数据计算系统，日处理数据量在上亿的规模。

为了方便大家集中注意力理解这个系统的生产环境的JVM相关的东西，所以对系统本身就简化说明了。

简单来说，这个系统就是会不停的从MySQL数据库以及其他数据源里提取大量的数据加载到自己的JVM内存里来进行计算处理，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/81181300_1578303738.cn/txdocpic/0/a2a44346a04ac9e835c45bc763f5fbe2/0)

这个数据计算系统会不停的通过SQL语句和其他方式从各种数据存储中提取数据到内存中来进行计算，大致当时的生产负载是每分钟大概需要执行500次数据提取和计算的任务。

但这是一套分布式运行的系统，所以生产环境部署了多台机器，每台机器大概每分钟负责执行100次数据提取和计算的任务。

每次会提取大概1万条左右的数据到内存里来计算，平均每次计算大概需要耗费10秒左右的时间

然后每台机器是4核8G的配置，JVM内存给了4G，其中新生代和老年代分别是1.5G的内存空间，大家看下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91008200_1578303738.cn/txdocpic/0/0f1c80d13cc35c5a59e006c125ab6471/0)

**3、这个系统到底多快会塞满新生代？**

现在明确了一些核心数据，接着我们来看看这个系统到底多快会塞满新生代的内存空间？

既然这个系统每台机器上部署的实例，每分钟会执行100次数据计算任务，每次是1万条数据需要计算10秒的时间，那么我们来看看每次1万条数据大概会占用多大的内存空间？

这里每条数据都是比较大的，大概每条数据包含了平均20个字段，可以认为平均每条数据在1KB左右的大小。

那么每次计算任务的1万条数据就对应了10MB的大小。所以大家此时可以思考一下，如果新生代是按照8:1:1的比例来分配Eden和两块Survivor的区域，那么大体上来说，Eden区就是1.2GB，每块Survivor区域在100MB左右，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/274900_1578303739.cn/txdocpic/0/27013616e76c608d90f90c4aa8839ad0/0)

基本上按照这个内存大小而言，大家会发现，每次执行一个计算任务，就会在Eden区里分配10MB左右的对象

一分钟大概对应100次计算任务，基本上一分钟过后，Eden区里就全是对象，基本就全满了。

所以说，回答这个小节的问题，新生代里的Eden区，基本上1分钟左右就迅速填满了。

**4、触发Minor GC的时候会有多少对象进入老年代？**

此时假设新生代的Eden区在1分钟过后都塞满对象了，然后在接着继续执行计算任务的时候，势必会导致需要进行Minor GC回收一部分的垃圾对象。

那么上篇文章给大家讲过这里在执行Minor GC之前会先进行的检查。

首先第一步，先看看老年代的可用内存空间是否大于新生代全部对象？

看下图，此时老年代是空的，大概有1.5G的可用内存空间，新生代的Eden区大概算他有1.2G的对象好了。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10763000_1578303739.cn/txdocpic/0/0c9b3fef4480068f6596d7e497b69f43/0)

此时会发现老年代的可用内存空间有1.5GB，新生代的对象总共有1.2GB，即使一次Minor GC过后，全部对象都存活，老年代也能放的下的，那么此时就会直接执行Minor GC了。

那么此时Eden区里有多少对象还是存活的，无法被垃圾回收呢？

大家可以考虑一下之前说的那个点，每个计算任务1万条数据需要计算10秒钟，所以假设此时80个计算任务都执行结束了，但是还有20个计算任务共计200MB的数据，还在计算中，那么此时就是200MB的对象是存活的，不能被垃圾回收掉，然后有1GB的对象是可以垃圾回收的，大家看下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/20231500_1578303739.cn/txdocpic/0/2c9dd6de204132c1782abb9befc0108f/0)

此时一次Minor GC就会回收掉1GB的对象，然后200MB的对象能放入Survivor区吗？

**不能**！

因为任何一块Survivor区实际上就100MB的空间，此时就会通过空间担保机制，让这200MB对象直接进入老年代去，占用里面200MB内存空间，然后Eden区就清空了，大家看下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/31476900_1578303739.cn/txdocpic/0/595cc6ef34ae543070f7c2a199041ada/0)

**5、系统运行多久，老年代大概就会填满？**

那么大家想一下，这个系统大概运行多久，老年代会填满呢？

按照上述计算，每分钟都是一个轮回，大概算下来是每分钟都会把新生代的Eden区填满，然后触发一次Minor GC，然后大概都会有200MB左右的数据进入老年代。

那么大家可以想一下，假设现在2分钟运行过去了，此时老年代已经有400MB内存被占用了，只有1.1GB的内存可用，此时如果第3分钟运行完毕，又要进行Minor GC会做什么检查呢？

如下图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40028300_1578303739.cn/txdocpic/0/5b5b675f627b6e4079b5a25c51fd7489/0)

此时会先检查老年代可用空间是否大于新生代全部对象，此时老年代可用空间1.1GB，新生代对象有1.2GB，那么此时假设一次Minor GC过后新生代对象全部存活，老年代是放不下的，那么此时就得看看一个参数是否打开了 。

如果“-XX:-HandlePromotionFailure”参数被打开了，当然一般都会打开，此时会进入第二步检查，就是看看老年代可用空间是否大于历次Minor GC过后进入老年代的对象的平均大小。

我们已经计算过了，大概每分钟会执行一次Minor GC，每次大概200MB对象会进入老年代。

那么此时发现老年代的1.1GB空间，是大于每次Minor GC后平均200MB对象进入老年代的大小的，所以基本可以推测，本次Minor GC后大概率还是有200MB对象进入老年代，1.1G可用空间是足够的。

所以此时就会放心执行一次Minor GC，然后又是200MB对象进入老年代。

转折点大概在运行了7分钟过后，7次Minor GC执行过后，大概1.4G对象进入老年代，老年代剩余空间就不到100MB了，几乎快满了，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52790100_1578303739.cn/txdocpic/0/4858e3f283323859740672320cd34233/0)

**6、这个系统运行多久，老年代会触发1次Full GC？**

大概在第8分钟运行结束的时候，新生代又满了，执行Minor GC之前进行检查，此时发现老年代只有100MB内存空间了，比之前每次Minor GC后进入老年代的200MB对象要小，此时就会直接触发一次Full GC。

Full GC会把老年代的垃圾对象都给回收了，假设此时老年代被占据的1.4G空间里，全部都是可以回收的对象，那么此时一次性就会把这些对象都给回收了，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/59742100_1578303739.cn/txdocpic/0/dfa1aa67d11adc2344b6eaf6b8f0afe5/0)

然后接着就会执行Minor GC，此时Eden区情况，200MB对象再次进入老年代，之前的Full GC就是为这些新生代本次Minor GC要进入老年代的对象准备的，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/71298900_1578303739.cn/txdocpic/0/ff8e4614f20b8854780172685c53ea1c/0)

按照这个运行模型，基本上平均就是七八分钟一次Full GC，这个频率就相当高了。因为每次Full GC速度都是很慢的，性能很差

**7、该案例应该如何进行JVM优化？**

相信通过这个案例，大家结合图一路看下来，对新生代和老年代如何配合使用，然后什么情况下触发Minor GC和Full GC，什么情况下会导致频繁的Minor GC和Full GC，大家都有了更加深层次和透彻的理解了。

对这个系统，其实要优化也是很简单的，因为这个系统是数据计算系统，每次Minor GC的时候，必然会有一批数据没计算完毕，但是按照现有的内存模型，最大的问题，其实就是 每次Survivor区域放不下存活对象。

所以当时我们就是对生产系统进行了调整，增加了新生代的内存比例，3GB左右的堆内存，其中2GB分配给新生代，1GB留给老年代

这样Survivor区大概就是200MB，每次刚好能放得下Minor GC过后存活的对象了，如下图所示。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/80775200_1578303739.cn/txdocpic/0/c6dd4fa67166c92a852d81497602b4bd/0)

只要每次Minor GC过后200MB存活对象可以放Survivor区域，那么等下一次Minor GC的时候，这个Survivor区的对象对应的计算任务早就结束了，都是可以回收的了，此时比如Eden区里1.6GB空间被占满了，然后Survivor1区里有200MB上一轮 Minor GC后存活的对象，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/89751400_1578303739.cn/txdocpic/0/1529a0eb09d15c59499ee08fdaf539a7/0)

然后此时执行Minor GC，就会把Eden区里1.4GB对象回收掉，Survivor1区里的200MB对象也会回收掉，然后Eden区里剩余的200MB存活对象会放入Survivor2区里，如下图。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98094400_1578303739.cn/txdocpic/0/6ea6749024c342adef6918b21d3751f6/0)

以此类推，基本上就很少对象会进入老年代中，老年代里的对象也不会太多的。

通过这个分析和优化，定时我们成功的把生产系统的老年代Full GC的频率从几分钟一次降低到了几个小时一次，大幅度提升了系统的性能，避免了频繁Full GC对系统运行的影响。

**8、运行程序用的示例JVM参数**

下面的参数唯一修改的就是“**-XX:PretenureSizeThreshold**”，把大对象阈值修改为了20MB，避免我们程序里分配的大对象直接进入老年代。

-XX:NewSize=104857600 -XX:MaxNewSize=104857600 -XX:InitialHeapSize=209715200 -XX:MaxHeapSize=209715200 -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=20971520 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log

**9、示例程序**

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15943800_1578303740.png)

简单给大家解释一下上面的程序

大概意思其实就是，每秒钟都会执行一次loadData()方法，他会分配4个10MB的数组，但是都立马成为垃圾，但是会有data1和data2两个10MB的数组是被变量引用必须存活的，此时Eden区已经占用了六七十MB空间了，接着是data3变量依次指向了两个10MB的数组，这是为了在1s内触发Young GC的。

**10、基于jstat分析程序运行的状态**

接着我们基于jstat分析程序运行的状态，启动程序后立马采用jstat监控其运行状态可以看到如下的信息：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29169200_1578303740.cn/txdocpic/0/f74cde8c1c0293bc12e99e53003d2043/0)

接着我们一点点来分析这个jvm的运行状态。首先我们先看如下一行截图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37791000_1578303740.cn/txdocpic/0/c73e70396eea1345f393591f49e23833/0)

在这里最后一行，可以清晰看到，程序运行起来之后，突然在一秒内就发生了一次Young GC，这是为什么呢？

很简单，按照我们上述的代码，他一定会在一秒内触发一次Young GC的。

Young GC过后，我们发现S1U，也就是一个Survivor区中有587KB的存活对象，这应该就是那些未知对象了。

然后我们明显看到在OU中多出来了30MB左右的对象，因此可以确定，在这次Young GC的时候，有30MB的对象存活了，此时因为Survivor区域放不下，所以直接进入老年代了。

我们接着看下面的截图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/49476100_1578303740.cn/txdocpic/0/273760104da7ff6bbdb1de96e7571f23/0)

大家看上图中红圈的部分，很明显每秒会发生一次Young GC，都会导致20MB~30MB左右的对象进入老年代

因为每次Young GC都会存活下来这么多对象，但是Survivor区域是放不下的，所以都会直接进入老年代。

此时看到老年代的对象占用从30KB一路到60MB左右，此时突然在60MB之后下一秒，明显发生了一次Full GC，对老年代进行了垃圾回收，因为此时老年代重新变成30MB了。

为啥会这样？

很简单，老年代总共就100MB左右，已经占用了60MB了，此时如果发生一次Young GC，有30MB存活对象要放入老年代的话，你还要放30MB对象，明显老年代就要不够了，此时必须会进行Full GC，回收掉之前那60MB对象，然后再放入进去新的30MB对象。

所以大家可以看到，按照我们的这段代码，几乎是每秒新增80MB左右，触发每秒1次Young GC，每次Young GC后存活下来20MB~30MB的对象，老年代每秒新增20MB~30MB的对象，触发老年代几乎三秒一次Full GC，是不是跟我们上面的案例中分析的场景很类似？Young GC太频繁，而且每次GC后存活对象太多，频繁进入老年代，频繁触发老年代的GC。

那么Young GC和Full GC的耗时呢？看下图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56950700_1578303740.cn/txdocpic/0/144953ec5db13d3c365a124860d5bd64/0)

大家看上图，有没有发现Young GC特别坑爹，28次Young GC，结果耗费了180毫秒，平均下来一次Young GC要6毫秒左右。但是14次Full GC才耗费34毫秒，平均下来一次Full GC才耗费两三毫秒。这是为什么呢？

很简单，按照上述程序，每次Full GC都是由Young GC触发的，因为Young GC过后存活对象太多要放入老年代，老年代内存不够了触发Full GC，所以必须得等Full GC执行完毕了，Young GC才能把存活对象放入老年代，才算结束。这就导致Young GC也是速度非常慢。

**11、对JVM性能进行优化**

接着我们按照之前学习的思路对JVM进行优化，很简单，他最大的问题就是每次Young GC过后存活对象太多了，导致频繁进入老年代，频繁触发Full GC

我们只需要调大年轻代的内存空间，增加Survivor的内存即可，看如下JVM参数：

-XX:NewSize=209715200 -XX:MaxNewSize=209715200 -XX:InitialHeapSize=314572800 -XX:MaxHeapSize=314572800 -XX:SurvivorRatio=2  -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=20971520 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log

我们把堆大小调大为了300MB，年轻代给了200MB，同时“-XX:SurvivorRatio=2”表明，Eden:Survivor:Survivor的比例为2:1:1，所以Eden区是100MB，每个Survivor区是50MB，老年代也是100MB。

接着我们用这个JVM参数运行程序，用jstat来监控其运行状态如下：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66804900_1578303740.cn/txdocpic/0/5ec8345ef77514a6f87fec74c3a81b69/0)

在上述截图里，大家可以清晰看到，每秒的Young gC过后，都会有20MB左右的存活对象进入Survivor，但是每个Survivor区都是50MB的大小，因此可以轻松容纳，而且一般不会过50%的动态年龄判定的阈值。

我们可以清晰看到每秒触发Yuong GC过后，几乎就没有对象会进入老年代，最终就600KB的对象进入了老年代里，其他就没有对象进入老年代了。

再看下面的截图：

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78901300_1578303740.cn/txdocpic/0/d7495599d2ad7d1f88eba015739abd62/0)

我们可以看到，只有Young GC，没有Full GC，而且11次Young GC才不过9毫秒，平均一次GC1毫秒都不到，没有Full GC干扰之后，Young GC的性能极高。

所以，其实这个案例就优化成功了，同样的程序，仅仅是调整了内存分配比例，立马就大幅度提升了JVM的性能，几乎把Full GC给消灭掉了。

**12、今日思考题**

这周内容学完之后，大家不用多说，直接自己按照文章的思路，写出来模拟代码，尝试用jstat去观察一下运行状态

尤其是今天的文章，需要你去亲自动手写代码，观察优化前频繁full gc的问题，然后优化jvm参数之后，再看看优化后的效果，相信大家基本从实操层面就完全明白jvm的优化方法了。

**End**

### 055、第8周作业

思考题，结合你们系统的业务来分析，遇到频繁GC该怎么处理？

本周的作业非常简单，学完了这周内容，希望大家务必制定好自己公司从开发、测试到上线的全流程的JVM优化规范，每个环节都需要进行JVM优化。

同时对生产环境的系统需要制定好JVM监控方案，无论是手动监控，还是基于监控系统，都需要有。

同时去对你们生产系统按照本周教的方法观察一下运行状态，有问题的话可以尝试优化一下，亲自动手实战一下。

### 056、第8周答疑：本周问题答疑汇总

**问题：**

我觉得有点虚了，我从今儿早上回来就一直试(工作准备加班完成了)，3个实验gc都和老师不一样，现在版本换成和老师的一样了，同样是不一样，老师的gc日志明显是老年代只有2m，我的不知道为什么gc后有5m。百思不得其解，这到底是为什么呢，是否有同学测试出来的结果和老师的一样？

**回答：**

别着急，说实话，细微的差别都是正常的，千万不要在jvm实验中钻牛角尖。因为eclipse、intellij idea、包括笔记本硬件、操作系统，其实都会对实验结果产生影响的。核心是理解我讲解的原理，然后结合自己的实验结果去分析大的原理，而不是扣细微的细节，jvm自身内置会产生一些对象，所以对象占用之类的，都是不一样的

**问题：**

实验了一下，又颠覆了我昨天的认知：触发了YoungGC，判断存活对象是否大于老年代剩余，即晋升失败 -ParNew (promotion failed)。如果晋升失败，这时触发Old GC，且所有的存活对象都会直接晋升到老年代。不管Survivor区放不放的下部分存活对象。而昨天提到的“部分晋升”的前提是晋升过程中不会触发Old GC

**回答：**

非常好，多动手试验

**问题：**

今天的课程有点颠覆我以前的认知了。。。我一直认为是发生OldGC后，一次性怼这个6M多的数据进入老年代的，按照老师你的讲解，它是分批进入老年代的呀

**回答：**

不是的，就是直接放入老年代，我分步骤讲解，是拆解那个过程给你理解的，不要认为是拆分开来走的

**学员思考题回答：**

系统如何尽量减少FULL GC？ 

一、首先说明什么情况下发生 full gc：

**1、Minor GC 前：**

1. 年轻代对象大小>老年代可用内存&&没开通内存分配担保情况 
2. 年轻代对象大小>老年代可用内存&&开通内存分配担保情况下，历次年轻代GC进入老年代大小平均值>老年代可用内存大小 

**2、Minor GC 后**，老年代放不下GC后存活的对象 

二、为了避免full gc：使每次minor gc后，存活的对象尽量能放在s区，不要放到老年代： 

1. 可以调大survivor区的大小。考虑到动态年龄判断，如果系统资源比较足，可以估算每次minor gc后，存活对象的大概大小，将survivor区内存设置为这个内存的一倍 
2. 如果系统运算时间比较长，导致对象的年龄比较大，可以适当调大"-XX:MaxTenuringThreshold"，使对象年龄大一些再进入老年代，这样也可以减少进入老年代的对象

**问题：**

老师你好，今天实验又发现个新情况：eden区放满触发了young GC，结果晋升的对象太多，触发了CMS，并把老年代放满了。后续又放入的对象把eden区占满了，再放入对象就直接放在了S区，而不是再次触发Young GC。

我花了好多时间去看gc log，然后推测 gc的算法，好费时间啊。老师有没有详细的垃圾收集算法，这样我就不用去推测了。推测了这么多，实际工作中好像又不太会遇到这种情况，挺浪费时间，但不搞清楚又不甘心

**回答：**你一定要明白一点，jvm的算法你永远没法完全搞明白，只要跟着文章理解大致思路和原理就行了。每个版本的算法都有区别，你要对你线上jvm的算法理解细节，只能一点一点的做测试和尝试

**问题：**

？？疑问，当晋升失败 - [ParNew (promotion failed): 15615K->19502K(20480K)，触发了OLd GC - [CMS: 5123K->5120K(6144K) ..]，这时全部的幸存对象进入老年代，S区是空的。

请问老师，此时回收的执行顺序是什么样的？ 本该放到S区的对象，现在是怎么处理的？

**回答：**先回收老年代，然后让young gc的幸存对象放入老年代中

**问题：**

回看这篇帖子，我终于知道我的疑问到底是什么了，也知道怎么提问了。 不想钻牛角尖，但是还是想大概明白这个问题是咋回事，希望能得到老师的指点。 所以我想问三个问题： 

1.在宏观上，其实仍然存在Eden+2S，举一个例子，现在Eden区满了，触发了一次新生代GC，于是把存活对象放到S1所属的Region。第二次，Eden区又满了，于是会触发第二次新生代GC，那么会把Eden+S1的幸存对象放入S2。 

那么第一个问题来：此时会清空S1的所有Region吗？ 

2.第二个问题：在宏观上，Eden区和S区是有比例的，比如默认的8：1：1，有800个属于Eden区的Region，那么理应有100个属于S1的Region和100个属于S2的Region。 

现在有一个新生代大小为2G的堆内存，每个Region大小为2m，Eden区占用了800个Region，两个S分别占用了100个Region。

比如在一次新生代GC过后，存活对象为100m，所以就会占用其中一个S区的50个Region。

那么为了符合Eden：S1：S2=8：1：1的比例，Eden区的Region是否应该变为400个Region，然后800-400=400的另外400个Region就成为了空闲的Region？ 

3.根据以上，是否可以得出一个结论：Survivor区是否随着Eden区的增长而变大？

**回答：**

1、当然会清空S1 

2、是有比例的 

3、对的，S会自然变大

**问题：**

大神，我之前统计系统内存就是直接使用jmeter去压测，把客户要求的tps（加了2倍去测试）。 

1. 1使用 top 监控一段时间进程使用的内存大小 
2. 2监控一段时间的 GC频率 
3. 3统计结果（客户接受这样测试结果） 

请问一下大神们，这样简单暴力的方式有什么坑吗？

**回答：**其实没大坑，基本上可以看出来系统的具体情况

**学员总结：**

打卡，前面已经学习了上线前如何预估系统的压力，现在直接通过工具分析更加准确，感谢老师提供这么好的思路，现在感觉自己越来越接近具备解决生产问题的能力了，有点小激动。

**回答：**后面全部是大量的代码实战案例，结合各种工具分析线上生产问题，然后做出优化的实战，加油

**问题：**

动态年龄判断里，“年龄1+年龄2+年龄n”这句是不是指从最小年龄的对象开始依次向上与大龄对象累加大小？

比如累加到10岁的对象时，对象累加大小总和大于survivor区空间大小的50%了，那么survivor区里大于等于10岁的对象都移到老年代里

**回答：**没错的

**问题：**

在同一台机器上，启动一个java项目，就启动一个JVM进程，同一个项目项目，都运营在同一个JVM进程中，老师是这样吗？

**回答：**对的，就是你理解的这样子

**问题：**

老师，在Tomcat中启动一个war，这是启动了一个JVM进程吗，还是多个？

**回答：**Tomcat自己就是一个JVM进程，我们写的war包不过就是一些类而已，Tomcat加载到自己的JVM进程里去执行类的代码逻辑

**问题：**

看第二遍的想法： 判断是否是垃圾对象都是从GC ROOT出发的，老年代为什么没有像eden区那样一步到位把全部对象标出是否是垃圾对象？ 

是因为年轻代绝大部分的对象通过年像老年代初始标识阶段就能知道是否垃圾对象了，即通过直接引用的判断就能知道是否是垃圾对象，剩下极少部分对象是需要追踪间接引用（耗时少），所以年轻代一步到位去标识。而老年代刚好相反，很多对象是直接引用，也有很多是间接引用（比较耗时），所以分了多个阶段。

请问老师，这样理解对吗？

**回答：**老年代那么做是因为他不能直接stop the world去回收，那样太慢了，他要尽可能把一些环节跟工作线程并行运行，这样可以适当减轻gc对工作线程的影响

**学员思考题总结：**

本地实践下来，除了长期存活的那几百K对象经过15次GC进入老年代，之后每次Young GC存活对象都为0.所以频繁YoungGC，但就是触发不了FullGC。 其实这里感觉应该将年轻代调大，老年代设的小一点。比如年轻代150，老年代50，这样YoungGC频率也就可以相对低一点了，而且依然不会触发OldGC。

**回答：**对的，思路正确

**学员总结：**

非常有意思，实验有小小不同，第一次ygc之后，有8b不知道啥东西晋升老年代了，不过这个无关大雅。

**问题：**java项目一般都是运营在Tomcat中的，设置参数的时候，是不是只配置Tomcat的即可，还是要在程序中单独配置。 还有，这这些参数配置的作用范围是什么，是当前的JVM进程吗？

**回答：**对的，一般就是配置tomcat的jvm参数的，作用范围就是tomcat的jvm进程

**问题：**

老师我这 S0C 、S1C 的值为什么一直变化呢，他的含义是Survivor0的容量，这个容量不应该是一个定值吗

**回答：**正常啊，程序运行，一直触发young gc，每次gc过后存活对象先进入S0，然后下一次gc后存活对象进入S1，不就来回交替了，看来之前的文章内容还是没理解透，可以回过头二刷以前的文章

**问题：**

根据文章开始的例子，from和to区域只有100m，每次young gc后，会有200m的存活对象，优化后把from和to调整到200m，young gc后，存活的对象可以放到fro区域了。

那么问题来了，动态年龄判断啥时候触发啊？200m的from区，200m的存活对象，不会触发动态年龄判断吗？

**回答：**你可以观察文章里的情况，优化过后，一般不会触发动态年龄判定，因为S区没到50%

**问题：**

单单通过改变SurvivorRatio为2也可以了，只不过YGC更频繁而已；老师我这里有个问题，我本机用CMS，老年代达到50%，最多到80%就开始垃圾回收了。

我通过设置CMSInitiatingOccupancyFraction改变占比也还是老样子，这种情况算正常吗？最郁闷的一点是这个垃圾回收触发的占比通过jstat来看，每次都有点不固定，最低50%，最高80%触发。。

**回答：**其实也是正常的，因为看那几个条件，可能你历次升入老年代的平均对象大小会不停改变，那么自然触发老年代GC的时机不一样了

**问题：**

请问一下我的JDK版本是1.8.0_91, 配置-XX:+HandlePromotionFailure 时报错“Unrecognized VM option 'HandlePromotionFailure' Did you mean '(+/-)PromotionFailureALot'?”，请问是哪里配置的不对吗？

**回答：**那个参数在JDK 1.6之后就不需要了，可以忽略了，他默认只需要比较历次young gc后升入老年代的平均对象大小和老年代的可用空间大小就可以了，把那个参数删除就可以了

**问题：**

老师您好，问下G1回收器什么情况下会自动降级成parnew+cms，反过来升级的情况也可能发生嘛？

**回答：**G1一般不会自动降级，这个用什么垃圾回收器，都是你自己决定的

**问题：**

老师有个问题，我看我们生产服务器配的堆大小2g，其他都是默认。jmap看新生代eden区和survivor区的比例为8，按这比例，survivor区应该是新生代的10分之一，但是eden区有680m,survivor区却只有10m,而且每次yonggc后，eden区和survivor区的总大小都在变化，为什么呢？

**回答：**

那你们肯定是允许堆大小动态调整了，那个需要禁止掉的，就是-Xmx和-Xms需要一样的值

**问题：**

在创建对象是在eden和survivor1中随机分配内存的么？不是的话，那顺序是什么？

猜想是在eden分配，慢慢晋升到survivor1中，最后yongGC后存活的1%转移到survivor2中，对么？

**回答：**在eden里分配对象，gc后存活对象放入Survivor中

### 057、案例实战：每秒十万QPS的社交APP 如何优化GC性能提升3倍？

**1、引子**

这篇文章开始，我们会开始用一系列真实的生产案例给大家还原出来各种各样不同的JVM优化场景

力求让大家在不同的业务背景下，对不同的原因产生的JVM性能问题进行分析和处理，进而积累出来大量不同场景下的JVM性能优化经验。

所有的JVM优化案例都是基于之前几十篇文章教给大家的核心原理以及优化手段来展开的，因此大家可以认为，之前几十篇文章学习完过后，你就已经有能力在生产系统上手进行JVM的性能分析以及优化了。

只不过大家如果再经过几十个真实的生产案例的锤炼，就能够有能力对未来自己可能遇到的各种不同的情况下的JVM生产问题进行分析和处理。

因为这些案例都是真实的生产案例，所以有的案例我们会用一些模拟代码还原出来当时的故障场景，并且带着大家用各种工具来进行分析，然后进行优化。

但是有的案例我们很难用模拟代码还原出来，此时还会延续采用一步一图的方式，尽量用大量的手绘图让大家理解JVM故障的产生背景、发生原因以及优化的手段。

跟之前一样，我们开始先用两三个较为简单的案例引入，让大家找找感觉，接着再切入比较复杂的案例。

**2、案例背景**

本案例的背景是一个有高峰期每秒十万QPS的社交APP，这是我曾经帮助一个朋友的公司处理过的一个JVM优化的案例。

大家都知道，其实现在社交APP有很多种，不光是大家熟悉的微信、QQ之类的，还有很多细分领域的明星社交APP，诸如陌生人社交，基于地理位置的社交，等等。

其实很多明星创业社交APP产品，也有每日数百万的日活用户，尤其在晚上高峰期的时候，APP的QPS也是很高的。

附带一句，可能有的同学不知道QPS是什么，其实英文全称就是“Query Per Second”，也就是每秒钟的查询数量，大致可以理解为是APP每秒钟的访问数量。

对于这个APP而言，流量最大的是哪个功能模块？

不知道大家有没有玩儿过陌生人社交类的APP，在这种APP中操作最多的就是浏览某个陌生人的个人页面。

一般这类APP都会通过各种方式来给你推荐一些周边的陌生人，然后你可能会看到一些感兴趣的人，就会进入他/她的个人主页去看一看。个人主页里可能就包含了那个人的一些自我介绍，照片之类的东西。

所以这类APP，在晚上高峰期，流量最大的一个模块，其实就是个人主页模块

会有大量的活跃用户在一个集中的时间段内频繁的访问各种个人主页数据，而且这类数据的量还通常很大，要包含很多的信息，通常一个个人主页的数据甚至可能有几MB。

随便给大家举个例子，一个个人主页里，可能有这个人每天发的一些心情、感悟之类的东西

那么一旦要把这个个人主页加载出来，必然会加载出来这个人最近N多天发的一些心情感悟之类的文字，这个文字的量还是比较多的。

所以我们大致可以认为，一次个人主页的查询，就会加载出来比如5MB的数据。

而且一般在高峰期内，有可能一些活跃用户他可能会连续不断的去点击他感兴趣的人的个人主页，比如连续1个小时都在不停的点击。

所以其实这类社交APP他的高峰期QPS时很高的。在当时的场景中，这个社交APP流量最大的个人主页模块高峰期最多每秒会有10万+的QPS。

当然在底层存储中，这些个人主页数据一定是基于缓存来存放的，也就是基于Redis缓存来查询这些个人主页数据。

所以，综合我们分析出来的这个背景，我们在这里可以用下面一幅图来让大家了解一下这个社交APP的一个情况。![01.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/59180200_1578303742.png)

**3、高并发查询导致对象快速进入老年代**

之前已经用大量的图示给大家分析过了JVM的运行原理了，所以在这个案例中就不再给大家描述的过于细节。

这里就直接给大家分析一下当时案例发生的一个场景，因为这个社交APP的日活用户涨的很快，所以导致他的高峰期QPS很快就飙升到了10万。

正是因为每秒并发量太高，这也直接导致了这个系统在高峰期的时候，年轻代的Eden区会迅速的被填满，并且频繁的触发Young GC，如下图所示。

![02.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70763800_1578303742.png)

而且每次在Young GC的时候，实际上还有很多请求是没处理完毕的，没办法，因为每秒请求量太多了，所以在你触发Young GC的时候，就这一瞬间，必然有很多请求是还没处理完毕的。

这就导致Eden区中其实每次触发Young GC的时候，都有很多对象是需要存活下来的

如下图所示：![03.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83598400_1578303742.png)

因此在高峰期的时候，其实经常会出现Young GC过后存活对象较多，在Survivor区中放不下的问题，如下图所示。

![04.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96313700_1578303742.png)

所以此时必然会导致大量的对象快速的进入老年代中，如下图所示。



![05.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/9021900_1578303743.png)

**4、老年代必然会触发频繁GC**

其实根据之前学过的知识，大家都清楚了一点，那就是一旦在高并发场景下Young GC后存活对象过多，导致对象快速进入老年代，必然会频繁触发老年代的GC，对老年代进行垃圾回收。

所以在上述社交APP高峰期高并发场景下，必然会导致个人主页服务对应的JVM频繁的发生老年代的GC，如下图所示。

![06.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23882200_1578303743.png)

**5、优化前的线上系统JVM参数**

其实大家都知道，针对上述场景，最核心的优化点，主要应该是增加机器，尽量让每台机器承载更少的并发请求，减轻压力。

同时，给年轻代的Survivor区域更大的内存空间，让每次Young GC后的存活对象务必停留在Survior中，别进入老年代。

但是在这里我们先不考虑上述优化，在优化前的线上系统中，对JVM有两个比较关键的参数大家可以看一下：

-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=5

大家可以看到这个是什么意思，其实非常明显，我们之前讲过这两个参数的含义。CMS垃圾回收器默认是采用标记-清理算法，所以是会造成大量的内存碎片的。

什么叫内存碎片？我们再来看下图回顾一下

比如现在老年代内存里有一些垃圾对象。

![07.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37450400_1578303743.png)

然后CMS垃圾回收器一次垃圾回收过后，回收掉了一些垃圾对象，此时可能内存里看起来跟下面这样：

![08.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55582100_1578303743.png)

大家注意上面那个红圈的地方，因为回收掉了一个对象，所以那里出现了一个内存碎片

虽然这里是空白内存，但是假如此时你要是要分配一个对象比较大，没法再上面红圈处放进去呢？那么红圈的那个内存碎片不就没任何意义了？

所以CMS正常垃圾回收，因为使用标记-清理算法，所以必然导致大量的内存碎片。

所以“-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=5”两个参数的含义，就是在5次Full GC之后会触发一次Compaction操作，也就是压缩操作

这个操作会把存活对象放到紧邻在一起，避免大量的内存碎片，如下图所示。

![09.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70368500_1578303743.png)

大家看下上图，是不是发现两个存活对象被挤压在一起了？然后红圈地方是不是多出来一大块连续可用的内存空间？不再是之前的一小片内存碎片了吧？

**6、频繁Full GC导致的大量内存碎片**

但是大家现在要明白一点，上述两个参数“-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=5”是设置的5次Full GC之后才会进行一次压缩操作，解决内存碎片的问题，空出来大片的连续可用内存空间。

所以这就直接导致在这5次Full GC的过程中，每一次Full GC之后都会产生大量的内存碎片。

大量的内存碎片会导致很多问题，其中一个问题，就是提高Full GC的频率。

为什么呢？因为大家之前应该还记得我们讲过，触发老年代GC的一个非常重要的条件，就是Young GC后的存活对象无法放入Survivior，就要放入老年代。

但是此时老年代假设也没足够内存放这些对象了，就必须触发Full GC了。

所以大家考虑一个场景，假设如下图所示，一次Full GC过后，老年代中有一部分内存里都是大量的内存碎片，没法放入完整的一些大对象了，只有部分内存是连续可用的内存空间。

如下图所示：

![10.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/82648800_1578303743.png)

这个时候，随着大量对象快速进入老年代，会导致一旦老年代的那块连续可用内存满了，此时很多内存碎片是无法放入更多对象的，就会立马触发下一次Full GC。

比如老年代有2G的内存，其中1.5G是连续可用内存，0.5G是很多内存碎片。

本来老年代如果都是连续空内存的话，那么可能可以对象占用到将近2G才会触发Full GC。

结果现在就是对象占用到了1.5G就需要触发Full GC了，剩下0.5G是没法放任何对象的。

所以这就会导致随着一次一次Full GC导致老年代产生更多的内存碎片，连续可用内存越来越少，触发下一次FUll GC的速度就会越快。

直到几次Full GC之后，才会触发一次Compaction操作去整理内存碎片。

**7、这个案例如何进行优化？**

其实对这个案例进行优化，非常的简单，无法就是用之前讲过的jstat分析一下各个机器上的jvm的运行状况，判断出来每次Young GC后存活对象有多少，然后就是增加Survivor区的内存，避免对象快速进入老年代。

另外一个，在当时对那个系统优化之后，增加了年轻代和Survivor区的大小，但还是会慢慢的有对象进入老年代里，毕竟系统负载很高，彻底让对象不进入老年代也很难做到。所以当时调优过后每小时还是会有一次Full GC。

所以当时第二个参数的优化就是针对CMS内存碎片问题的

在降低了Full GC频率之后，务必设置如下参数“-XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0”，每次Full GC后都整理一下内存碎片。

否则如果每次Full GC过后，都造成老年代里很多内存碎片，那么必然导致下一次Full GC更快到来，因为内存碎片会导致老年代可用内存变少。

也许第一次Full GC是一小时才有，第二次Full GC也许是40分钟之后，第三次Full GC可能就是20分钟之后，要是不解决CMS内存碎片问题，必然导致Full GC慢慢变得越来越频繁。

**8、今日思考题**

今天给大家留一个小的思考题：去看看你们线上系统的JVM参数，如果用的是CMS垃圾回收器的话，压缩这块是如何设置的？

考虑一下如果不是每次FUll GC后都压缩一次解决内存碎片，会对你们的系统有什么影响？

**End**

### 058、案例实战：垂直电商APP后台系统，如何对Full GC进行深度优化？

**1、垂直电商业务背景**

今天给大家讲的这个案例，是我协助另外一个朋友的公司进行全公司级别的JVM性能优化的案例，很多核心的思想其实也跟之前是相同的，只不过在优化的过程中会带出来一些比较高级的参数的调优。

先给大家说一下这个公司的大致业务背景，这是一个垂直电商公司，做的是一个垂直电商APP

之前的文章里给大家提过，其实现在除了淘宝、京东、天猫、唯品会这些超大型的电商平台之外，国内还是有很多中小型的垂直类电商公司的。

他们做的主要是一些细分领域的电商业务，比如说有的APP专门做消费分期类的电商业务，在他们的APP里你主要是进行购物，然后可以分期付费。

还有的APP，他专门是做服装定制的，他可能是会在APP里让你选购商品，然后有人上门给你定制化的测量身体，然后给你做定制的衬衫或者西装之类的。

还有的APP，他是做时尚潮流服饰的，就是专门售卖针对年轻人的一些潮牌、设计师的品牌，等等。

所以这个朋友的公司就是类似上述的那种垂直电商APP，专门做某个细分领域的。

说实话，这个垂直电商APP大致注册用户量有就数百万的规模，不算多大，而且每日活跃用户数量也就几十万而已，每天APP的整体请求量也就小几千万的级别，也并不大。高峰期的QPS也就每秒数百请求罢了。

但即使如此的一个普通APP的后台系统，感觉上压力一点儿都不大，是不是真的就没有JVM的性能问题呢？

当然不是了，这个APP虽然不大，但是他同样有JVM相关的性能问题，而且也需要一些细致的优化才可以。

**2、垂直电商APP的JVM性能问题**

那么类似这样的一个垂直电商APP，他的JVM性能问题在哪里呢？

很简单，问题就出在类似这样的一个创业型互联网公司，虽然有少数几个技术比较好的架构师，但是架构师往往没那么大精力把控到特别细节的地方

所以大部分的一线普通工程师可能都对JVM这块没有那么的精通，起码说对我们的专栏里讲解的JVM相关原理和优化手段，都没了解。

所以这直接导致一个很大的问题，那就是大部分的一线工程师开发完一个系统之后，部署生产环境的时候往往就不会对JVM进行什么参数的设置，可能很多时候就是用一些默认的JVM参数。

默认的JVM参数绝对是系统负载逐渐增高的时候一个最大的问题

如果你不设置-Xmx、-Xms之类的堆内存大小的话，你启动一个系统，可能默认就给你几百MB的堆内存大小，新生代和老年代可能都是几百MB的样子。

所以当时这个垂直电商APP的很多后台系统，基本都是用的默认JVM参数部署启动的，前期是没什么问题，但是中后期开始，当有一定用户量，有一定负载了，此时就会出现一些问题了。

大家通过之前大量的学习，哪怕现在不画图，脑子里都有一个概念了，那就是新生代内存过小，会导致Survivor区域内存过小，同时Eden区域也很小。

Eden区域过小，自然会导致频繁的触发Young GC，Survivor区域过小，自然会导致经常在Young GC之后存活对象其实也没多少，但就是Survivor区域放不下。

此时必然会导致对象经常进入老年代中，因此也必然会导致老年代过一段时间就放满了，然后就会触发Full GC。

所以当时这个垂直电商APP的各个系统通过jstat分析JVM GC之后发现，基本上高峰期的时候，Full GC每小时都会发生好几次。

Full GC一般在正常情况下，都是以天为单位发生的，比如每天发生一次，或者是几天发生一次Full GC。

要是每小时都发生几次Full GC，那么就会导致系统每小时都卡顿好几次。这个时候必然是不行的。

在这个背景下，当时我的朋友是那家公司的架构师，找到我帮忙，在分析系统情况过后，定制了一套公司级别的JVM参数模板

在大部分工程师都对JVM优化不是很精通的情况下，通过推行一个JVM参数模板，让各个系统短时间内迅速就优化了JVM的性能。

**3、公司级别的JVM参数模板**

其实这个公司级别的或者团队级别的JVM参数模板，是一个很有用的东西，因为大家要知道，并不是每个人都会来学习我们的这个JVM专栏，也就意味着并不是每个人都是可以比较精通JVM的核心运行原理和性能优化的。

所以如果你是一个团队的leader，或者是一个中小型公司的架构师，那么必然是需要为团队或者公司定制一套基本的JVM参数模板的

然后尽量让大部分系统套用这个模板，基本保证JVM性能别太差，避免很多初中级工程师直接使用默认的JVM参数，可能一台8G内存的机器上，JVM堆内存就分配了几百MB。

下面我们可以来看看当时我和那位朋友一起定制出来的适合他们公司的JVM参数模板：

-Xms4096M -Xmx4096M -Xmn3072M -Xss1M  -XX:PermSize=256M -XX:MaxPermSize=256M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=92 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0

为什么如此定制JVM参数模板呢？

首先，8G的机器上给JVM堆内存分配4G就差不多了，毕竟可能还有其他进程会使用内存，一般别让JVM堆内存把机器内存给占满。

然后年轻代给到3G，之所以给到3G的内存空间，就是因为让年轻代尽量大一些，进而让每个Survivor区域都达到300MB左右。

根据当时对这个业务系统的分析，假设用默认的JVM参数，可能年轻代就几百MB的内存，Survivor区域就几十MB的内存

那么每次垃圾回收过后存活对象可能会有几十MB，这是因为在垃圾回收的一瞬间可能有部分请求没处理完毕，此时会有几十MB对象是存活的，所以很容易触发动态年龄判定规则，让部分对象进入老年代。

所以在分析过后，给年轻代更大内存空间，让Survivor空间更大，这样在Young GC的时候，这一瞬间可能有部分请求没处理完毕，有几十MB的存活对象，这个时候在几百MB的Survivor空间中可以轻松放下，绝对不会进老年代。

基本上在这个内存分配之下，对于这个垂直电商APP的大部分后台业务系统，都是可以轻松hold住的

不同的系统运行时的情况略有不同，但是基本上都是在每次Young GC过后存活几MB~几十MB的对象，所以此时在这个参数模板下，都可以抗住。

只要把内存分配完毕，那么对象进入老年代的速度是极慢极慢的，经过这个参数模板在朋友公司全部系统的重新部署和上线，各个团队通过jstat观察，基本上发现各个系统的Full GC都变成了几天才会发生一次。

此时在参数模板里还会加入Compaction相关的参数，保证每次Full GC之后都会执行一次压缩，解决内存碎片的问题。

关于内存碎片的影响和优化，上一篇文章刚刚分析过，那是为另外一个朋友的公司专门做优化的时候调整的参数。

**4、如何优化每次Full GC的性能？**

这里给大家再介绍一下当时帮那位朋友做优化的时候调整的另外两个参数，这个两个参数可以帮助优化FUll GC的性能，把每次Full GC的时间进一步降低一些。

一个参数是“-XX:+CMSParallelInitialMarkEnabled”，这个参数会在CMS垃圾回收器的“初始标记”阶段开启多线程并发执行。

大家应该还记得初始标记阶段，是会进行Stop the World的，会导致系统停顿，所以这个阶段开启多线程并发之后，可以尽可能优化这个阶段的性能，减少Stop the World的时间。

另外一个参数是“-XX:+CMSScavengeBeforeRemark”，这个参数会在CMS的重新标记阶段之前，先尽量执行一次Young GC。

**这样做有什么作用呢？**

其实大家都记得，CMS的重新标记也是会Stop the World的，所以所以如果在重新标记之前，先执行一次Young GC，就会回收掉一些年轻代里没有人引用的对象。

所以如果先提前回收掉一些对象，那么在CMS的重新标记阶段就可以少扫描一些对象，此时就可以提升CMS的重新标记阶段的性能，减少他的耗时。

所以当时在JVM参数模板中，同样加入了这两个参数：

-Xms4096M -Xmx4096M -Xmn3072M -Xss1M  -XX:PermSize=256M -XX:MaxPermSize=256M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFaction=92 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSScavengeBeforeRemark

**5、垂直电商APP全公司采用JVM参数模板之后的效果**

上述JVM参数模板，推广到了朋友公司的全部系统中，因为当时公司里几乎一线工程师对JVM优化的理解都达不到这个专栏讲解的水准，很多人都理解非常粗浅的，所以这套JVM模板参数是全部推行的。

经过各个团队采用jstat观察JVM GC情况，发现明显有了很大的好转，基本上各个系统的Young GC都在几分钟一次，或者十几分钟一次，每次耗时就几十毫秒而已。

Full GC基本都在几天一次，每次耗时在几百毫秒的样子。

基本上各个系统的JVM达到这个性能，就对线上系统没多大影响了。哪怕是不太懂JVM优化的普通工程师只要套用这个模板，对一些普通的业务系统，都能保证其JVM性能不会出现大的问题，比如频繁的Young GC和Full GC导致的系统频繁卡顿。

**6、今日思考题**

今天让大家思考一下：

- 你们公司有没有类似这里讲的JVM参数模板？

- 假如你是公司的架构师，结合你们公司的大部分业务系统的实际情况，会如何定制一套JVM参数模板？
- 是否你们公司有各种不同配置的机器？
- 针对不同配置的机器如何定制JVM参数模板？
- 你们公司有没有那种特例的系统，比如并发量特别高或者数据量非常大？
- 对特例系统该如何进行优化?

希望大家对以上问题积极思考，并将自己的答案发送到评论区交流。

**有什么疑问，也欢迎大家在评论区给我留言！**

**End**

### 059、案例实战：新手工程师不合理设置JVM参数，是如何导致频繁Full GC的？

**1、本文背景**

本文会给大家讲解一个比较特殊的JVM优化案例，这个优化案例本身是因为新手工程师对JVM优化可能了解了一个半吊子，然后不知道从哪里找来了一个非常特殊的JVM参数错误的设置了一下，就导致线上系统频繁的出现Full GC的问题。

但是我们后续大量的优化案例其实都是各种各样奇形怪状的场景，因为正是各种奇怪场景才能让大家逐步积累出来较为丰富的JVM优化实战经验

了解的场景越多，自己未来在处理JVM性能问题的时候才能更是得心应手。

**2、问题的产生**

这个场景的发生大致如下过程：某天团队里一个新手工程师大概是心血来潮，觉得自己网上看到了某个JVM参数，以为学会了绝世武功秘籍，于是就在当天上线一个系统的时候，自作主张设置了一个JVM参数

这个参数是什么呢？

不用急，跟着看下面的案例分析即可，现在只要知道他设置了一个奇怪的参数，接着事故就发生了。

因为一般中大型公司都是接入类似Zabbix、OpenFalcon或者公司自研的一些监控系统的，监控系统一般都做的很好，可以让你的系统直接接入进去，然后在上面可以看到每台机器的CPU、磁盘、内存、网络的一些负载。

而且可以看到你的JVM的内存使用波动折线图，还有你的JVM GC发生的频率折线图。包括如果你自己上报某个业务指标，也可以在监控系统里看到。

而且一般都会针对线上运行的机器和系统设置一些报警，比如说，你可以设置如果10分钟内发现一个系统的JVM发生了超过3次Full GC，就必须发送报警给你，可以发送给你的短信、邮箱或者是钉钉之类的IM工具。

类似这样的监控系统不在我们的专栏范畴内，建议大家自己可以去查阅资料，其实基于我们讲解的命令行工具，比如jstat，你可以通过linux上的一些命令，让jstat自动对jvm进行监控，把监控结果可以输出到机器的某个文件里去。

然后第二天你就可以去查阅那个文件，也可以看到那台机器的jvm的一些gc统计。

所以说，没有可视化工具，用最简单的命令行工具，其实同样可以起到类似的效果。

所以那天那个工程师设置了一个JVM参数之后，直接导致线上频繁接到JVM的Full GC的报警，大家就很奇怪了，于是就开始排查那个系统了。

**3、查看GC日志**

之前已经给大家讲解过如何在启动系统的时候让他输出GC日志，所以一旦发现报警，直接登录到线上机器，然后就看到对应的GC日志了。

此时我们看到在GC日志中有大量的Full GC的记录。

那么是为什么导致的Full GC呢？

在日志里，看到了一个“Metadata GC  Threshold”的字样，类似于如下日志：

【Full GC（Metadata GC  Threshold）xxxxx, xxxxx】

从这里就知道，这频繁的Full GC，实际上是JDK 1.8以后的Metadata元数据区导致的，也就是类似我们之前说的永久代。

这个Metadata区域一般是放一些加载到JVM里去的类的。

所以此时就很奇怪了，为什么会因为Metadata区域频繁的被塞满，进而触发Full GC？而且Full GC大家都知道，会带动CMS回收老年代，还会回收Metadata区域本身。

我们先看看下图：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/58857700_1578386336.cn/txdocpic/0/0736b7cba40d8420b1b9f685f8e7c737/0)   

**4、查看Metaspace内存占用情况**

接着我们当然是想看一看Metaspace区域的内存占用情况了，简单点你可以通过jstat来观察，如果有监控系统，他会给你展示出来一个Metaspace内存区域占用的波动曲线图，类似下面这种。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/71243500_1578386336.cn/txdocpic/0/a6542d9da6fbe98fc0a10c48be4adea9/0)       

看起来Metaspace区域的内存呈现一个波动的状态，他总是会先不断增加，达到一个顶点之后，就会把Metaspace区域给占满，然后自然就会触发一次Full GC，Full GC会带着Metaspace区域的垃圾回收，所以接下来Metaspace区域的内存占用又变得很小了。

**5、一个综合性的分析思路**

看到这里，相信大家肯定有一点感觉了，这个很明显是系统在运行过程中，不停的有新的类产生被加载到Metaspace区域里去，然后不停的把Metaspace区域占满，接着触发一次Full GC回收掉Metaspace区域中的部分类。

然后这个过程反复的不断的循环，进而造成Metaspace区域反复被占满，然后反复导致Full GC的发生，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/85028100_1578386336.cn/txdocpic/0/1f8023779c2a550de4c4009f0474f720/0)       

**6、到底是什么类不停的被加载？**

接着我们就有点奇怪了，到底是什么类不停的被加载到JVM的Metaspace区域里去？

这个时候就需要在JVM启动参数中加入如下两个参数了：

“-XX:TraceClassLoading -XX:TraceClassUnloading”

这两个参数，顾名思义，就是追踪类加载和类卸载的情况，他会通过日志打印出来JVM中加载了哪些类，卸载了哪些类。

加入这两个参数之后，我们就可以看到在Tomcat的catalina.out日志文件中，输出了一堆日志，里面显示类似如下的内容：

【Loaded sun.reflect.GeneratedSerializationConstructorAccessor from __JVM_Defined_Class】

明显可以看到，JVM在运行期间不停的加载了大量的所谓“GeneratedSerializationConstructorAccessor”类到了Metaspace区域里去

如下图所示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/95839800_1578386336.cn/txdocpic/0/4fe746fdece2588511a2fc35d46a6673/0)   

相信就是因为JVM运行期间不停的加载这种奇怪的类，然后不停的把Metaspace区域占满，才会引发不停的执行Full GC的。

**这是一个非常实用的技巧，各位同学一定要掌握**，频繁Full GC不光是老年代触发的，有时候也会因为Metaspace区域的类太多而触发。

到此为止，已经慢慢接近真相了。

**7、为什么会频繁加载奇怪的类？**

接着遇到类似这种问题，我们就应该找一下Google或者是百度了，当然推荐是用Google。你完全可以看看那种不停加载的类，到底是什么类，是你自己写的类？还是说JDK内置的类？

比如上面的那个类，如果你查阅一些资料，很容易就会搞明白，那个类大概是在你使用Java中的反射时加载的，所谓反射代码类似如下所示。

Method method = XXX.class.getDeclaredMethod(xx,xx);

method.invoke(target,params);

友情提示一下，反射是Java中最最基础的一个概念，不懂的朋友自己查一下资料。

简单来说，就是通过XXX.class获取到某个类，然后通过geteDeclaredMethod获取到那个类的方法。

这个方法就是一个Method对象，接着通过Method.invoke可以去调用那个类的某个对象的方法，大概就这个意思。

在执行这种反射代码时，JVM会在你反射调用一定次数之后就动态生成一些类，就是我们之前看到的那种莫名其妙的类

下次你再执行反射的时候，就是直接调用这些类的方法，这是JVM的一个底层优化的机制。

看到这里，有的小伙伴是不是有点蒙？

其实这倒无所谓，这段话看的蒙丝毫不影响你进行JVM优化的

**你只要记住一个结论：如果你在代码里大量用了类似上面的反射的东西，那么JVM就是会动态的去生成一些类放入Metaspace区域里的。**

所以上面看到的那些奇怪的类，就是由于不停的执行反射的代码才生成的，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8055300_1578386337.cn/txdocpic/0/60712831855deba847abf7e93314a137/0)       

**8、JVM创建的奇怪类有什么玄机？**

那么接下来我们就很奇怪一件事情，就是JVM为什么要不停的创建那些奇怪的类然后放入Metaspace中去？

其实这就要从一个点入手来分析一下了，因为上面说的那种JVM自己创建的奇怪的类，他们的Class对象都是SoftReference，也就是软引用的。

大家可千万别说连类的Class是什么都没听说过？简单来说，每个类其实本身自己也是一个对象，就是一个Class对象，一个Class对象就代表了一个类。同时这个Class对象代表的类，可以派生出来很多实例对象。

举例来说，Class Student，这就是一个类，他本身是由一个Class类型的对象表示的。

但是如果你走一个Student student = new Student(），这就是实例化了这个Student类的一个对象，这是一个Student类型的实例对象。

所以我们这里所说的Class对象，就是JVM在发射过程中动态生成的类的Class对象，他们都是SoftReference软引用的。

所谓的软引用，最早我们再一篇文章里说过，正常情况下不会回收，但是如果内存比较紧张的时候就会回收这些对象。

那么SoftReference对象到底在GC的时候要不要回收是通过什么公式来判断的呢？

是如下的一个公式：**clock - timestamp <= freespace \* SoftRefLRUPolicyMSPerMB**。

这个公式的意思就是说，“clock - timestamp”代表了一个软引用对象他有多久没被访问过了，freespace代表JVM中的空闲内存空间，SoftRefLRUPolicyMSPerMB代表每一MB空闲内存空间可以允许SoftReference对象存活多久。

举个例子，假如说现在JVM创建了一大堆的奇怪的类出来，这些类本身的Class对象都是被SoftReference软引用的。

然后现在JVM里的空间内存空间有3000MB，SoftRefLRUPolicyMSPerMB的默认值是1000毫秒，那么就意味着，此时那些奇怪的SoftReference软引用的Class对象，可以存活3000 * 1000 = 3000秒，就是50分钟左右。

当然上面都是举例而已，大家都知道，一般来说发生GC时，其实JVM内部或多或少总有一些空间内存的，所以基本上如果不是快要发生OOM内存溢出了，一般软引用也不会被回收。

所以大家就知道了，按理说JVM应该会随着反射代码的执行，动态的创建一些奇怪的类，他们的Class对象都是软引用的，正常情况下不会被回收，但是也不应该快速增长才对。

**9、为什么JVM创建的奇怪的类会不停的变多？**

那么究竟为什么JVM创建的那些奇怪的类会不停的变多呢？

原因很简单，因为文章开头那个新手工程师不知道从哪里扒出来了SoftRefLRUPolicyMSPerMB这个JVM启动参数，他直接把这个参数设置为0了。

他想的是，一旦这个参数设置为0，任何软引用对象就可以尽快释放掉，不用留存，尽量给内存释放空间出来，这样不就可以提高内存利用效率了么？

**真是想的很傻很天真。**

实际上一旦这个参数设置为0之后，直接导致clock - timestamp <= freespace * SoftRefLRUPolicyMSPerMB这个公式的右半边是0，就导致所有的软引用对象，比如JVM生成的那些奇怪的Class对象，刚创建出来就可能被一次Young GC给带着立马回收掉一些。

如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23142400_1578386337.cn/txdocpic/0/c86edc96839d4a9d0faf1f01a0ced8d8/0)       

比如JVM好不容易给你弄出来100个奇怪的类，结果因为你瞎设置软引用的参数，导致突然一次GC就给你回收掉几十个类

接着JVM在反射代码执行的过程中，就会继续创建这种奇怪的类，在JVM的机制之下，会导致这种奇怪类越来越多。

也许下一次gc又会回收掉一些奇怪的类，但是马上JVM还会继续生成这种类，最终就会导致Metaspace区域被放满了，一旦Metaspace区域被占满了，就会触发Full GC，然后回收掉很多类，接着再次重复上述循环，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/39997500_1578386337.cn/txdocpic/0/f5c56edb0355e350898a96633dcc9089/0)       

其实很多人会有一个疑问，到底为什么软引用的类因为错误的参数设置被快速回收之后，就会导致JVM不停创建更多的新的类呢？

其实大家不用去扣这里的细节，这里有大量的底层JDK源码的实现，异常复杂，要真的说清楚，得好几篇文章才能讲清楚JDK底层源码的这些细节。

大家只要记住这个结论，明白这个道理就好。

**10、如何解决这个问题？**

虽然底层JDK的一些实现细节我们没分析，但是大致梳理出来了一个思路，大家也很清楚问题所在和原因了

解决方案很简单。在有大量反射代码的场景下，大家只要把

-XX:SoftRefLRUPolicyMSPerMB=0

这个参数设置大一些即可，千万别让一些新手同学设置为0，可以设置个1000，2000，3000，或者5000毫秒，都可以。

提高这个数值，就是让反射过程中JVM自动创建的软引用的一些类的Class对象不要被随便回收，当时我们优化这个参数之后，就可以看到系统稳定运行了。

基本上Metaspace区域的内存占用是稳定的，不会来回大幅度波动了。

**11、今日思考题**

结合昨天的内容，**大家思考一下这个线上事故的本质是什么？**

其实说白了不是JVM的问题，是人的问题。

大家可以考虑一下，如果你是公司的架构师，是否应该严格审核各个系统的生产环境JVM参数？

比如完全可以推行一套JVM参数模板，如果有人要做定制的JVM优化，是不是应该先在测试环尝试一下，然后还得交给你们高级别的架构师来审核？

如果有人审核，那么就不会发生类似之类的血案了。

**End**

### 060、案例实战：一次线上系统每天数十次Full GC导致频繁卡死的优化实战！

**1、案例开始前的说明**

今天的这个案例也是我们之前线上系统经历过的一个真实的生产JVM优化案例，这个优化的过程比较复杂，经过了多次优化，当然核心原理和知识其实还是之前给大家讲解过的那些东西。

只不过这个真实生产系统优化的过程大家如果能理解透彻，那么对于大家利用学过的知识和掌握的工具自己去进行JVM优化的时候，肯定是大有好处的。

这个线上系统是一个团队开发的，那个团队开发完一个新系统上线之后发现一天的Full GC次数多大数十次，甚至有的时候会上百次，大家可想而知这是什么概念！

通常来说，我们建议的一个比较良好的JVM性能，应该是Full GC在几天才发生一次，或者是最多一天发生几次而已。

所以当时这个新系统在线上的表现非常不好，明显是有经常性的卡顿的，因此针对这个系统，我们进行了一连串的排查、定位、分析和优化

下面就给大家分析一下整个优化的过程。

**2、未优化前的JVM性能分析**

大家还记得之前带着大家动手实操过的jstat工具吧？那个工具是非常好用，非常实用，也是非常常用的一个工具。

因为很多中小型公司都没上那种可视化的监控平台，没法直接可视化的看到JVM各个区域的内存变化，GC次数和GC耗时。

当然，如果有办法的话，我建议大家可以给自己所在公司推荐一下类似Zabbix、Ganglia、Open-Falcon、Prometheus之类的可视化监控平台，其实接入都非常简单，如果把线上系统接入了这些平台，可以直接图形化看到JVM的表现。

但是哪怕你有了可视化监控平台，有时候直接对线上系统进行分析的时候，还是jstat更加好用和直接。

所以当时我们通过监控平台+jstat工具分析，直接得出当时没优化过的系统的JVM性能表现大致如下：

- 机器配置：2核4G
- JVM堆内存大小：2G
- 系统运行时间：6天
- 系统运行6天内发生的Full GC次数和耗时：250次，70多秒
- 系统运行6天内发生的Young GC次数和耗时：2.6万次，1400秒

综合分析一下，就可以知道，大致来说每天会发生40多次Full GC，平均每小时2次，每次Full GC在300毫秒左右；

每天会发生4000多次Young GC，每分钟会发生3次，每次Young GC在50毫秒左右。

上述数据对任何一个线上系统，用jstat可以轻松看出来，因为jstat显示出来的Full GC和Young GC的次数都是系统启动以来的总次数，耗时都是所有GC加起来的总耗时，所以直接可以拿到上述数据，略微分析一下就知道具体情况了。

基本看起来，这个系统的性能是相当差了，每分钟3次Young GC，每小时2次Full GC，这种系统是必须得进行优化的。

**3、未优化前的线上JVM参数**

下面是未优化前的线上JVM参数，大致如下：

-Xms1536M -Xmx1536M -Xmn512M -Xss256K -XX:SurvivorRatio=5 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=68 -XX:+CMSParallelRemarkEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC

其实基本上跟我们之前看到的参数没多大的不同，一个4G的机器上，给JVM的堆内存是设置了1.5G的大小，其中新生代是给了512M，老年代是1G。

比较关键的是“-XX:SurvivorRatio”设置为了5，也就是说，Eden:Survivor1:Survivor2的比例是5:1:1

所以此时Eden区域大致为365M，每个Survivor区域大致为70MB。

而且这里有一个非常关键的参数，那就是“-XX:CMSInitiatingOccupancyFraction”参数设置为了68

所以一旦老年代内存占用达到68%，也就是大概有680MB左右的对象时，就会触发一次Full GC。

此时整个系统的内存模型图如下所示：   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/3899400_1578303858.cn/txdocpic/0/cc767188e28d05a62acc1801460915e6/0)       

**4、根据线上系统的GC情况倒推运行内存模型**

接着我们可以根据系统的内存模型以及GC情况，直接根据学习过的知识推导出系统运行时的内存模型了。

首先我们知道每分钟会发生3次Young GC，说明系统运行20秒就会让Eden区满，也就是产生300多MB的对象，平均下来系统每秒钟会产生15~20MB的对象，如下图所示。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17473800_1578303858.cn/txdocpic/0/c1a252d79fd3c30f4c8ea8e9b7ac1541/0)       

所以20秒左右就会导致Eden区满，然后触发一次Young GC。

接着我们根据每小时2次Full GC推断出，30分钟会触发一次Full GC

根据“-XX:CMSInitiatingOccupancyFraction=68”参数的设置，应该是在老年代有600多MB左右的对象时大概就会触发一次Full GC，因为1GB的老年代有68%空间占满了就会触发CMS的GC了。

所以系统运行30分钟就会导致老年代里有600多MB的对象，进而触发CMS垃圾回收器对老年代进行GC，如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27932000_1578303858.cn/txdocpic/0/b1e23c04edc338a5c0e63f52997afed6/0)       

所以基本上我们就能根据推导出的运行内存模型得出一个结论：

每隔20秒会让300多MB的Eden区满触发一次Young GC，一次Young GC耗时50毫秒左右。

每隔30分钟会让老年代里600多MB空间占满，进而触发一次CMS的GC，一次Full GC耗时300毫秒左右。

但是到这里大家先暂停一下，有的朋友可能立马会推断了，他会说，是不是因为Survivor区域太小了，导致Young GC后的存活对象太多放不下，就一直有对象流入老年代，进而导致30分钟后触发Full GC？

实际上仅仅只是分析到这里，绝对不能草率下这个判断的。

因为老年代里为什么有那么多的对象？有可能是每次Young GC后的存活对象较多，Survivor区域太小，放不下了

也有可能是有很多长时间存活的对象太多了，都积累在老年代里，始终回收不掉，进而导致老年代很容易就达到68%的占比触发GC。

所以仅仅分析到这里，绝对不能轻易下结论。

**5、老年代里到底为什么会有那么多的对象？**

分析到这里，说句实话，仅仅根据可视化监控和推论是绝对没法往下分析了，因为我们并不知道老年代里到底为什么会有那么多的对象

此时就完全可以用jstat在高峰期观察一下JVM实际运行的情况。

通过jstat的观察，我们当时可以明确看到，每次Young GC过后升入老年代里的对象很少

一般来说，每次Young GC过后大概就存活几十MB而已，那么Survivor区域因为就70MB，所以经常会触发动态年龄判断规则，导致偶尔一次Young GC过后有几十MB对象进入老年代。

我们看下图的图示。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/41072400_1578303858.cn/txdocpic/0/d6c042b0833f47495b08b88f0cffcbd9/0)       

因此分析到这里很奇怪，因为通过jstat追踪观察，并不是每次Young GC后都有几十MB对象进入老年代的，而是偶尔一次Young GC才会有几十MB对象进入老年代，记住，是偶尔一次！

所以正常来说，应该不至于30分钟就导致老年代占用空间达到68%。

那么老年代里到底为什么有那么多对象呢？

这个时候我们通过jstat运行的时候就观察到一个现象，就是老年代里的内存占用在系统运行的时候，不知道为什么系统运行着运行着，就会突然有几百MB的对象占据在里面，大概有五六百MB的对象，一直占据在老年代中

大家看下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56646000_1578303858.cn/txdocpic/0/b5f28da60d71dd64ee40ed5f3c90d34b/0)       

正是因为系统运行的时候，不知道为什么突然有有几百MB对象进入老年代中，所以才导致Young GC偶尔一次让几十MB对象升入老年代，平均30分钟左右就会触发一次Full GC！！！

那么我们就很奇怪了，为什么系统运行着会突然有几百MB的对象进入老年代？

答案已经呼之欲出了，**大对象！**

一定是系统运行的时候，每隔一段时间就会突然产生几百MB的大对象，直接进入老年代，不会走年轻代的Eden区域。

然后再配合上年轻代还偶尔会有Young GC后几十MB对象进入老年代，所以才会30分钟触发一次Full GC！

大家看如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/67883500_1578303858.cn/txdocpic/0/a6fcc129ee368c8fda0a3b45ce157d9d/0)       



**6、定位系统的大对象**

分析到这里，就很简单了，我们只需要采用之前给大家介绍的jmap工具，通过后台jstat工具观察系统，什么时候发现老年代里突然进入了几百MB的大对象，就立马用jmap工具导出一份dump内存快照。

接着可以采用之前说过的jhat，或者是Visual VM之类的可视化工具来分析dump内存快照

关于Visual VM之类的工具，大家自行百度即可，非常简单易用，其实本质就是让你分析导出的内存快照。

通过内存快照的分析，直接定位出来那个几百MB的大对象，就是几个Map之类的数据结构，这是什么东西？直接让负责写那个系统代码的几个同学分析了一下，明显是从数据库里查出来的！

因为那个系统仅仅就是操作数据库而已，不存在别的什么特殊操作。

然后这个时候也没太好的办法了，直接笨办法，几个人地毯式排查这个系统的所有SQL语句，结果还真的有一个人发现，自己的一个SQL居然在某种特殊的场景下，会类似如下所示：

select * from tbl。

这是啥意思？就是没有where条件！

没有where条件，就代表这个SQL可能会把表中几十万条数据直接全部查出来！

正是因为这个代码层面的bug，导致了每隔一段时间系统会搞出几个上百MB的大对象，这些对象是会全部直接进入老年代的！

然后过一会儿随着偶尔几次Young GC有几十MB对象进入老年代，所以平均几十分钟就会触发一次Full GC！！！

**7、针对本案例的JVM和代码优化**

其实分析到这里，这个案例如何优化已经呼之欲出了！

非常简单，分成两步走

第一步，让开发同学解决代码中的bug，避免一些极端情况下SQL语句里不拼接where条件，务必要拼接上where条件，不允许查询表中全部数据。彻底解决那个时不时有几百MB对象进入老年代的问题。

第二步，年轻代明显过小，Survivor区域空间不够，因为每次Young GC后存活对象在几十MB左右，如果Survivor就70MB很容易触发动态年龄判定，让对象进入老年代中。所以直接调整JVM参数如下：

-Xms1536M -Xmx1536M -Xmn1024M -Xss256K -XX:SurvivorRatio=5 -XX:PermSize=256M -XX:MaxPermSize=256M  -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=92 -XX:+CMSParallelRemarkEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC

直接把年轻代空间调整为700MB左右，每个Surivor是150MB左右，此时Young GC过后就几十MG存活对象，一般不会进入老年代。

反之老年代就留500MB左右就足够了，因为一般不会有对象进入老年代。

而且调整了参数“-XX:CMSInitiatingOccupancyFraction=92”，避免老年代仅仅占用68%就触发GC，现在必须要占用到92%才会触发GC。

最后，就是主动设置了永久代大小为256MB，因为如果不主动设置会导致默认永久代就在几十MB的样子，很容易导致万一系统运行时候采用了反射之类的机制，可能一旦动态加载的类过多，就会频繁触发Full GC。

这几个步骤优化完毕之后，线上系统基本上表现就非常好了，基本上每分钟大概发生一次Young GC，一次在几十毫秒；

Full GC几乎就很少，大概可能要运行至少10天才会发生一次，一次就耗时几百毫秒而已，频率很低。

**8、今日小思考题**

今天这个案例的特点，是搭配上了大对象的问题排查，当你发现Young GC过后并不是每次都有很多存活对象进入老年代的时候，就得从别的角度考虑一下到底为什么会有那么多的对象进入老年代了。

希望大家可以自己结合这个案例的思路，动手画画图，对这个案例进行一定的分析和推导，自己动手做一下分析，一定可以让你i积累出来JVM优化的思路和经验的！

同样老规矩，对文章有什么疑问，或者自己对思考题的答案，都可以在评论区留言，我会逐一回复大家。

**End**

### 061、案例实战：电商大促活动下，严重Full GC导致系统直接卡死的优化实战

今天这个案例因为是延迟了一天发布在周末，而且周末很多同学一般都会出去玩儿，所以我们今天发一个比较简单轻松的优化小案例，让大家一下子就能看完的，不烧脑子的，对应了一个小的JVM优化的知识点。

案例是这样，有一次一个新系统上线，平时都还算正常，结果有一次大促活动的时候，这个系统就直接卡死不动了

大家注意，是直接卡死不动！也就是说，所有请求到这个系统就直接卡住无法处理，无论如何重启这个系统都没任何效果。 

这个时候我们当然会想，是不是按照之前的思路，一点一点去分析JVM的GC问题，考虑是不是过于频繁的GC问题导致了系统被卡死？ 

那当然是会按照之前的思路去分析的，首先使用jstat去看一下系统运行情况，令人吃惊的事情是：JVM几乎每秒都执行一次Full GC，每次都耗时几百毫秒。

我们当时就惊呆了，为什么每秒都有一次Full GC？ 

结果更加令人吃惊的事情还在后面：我们通过jstat看了一下JVM各个内存区域的使用量，基本都没什么问题，年轻代对象增长并不快，老年代才占用了不到10%的空间，永久代也就使用了20%左右的空间

那。。。为什么会频繁触发Full GC呢？ 

这个时候我们立马想到了一个问题，是不是有人在系统里写了一行致命的代码：**System.gc()** 

这个“System.gc()”可不能随便瞎写，他每次执行都会指挥JVM去尝试执行一次Full GC，连带年轻代、老年代、永久代都会去回收。

所以我们立马找到那个系统负责开发的同学，让他排查了一下自己的代码。 

其实说是排查，无非就是在开发IDE中开启代码全局搜索找一下是否有“System.gc()”，结果没想到还真的找到了！ 

所以致命的问题就在这里，当时这个同学写这行代码想的特别的天真和可爱，出发点也是好的

他是这么考虑的，大家可以看看他当时的思路：他在代码里经常会一下子加载出来一大批数据，一旦请求处理完毕之后，他就觉得，一大批数据就废弃不用了，占据内存太多了，完全可以主动用“System.gc()”代码触发一次GC，把他们回收掉！ 

结果呢，平时系统运行时，访问量很低，基本还不会出大乱子！但是在大促活动的时候，访问量一高，立马由“System.gc()”代码频繁触发了Full GC，导致了这个系统直接被卡死了！ 

原来关键点就在于这里！ 

这个案例很短，也很简单，发在周末，让大家轻松看一看，不太烧脑，这周之前的几个案例，都略微有点烧脑，而且都在4000字以上，甚至需要大家看两遍才能理解其中的内容和精髓

这篇文章轻松一点，就带给大家一个小的知识点，相信大家看着也很容易。 

所以，针对这个问题，一方面大家平时写代码的时候，不要自己使用“System.gc()”去随便触发GC，一方面可以在JVM参数中加入这个参数：-XX:+DisableExplicitGC。这个参数的意思就是禁止显式执行GC，不允许你来通过代码触发GC。 

所以推荐大家将“**-XX:+DisableExplicitGC**”参数加入到自己的系统的JVM参数中，或者是加入到公司的JVM参数模板中去。避免有的开发工程师好心办坏事，代码中频繁触发GC就不好了。

**End**

### 062、第9周作业

复习题，自己总结本周生产案例的问题定位、原因分析以及解决思路！

### 063、第9周答疑以及学员思考题总结汇总

**问题：**

老师，我这边发生第一次Full GC的时间是在老年代占满50M的时候，请问是为什么？还是说jstat打印出来的第一个FGC=1不算的。。我理了几个会触发FullGC的条件都不成立，第二次Full GC就能理解。

**回答：**

是的，第一次full gc是不太靠谱的，其实没太大必要，可以忽略掉他，关注第二次开始就好了

**问题：**

老师，stw的时候，系统停止，请求发生阻塞。那么老年代比如stw时间比较长，阻塞了很多请求。等这一次垃圾回收完之后，被阻塞的请求开始处理，又会创建很多对象，对jvm造成压力，然后老年代又要gc。所以stw时间久的话，会变相的给系统制造更高的并发，我的理解对吗？

**回答：**

也可以这么来理解，因为阻塞了很多请求，确实会造成瞬时处理请求过多

**问题：**

老师想请问下,2核4G，4核8G的linux机器，能够分配给堆内存的大小最大能有多少呢?

**回答：**

一般不能给到最大，操作系统和其他进程都要占用一些，比如4G的机器，给JVM个2G~3G，8G的机器，给JMV个4G左右，就差不多了

**问题：**

1.老师，g1对于大对象的判定规则是超过region的50%，有参数可以指定超过60%吗，还是通过参数指定region大小从而来指定大对象大小？ 

2.老师，这样理解对吗，parnew回收器和serial回收器的步骤是一样的，只不过parnew是并行的，g1的younggc也跟它们一样，不过它可以指定停顿时间，这中间是串行还是并行呢

**回答：**

1、g1有参数可以控制大对象的，但是建议不要改变 

2、对的，你理解没错，其实都是类似的，多线程的话是并发回收的

**问题：**

像照片这类的图片我们一般都存在阿里云的OSS上，在数据库里只保存一个地址，这样的话个人主页只是文字和连接，最多也就1k吧，请问案例中为啥个人主页的数据会有几MB？

**回答：**

其实文章里解释了，个人主页除了图片还有大量文字，就是每天发的心情说说之类的东西

**问题：**

有一个有趣的现象，为什么 -XX:CMSFullGCsBeforeCompaction=5是大部分公司的设置呢？

我看了下我们公司也是设置为5，据我所知，这个参数模式是0的吧。设置为5这个“规则”是在那本书或者那个文章说出来，然后大家都遵守的吗？感觉是是不是误导了大家。

**回答：**

是的，是有人推荐设置为5，但是其实要考虑一下，如果通过优化之后，让full gc的频率很低，其实就完全可以让他每次full gc之后都compaction一次，那一次fullgc慢点而已，但是不至于大量内存碎片导致下一次fullgc提前到来

**问题：**

老师，访问量和PV这个我可以理解，但是QPS和TPS就区分不出来了。百度了一下，感觉说的都一样，太官方了，看不明白。您可以大白话详细说说QPS和TPS区别吗？ 用压测工具对一个接口压测，出来的是QPS还是TPS

**回答：**

压测工具的是QPS，就是你每秒多少个请求，TPS是每秒的事务量，这个一般用于数据库层面，就是数据库每秒多少个事务

**问题：**

测试服务器，1G的新生代，1G老年代，E区和S区6:2，项目启动的时候，进行了6次YGC,4次FGC，感觉太难优化了

**回答：**

启动就多次gc，一般就是启动的时候系统内置对象太多，只能考虑增加机器内存了，分配更大内存空间

**问题：**

如果用G1回收，它选择回收一部分region，会不会在老年代中对象块与块之间存在空缺部分（块1和块3之间的块2被回收了），下次在分配对象时发现块与块之间的空隙部分放不下新对象（这空隙部分算不算碎片）？

**回答：**

是选择一部分的region，不是选择region中的一部分对象来回收，所以g1是没有碎片问题的。任何一个region回收的时候，都是复制算法，他会把region里的存活对象拷贝到其他region区，然后直接清空这个reigon剩余的垃圾对象。

**问题：**

分析了下我们的系统，服务器不接受任何请求的情况下，大概16分钟左右1次YGC，1.5天左右1次FGC，每次YGC大概8M左右对象进入老年代。但是服务器每次启动都会有3-4次FGC，不知道怎么样去优化

**回答：**

服务器启动的时候很多内置对象，初始化之类的，那个不需要优化，核心是运行期间的优化

**问题：**

例子还是忽略了200M大小大于了S区百分之五十，从而下一次minor gc触发后，会直接把200M放入old的情况吧？

**回答：**

不是的，当时是举例子，你可以把Survior认为是调整更大一些，不触发动态年龄判定

**问题：**

老师，QPS跟TPS的关系是什么?是不是可以说一个TPS可能包含对个QPS

**回答：**

其实一般系统都说QPS，TPS一般用于数据库里的概念，数据库层面每秒多少个事务

**问题：**

所以说高并发加慢处理是导致full gc的元凶之一，从而带来更多的性能问题

**回答：**

对的，其实后续大量的案例都是围绕这个中心点来展开的，反复用大量的案例把最根本的jvm优化原理强化到你们的大脑里去

**问题：**

老师，你有遇到过压测系统的时候，响应时间很慢，但是此时单独打开浏览器访问却很快的情况吗？这个是什么道理，请教下，谢谢

**回答：**

压测的时候慢是因为最高负载把你的机器资源打满了，包括网络和CPU，此时当然系统响应速度变慢了

**问题：**

请教一下 G1相对其他回收器有什么劣势吗，还是说很多地方不用G1只是因为没必要呢

**回答：**

G1未来会成为一个默认的垃圾回收器，好处就是只要指定一个垃圾回收的停顿时间，就可以让g1自动优化了，你一般不用过渡优化。

坏处是你没法精准把控内存分配、使用、垃圾回收，所以有的时候优先使用cms+parnew，如果没问题就不一定用g1。

**问题：**

这些心情啥的我感觉也是是不是也应该分页展示，比如先加载十条，下拉再加载十条这样，感觉这样一次好像也就几k就够了

**回答：**很多时候他会一次性加载出来一大批，那也是够多的了

**问题：**

老师，大堆使用G1 但是设为了不影响系统性能，g1设置了回收允许停顿的时间，每次回收不多少内存， 也就是腾出来的内存并不多，这样会不会提现不出大堆的优势

**回答：**

不会的，大堆你可以尽情的用，但是每次回收一小部分而已，保证垃圾回收不要影响太大

**问题：**

CMS不是扫描老年代的对象吗？那在重新标记之前进行一次YGC，YGC回收年轻代的对象，对提升重新标记的性能有所帮助吗？

**回答：**

CMS扫描老年代的对象是没错的，但是有的时候年轻代和老年代之间的对象有引用关系呢？是不是就会扫描到年轻代去了？

所以提前young gc一次，可以清理掉一些年轻代对象，是有助于提升CMS的重新标记阶段的性能的

**问题：**

不同的机器配置应该是给出不同的jvm参数的 

1、JVM内存超过4G且对系统响应时间敏感的是不是应该采用G1？ 

2、对高并发，容易产生阻塞的系统，是不是考虑减小SurvivorRatio的值？这样可以给S区分配更大的空间，避免短命的对象进入老年代。这样可能会导致YGC会更频繁些，但YGC很快，关系不太大

**回答：**

1、没错，作为架构师或者team leader应该在团队内或者公司内推行统一的jvm参数模板 

2、超过4G还不至于必须用G1，一般超过16G以上的机器可以考虑用G1，普通的机器都是2C4G，或者4C8G的，哪怕是CMS+ParNew也没问题的 

3、高并发、大数据量的系统，建议是让系统负责人根据实际情况去优化各种参数，具体方法参加之前我们讲解的那套理论和思路即可

**学员总结：**

对并发高且核心的系统，还是应该采用之前文章讲授的方法：预估系统的并发量 ---选择合适的服务器配置 ---根据JVM内存模型---设定初始参数---对系统进行压测---观察高峰期对象的增长速率 、GC频率、GC后的存活

然后根据实际的情况来设定jvm参数 --- 最后做好线上jvm监控 。 我能想到的就这么多，请老师补充

**回答：**对的，核心系统一定是要走一整套思路来优化JVM参数的

**问题：**

学到这里我有个想法，就是把老师讲授的优化思路做成一个监控系统，系统通过jstat获取数据，然后给出jvm参数设置建议值，并给出这么设置的理由。不知道有没有这个必要？我感觉还挺好的

**回答：**

其实一般没必要，用一个统一的jvm参数模板就能解决80%的系统问题了，一般合理的参数可以让普通系统都没什么JVM的性能问题。但是如果要做一个类似的工具也没问题的

**学员总结：**

打卡。以前没有注意有个倒序的按钮，每次都要下拉好久，经过老师点评，今天进来就倒排，很方便。

今日思考题来说，目前没有通用设置JVM模板，基本都是默认参数，可能是用户量并不大的缘故吧。不过学习这个专栏以后，可以注意很多细节，学习的知识到下个公司就成了救火队长了，哈哈哈。

**回答：**

非常好，你其实可以根据你们公司各个系统的情况，去线上看一看，尽量站在架构师的角度给出一个较为合理的JVM参数模板，这样无论是对于实际的工作还是跳槽面试都是很有好处的

**问题：**

CMSScavengeBeforeRemark这个参数本意是希望在CMS GC remark之前做一次YGC，正常情况下其实是会做一次YGC的

这个参数的好处是如果YGC比较有效果的话是能有效降低remark的时间长度，可以简单理解为如果大部分新生代的对象被回收了，那作为根的部分少了，从而提高了remark的效率

**回答：**对的，理解正确

**问题：**

动态年龄判断是超过了Survivor区域的50%.请问这区域是指两个Survivor的50%，也就是一个Survivor的内存容量还是一个Survivor的50%？

**回答：**是指代的一个Survivor区域的50%

**问题：**

可能年轻代的某个 GC Root，它引用了老年代的某个对象，这个对象就不能清除，所以CMS应该也要扫描年轻代GC Root，再进行一次YGC就可以减少扫描的年轻代GC链路。

另外G1基于Region收集，通过RememberSet记录引用关系来避免全堆扫描。 不知道我解答的对不对，老师看到的话麻烦补充下。

**回答：**解答的基本没什么问题，很好

**学员总结：**

到了这里和以前的零散经验链接起来了。由于CMS remark阶段需要扫描新生代(原因太长，不说了)，所以整个堆中数量会影响remark阶段的耗时，所以remark之前添加一次可中断的并发预清理(其实就是继续执行并发标记)

另外为了防止并发预清理阶段等太久都不发生young gc，提供了CMSMaxAbortablePrecleanTime 参数来设置等待多久没有等到young gc就强制remark。默认是5s。

但是最终一劳永逸的办法是，添加参数CMSScavengeBeforeRemark，让remark之前强制Minor GC

**回答：**是的，总结的非常好

**问题：**

为什么重标记的时候提前做一次young gc会提高效率？重标记不是只针对老年代的对象进行标记的吗？就算young gc减少了部分对象，重标记也不会去新生代里查找的呀

**回答：**

老年代扫描的时候要确认老年代里哪些对象是存活的，这个时候必然会扫描到年轻代，因为有些年轻代的对象可能引用了老年代的对象，所以提前做young gc可以把年轻代里一些对象回收掉，减少了扫描的时间，可以提升性能

**问题：**

老师看到这里我又产生很多问题了

1.文章中提到大量matedata的软引用经过young gc就回收了。之前我们都很清楚young gc,old gc的触发条件，那么matedata什么时候触发呢？

2.文中的案例中，虽然反射会不断的创建各种奇怪的类，难道每次调反射都会创建不一样的奇怪类吗？如果不是的话，他直接引用已经创建过的奇怪类，也不至于内存一直暴涨啊。

3.这是一个比较非常规的案例，老师对于这种非常规的情况，我们的分析思路与之前要有所改变吗？

**回答：**

1、metadata区域的gc就是他自己满了就触发 

2、从jdk底层实现机制而言，他会创建各种不同的类 

3、其实非常规案例，你们也得了解一下，但是排查思路没什么变化，看文章里的思路，其实还是一步一步去排查的

**问题：**

请问老师，每次Full GC都会回收永久代么？还是说永久代满触发的FullGC才会回收永久代？1.8前的永久代和1.8的Metadata区域的回收时机是一样的么？

**回答：**

无论是old区满了触发的full gc，还是metadata区域满了触发的full gc，都会执行metadata区域的gc

**问题：**

老师，动态生成的类是什么类加载器加载的呀？是系统类加载器吗，是的话，那之前不是说一个class的回收需要加载他的类加载器已经回收了，这个时候系统类加载器是回收了吗？

**回答：**对的，已经回收了

**问题：**

打卡。今天这个案例应该是反射生产的软引用类在触发young gc时实例对象被立刻回收了，而方法区的类对象还在，下次再反射调用的时又得重新加载这个类对象并在年轻代实例化，这样就导致方法区同样的类对象越来越多，很快就需要进行垃圾回收。是这样吗？

如果不是这样那方法区为什么会一直在增长？这些增长的类对象都是不同反射调用的产生的吗？好多疑问。

**回答：**感觉你可能还是没完全理解透彻，可以把本文多看两遍，他其实讲的是对类的回收，类自己本身就是一个Class对象

**问题：**

打卡 本篇内容应该讲的是因为设置了这个参数 ，导致young gc时直接回收了大部分反射时jvm创建的软引用对象 ，导致下一次调用反射继续创建类 和class 。而class被放在元空间，导致元空间很快就满了，接着就是full gc

**回答：**对的，你理解的完全正确

**学员实践总结：**

非常感谢老师的回复。排查了survivor 区放不下的原因，并把 survivor 加大了内存，然后从新捋了一下问题

项目刚启动之后对象都在 Eden区( 很大一部分是永久存活的对象 )，压测之后永久存活的对象达到条件进入老年代了

老年代我分了20M，发生第一次FullGC 之后老年代基本满了(基本都是永久存活的对象，快放不下的感觉)

后面继续触发所有的 MinorGC 时，符合的条件: 老年代可用空间小于历次进入老年代总对象平均大小，所以导致一直是 FullGC

针对这个问题，我把老年代设置成了 100M(足够存永久存活的对象了), 继续压力测试，对象正常走 E-S0-S1 了，使用 jstat 看情况，感觉没有10天半个月都不会触发 fullgc 了。

**回答：**非常好，活学活用，直接就优化上自己的系统了

**问题：**

老师，这个系统运行逻辑是这样吗。比如我点击某个用户的个人主页，如果在redis缓存中存在，则直接返回，如果不存在，则去数据库请求，这个过程会创建一系列对象，多达几m，然后放到redis缓存去。

由于方法生成这些对象后出栈，从而让堆对象成了垃圾对象，以此类推来塞满堆从而触发gc。这样理解对吗老师

**回答：**

是的，就是你理解的这个样子

**问题：**

这篇看了几遍还是比较疑惑： 

1、正常加载的class与文章所说通过反射加载的class，全部都是软引用吗？ 

2、“奇怪的Class对象”是指XXX.class？还是类似GeneratedSerializationConstructorAccessor.class？ 

3、Young GC为什么也会带着回收metaspace里的class对象？ 

4、即使gc后反复重新创建，不是应该只创建不存在的class对象吗？也不会超过最开始的100啊？难道会重复创建相同的class对象？

**回答：**

1、这里仅仅说的是JVM自己生成的Class是软引用 

2、是指那个很长名字的类，其实除了那个类以外还有别的类似的生成的奇怪的类 

3、GC的时候会去检查软引用这种特殊的对象，如果满足条件就会回收，那些Class都是软引用的 

4、对的，因为JDK源码里的实现有一些问题，所以导致并发环境下会重复创建一些Class

**问题：**

老师，这个例子是不是理解下来就是对full gc由metaspace触发的场景讲解。使得我们明白full gc触发不仅仅只是堆空间的old区域不足导致。而原理都是一样，都是对应的区域放满了，放不下了新的对象触发full gc。只是这里的情况是反射导致的对class对象软引用是引用的在非堆的方法区？

如理解有误，望老师指出，指正，谢谢

**回答：**对的，你理解的没错

**问题：**

老师好，后面自己试了下，补了下课。简单来说是不是可以理解为，对于反射的大量，多次调用，会因为nativemethodaccessor的次数影响，默认15次，从而生成最终的generateXXXaccessorXXX的类

这个类的作用在于反射的方法调用转化为本地调用，提升性能。但对于该类的method方法的软引用被回收了，所以导致元数据区多次生成相同的类。导致full gc。感觉这块讲不清楚了。。。

**回答：**其实你理解的没问题，就是这个意思

**学员总结：**

初始标记：标记由4种gc roots直接关键的对象。 

并发标记：对老年代所有的对象进行trace，看是否能与gc roots建立间接关系，我就是gc roots是否可达。 

最终标记：标记并发标记阶段引用变动的对象。 

并发清理：并发清理掉可回收的内存，但是因为用户线程依旧在运行，所以每次full gc都会清理不干净，产生浮动垃圾。 

**回答：**总结的很好

**问题：**

你好，软引用对象的回收，为什么会导致下一次调用反射会继续创建类呢，这里的class放在元空间是说的class还是class对象?

**回答：**Class就是Class对象，具体为什么会因为软引用回收之后继续有新的Class生成，其实是JDK内部本身的一个缺陷，他会在这个场景下生成很多的Class。

具体JDK源码我没分析，因为那个太繁琐，大家其实在这里记住反射场景下的这个问题即可

**问题：**

请问一下dmp打印的内容怎么分析,我用jhat命令查看得到的是下面这样的结果,对象后面只有一个地址,没有显示对象大小,点进去看一个对象的详情也没有显示对象的大小

而且新生代和老年代的对象都混在一起, 那从哪些信息可以判断老年代的哪个对象占用的空间最大

**回答：**jhat是显示出来对象数量和大小的，后面案例中我们会分析MAT、Visual VM之类的更好的工具，别着急

**学员总结：**

1.分析机器情况（机器配置，堆内存大小，运行时长，FullGC次数、时间，YoungGC次数、时间） 

2.查看具体的jvm参数配置 

3.然后根据JVM参数配置梳理出JVM模型，每个区间的大小是多少，画出来JVM模型（考虑每个设置在申请情况下会执行GC） 

4.结合jstat查看的GC情况，在结合JVM模型进行二次分析 

5.jmap dump内存快照，通过jhat或者Visual VM之类的工具查看具体的对象分类情况 

6.根据分析的情况再具体到问题（Bug、或者参数设置等问题） 

7.修复Bug，优化JVM参数

**回答：**总结的非常好

### 064、案例实战：一次线上大促营销活动导致的内存泄漏和Full GC优化

**1、线上故障场景**

先简单说一下业务背景：一次我们线上推了一个大促销活动，大致就是类似于在某个特定节日里，突然给所有用户发短信、邮件、APP Push消息，说现在有个特别优惠的活动，如果参与的话肯定可以得到很大的实惠！

这类大促活动一般都会吸引比平时多几倍的用户短时间内突然登录APP来参与，所以系统一般在这个时候压力会比平时大好几倍。

但是因为从系统的整体设计角度而言，其实给的一些数据库、缓存和机器的资源都是足够的，所以通常而言不该有什么问题。

但是那次大促活动开始之后，直接导致线上一个系统的CPU使用率飙升，而且因为CPU使用率太高，导致系统几乎陷入卡死的状态，无法处理任何请求！

在重启系统之后，会好一段时间，但是很快又立马发现机器的CPU使用率飙升，继续导致系统卡死！

这就是那次大促活动开始之后，那个系统在线上的一个真实的情况。

有人可能会问，那么CPU使用率是怎么观察到飙升的？怎么收到报警的？

其实这个之前已经说过很多次了，中大型公司都会有Zabbix、Open-Falcon、Prometheus之类的监控和告警系统，一旦机器的CPU使用率过高，会直接发送报警给你的短信、邮箱和IM工具（比如钉钉）

所以上面说的大促活动开始之后，某个线上系统的CPU使用率飙升，其实就是得到了报警才知道的，然后在监控系统上还可以去观察CPU的负载曲线，是一个折线图，可以看到CPU负载很高。

**2、初步排查CPU负载过高的原因**

这里给大家说一下线上系统的机器CPU负载过高的两个常见的场景。

第一个场景，是你自己在系统里创建了大量的线程，这些线程同时并发运行，而且工作负载都很重，过多的线程同时并发运行就会导致你的机器CPU负载过高。

第二个场景，就是你的机器上运行的JVM在执行频繁的Full GC，Full GC是非常耗费CPU资源的，他是一个非常重负载的过程

所以一旦你的JVM有频繁的Full GC，带来的一个明显的感觉，一个是系统可能时不时会卡死，因此Full GC会带来一定的“Stop the World”问题，一个是机器的CPU负载很高。

所以一旦知道CPU负载过高的两个原因，就很容易进行排查了。

大家完全可以使用排除法来做，首先看一下JVM Full GC的频率，通过jstat也好，或者是监控平台也好，很容易看到现在Full GC的频率。如果Full GC频率过高，那么就是Full GC引起的CPU负载过高。

那么如果JVM GC频率很正常呢？那就肯定是你的系统创建了过多线程在并发执行负载很重的任务了！

所以当时我们直接通过监控平台就可以看到，JVM的Full GC频率突然变得极为频繁，几乎是每分钟都有一次Full GC。

大家都知道，每分钟一次Full GC，一次至少耗时几百毫秒，这个系统性能绝对很糟糕，而且对机器的CPU负载也是很高的！

既然发现了频繁Full GC了，那肯定就不用去怀疑是系统自己创建过多线程了！

**3、初步排查频繁Full GC的问题**

大家通过之前大量的案例和文章已经初步可以得到结论，如果有频繁Full GC的问题，一般可能性有三个：

1. 内存分配不合理，导致对象频繁进入老年代，进而引发频繁的Full GC；
2. 存在内存泄漏等问题，就是内存里驻留了大量的对象塞满了老年代，导致稍微有一些对象进入老年代就会引发Full GC；
3. 永久代里的类太多，触发了Full GC

当然还有之前案例说过，如果上述三个原因都不存在，但是还是有频繁Full GC，也许就是工程师错误的执行“System.gc()”导致的

但是这个一般很少见，而且之前讲过，JVM参数中可以禁止这种显式触发的GC。

所以一般排查频繁Full GC，核心的利器当然是jstat了，之前我们有大量文章带大家做过jstat分析JVM的实战，这里就不赘述了。

当时我们用jstat分析了一下线上系统的情况，发现并不存在内存分配不合理，对象频繁进入老年代的问题，而且永久代的内存使用也很正常，所以上述三个原因中的两个就被排除掉了。

**那么我们来考虑最后一个原因：老年代里是不是驻留了大量的对象给塞满了？**

对，当时系统就是这个问题！

我们明显发现老年代驻留了大量的对象，几乎快塞满了，所以年轻代稍微有一些对象进入老年代，很容易就会触发Full GC！而且Full GC之后还不会回收掉老年代里大量的对象，只是回收一小部分而已！

所以很明显老年代里驻留了大量的本不应该存在的对象，才导致频繁触发Full GC的。接下来就是要想办法找到这些对象了

之前我们介绍过jmap+jhat的组合来分析内存里的大对象，今天我们介绍另外一个常用的强有力的工具，**MAT**。

因为jhat适合快速的去分析一下内存快照，但是功能上不是太强大，所以一般其实常用的比较强大的**内存分析工具**，就是MAT。

**4、对线上系统导出一份内存快照**

既然都发现线上系统的老年代中驻留了过多的对象的问题，那么肯定要知道这些对象是谁！

所以先用jmap命令导出一份线上系统的内存快照即可：

jmap -dump:format=b,file=文件名 [服务进程ID]

拿到了内存快照之后，其实就是一份文件，接着就可以用jhat、MAT之类的工具来分析内存了。

**5、MAT是如何使用的？**

不少人是通过Eclipse集成的MAT插件来使用的，但是很多人其实开发是用IntelliJ IDEA的，所以这个时候可以直接下载一个MAT来使用即可

给大家官网的下载地址：

https://www.eclipse.org/mat/downloads.php

在这个地址中，就可以下载MAT的最新版本了。

大家选择自己的笔记本电脑的操作系统对应的版本就可以了，他是支持Windows、Mac、Linux三种操作系统的。

下载好MAT后，在他的安装目录里，可以看到一个文件名字叫做：MemoryAnalyzer.ini

这个文件里的内容类似如下所示：

-startup

../Eclipse/plugins/org.eclipse.equinox.launcher_1.5.0.v20180512-1130.jar

--launcher.library

../Eclipse/plugins/org.eclipse.equinox.launcher.cocoa.macosx.x86_64_1.1.700.v20180518-1200

-vmargs

-Xmx1024m

-Dorg.eclipse.swt.internal.carbon.smallFonts

-XstartOnFirstThread

大家务必要记得，如果dump出来的内存快照很大，比如有几个G，你务必在启动MAT之前，先在这个配置文件里给MAT本身设置一下堆内存大小，比如设置为4个G，或者8个G，他这里默认-Xmx1024m是1G。

接着大家直接启动MAT即可，启动之后看到的界面中有一个选型是：Open a Heap Dump，就是打开一个内存快照的意思，选择他，然后选择本地的一个内存快照文件即可。

**6、基于MAT来进行内存泄漏分析**

使用MAT打开一个内存快照之后，在MAT上有一个工具栏，里面有一个按钮，他的英文是：Leak Suspects，就是内存泄漏的分析。

接着MAT会分析你的内存快照，尝试找出来导致内存泄漏的一批对象。

这时明显可以看到他会显示给你一个大的饼图，这里就会提示你说，哪些对象占用内存过大。

这个时候直接会看到某种自己系统创建的对象占用量过大，这种对象的实例多达数十万个，占用了老年代一大半的内存空间。

接着当然是找开发工程师去排查这个系统的代码问题了，为什么会创建那么多的对象，而且始终回收不掉？

**这就是典型的内存泄漏！**即系统创建了大量的对象占用了内存，其实很多对象是不需要使用的，而且还无法回收掉。

后来找到了一个原因，是在系统里做了一个JVM本地的缓存，把很多数据都加载到内存里去缓存起来，然后提供查询服务的时候直接从本地内存走。

但是因为没有限制本地缓存的大小，并且没有使用LRU之类的算法定期淘汰一些缓存里的数据，导致缓存在内存里的对象越来越多，进而造成了内存泄漏。

解决问题很简单，只要使用类似EHCache之类的缓存框架就可以了，他会固定最多缓存多少个对象，定期淘汰删除掉一些不怎么访问的缓存，以便于新的数据可以进入缓存中。

**7、今日文章总结**

之前给大家讲过，我们的文章会不停的在后面的案例中重复一些之前案例里的内容，原因是学习周期很长，通过这种方式可以定期帮助大家复习和总结。

这篇文章就给大家分析总结了一下之前学习过的几种频繁Full GC的原因，以及分析的方法和思路，这个可以看做是一个复习性的文章。

同时给大家初步介绍了一下MAT这种内存分析的工具，其使用是非常简单的，里面有很多的功能。

而今天之所以没有给大家很多截图，一步一步教大家来用这个工具，就是希望给大家课后留一个作业。

今天的作业就是，希望大家可以自己动手玩一玩MAT，你可以自己运行一段代码，模拟生成一种对象特别多的实例，然后导出一份内存快照，基于MAT来分析一下，就可以看到他是如何清晰的告诉你系统中哪种对象实例过多了！

另外，之后我们会有更多案例，我会带着大家一步一图，使用MAT、Visual VM等工具来深度分析JVM的内存快照，找到一些内存泄漏的问题，后续都会有，咱们一步步来，敬请期待。

**End**

### 065、案例实战：百万级数据误处理导致的频繁Full GC问题优化

**1、事故场景**

有一次一个线上系统进行了一次版本升级，结果升级过后才半小时，突然之间收到运营和客服雪花般的反馈，说这个系统对应的前端页面无法访问了，所有用户全部看到的是一片空白和错误信息。

这个时候通过监控报警平台也收到雪花般的报警，发现线上系统所在机器的CPU负载非常高，持续走高，甚至直接导致机器都宕机了。所以系统对应的前端页面当然是什么都看不到了。

**2、CPU负载高原因分析**

上篇文章已经给大家总结过CPU负载高的原因了，这里我们直接说结论，看了一下监控和jstat，发现Full GC非常频繁，基本上两分钟就会执行一次Full GC，而且每次Full GC耗时非常长，在10秒左右！

所以直接入手尝试进行Full GC的原因定位。

**3、Full GC频繁的原因分析**

上篇文章也给大家总结过Full GC频繁的几种常见原因了，其实分析Full GC频繁的原因，最好的工具不是监控平台，就是最实用的jstat工具，直接看线上系统运行时候的动态内存变化模型，什么问题都立马出来了。

基于jstat一分析发现了很大的问题，当时这个系统因为主要是用来进行大量数据处理然后提供数据给用户查看的，所以当时可是给JVM的堆分配了20G的内存，其中10G给了年轻代，10G给了老年代，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/90443900_1578303867.cn/txdocpic/0/44bb9d1ba2f981ebc546e8b7a4b49418/0)   

这么大的年轻代，结果大家能猜到jstat看到什么现象吗？Eden区大概1分钟左右就会塞满，然后就会触发一次Young GC，而且Young GC过后有几个GB的对象都是存活的会进入老年代！

如下图所示。     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/3986300_1578303868.cn/txdocpic/0/8050e38ea4b660d31b3585c3ad3d6918/0)       

这说明什么？这说明系统代码运行的时候在产生大量的对象，而且处理的极其的慢，经常在1分钟过后Young GC以后还有很多对象在存活，才会导致大量对象进入老年代中！这真是让人非常的无奈。

所以就是因为这个内存运行模型，才导致了平均两分钟就会触发一次Full GC，因为老年代两分钟就塞满了，而且老年代因为内存量很大，所以导致一次Full GC就要10秒的时间！

大家想象一下，系统每隔2分钟就要暂停10秒，对用户是什么感觉！

更有甚者，普通的4核机器根本撑不住这么频繁，而且这么耗时的Full GC，所以这种长时间Full GC直接导致机器的CPU频繁打满，负载过高，也导致了用户经常无法访问系统，页面都是空白的！

**4、以前那套GC优化策略还能奏效吗？**

那么大家想一下，以前我们给大家说过的那套GC优化策略此时还能奏效吗？

即把年轻代调大一些，给Survivor更多的内存空间，避免对象进入老年代。

明显不行，这个运行内存模型告诉我们，即使你给年轻代更大空间，甚至让每块Survivor区域达到2GB或者3GB，但是一次Young GC过后，还是会因为系统处理过慢，导致几个GB的对象存活下来，你Survivor区域死活都是放不下的！

所以这个时候就不是简单优化一下JVM参数就可以搞定的。

这个系统明显是因为代码层面有一定的改动和升级，直接导致了系统加载过多数据到内存中，而且对过多数据处理的还特别慢，在内存里几个GB的数据甚至要处理一分多钟，才能处理完毕。

这是明显的代码层面的问题了，其实要优化这个事故，就必须得优化代码，而不是简单的JVM参数！

我们需要避免代码层面加载过多的数据到内存里去处理，这是最核心的一个点。

**5、复杂的业务逻辑，自己都看不懂了怎么办？**

说优化代码，说起来很简单，但是实际做起来呢？

有很多系统的代码都特别的复杂，别说别人看不懂了，自己可能写的代码过了几个月自己都看不懂了！所以直接通过走读代码来分析问题所在是很慢的！

我们必须有一个办法可以立马就定位到系统里到底是什么样的对象太多了占用了过多的内存，这个时候就得使用一个工具了：**MAT**

这个工具我们上篇文章介绍过，今天给大家深入讲讲他的使用，给出一些界面的截图出来。

**6、准备一段示范用的代码**

现在我们来准备一段示范用的代码，在代码里创建一大堆的对象出来，然后我们尝试获取他的dump内存快照，再用MAT来进行分析。

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17697200_1578303868.png)

这段代码大家可以看到，非常的简单，就是在代码里创建10000个自定义的对象，然后就陷入一个阻塞状态就可以了，大家可以把这段代码运行起来。

**7、获取jvm进程的dump快照文件**

先在本地命令行运行一下jps命令，查看一下启动的jvm进程的PID，如下所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27616600_1578303868.cn/txdocpic/0/78ecb291957d1826cd6e1cfd8df8d5cc/0)       

明显能看到，我们的Demo1这个类的进程ID为1177

接着执行下面的jmap命令可以导出dump内存快照：

jmap -dump:live,format=b,file=dump.hprof 1177

**8、使用MAT分析内存快照**

大家在下一个步骤，务必要注意到，如果是线上导出来的dump内存快照，很多时候可能都是几个GB的

比如我们这里就是8个多GB的内存快照，所以就务必按照上节课所说的，在MAT的MemoryAnalyzer.ini文件里，修改他的启动堆大小为8GB。

接着就是打开MAT软件，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40422200_1578303868.cn/txdocpic/0/c47c43c700c1786b76e41824f3517a77/0)       



然后选择其中的“Open a Heap Dump”打开自己的那个dump快照即可

接着会看到下图的样子，大家如果跟我一样用的是最新版本的MAT，那么打开dump快照的时候就会提示你，要不要进行内存泄漏的分析，就是Leak Suspects Report，你一般勾选是即可。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55402700_1578303868.cn/txdocpic/0/2e9e48b9bb07614c1f096527c8b5d1cc/0)       

接着大家会看到如下的图：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75354800_1578303868.cn/txdocpic/0/1adbf543a5979acc4a8f5ee81904c410/0)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/93820200_1578303868.cn/txdocpic/0/2f5687367037df7f243124d08262afff/0)       



从上面图里可以很清晰的看到了，别人都告诉你了，可能存在的内存泄漏的问题，尤其是第一个问题，“Problem Suspect 1”，他的英文里很清晰的告诉你了，java.lang.Thread main，就是说main线程，通过局部变量引用了占据24.97%内存的对象。

而且他告诉你那是一个java.lang.Object[]数组，这个数组占据了大量的内存。

那么大家肯定想要知道，到底这个数组里是什么东西呢？

大家可以看到上面的“Problem Suspect 1”框框的里面最后一行是一个超链接的“**Details**”，大家点击进去就可以看到详细的说明了。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11634600_1578303869.cn/txdocpic/0/e379fb6eb87cb776ae77e2eba726a039/0)       



通过这个详细的说明，尤其是那个“Accumulated Objects in Dominator Tree”，在里面我们可以看到，java.lang.Thread main线程中引用了一个java.util.ArrayList，这里面是一个java.lang.Object[]数组，数组里的每个元素都是Demo1$Data对象实例。

到此为止，就很清楚了，到底是什么对象在内存里占用了过大的内存。所以大家通过这个小小的范例就可以知道，你的系统中那些超大对象到底是什么，用MAT分析内存是非常方便的。

**9、追踪线程执行堆栈，找到问题代码**

一旦发现某个线程在执行过程中创建了大量的对象之后，就可以尝试找找这个线程到底执行了哪些代码才创建了这些对象

如下图所示，先点击页面中的一个“See stacktrace”，可然后就会进入一个线程执行代码堆栈的调用链：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36323900_1578303869.cn/txdocpic/0/6aaab2d098e3a724e808a6fe4e2b8ef9/0)       



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/50714100_1578303869.cn/txdocpic/0/ce3e0342c2fdedc817671972933e35c0/0)       

在当时而言，我们就是按照这个方法追踪到了线上系统某个线程的执行堆栈，最终发现的是这个线程执行“String.split()”方法导致产生了大量的对象

那么到底是为什么呢？接着我们来分析一下“String.split()”这个方法。

**10、为什么“String.split()”会造成内存泄漏？**

其实原因很简单，当时这个线上系统用的是JDK 1.7。

在JDK 1.6的时候，String.split()方法的实现是这样子的，比如你有一个字符串，“Hello World Ha Ha”，然后你按照空格来切割这个字符串，应该会出来四个字符串“Hello”“World”“Ha”“Ha”，大家应该理解这个吧？

那么JDK 1.6的时候，比如“Hello World Ha Ha”这个原始字符串底层是基于一个数组来存放那些字符的

比如[H,e,l,l,o,,W,o,r,l,d,,H,a,,H,a]这样的数组。然后切割出来的“Hello”字符串他不会对应一个新的数组，而是直接映射到原来那个字符串的数组，采用偏移量表明自己是对应原始数组中的哪些元素。

比如说“Hello”字符串可能就是对应[H,e,l,l,o,,W,o,r,l,d,,H,a,H,a]数组中的0~4位置的元素。

但是JDK 1.7的时候，他的实现是给每个切分出来的字符串都创建一个新的数组，比如“Hello”字符串就对应一个全新的数组，[H,e,l,l,o]。

所以当时那个线上系统的处理逻辑，就是加载大量的数据出来，可能有的时候一次性加载几十万条数据，数据主要是字符串

然后对这些字符串进行切割，每个字符串都会切割为N个小字符串。

这就瞬间导致字符串数量暴增几倍甚至几十倍，这就是系统为什么会频繁产生大量对象的根本原因！！！

因为在本次系统升级之前，是没有String.split()这行代码的，所以当时系统基本运行还算正常，其实一次加载几十万条数据量也很大，当时基本上每小时都会有几次Full GC，不过基本都还算正常。

只不过系统升级之后代码加入了String.split()操作，瞬间导致内存使用量暴增N倍，引发了上面说的每分钟一次Young GC，两分钟一次Full GC，根本原因就在于这行代码的引入。

**11、代码如何进行优化？**

后来紧急对这段代码逻辑进行了优化，避免对几十万条数据每一条都执行String.split()这行代码让内存使用量暴增N倍，然后再对那暴增N倍的字符串进行处理。

就当时而言，其实String.split()这个代码逻辑是可用可不用的，所以直接去除就行了。

但是如果从根本而言，说白了，还是这种大数据量处理的系统，一次性加载过多数据到内存里来了，所以后续对代码还是做了很多的优化

比较核心的思路，就是开启多线程并发处理大量的数据，尽量提升数据处理完毕的速度，这样到触发Young GC的时候避免过多的对象存活下来。

**12、今日思考题**

今天给大家留一个小作业，可以去自己线上系统dump一个内存快照出来，用MAT练习一下分析自己系统的内存

看看都有哪些较大的对象，什么线程创建的，看看线程执行堆栈，体验一下分析内存的感觉。

**End**

### 066、阶段性复习：JVM运行原理和GC原理你真的搞懂了吗？

**1、阶段性复习**

最近三天会进行一个阶段性的复习，因为我们已经把完整的JVM运行原理、GC原理以及GC优化的原理，还有线上发生GC问题的各种优化案例，都给大家完整的分析完了，所以到这里务必停一停脚步，整理一下学习过的知识脉络，让大家进行一点复习。反复的复习，才能让大家真正吃透和消化掉这些知识。

这几篇文章我们不会把之前文章的东西大幅度的整理出来，主要是给大家提点一些思路，给你一些引导，希望每个人看到这里的时候，都停一停脚步，顺着我们带出来的思路，把过往学习过的知识，自己整理一下。

**2、JVM和GC的运行原理，你都能搞懂了吗？**

对于JVM的学习，首先大家务必要搞清楚一点，JVM是如何运行起来的。

相信大家认真看过之前文章的，应该都清楚一点，JVM的内存区域划分，最核心的就是这么几块了：年轻代、老年代、Metaspace（也就是以前的永久代）。

其中年轻代又分成了Eden和2个Survivor，默认比例是8：1：1，如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/71563200_1575454975.cn/txdocpic/0/bb6655f072e34c41aa772f971d2458f2/0)       

接着我们来思考一下，我们写好的系统会不停的运行，运行的时候是不是就会不停的在年轻代的Eden区域中创建各种对象？

如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/83467800_1575454975.cn/txdocpic/0/7f6fef7647898896bb621af83e9b368f/0)       

而且一般创建对象都是在各种方法里执行的，一旦方法运行完毕，方法局部变量引用的那些对象就会成为Eden区里的垃圾对象，就是可以被回收的状态，大家务必要清楚这个过程

如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/93060500_1575454975.cn/txdocpic/0/0a11a961ea67fee98ddc4e48d5d1f98b/0)       接着随着Eden区不断的创建对象，就会逐步的塞满，当然这个时候可能塞满Eden区的对象里大多数都是垃圾对象。一旦Eden区塞满之后，就会触发一次Young GC。

Young GC会采用复制算法，从GC Roots（方法的局部变量、类的静态变量）开始追踪，标记出来存活的对象。

然后把存活对象都放入第一个Survivor区域中，也就是S0区域，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/4064900_1575454976.cn/txdocpic/0/4b1f88fbd0cde29f7904c8857d5ec053/0)       

接着垃圾回收器就会直接回收掉Eden区里剩余的全部垃圾对象，在整个这个垃圾回收的过程中全程会进入Stop the Wold状态，也就是暂停系统工作线程，系统代码全部停止运行，不允许创建新的对象

只有这样，才能让垃圾回收器专心工作，找出来存活对象，回收掉垃圾对象，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/18679300_1575454976.cn/txdocpic/0/21120a5883b9f8584bea513c00a09054/0)       

一旦垃圾回收全部完毕之后，也就是存活对象都进入了Survivor区域，然后Eden区都清空了，那么Young GC执行完毕，此时系统恢复工作，继续在Eden区里创建对象，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/29192300_1575454976.cn/txdocpic/0/6dd5f6f0f5a6dbca2e6ffca5d2418df3/0)       

下一次如果Eden区满了，就会再次触发Young GC，把Eden区和S0区里的存活对象转移到S1区里去，然后直接清空掉Eden区和S0区中的垃圾对象

当然这个过程中系统是禁止运行的，处于Stop the World状态，如下图所示。

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/43417900_1575454976.cn/txdocpic/0/029ae7686d8cfc2c0092bae085654fce/0)       

负责Young GC的垃圾回收器有很多种，但是常用的就是ParNew垃圾回收器，他的核心执行原理就如上所述，只不过他运行的时候是基于多线程并发执行垃圾回收的，大家只要记得这点就可以。

这就是最基本的JVM和GC的运行原理，大家都搞懂了吗？

**3、对象什么时候进入老年代？**

但是大家觉得光是一块年轻代和Young GC配合起来，就足够JVM来使用了吗？

No！

实际JVM运行过程中，有很多意外的情况会发生的，会导致对象进入老年代区域中，如下所述几种情况，反复给大家总结过，务必要记得很清晰：

1. 一个对象在年轻代里躲过15次垃圾回收，年龄太大了，寿终正寝，进入老年代
2. 对象太大了，超过了一定的阈值，直接进入老年代，不走年轻代
3. 一次Young GC过后存活对象太多了，导致Survivor区域放不下了，这批对象会进入老年代
4. 可能几次Young GC过后，Surviovr区域中的对象占用了超过50%的内存，此时会判断如果年龄1+年龄2+年龄N的对象总和超过了Survivor区域的50%，此时年龄N以及之上的对象都进入老年代，这是动态年龄判定规则

上面4个条件就是最常见的对象进入老年代的情况，那种长期存活的躲过15次Young GC的对象毕竟是少数的，大对象一般在特殊情况下会有，对于那种加载大量数据长时间处理以及高并发的场景，很容易导致Young GC后存活对象过多的。

所以对于这些情况，都会导致对象进入老年代中，老年代对象可能会越来越多，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/ueditor/56011700_1575454976.cn/txdocpic/0/d1de14e8754e8892bf79a5ae76a402e7/0)       

**4、老年代的GC是如何触发的？**

一旦老年代对象过多，就可能会触发Full GC，Full GC必然会带着Old GC，也就是针对老年代的GC

而且一般会跟着一次Young GC，也会触发永久代的GC。

大家还记得Full GC触发的几个条件吗？

1. 老年代自身可以设置一个阈值，有一个JVM参数可以控制，一旦老年代内存使用达到这个阈值，就会触发Full GC，一般建议调节大一些，比如92%
2. 在执行Young GC之前，如果判断发现老年代可用空间小于了历次Young GC后升入老年代的平均对象大小的话，那么就会在Young GC之前触发Full GC，先回收掉老年代一批对象，然后再执行Young GC。
3. 如果Young GC过后的存活对象太多，Survivor区域放不下，就要放入老年代，要是此时老年代也放不下，就会触发Full GC，回收老年代一批对象，再把这些年轻代的存活对象放入老年代中

触发Full GC几个比较核心的条件就是这几个，总结起来，其实就是老年代一旦快要搞满了，空间不够了，必然要垃圾回收一次。

老年代的垃圾回收通常建议走CMS垃圾回收器，回收机制比较复杂，此处建议大家自行复兴和总结一下

总之，Old GC的速度是很慢的，少则几百毫秒，多则几秒。所以一旦Full GC很频繁，就会导致系统性能很差，因为频繁要停止系统工作线程，导致系统看起来一直有卡顿的现象。

而且频繁Full GC还会导致机器CPU负载过高，导致机器性能下降，处理请求能力降低。

所以优化JVM的核心就是减少Full GC的频率。

**5、正常情况下的系统**

正常情况下的系统，会有一定频率的Young GC，一般在几分钟一次Young GC，或者几十分钟一次Young GC，一次耗时在几毫秒到几十毫秒的样子，都是正常的。

正常的Full GC频率在几十分钟一次，或者几个小时一次，这个范围内都是正常的，一次耗时应该在几百毫秒的样子。

所以大家如果观察自己线上系统就是这个性能表现，基本上问题都不太大。

当然，实际线上系统很多时候回遇到一些JVM性能问题，就是Full GC过于频繁，每次还耗时很多的情况，此时就需要一些优化了。

明天我们会继续总结Full GC优化的一些常用手段以及生产环境下的GC优化的方法。

**End**

### 067、阶段性复习：JVM性能优化到底该怎么做？

**0、写在前面的话**

最近有个别同学后台留言问为什么最近的文章的一些顺序和标题，跟大纲里的有点出入，我在这里解释一下。

因为在实际写作的过程中，会把一些文章的顺序做一些交换，比如之前把Week 10里的内容放到了Week 9。

同时Week 9里有一些案例，在写作过程中考虑了一下，感觉有些东西没太大必要了，所以替换成了别的几个案例放在了Week 10里，因此有一些案例的标题上会有一些变更。

另外Week 10里把原定的3个案例替换为了3讲内容用来进行阶段性的复习，这个也是因为发现很多同学对我们专栏里的阶段性定时复习，定时停下脚步做点总结和梳理，非常的好评和认可，因此我们临时在这里加入了3讲内容进行阶段性梳理、总结和复习。

相信大部分人都认可一个观点：学习不是不停的学习新的内容，更重要的是在学新内容的同时，要经常性的停下脚步来复习之前的内容，这样反复的周期性的复习和总结，不断的重复和强化一些核心的东西，最后在三四个月的专栏学习完毕之后，才能彻底吃透和消化掉这块知识。

这就是我对原定大纲做出一些顺序变换以及少数案例替换为阶段性复习的原因和说明，希望大家理解我的良苦用心，好好学新内容，也好好复习老内容，消化成自己的东西，最后才真正有所收获。

**1、一个新系统开发完毕之后如何设置JVM参数？**

之前花费了很多的精力给大家介绍，在一个新系统开发完毕之后，到底该如何预估性的合理设置JVM参数？

毕竟直接用默认的JVM参数部署上线再观察，是非常的不靠谱的。很多公司也没有所谓的JVM参数模板。

首先大家应该估算一下自己负责的系统每个核心接口每秒多少次请求，每次请求会创建多少个对象，每个对象大概多大，每秒钟会使用多少内存空间？

这样接着就可以估算出来Eden区大概多长时间会占满？

然后就可以估算出来多长时间会发生一次Young GC，而且可以估算一下发生Young GC的时候，会有多少对象存活下来，会有多少对象升入老年代里，老年代对象增长的速率大概是多少，多久之后会触发一次Full GC。

通过一连串的估算，就可以合理的分配年轻代和老年代的空间，还有Eden和Survivor的空间

原则就是：尽可能让每次Young GC后存活对象远远小于Survivor区域，避免对象频繁进入老年代触发Full GC。

最理想的状态下，就是系统几乎不发生Full GC，老年代应该就是稳定占用一定的空间，就是那些长期存活的对象在躲过15次Young GC后升入老年代自然占用的。然后平时主要就是几分钟发生一次Young GC，耗时几毫秒。

**2、在压测之后合理调整JVM参数**

任何一个新系统上线都得进行压测，此时在模拟线上压力的场景下，可以用jstat等工具去观察JVM的运行内存模型：

- Eden区的对象增长速率多块？
- Young GC频率多高？
- 一次Young GC多长耗时？
- Young GC过后多少对象存活？
- 老年代的对象增长速率多高？
- Full GC频率多高？
- 一次Full GC耗时？

压测时可以完全精准的通过jstat观察出来上述JVM运行指标，让我们对JVM运行时的情况了如指掌。然后就可以尽可能的优化JVM的内存分配，尽量避免对象频繁进入老年代，尽量让系统仅仅有Young GC。

**3、线上系统的监控和优化**

系统上线之后，务必进行一定的监控，高大上的做法就是通过Zabbix、Open-Falcon之类的工具来监控机器和JVM的运行，频繁Full GC就要报警。

比较差一点的做法，就是在机器上运行jstat，让其把监控信息写入一个文件，每天定时检查一下看一看。

一旦发现频繁Full GC的情况就要进行优化，优化的核心思路是类似的：通过jstat分析出来系统的JVM运行指标，找到Full GC的核心问题，然后优化一下JVM的参数，尽量让对象别进入老年代，减少Full GC的频率。

**4、线上频繁Full GC的几种表现**

其实通过之前的各种案例，大家可以总结出来，一旦系统发生频繁Full GC，大概看到的一些表象如下：

- 机器CPU负载过高；
- 频繁Full GC报警；
- 系统无法处理请求或者处理过慢

所以一旦发生上述几个情况，大家第一时间得想到是不是发生了频繁Full GC。

**5、频繁Full GC的几种常见原因**

之前给大家分析过多个案例，通过那些案例的总结和归纳，可以得出下面几个常见的频繁Full GC的原因：

1. 系统承载高并发请求，或者处理数据量过大，导致Young GC很频繁，而且每次Young GC过后存活对象太多，内存分配不合理，Survivor区域过小，导致对象频繁进入老年代，频繁触发Full GC。
2. 系统一次性加载过多数据进内存，搞出来很多大对象，导致频繁有大对象进入老年代，必然频繁触发Full GC
3. 系统发生了内存泄漏，莫名其妙创建大量的对象，始终无法回收，一直占用在老年代里，必然频繁触发Full GC
4. Metaspace（永久代）因为加载类过多触发Full GC
5. 误调用System.gc()触发Full GC

其实常见的频繁Full GC原因无非就上述那几种，所以大家在线上处理Full GC的时候，就从这几个角度入手去分析即可，核心利器就是jstat。

如果jstat分析发现Full GC原因是第一种，那么就合理分配内存，调大Survivor区域即可。

如果jstat分析发现是第二种或第三种原因，也就是老年代一直有大量对象无法回收掉，年轻代升入老年代的对象病不多，那么就dump出来内存快照，然后用MAT工具进行分析即可

通过分析，找出来什么对象占用内存过多，然后通过一些对象的引用和线程执行堆栈的分析，找到哪块代码弄出来那么多的对象的。接着优化代码即可。

如果jstat分析发现内存使用不多，还频繁触发Full GC，必然是第四种和第五种，此时对应的进行优化即可。

**6、一个统一的JVM参数模板**

为了简化JVM的参数设置和优化，建议各个公司和团队leader做一份JVM参数模板出来，设置一些常见参数即可

核心就是一些内存区域的分配，垃圾回收器的指定，CMS性能优化的一些参数（比如压缩、并发，等等），常见的一些参数，包括禁止System.gc()，打印出来GC日志，等等。

这些常见的参数，之前基本都讲过了，建议大家自行整理出来一份模板即可。

**End**

### 068、如何为你的面试准备自己负责的系统中的JVM优化案例？

**1、面试中关于JVM的一些痛点**

很多人都跟我以及我的一些朋友反馈过一些自己面试中对于JVM这块的一些痛点，一些常见的理论知识，比如JVM内存模型，垃圾回收算法，垃圾回收器，类加载，这些常见的知识，都背的滚瓜烂熟。

但是呢？面试官常问的就是，说说你平时在工作中如何进行JVM优化的？

此时很多人直接两眼发蒙。原因很简单，在我们这个JVM专栏推出之前，平心而论，国内Java工程师中，真的懂JVM优化的仅仅是少数人而已。

大部分工程师对JVM的掌握仅仅停留在JVM的一些理论知识，但是这些理论知识在你的系统运行时候的运行模型和原理，以及跟GC的各种关系，完全没有串联起来，也就是从理论到实践完全是脱节的。

而且对线上系统如何监控JVM GC以及如何定位、分析以及解决频繁GC问题，完全没任何成体系的思路。之前仅仅看过网上一些凌乱的博客，看过一些所谓的JVM优化参数，仅仅就是背参数而已，以为设置了一些参数就一定没问题了。

上述问题，跟国内缺乏JVM实战性的资料有很大的关系。因此很多人出去面试的时候，一旦问到自己平时如何在生产环境进行JVM生产优化，根本不知道怎么回答，这就是最大的痛点！

**2、现在的你应该如何在面试中回答JVM生产优化问题？**

现在的你已经学习了这么多的内容，应该如何在面试中回答JVM生产优化问题？

一种比较常见的做法，就是把之前学习过的知识，归纳总结出来一套通用的方法付论，然后面试的时候就聊这套通用方法论即可。

这个方法没有问题，很多面试官其实听到这套回答已经眼前一亮了，因为国内很少有人能把JVM生产优化的方法论总结的如此之系统的。

但是还不够，因为面试官想听的，实际上是你自己负责的系统是如何进行JVM优化的！

**3、如果你的系统访问量和数据量暴增10倍或者100倍**

所以你在这里应该思考的一个问题，就是你负责的系统，假设数据量和访问量暴增10倍，或者100倍，此时会不会出现频繁Full GC的问题？

利用学习过的知识去倒推一下，其实很可能的，在有限的机器资源下，一旦压力增长，很可能因为内存分配不合理，导致频繁Full GC的！

上面我们说过好几种频繁Full GC的触发条件，你是不是都可以放在自己的系统里去思考一下，自己的系统有没有可能会发生上述几种场景下的频繁Full GC？

如果会的话，那么一旦发生了，如何定位、分析和解决？

你应该把频繁Full GC问题和你自己的业务系统结合起来，自己深度思考，自己整理出来几个自己系统可能发生的JVM性能问题，然后整理出一套解决方案出来。

未来在面试的时候，应该结合自己的系统去跟面试官聊，说自己的系统可能在哪些情况下发生频繁Full GC，在压测的时候就发现了这些问题，然后你是如何进行JVM性能优化的！

这样面试官一定会认可你对JVM这块技术的掌握和实践经验的！

**4、最后提点一句JVM的优化注意点**

网上有很多博客会让你设置一些非常少见的JVM参数，比如之前有个案例就讲了，有人设置了软引用的一个参数，还有一些奇怪的参数，比如pagecache的参数之类的，以为JVM优化就是调节奇怪的参数，搞的很牛一样。

其实完全不是如此，真正的JVM优化，就是一些内存分配+垃圾回收器的选择（ParNew、CMS、G1）+垃圾回收器的常见参数设置，还有就是一些代码层面的内存泄漏问题，其实搞定这些问题，99%的JVM性能问题你都能搞定了！

所以大家千万别胡乱设置一些奇怪的参数，很可能会适得其反！

我们这个专栏后面还有很多内容是关于JVM OOM的，JVM平时最容易产生的就是两类问题，JVM频繁GC和JVM OOM，所以我们专栏也专注在这两块内容的实战上。当大家把专栏全部内容都学习完毕之后，日常生产环境里的问题基本都能搞定了。

**End**

### 069、关于作业的说明

不知不觉，JVM实战专栏已更新10周

通过每天在后台回复大家的留言，在和大家的互动中，也了解到了更多的典型问题，我会根据这些问题对咱们的专栏内容进行微调，目标只有一个，让所有购买了专栏的同学最大限度的吃透课程。

其中决定对专栏做的一个调整，就是取消掉每周六的作业，因为从后台的数据看，每周看过作业的同学实在是少的可怜，10分之1都不到

后来我想了一下，每周5篇干货文章，加上一篇答疑，这个量已经足够了，况且布置作业这种方式看上去太重，不如每天文末留下的，看起来更轻的思考题。

大家可以看了文章，立即思考，马上讨论，这样的短平快的过程，我认为效果是更好的

所以从本周开始，周六的作业都取消，大家就认真吃透每周5篇干货文章，思考每篇文章末尾留下的思考题，坚持4个月，你一定能成为jvm实战高手！

### 070、第10周答疑汇总

**学员总结：**

总结一下老师这几天提交的参数：

- -XX:+CMSParallelInitialMarkEnabled表示在初始标记的多线程执行，减少STW；
- -XX:+CMSScavengeBeforeRemark：在重新标记之前执行minorGC减少重新标记时间；
- -XX:+CMSParallelRemarkEnabled:在重新标记的时候多线程执行，降低STW；
- -XX：CMSInitiatingOccupancyFraction=92和-XX:+UseCMSInitiatingOccupancyOnly配套使用，如果不设置后者，jvm第一次会采用92%但是后续jvm会根据运行时采集的数据来进行GC周期，如果设置后者则jvm每次都会在92%的时候进行gc；
- -XX:+PrintHeapAtGC:在每次GC前都要GC堆的概况输出

**回答：**总结的非常好

====================================================

**问题：**

这问题看似简单，其实我也碰到过，当时使用jxl导出excel的时候，jxl会默认调用gc方法，当时花了不少时间才发现原来是这个问题...

**回答：**

是的，就是如此，还有的同学一时手痒会自己去执行这行代码，就怕有的人自己瞎玩儿

====================================================

**问题：**

果然精短轻松😊，之前我还在想这个参数的意义在哪里。老师，像这种情况jstick里会有什么异常吗？

**回答：**

jstat其实是用在死锁里的，一般可以是看死锁哪里线程卡住的

====================================================

问题：

老师，永久带里面存放的类信息具体是一些什么样的信息，可以给讲解几种吗？谢谢

回答：

这个具体类的信息，其实你可以理解为就是类所带的一些变量和方法

====================================================

总结：

当多个服务需要竞争一个单体资源时，可以考虑加上分布式锁。如果并发量高的话，可以考虑拆分掉那个单体资源，50个拆成5个10个资源，从而缩小锁的粒度，提高吞吐量。

总结的很好

====================================================

总结：

一般redis的分布式锁都可以使用redisson框架来做。 使用：直接lock("key") 

原理：当lock的时候，当前客户端会生成一串lua脚本发送到redis服务端。服务端根据这个key是否存在以及key的value状态判断是否已被加锁。

如果加锁成功（有效期30s）：生成的value里面带有当前客户端的id，实现可重入效果。然后在这个线程执行过程中，一旦加锁成功还会有一个watch dog每10s去刷新锁的状态。直到释放或者宕机，如果当前线程死锁或阻塞，那么这个锁就一起死锁。如果加锁失败：自旋到获取锁。

总结的很好

====================================================

问题：

老师，我有个关于锁的问题。在我看来，所有锁都是基于一个互斥量，这个互斥量可以随意选取为一个支持加锁或者原子操作的第三方组件。那么经常使用zk/redis去是实现分布式锁，而不使用db去加锁。是因为db的吞吐量太低了吗，还是有其他考虑？

回答：

db不适合配合系统做加锁，从功能层面，db就不是干这个的

====================================================

学员思考题回答：

出现原因：

因为redis主从异步复制的实现方式是可能出现在主节点宕机时未完成同步而出现的锁丢失。 这个时候watch dog发现了也没办法，只能说是自己停掉了。 

解决办法：

因为主从异步复制出现的问题，那么保证主从写强一致就可以了。但是如果写强一致的话，那么吞吐量就会因为这些实时同步而急剧降低。

另外出现了C，那么A这个可用性就会受到一定的影响，因为在实时同步的时候，要是失败了，这个服务可能暂时就无法提供正常的服务。

回答：

总结的很好

====================================================

总结：

zk一般使用curator去实现分布式锁。 原理：向zk发起请求，在一个目录（/locks/pd_1_stock）下，创建一个临时节点，也是带有自己客户端的id。

如果目录是空的，自己创建出来的节点就是第一个节点，那么加锁成功。如果成功执行则释放（节点删除）。

如果宕机了，基于zk的心跳机制，那个临时节点也会被删除。第二个客户端请求锁时，也创建一个节点，如果不是第一节点，那么向上一节点加一个watcher监听器。如果上一节点被删除立马会感知到，然后在判断自己是不是第一节点，如果不是再监听上一级（公平实现）。完事后陷入等待，直到获取锁。

总结的很好

====================================================

总结： 

羊群效应：当几十个节点争抢同一把基于zk的锁时，如果都是监听第一个节点，那么当释放锁时，zk会同时反向通知所有客户端又来重新增强。 

影响：

主要是多了很多没必要的请求，从而会加重网络的负载。 

解决：

就基于curator去做就好了，通过监听上一级节点，降低了争抢次数，还实现了公平锁。 

redis：redis因为是客户端自己主动隔一段时间去尝试加锁，所以羊群效应影响不大，因为请求都错开了，而不是一群一拥而上。

总结的很好

====================================================

问题：

老师，关于g1我有个疑惑，就是如果选择减小gc耗时，那是不是意味gc的次数会增多。那选择减少时间是因为它的收益大于gc次数的增加。

回答：

是的，你理解的没问题，gc频率多点，但是每次gc耗时很短，没事的

====================================================

问题：

老师能不能对 这种类型的问题上一课 最近金九银十 肯定有很多同学要面试 比如这个题目 凌晨三点发生了一次oom 怎么追踪解决

回答：

这个问题很好，这个在后面的OOM相关案例里会分析的

====================================================

问题：

老师好，刚才看到有同学的总结： 

1. G1，新生代未达到60%，老年代未达到45%，则不进行GC 您的回答是没问题，但是从您的文章中找到的，新生代不到60%，只要ygc时间接近预设GC时间时就会进行GC。

2.  看了一下我们的线上系统，发现ygc之后，survivor区已经达到了将近90%，但是没有对象进入老年代，这是不是意味着要么所有对象都是同岁的，要么至少41%的对象是大年龄且同岁的。

   

回答：

有的回复可能是没看清楚他们的问题，G1的话，有可能达到预期的gc时间后他就会触发gc，不是固定的

====================================================

问题：

看到老年代驻留大量对象，想到的竟然是老年代空间太小，需要扩大，而不是其他问题，看来还是没学到家呀

回答：

是的，所以要多学案例，积累经验

====================================================

问题：

打卡，今天讲解了cpu负载高的原因，复习了频繁fullGC的原因，给了一个分析堆内存的实用工具MAT。给力。

回答：

是的，今天其实以复习为主

====================================================

问题：

老师我非常激动，今天出现一个问题，我发现和上周的案例基本一致，就是极端情况下，SQL中in条件语句包含十几万个ID，一个SQL语句的String对象很大，导致Full GC，然后还有一种场景in条件是空 ，SQL是select * from DB的(代码我写的，非常惭愧)一下加载十几万的数据出来，导致full gc。

除了这两个问题，今天的例子也和我们系统很想：通过cat观察cms管理的内存平时一般都在800m左右，但是最近一直涨到1300m左右，持续了很久都不会降低，老年代2000m，68%就会触发Full GC，也就是1300左右就会发生GC，我初步怀疑是哪里内存泄漏了，导致这里占了几百MB内存不释放，这个还有待查询。准备将这个泄漏找出来，然后调整一下JVM参数，优化一波。

回答：

非常好，看来案例中碰到的例子，你自己工作里都碰到了，一个是SQL误操作加载过多数据触发的Full GC，一个是内存泄漏导致老年代里一直占用了过多内存。可以dump一个内存快照出来，用MAT分析一下

====================================================

问题：

另外有点疑问，大对象进入老年代，我网上找资料说是默认大对象阈值是0，也就是不会直接进入老年代，这个是真的吗。

我感觉我的问题也是因为很大的String直接进入了老年代才导致几百MB无法释放

回答：

大对象会直接进入老年代的，但是具体要看不同版本的JDK默认参数是什么

====================================================

问题：

老师，你好，我数据平台有一个报表导出，导出的时候因为堆内存不足而失败，但是后面CMS，会一直进行标记清理，老年代基本上是占满的，S1,S2没有任何占用，jstat 看到jvm一直在进行 YGC 和 FGC

但是老年代一直不减少，只有在eden区占满以后执行后的YGC，才会将老年代的占用真正的回收，请问能否解释一下这个是为什么？ 

-XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:NewSize=838860800 -XX:MaxNewSize=838860800 -XX:MetaspaceSize=104857600 -XX:SoftRefLRUPolicyMSPerMB=2000 -XX:SurvivorRatio=2 -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=20971520 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=95 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+TraceClassLoading -XX:+TraceClassUnloading -XX:+PrintGCDetails -XX:+PrintGCDateStamps -verbose:gc -Xloggc:./filebeat/gc.log -XX:ErrorFile=./logs/jvm_error.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./logs/heapdump.hprof 204800.0 204800.0 0.0 0.0 409600.0 313375.4 229376.0 229352.0 � � 252 1.854 467 54.764 56.618 还有就是元数据区是乱码的

回答：

这个本质还是你代码的问题，应该是大量的数据占满了老年代，没处理完就回收不掉

====================================================

问题：

请问一下[Loaded sun.reflect.GeneratedMethodAccessor129 from __JVM_DefineClass__]从这行日志是咋看出来GeneratedMethodAccessor会被加载到MetaSpace的, 这行日志好像没有提供类被加载到哪里的信息

回答：

第一个英文单词，Loaded，就是加载的意思

====================================================

总结： 

cms存在的问题，浮动垃圾因为并发清除，空间碎片因为标记整理算法，并发执行失败也是因为并发清除的设计可能存在预留的老年代空间不足。但是cms对空间碎片进行了优化，提供了内存的整理，这个操作可以通过参数去控制，默认是开启，并且每次fullgc后都去整理内存，但是就需要stw。 

最后，fullgc的发生情况： 

1.老年代可用内存小于新生代全部对象的大小，又没有开启空间担保，就会直接触发fullgc。 

2.如果新生代存活大小大于老年代空间，并且老年代空间小于历次晋升的平均内存大小，也会执行fullgc。 

3.大对象或者动态年龄进入老年代，而老年代空间不足，也会执行fullgc。 

4.如果是cms回收器，那么老年代内存使用到92%之后，就会触发fullgc，因为并发清除阶段需要给用户线程预留内存空间。

回答：

总结的很好

====================================================

问题：

一般公司线上系统是禁用dump内存快线的吗。我们被运维禁了

回答：

是的，线上机器一般来说会禁止执行dump，因为dump的时候可能会导致系统停机几秒钟，或者几百毫秒，可以跟运维沟通，让他们给你dump一个快照出来

====================================================

问题：

打卡。MAT好用，但是功能多需要慢慢摸索

回答：

是的，非常好用，不过文章里介绍的几个常用功能基本够用了，其实一般主要就是看占用内存最多的对象，追踪一下哪个线程，然后看一下线程执行堆栈

====================================================

问题：

老师您好！我最近遇到一个问题：

有一个定时任务，每5秒去读取一张表中的数据，这张表大概有7000多行，每行大概3KB

理论上，加载到内存，最大差不多30MB左右，但我通过jstat发现，到底每5秒钟，加载了大概300MB的数据，我对此很困惑，望老师或者其他同学能够解答一下～

回答：

这个很正常的，因为实际加载到内存里，你有不同的数据结构来存储，还有一些对象本身也会占据内存空间，实际内存消耗往往比你数据本身大小要大很多

====================================================

总结：

1. JVM是什么 答：本质上也是一个程序，负责运行java编译好的字节码文件(.class)。
2. JVM跟我们平时运行在机器上的系统之间是什么关系 答：JVM具有跨平台性，可以在多种系统上执行。
3. 类加载器的概念 答：把编译好的那些".class"字节码文件给加载到JVM中。
4. 字节码执行引擎的概念 答：针对加载进内存的类进行代码的执行。

回答：

总结得很好

====================================================

问题：

现在不是很清楚年轻代的gc root跟踪和老年代的gc root跟踪，是不是都是从gc root去找引用的对象？还是年轻代的直接从gc root遍历，老年代得并发标记，从老年代中的对象去找gc root

回答：

都是从gc roots开始追踪的

====================================================

问题：

老师你好，我在动态年龄判断那块有点疑问，假如survivor内存100m，现在已经占用了45m,那么此时来了一共55m的对象，那么之前所有的对象全部进入老年代，但是这55m还是大于一半，假如都是1岁会怎么处理呢，按对象的创建时间进入到老年代吗？

回答：

对的，也有可能会进入老年代

====================================================

问题：

老师好，关于动态对象年龄判断，我理解的还是不清晰，我现在理解的是判断各个年龄段之和是不是超过了survivor的一半，但是“各个年龄”包含本次gc之后存活的对象吗，如果超过了一半，那么本次gc之后的对象会进入老年代吗

回答：

其实关于这里的实现，不用过于找里面的细节，因为不同JDK版本实现都不同。你只要知道几点

第一，有动态年龄判定规则

第二，有可能因为young gc后存活对象过多导致进入survivor对象太多，触发这个规则

第三，可能因为这个规则导致很多对象进入老年代

====================================================

问题：

学这个专栏，感觉就在上课，不断学习，不断的复习

回答：

是的，一味的讲新内容往前赶进度是不靠谱的，要不断的有新的知识，同时不断带着之前旧的知识反复强化，还要定期复习，最后跟着看完，对核心知识绝对都消化的很彻底，当然自己也得注意复习和理解，还有结合自己的实际工作情况去积累

====================================================

学员思考题回答： 

1.调整s区到能够装下每次minorgc后存活的对象。 

2.调整动态年龄判断，可以考虑减小默认值。 

3.设置大对象进入老年代的判断，可以考虑设置为1m。 

4.设置回收器组合，一般响应优先就使用parnew+cms

回答：总结的很好

====================================================

问题：

系统给了3g堆内存，但是dump出来的文件打开看只有500mb，这个怎么解释

回答：

这个很正常，说明你的内存占用并没有占用满3g，就使用了500mb左右的内存，另外一个dump快照也会做一些二进制存储，不一定跟3g一样

====================================================

总结：

关于老年代的优化，都是建立在新生代已经进行优化的前提下。然后分析什么情况下对象依旧会进去老年代，多久会填满老年代，再根据垃圾回收器的特性，去设置对应的参数。

一般来说老年代的参数保持默认就好了，新生代优化好了，很少甚至几乎不会发生fullgc。

回答：

总结的很好

====================================================

问题：

老师，我发现一次young gc并没有将Eden区清空，而是会有部分保留是怎么回事呢？是我弄的不对还是有什么说道的？

回答：

不是的，实际上你说的那种是他刚ygc完，然后就有对象进入了eden，所以你就看到那样子了

====================================================

总结：

g1是以垃圾回收优先的回收器，主要是通过将内存区域划分成region然后维护一个回收价值列表，建立一个可预测的时间停顿模型。内部保留了年代，但是仅仅是逻辑上进行了保留，新生代和老年代会根据需要进行变化。

回答：

总结的很好

====================================================

总结: 

rabbitmq丢失数据的情况： 

1、生产者端。消息因为网络问题丢失或者发送到rabbitmq时出错了。 

2、rabbitmq服务端。未做持久化。 

3、消费者端。打开了autoAck，在未完成消费之前就自动回复了。 

rabbitmq丢失数据的解决： 

1、生产者端。

通过confirm模式异步确认消息发送成功，在失败后的回调函数中处理失败的逻辑。 

2、服务端。

打开持久化机制。这里涉及到两个参数，一个是建立queue的时候，持久化那个queue。

另外一个是生产者发送消息的时候，把deliveryMode设置为2，让MQ把这条数据也给持久化。

但是尽管如此，如果在极端情况下，在rabbitmq中内存写成功，但是还没来及持久化时，rabbitmq宕机，这部分在内存里面的数据也会丢失，不过几率很小。 

3、在消费者端，去掉autoAck，在自己完成逻辑后手动提交ack。

回答：总结的很好

====================================================

总结： 

kafka出现数据丢失的情况： 

1、生产者端：和rabbitmq类似，如果没能确认写成功，也没有重发那么也会丢失。 

2、服务端：如果未来得及和从节点同步数据就宕机了，那么这部分数据就会丢失。 

3、消费者端：和rabbitmq类似，如果自动提交offset依旧会出现丢失。 

kafka出现数据丢失的解决： 

1、生产者端：设置参数，要求每个从节点都写成功后才任务成功，另外如果发送失败，重试次数设置一个很大的值。 

2、服务端：设置参数，要求从节点起码大于1，且至少有一个能被感知到。 

3、消费者端：取消掉自动回复。 不过，强一致的保证消息不丢失，必然会影响到吞吐量。

回答：

总结的很好

====================================================

问题：

打卡，jvm性能优化怎么做？用jstat呀，哈哈，神器在手，再来个mat工具，还找不到问题？妖怪吧。

回答：

是的，基本gc日志+jstat+MAT就足够了

====================================================

6.JVM在什么情况下会加载一个类 

答: 

a.JVM进程启动之后，代码中包含"main()"方法的主类一定会被加载到内存。 

b.执行"main()"方法代码的过程中，遇到别的类也会从对应的".class"字节码文件加载 对应的类到内存里面。 

7.一个类从加载到使用，一般会经历哪些过程 

答：加载->验证->准备->解析->初始化->使用->卸载 

a.加载:将编译好的".class"字节码文件加载到JVM中 

b.验证:根据JVM规范，校验加载进来的".class"字节码文件 

c.准备:给类和类变量分配一定的内存空间，且给类变量设置默认的初始值(0或者nul) 

d.解析:把符号引用替换为直接引用的过程 

e.初始化:根据类初始化代码给类变量赋值 

注：执行new函数来实例化类对象会触发类加载到初始化的全过程；或者是包含"main()"方 法的主类，必须是立马初始化的。如果初始化一个类的时候，发现他的父类还没初始化， 那么必须先初始化他的父类。 

8.Java里有哪些类加载器 

答: a.启动类加载器: 主要负责加载我们在机器上安装的Java目录(lib目录)下的核心类库 

b.扩展类加载器：主要负责加载Java目录下的"lib/ext"目录中得类 

C.应用程序类加载器：主要负责加载"ClassPath"环境变量所指定的路径中的类，大致 可以理解为加载我们写好的java代码 

d.自定义类加载器：根据自己的需求加载类 

9.什么是双亲委派机制 

答：JVM的类加载器是有亲子层级结构的，启动类加载器最上层，扩展类加载器第二层， 应用程序类加载器第三层，自定义类加载器第四层。

当应用程序类加载器需要加载一个类时， 他会先委派给自己的父类加载器去加载，最终传导到顶层的类加载器去加载，但是如果父类 加载器在自己负责加载的范围内，没找到这个类，那么就会下推加载权利给自己的子类加载器

回答的很好

====================================================

问题：

老师能说说RememberSet吗，我在深入理解JVM这本书上看到了老年代引用新生代对象的时候，是用RememberSet来避免全堆扫描，那进行Old GC的时候，新生代引用老年代，是直接扫描整个新生代吗？我看有的博客有说道CMS的并发预清理跟可中断并发预清理，这又是什么。

回答：

这是一些JVM底层源码级别的原理，我们这个专栏定位是实战型的，基本把日常生产实战的内容都分析过了，而且都是结合案例的。有一些是属于深入底层的技术细节，但是跟平时实战优化关系不大，所以实际上我们没有讲这些内容。也许未来会出那种JVM源码级别的深入的课程，会说到你说的那些内容。

====================================================

问题：

你的总结正是我心中想总结的，预料之中。jvm运行原理、什么时候yong gc、什么时候进入老年代、什么时候full gc、为什么会频繁full gc、分析制定jvm模板

回答：

是的，定期复习和总结非常的重要

====================================================

问题：

之前也看过jvm的书，看了一些视频，但怎么优化还是一头雾水。很幸运遇到了老师的专栏，现在不管对实战还是面试都充满信心。

每天都要花3-4个小时学习总结，现在看文章还算轻松，后续要把老师所有的案例总结成自问自答的面试题。也根据所学，基于jstat初步做了个监控小工具，后面继续完善，也算是对专栏最好的总结。谢谢老师！！！！！！

回答：

好的，你真是太棒了，你们学有所成，真正掌握JVM的优化实战，提升国内工程师的技术水平，就是我们希望做的事情

====================================================

问题：

老师好，线上也出现了FUll GC的告警，日志如下: 

2019-09-05T17:26:15.161+0800: 85779.869: [Full GC (Metadata GC Threshold) 2019-09-05T17:26:15.161+0800: 85779.869: [CMS: 472256K->445559K(3022848K), 2.0333919 secs] 1425388K->445559K(4910336K), [Metaspace: 277217K->277217K(1511424K)], 2.0355295 secs] [Times: user=2.01 sys=0.01, real=2.03 secs] 2019-09-05T17:26:17.197+0800: 85781.905: [Full GC (Last ditch collection) 2019-09-05T17:26:17.197+0800: 85781.905: [CMS: 445559K->382990K(3022848K), 1.6770458 secs] 445559K->382990K(4910336K), [Metaspace: 276037K->276037K(1511424K)], 1.6863552 secs] [Times: user=1.63 sys=0.01, real=1.68 secs] 2019-09-05T17:26:18.886+0800: 85783.594: [GC (CMS Initial Mark) [1 CMS-initial-mark: 382990K(3022848K)] 382992K(4910336K), 0.0134842 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] 

初步确认是每次发布groovy脚本，classload的时候才会彪高，这种问题基本定位到了，但是需要怎么去优化和解决呢?

回答：

这个很明确了，你的metaspace太小了，所以classload太多的时候会触发full gc，只要给metaspace区域更大空间就可以了

====================================================

问题：

这个课程真的是我学习过的JVM课程和看过的书中最好的,没有之一,其他的课程都感觉是隔靴搔痒,这个课是直击要害,而且每个问题都能得到回复, 非常感谢

回答：

多谢你的支持

====================================================

问题：

现在很渴望去解决线上问题，当别人搞不定的时候我来上，可惜现在系统的量太少，基本上堆内存6G,老年代和年轻代分别3G，就不会产生性能问题。只能假设它的量增加100倍了。

回答：

是的，因为系统量太小，就是Eden区都慢慢才满，满了以后存活对象也很少，几乎很少对象进入老年代的，所以一般量小的系统没有JVM的问题

====================================================

问题：

请问一下如果访问量增加100倍, 比如QPS从100, 猛增到10000, 而我Tomcat的MaxThread设置的是200, 而我每个请求的处理时间加入是100ms, 那理论上系统1S中可以处理的请求数就是2000, 这已经确定了啊, 即使QPS达到10000,多余的请求只会在OS中的Epoll模型中排队吧,

我感觉好像不会轮到JVM的问题暴露, 所以优化jvm参数的时候只要考虑到最大QPS是2000就可以了, 请问我这样理解对不对?

回答：

我给你举个例子怎么思考，比如你现在有一个系统，部署在一台机器上，机器能承载的极限并发量是每秒处理1000个请求，结果你现在系统量很小，每秒才处理10个请求。此时你直接假设你的机器资源被打满，每秒处理1000个请求，同样的内存下，请求量大了1000倍，你的JVM会如何运行？

====================================================

问题：

今天是分析而不讨论。让我们去回顾之前所学的知识，然后结合自己现有系统去准备这方面的面试技巧。

回答：

是的，加油好好复习和准备，多结合自己工作的系统去思考JVM的优化，放大100倍压力思考一下

====================================================

11.什么是JVM内存区域划分 

答：加载进来的类信息，需要放在某个内存区域。方法运行时，方法里面很多变量之类的 东西需要放在某个内存区域。代码里创建的对象，也需要内存空间来存放。所以JVM中必须 划分出来不同的内存区域，为了让我们写好的代码在运行过程中更方便的根据需要来使用。 

12.JVM中有哪些内存区域 答：

a.方法区: 在JDK1.8以后,这块区域的名字改了,叫做"Metaspace"。主要存放我们自己写的 各种类相关的信息。 

b.程序计数器：字节码指令通过字节码执行引擎被一条一条执行，才能实现我们写好的代码 执行的效果。

程序技术器就是用来记录当前执行的字节码指令的位置，也就是记录目前执行到了哪一条字节码指令。

JVM是支持多个线程的，所以就会 有多个线程来并发执行不同的代码指令，因此，每个线程都会有自己的一个 程序计数器，专门记录当前这个线程目前执行到了哪一条字节码指令。 

c.Java虚拟机栈：保存每个方法内的局部变量等数据。每个线程都会有自己的Java虚拟机栈。 

如果线程执行了一个方法，就会对这个方法调用创建对应的一个栈帧(栈帧 里就有这个方法的局部变量表,操作数栈,动态链接,方法出口等)，然后压入 线程的Java虚拟机栈。方法执行完毕之后就从Java虚拟机栈出栈。

因此，每个线程在执行代码时，除了程序计数器以外，还搭配了一个Java虚拟机 内存区域来存放每个方法中得局部变量。 

d.Java堆内存：存放我们在代码中创建的各种对象。对象实例里面会包含一些数据。而Java 虚拟机栈的栈帧局部变量表里面的对象，其实是一个引用类型的局部变量，存放 了对应Java堆内存对象的地址。可以理解为局部变量表里的对象指向了Java

回答：

总结得很好

====================================================

问题：

老师，那以后市面上都用G1清理，那我们是不是在jvm优化上就得靠边站了？

回答：

是的，使用G1的时候，其实能做的事情很少，因为你想，他所有的内存分配和GC时机都是动态变化的，你怎么去调优？实际上他一切都是自动运行的。只要他能保证每次GC的耗时在你指定范围就可以了。但是其实现在G1也未必就已经很稳定了，所以一般还是用CMS+ParNew就可以了，比较可控一些。

### 071、Java程序员的梦魇：线上系统突然挂掉，可怕的OOM内存溢出！

**1、Java程序员的梦魇：线上系统突然挂掉**

大家作为一个Java程序员，平时开发系统，测试系统，上线部署系统，为了公司拼命的加班，任劳任怨的干活，然后。。。平时最害怕的是个什么事情？

想必不用我说，大家自己也知道了，就是出事故！

很多大公司管事故叫做Case，如果系统一旦出一个事故，比如线上核心系统突然宕机不可用，然后导致几个小时内用户无法下订单，进而导致公司损失几百万，甚至几千万。

或者公司的某个单点登录系统突然不可用，所有用户无法登录APP，也导致无法下单。

或者公司的缓存集群突然全面故障，然后导致公司的全部系统一起瘫痪。

或者因为某个明星突然出轨，结果导致流量集中访问某台服务器，直接把数据库搞挂了。

凡此种种，都是重大的Case。一旦有Case，就会有程序员被拉出去祭天，这是网上常见的一个段子，是不是？

其实祭天不至于那么夸张，但是一旦出了事故，总有人得去承担这个责任，去分析这个事故为什么会发生，谁的责任，后续如何改进

所以，自己负责的线上系统，或者负责维护的缓存集群，或者负责维护的数据库集群，突然莫名其妙挂掉，不可用，导致公司核心业务流程彻底中断，这个就是程序员平时最害怕的事情。

**2、Java程序员平时最常遇到的故障：系统OOM**

那么作为咱们Java程序员而言，先不考虑自己系统外部依赖的缓存、消息队列、数据库等等东西挂掉，就我们自己系统本身而言，最常见的挂掉的原因是什么？

其实就是系统OOM，也就是所谓的内存溢出！

大家之前跟着我们的专栏学习，已经对JVM的运行原理都比较了解了，那么现在我们就来想一想，所谓的JVM OOM内存溢出到底是什么？

其实说白了，也非常非常的简单，一句话形容，你的JVM内存就这么点，结果你拼命的往里面塞东西，结果内存塞不下了，不就直接溢出了吗？

看看下面的这个图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38223600_1578303878.cn/txdocpic/0/bc32f5a83c01c7cc1202d35da20023b6/0)      

至于到底JVM是如何放不下对象导致内存溢出的，这个大家不要着急，情况有很多种，我们后面会一步一图一点点给大家分析的

此外我们还会结合一些案例用代码给大家演示出来发生内存溢出的一些场景，最后还会告诉大家平时对线上系统的内存溢出一般怎么来处理和解决。

一旦你的系统代码不停的往JVM内存里塞入大量的东西，JVM实在是放不下之后，JVM就会告诉你，OutOfMemory，内存溢出了，我实在放不下那么多东西了，我就直接瘫痪不能工作了。

通常而言，内存溢出这个问题可能对你的系统是毁灭性的打击，他代表你的JVM内存不足以支撑你的代码的运行

所以一旦发生这个情况，就会导致你的系统直接停止运转，甚至会导致你的JVM进程直接崩溃掉，进程都没了！

这个时候对于线上看起来的场景就是，用户突然发现很奇怪，为什么点击APP、点击网页，都没反应了呢？

然后大量的投诉和反馈给到客服，客服直接转移投诉给到运营，运营会直接反馈给技术人员。

这个时候技术人员往往得知这个消息会直接目瞪口呆，最害怕的事情发生了，自己负责的线上系统居然挂掉了，今年的年终奖。。。也许是泡汤了，弄不好还得提前出去找工作去了。。。

**3、很多工程师都不知道如何处理OOM**

最可怕的并不是线上系统发生了OOM，最可怕的是很多工程师压根儿就没有这种处理线上系统故障的经验

当发生OOM之后，根本不知道系统到底为什么会突然OOM？系统代码到底产生了多少对象？为什么会产生这么多对象？JVM为什么会放不下这么多对象？到底怎么去排查这个问题？又如何解决呢？

不知道，全都不知道！

因此我们的专栏接下来的几周内容，将全面围绕JVM OOM的问题展开。

我们先用一周时间带着大家一步一图分析各种可能发生OOM的情况，接着用一周的时间带着大家从一些案例入手来通过模拟代码真实感受一下OOM的发生

接着用一周的时间带着大家去学习如何监控、定位、排查、分析和解决JVM OOM的问题，最后用两周的时间带着大家去用各种真实的生产案例去体验不同场景下的OOM问题。

相信经过专栏最后几周的学习之后，每个人都能游刃有余的处理线上系统的JVM OOM问题的，最终成为JVM的实战高手。

**End**

### 072、大厂面试题：什么是内存溢出？在哪些区域会发生内存溢出？

**1、前言**

这篇文章，我们来聊一个面试常常被问的问题：JVM里的内存溢出到底是指的什么，哪些区域有可能会发生内存溢出？

要搞明白这个问题，那我们就得从头儿开始来捋一下了，我们这篇文章一步一图，从JVM的核心运行原理出发，然后给大家带出来到底哪些地方可能会发生内存溢出。

**2、运行一个Java系统就是运行一个JVM进程**

首先的话呢，大家得先搞明白一个事情，就是我们平时说启动一个Java系统，其实本质就是启动一个JVM进程。

咱们就用最最基本的情况来给大家演示一下好了，比如说下面的一段代码，是每个Java初学者都会写的一段代码：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21141100_1578303885.png)

那么大家知道，当你在Eclipse或者Intellij IDEA中写好这个代码，然后通过IDE来运行这个代码的时候，会发生哪些事情吗？

首先，我们专栏最早的几篇文章就给大家说过，我们写好的代码他都是后缀为“.java”的源代码，这个代码是不能运行的。

所以第一步就是这份“.java”源代码文件必须先编译成一个“.class”字节码文件，这个字节码文件才是可以运行的，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/30679700_1578303885.cn/txdocpic/0/e15355b40e0dc51e03a5b882236e9cd1/0)   接着对于这种编译好的字节码文件，比如HelloWorld.class，如果里面包含了main方法，接下来我们就可以用“java命令”来在命令行执行这个字节码文件了

实际上一旦你执行“java命令”，相当于就会启动一个JVM进程。这个JVM进程就会负责去执行你写好的那些代码，如下图所示。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40294300_1578303885.cn/txdocpic/0/ae17953d022ad23217f71a1bda99b67b/0)

所以首先要清楚第一点，运行一个Java系统，本质上就是启动一个JVM进程，这个JVM进程负责来执行你写好的一大堆代码。只要你的Java系统中包含一个main方法，接着JVM进程就会从你指定的这个main方法入手，开始执行你写的代码。

**3、到底执行哪些代码：JVM得加载你写的类**

下一个问题，JVM进程怎么执行你写的那些代码呢？

大家都知道，Java是一个面向对象的语言，所以最最基本的代码组成单元就是一个一个的类，平时我们说写Java代码，不就是写一个一个的类吗？是不是。

然后在一个一个的类里我们会定义各种变量，方法，数据结构，通过if else之类的语法，写出来各种各样的系统业务逻辑，这就是所谓的编程了。

所以JVM既然要执行你写的代码，首先当然得把你写好的类加载到内存里来啊！

所以JVM的内存区域里大家都知道，有一块区域叫做永久代，当然JDK 1.8以后都叫做Metaspace了，我们也用最新的说法好了。

这块内存区域就是用来存放你系统里的各种类的信息的，包括JDK自身内置的一些类的信息，都在这块区域里。

JVM有类加载器和一套类加载的机制，我们在专栏最开始的时候都说过了，这里不再赘述，他会负责把我们写好的类从编译好的“.class”字节码文件里加载到内存里来，如下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52465100_1578303885.cn/txdocpic/0/c231793f708c0f419d80e3f40068cf36/0)       

好，那么既然有这么一块Metaspace区域是用来存放类信息的，**那是不是有可能在这个Metaspace区域里就会发生OOM？**

没错，是有这种可能的。

**4、Java虚拟机栈：让线程执行各种方法**

大家都知道，我们写好的那些Java代码虽然是一个一个的类，但是其实核心的代码逻辑一般都是封装在类里面的各种方法中的

比如JVM已经加载了我们写好的HelloWorld类到内存里了，接着怎么执行他里面的代码呢？

Java语言中的一个通用的规则，就是一个JVM进程总是从main方法开始执行的，所以我们既然在HelloWorld中写了一个main()方法，那么当然得执行这个方法中的代码了。

但是等一等，JVM进程里的谁去执行main()方法的代码？

其实我们所有的方法执行，都必须依赖JVM进程中的某个线程去执行，你可以理解为线程才是执行我们写的代码的核心主体。

JVM进程启动之后默认就会有一个main线程，这个main线程就是专门负责执行main()方法的。

大家如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/67969700_1578303885.cn/txdocpic/0/d6a9ad82a80a9ed7c56da206869a761c/0)   现在又有一个问题了，在main()方法里定义了一个局部变量，“message”，那么大家回忆一下，这些方法里的局部变量可能会有很多，那么这些局部变量是放在哪里的呢？

很简单，每个线程都有一个自己的虚拟机栈，就是所谓的栈内存。

然后这个线程只要执行一个方法，就会为方法创建一个栈桢，将栈桢放入自己的虚拟机栈里去，然后在这个栈桢里放入方法中定义的各种局部变量，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78459400_1578303885.cn/txdocpic/0/a69acbe3d7c357327951d3a96ee225dd/0)   好，现在问题来了，大家如果还记得之前我们讲过的一个参数，应该都知道，我们是可以设置JVM中每个线程的虚拟机栈的内存大小的，一般是设置为1MB。

那么既然每个线程的虚拟机栈的内存大小是固定的，是否可能会发生虚拟机栈的内存溢出？

没错，所以**第二块可能发生OOM的区域，就是每个线程的虚拟机栈内存。**

**5、堆内存：放我们创建的各种对象**

最后我们知道，我们写好的代码里，特别在一些方法中，可能会频繁的创建各种各样的对象，这些对象都是放在堆内存里的，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94353400_1578303885.cn/txdocpic/0/379a9f83efe32446ecea99fe551182d8/0)       

而且我们通过之前的学习，也都知道了一点，通常我们在JVM中分配给堆内存的空间其实一般是固定的

既然如此，我们还不停在堆内存里创建对象，是不是说明，堆内存也有可能会发生内存溢出？

没错，**第三块可能发生内存溢出的区域，就是堆内存空间！**

**6、本文总结**

这篇文章我们从Java代码的运行的角度去分析了一下，我们写好的代码在运行的过程中涉及到了哪几块内存区域，然后这几块内存区域是不是就是有可能发生内存溢出的区域

大家先通过这篇文章的一步一图分析回顾一下，脑子里慢慢形成一些流动的图形，**接下来三天，我们就分别来分析一下Metaspace、Java虚拟机栈和堆内存这几块内存区域到底在什么情况下会发生内存溢出。**

**End**

### 073、Metaspace区域是如何因为类太多而发生内存溢出的？

**1、前文回顾**

上一篇文章我们已经把JVM的运行原理重新回顾了一遍，大家脑子里应该能重新浮现出JVM运行我们写的那些代码的流程图了。

而且结合那个运行流程，我们也点出来了有哪几块区域可能会发生内存溢出，那么今天我们就来着重分析一下Metaspace区域到底为什么会发生内存溢出。

**2、Metaspace区域是如何触发内存溢出的？**

好，我们通过之前的学习都知道，在启动一个JVM时是可以设置很多参数的，其中有一些参数就是专门用来设置Metaspace区域的内存大小的，大家如果有遗忘的回顾一下之前的文章即可。

如下两个参数就是用来设置Metaspace区域大小的：

-XX:MetaspaceSize=512m 

-XX:MaxMetaspaceSize=512m

我们看下图，图中我们就限定了Metaspace区域的内存大小为512m。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96003700_1578303894.cn/txdocpic/0/ebae405b97395f3b45bfc971638d1679/0)       

所以实际上来说，在一个JVM中，Metaspace区域的大小是固定的，比如512MB。

那么一旦JVM不停地加载类，加载了很多很多的类，然后Metaspace区域放满了，此时会如何？大家看下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8034700_1578303895.cn/txdocpic/0/ad484e64ea9c2a8b1c9c8356cd7a49f1/0)       

大家如果还记得之前我们说过的频繁Full GC触发的几个问题，其中之一就是Metaspace区域满就会触发Full GC，Full GC会带着一块进行Old GC就是回收老年代的，也会带着回收年轻代的Young GC。

当然，Full GC的时候，必然会尝试回收Metaspace区域中的类，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17344900_1578303895.cn/txdocpic/0/fd5afd50c59eda1fe936a82f9fae01ef/0)       

所以一旦Metaspace区域满了，此时会触发Full GC，连带着回收Metaspace里的类。

那么什么样的类才是可以被回收的呢？

这个条件是相当的苛刻，包括不限于以下一些：比如这个类的类加载器先要被回收，比如这个类的所有对象实例都要被回收，等等。

所以一旦你的Metaspace区域满了，未必能回收掉里面很多的类

那么一旦回收不了多少类，此时你的JVM还在拼命的加载类放到Metaspace里去，你觉得此时会发生什么事情？

显而易见，一旦你尝试回收了Metaspace中的类之后发现还是没能腾出来太多空间，此时还要继续往Metaspace中塞入更多的类，直接就会引发内存溢出的问题。因为此时Metaspace区域的内存空间不够了。

一旦发生了内存溢出就说明JVM已经没办法继续运行下去了，此时可能你的系统就直接崩溃了，这就是Metaspace区域发生内存溢出的一个根本的原理。

**3、到底什么情况下会发生Metaspace内存溢出？**

平心而论，Metaspace这块区域一般很少发生内存溢出，如果发生内存溢出一般都是因为两个原因：

- 第一种原因，很多工程师他不懂JVM的运行原理，在上线系统的时候对Metaspace区域直接用默认的参数，即根本不设置其大小

  这会导致默认的Metaspace区域可能才几十MB而已，此时对于一个稍微大型一点的系统，因为他自己有很多类，还依赖了很多外部的jar包有有很多的类，几十MB的Metaspace很容易就不够了

- 第二种原因，就是很多人写系统的时候会用cglib之类的技术动态生成一些类，一旦代码中没有控制好，导致你生成的类过于多的时候，就很容易把Metaspace给塞满，进而引发内存溢出

对于第一种问题，通常来说，有经验的工程师上线系统往往会设置对应的Metaspace大小，推荐的值在512MB那样，一般都是足够的。

对于第二种问题，我们下周就会用模拟代码给大家演示那种不停的生成大量的类的情况，让大家亲眼看到这种情况下是如何触发Metaspace内存溢出的。

**4、本文总结**

今天的文章给大家分析了一下Metaspace区域发生内存溢出的原理，同时给出了大家两种常见的触发Metaspace内存溢出的场景

大家以后只要记得，合理分配Metaspace区域，同时避免无限制的动态生成类，一般这块区域其实都是比较安全的，不至于会触发内存溢出的。

**End**

### 074、无限制的调用方法是如何让线程的栈内存溢出的？

**1、前文回顾**

上一篇文章我们已经分析了Metaspace区域内存溢出的原理和两种情况，这篇文章我们就顺着JVM的运行原理继续分析一下，线程的栈内存是如何内存溢出的。

因为在JVM加载了我们写的类到内存里之后，下一步就是去通过线程执行方法，此时就会有方法的入栈出栈相关的操作，那么我们来分析一下线程的栈内存到底是因为什么原因会导致溢出呢？

**2、一个线程调用多个方法的入栈和出栈**

大家先回顾一下之前我们画好的图，那个图是一个相对较为完整的JVM运行原理图，如下所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55744500_1578303902.cn/txdocpic/0/185b58568f2dd548384149ab971913cf/0)       

现在我们来看下面的代码：



![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70916100_1578303902.png)



按照我们之前所说的，JVM启动之后，HelloWorld类被加载到了内存里来，然后就会通过main线程执行main()方法

此时在main线程的虚拟机栈里，就会压入main()方法对应的栈桢，里面就会放入main()方法中的局部变量。

大家看看上面的图，在图里是不是有main线程的虚拟机栈和main()方法的栈桢的概念？

而且我们还知道一个概念，就是我们是可以手动设置每个线程的虚拟机栈的内存大小的，一般来说现在默认都是给设置1MB

所以看下图，main线程的虚拟机栈内存大小一般也是固定的。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92786200_1578303902.cn/txdocpic/0/e8f62c3949a55c139a00266f96ecfbf6/0)       

现在回过头思考一下上面的代码，代码中是不是在main()方法中又继续调用了一个sayHello()方法？

而且sayHello()方法中也会自己的局部变量，所以此时会继续将sayHello()方法的栈桢压入到main线程的虚拟机栈中去，如下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5216100_1578303903.cn/txdocpic/0/50dcf95cde3e9794d94cefc343565285/0)       

接着sayHello()方法如果运行完毕之后，就不需要为这个方法在内存中保存他的一些局部变量之类的东西了，此时就会将sayHello()方法对应的栈桢从main线程的虚拟机栈里出栈，如下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26585000_1578303903.cn/txdocpic/0/6aa85ce80092ce7cb6f9e38d46cad179/0)       

再接着，一旦main()方法自己本身也运行完毕，自然会将main()方法对应的栈桢也从main线程的虚拟机栈里出栈，这里我们就不在图里表示出来了。

**3、一个重要的概念：每次方法调用的栈桢都是要占用内存的**

在这里，要给大家明确一个重要的概念，那就是每个线程的虚拟机栈的大小是固定的，比如可能就是1MB，然后每次这个线程调用一个方法，都会将本次方法调用的栈桢压入虚拟机栈里，这个栈桢里是有方法的局部变量的。

虽然说一些变量和其他的一些数据占用不了太大的内存，但是大家要记得，每次方法调用的栈桢实际上也是会占用内存的！

这是非常关键的一点，哪怕一个方法调用的栈桢就占用几百个字节的内存，那也是内存占用！

**4、到底什么情况下会导致JVM中的栈内存溢出？**

既然明确了上述前提之后，那么大家思考一下，到底什么情况下JVM中的栈内存会溢出呢？

其实非常简单，既然一个线程的虚拟机栈内存大小是有限的，比如1MB，那么假设你不停的让这个线程去调用各种方法，然后不停的把方法调用的栈桢压入栈中，是不是就会不断的占用这个线程1MB的栈内存？

如下图所示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40985800_1578303903.cn/txdocpic/0/69eae644832e9b3719a9bb0c69bd0e0d/0)       

那么如果不停的让线程调用方法，不停的往栈里放入栈桢，此时终有一个时刻，大量的栈桢会消耗完毕这个1MB的线程栈内存，最终就会导致出现栈内存溢出的情况。

**5、一般什么情况下会发生栈内存溢出？**

那么一般什么情况下会发生栈内存溢出呢？

通常而言，哪怕你的线程的虚拟机栈内存就128KB，或者256KB，通常都是足够进行一定深度的方法调用的。

但是如果说你要是走一个递归方法调用，那就不一定了，看下面的代码。

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52457600_1578303903.png)

一旦出现上述代码，一个线程就会不停的调用同一个方法，即使是同一个方法，每一次方法调用也会产生一个栈桢压入栈里，比如说对sayHello()进行100次调用，那么就会有100个栈桢压入中。

所以如果疯狂的运行上述代码，就会不停的将sayHello()方法的栈桢压入栈里，最终一定会消耗掉线程的栈内存，引发内存溢出。

所以一般来说，其实引发栈内存溢出，往往都是代码里写了一些bug才会导致的，正常情况下发生的比较少。

**6、今日文章总结**

今天我们分析了栈内存溢出的根本原理和可能触发的一个场景，就是方法递归调用

但是一般来说，其实只要注意一下代码的编写，避免出现无限制的方法递归，就一般可以避免栈内存的溢出。

**End**

### 075、对象太多了！堆内存实在是放不下，只能内存溢出！

**1、前文回顾**

之前的文章已经分析了Metaspace和栈内存两块内存区域发生内存溢出的原理，同时给出了一些较为常见的引发他们内存溢出的场景，一般只要代码上注意一些，不太容易引发那两块区域的内存溢出。

本周的重点要来了，真正最容易引发内存溢出的，说白了就是平时我们系统创建出来的对象实在是太多了，最终就导致了系统的内存溢出！

**2、从对象在Eden区分配开始讲起**

如果要把这大量的对象是如何导致堆内存溢出的给讲清楚，那就得从系统运行，在Eden区创建对象开始讲起了。

咱们都知道，平时系统运行的时候一直不停的创建对象，然后大量的对象会填满Eden区

一旦Eden区满之后，就会触发一次Young GC，然后存活对象进入S区。

如下图所示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8886100_1578303910.cn/txdocpic/0/123398ab990b4e3da0a4d747241435be/0)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/20481400_1578303910.cn/txdocpic/0/9d1eb78383a489aa019d2e219226bd8b/0)       



**3、高并发场景下导致ygc后存活对象太多**

当然因为各种各样的情况，一旦出现了高并发场景，导致ygc后很多请求还没处理完毕，存活对象太多，可能就在Survivor区域放不下了，此时就只能进入到老年代里去了，老年代很快就会放满了，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/32987300_1578303910.cn/txdocpic/0/3121455f180c36c70f796f603365288d/0)       

一旦老年代放满了就会触发Full GC，如下图所示。



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/47236400_1578303910.cn/txdocpic/0/5c40cb267d2051f97ad403b1e28fa768/0)       

我们假设ygc过后有一批存活对象，Survivor放不了，此时就等着要进入老年代里，然后老年代也满了，那么就得等着老年代进行CMS GC，必须回收掉一批对象，才能让年轻代里存活下来的一批对象进去。

但是呢，不幸的事情发生了，老年代GC过后，依然存活下来了很多的对象！如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/64763300_1578303910.cn/txdocpic/0/09d52a1cc4451d47344a9e8225d455f0/0)       



这个时候如果年轻代还有一批对象等着放进老年代，人家GC过后空间还是不足怎么办？

还能怎么办！只能是内存溢出了！如下图所示！

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83165400_1578303910.cn/txdocpic/0/e0ca94f7677f1dbeaebe1d1ed960542e/0)       

所以这个时候，老年代都已经塞满了，你还要往里面放东西，而且触发了Full GC回收了老年代还是没有足够内存空间，你坚持要放？那只能给你一个内存溢出的异常了！JVM跑不动了，崩溃掉。

这个就是典型的堆内存实在放不下过多对象的内存溢出的一个典型范例。

**4、什么时候会发生堆内存的溢出？**

发生堆内存溢出的原因其实总结下来，就一句话：

有限的内存中放了过多的对象，而且大多数都是存活的，此时即使GC过后还是大部分都存活，所以要继续放入更多对象已经不可能了，此时只能引发内存溢出问题。

所以一般来说发生内存溢出有两种主要的场景：

1. 系统承载高并发请求，因为请求量过大，导致大量对象都是存活的，所以要继续放入新的对象实在是不行了，此时就会引发OOM系统崩溃
2. 系统有内存泄漏的问题，就是莫名其妙弄了很多的对象，结果对象都是存活的，没有及时取消对他们的引用，导致触发GC还是无法回收，此时只能引发内存溢出，因为内存实在放不下更多对象了

因此总结起来，一般引发OOM，要不然是系统负载过高，要不然就是有内存泄漏的问题

这个OOM问题，一旦你的代码写的不太好，或者设计有缺陷，还是比较容易引发的，所以这个问题也是我们后面要重点分析的。

**5、本文总结**

今天的文章我们分析了发生堆内存OOM的根本原因，即对象太多，且都是存活的，即使GC过后还是没有空间了，此时放不下新的对象，JVM只能选择崩溃！

希望大家好好理解本周对各种内存溢出问题的本质原理分析。

接下来我们就会用代码模拟出来各种内存溢出的场景，同时结合一些真实的案例，让大家去体验一下OOM的感觉，敬请期待

**End**

### 076、动手实验：自己模拟出JVM Metaspace内存溢出的场景体验一下！

**1、前文回顾**

上一周我们已经把内存溢出问题产生的根本原理都给大家做了详细的说明，这一周我们就进入实际的动手实验环境，带着大家来动手体验一下Metaspace、栈内存、堆内存分别都是怎么溢出的，溢出的时候会出现什么样的错误。

然后我们本周稍晚还会给出两个真实的生产案例来告诉大家平时在工作的时候，什么样的场景下可能会导致这三个区域的内存溢出。

**2、Metaspace内存溢出原理回顾**

首先，我们来回顾一下之前一直用的一张图片，大家看下图：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69483100_1578304044.cn/txdocpic/0/0e845961e8189ad655f570c970ecde7d/0)       

在这张图片里，我们通过之前的讲解，可以清晰的看到JVM整体的运行原理，包括类加载，线程执行方法，虚拟机栈，堆内存创建对象，GC以及对象转移老年代，等等。而且在这个图里，我们也清晰标志出来了哪些环节可能会发生内存溢出。

因此在这里我们来回顾一下，Metaspace区域发生内存溢出的一个场景，说白了就是如果我们在程序里不停的动态生成类，就会导致不停的加载类到Metaspace区域里去，而且这些动态生成的类必须还得是不能被回收掉的。

接着一旦Metaspace区域满了，就会触发Full GC连带着回收Metaspace中的类，但是此时大量的类是不能被回收的。

因此即使触发过Full GC过后，Metaspace区域几乎还是不能放下任何一个类，此时必然会触发Metaspace区域的内存溢出，导致JVM也是崩溃掉，无法继续运行了。

**3、到底什么是动态生成类？**

可能有的人不太理解什么叫做动态生成类，其实很简单，我们平时正常情况下，类都是通过自己的双手一行一行代码写出来的，而且都是写的“.java”后缀的源代码文件，大家想想是不是这样？

所以很多人可能想当然的以为在JVM中的类都是我们双手写出来的，其实并不是这样子。

大家回忆一下，平时我们自己写出来的类大致长什么样子？是不是一般都包含一些静态变量、实例变量、静态方法、实例方法，里面还有一大堆的业务逻辑？大致其实类就是这么个东西。

所以既然你双手都能写出来这种普普通通的类，那么当然是有办法可以借助一些方法在系统运行的时候，通过程序动态的生成出更多的类了，这是没有问题的。

所以一旦我们程序中拼命的生成大量的类，而且这些类还不能被回收，那么必然会最终导致Metaspace区域被占满，进而导致Metaspace内存溢出了。

接着我们就来实际看看代码层面上，动态生成类到底是怎么做的吧！

**4、一段CGLIB动态生成类的代码示例**

相信大家平时无论用Eclipse还是用IntelliJ IDEA，应该都大部分人都是基于Maven来进行项目构建的，当然现在也有一些公司在用Gradle进行项目构建。

我们这里就以Maven来举例好了，如果要用CGLIB来动态生成一些类，那么必须在你项目的pom.xml中引入以下的一些依赖。

<dependency>

  <groupId>cglib</groupId>

  <artifactId>cglib</artifactId>

  <version>3.3.0</version>

</dependency>

接着我们就可以使用CGLIB来动态生成类了，大家看下面的代码：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/80379300_1578304044.png)



上面那段代码稍微有点复杂是不是？

没关系，给大家解释一下，这个所谓的动态生成类大概是个什么意思。

首先我们可以看到我们在这里定义了一个类，代表了一个汽车，他有一个run()方法，执行的时候就会启动汽车，开始让汽车行驶，大家看下面的这个代码片段：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/90647100_1578304044.png)

相信关于小汽车的这个类的定义，大家都是没有问题的。那么我们接着来看下面的代码片段，我们通过CGLIB的Enhancer类生成了一个Car类的子类

注意，从这里开始，就是开始动态生成类了，大家要仔细看，看下面的代码片段：



![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98572100_1578304044.png)



你权且当做Enhancer是用来生成类的一个API吧，看到片段里我们给Enhancer设置了一个SuperClass没有？这里的意思就是说Enhancer生成的类是Car类的子类，Car类是生成类的父类。至于那个UseCache是什么意思，就先别管了。

既然Enhancer动态生成的类是Car的子类，那么是不是Car有的方法子类都有？所以子类是不是也有Car的run()方法？

答案是肯定的，但我们现在想要在调用子类的run()方法的时候做点手脚，如下面代码片段：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10222900_1578304045.png)

这个片段的意思是：如果你调用子类对象的run()方法，会先被这里的MethodInterceptor拦截一下，拦截之后，各位看里面的代码，是不是判断了一下，如果你调用的Method是run方法，那么就先对汽车做一下安全检查。

安全检查做完之后，再通过“methodProxy.invokeSuper(o, objects);”调用父类Car的run()方法，去启动汽车，这行代码就会执行到Car类的run()方法里去了。

到此为止，我们就已经通过CGLIB的Enhancer生成了一个Car类的子类了，而且定义好了对这个子类调用继承自父类的run()方法的时候先干点别的，再调用父类的run()方法。

这么一搞，是不是跟下面这种在IDE里手写一个Car的子类是类似的？

看看下面的手写版本的代码：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/18654700_1578304045.png)

看看上面那个SafeCar作为Car的子类，是不是干了一样的事？

但是这个类需要你用双手提前写出来代码，而CGLIB Enhancer那种模式可以在系统运行期间动态的创建一个Car的子类出来，实现一样的效果

看到这里，各位应该理解这个动态创建类了！

**5、限制Metaspace大小看看内存溢出效果**

接着我们可以设置一下这个程序的JVM参数，限制他的Metaspace区域比较小一点，如下所示，我们把这个程序的JVM中的Metaspace区域设置为仅仅10m：

-XX:MetaspaceSize=10m -XX:MaxMetaspaceSize=10m

接着我们可以在上述代码中做点手脚，大家看到上面的代码是有一个while循环的，所以他会不停的创建Car类的子类

我们在里面可以加入一个计数器，就是看看当前创建了多少个Car的子类了，如下所示：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/28068500_1578304045.png)

在while循环外面加一个计数器，然后打印出当前创建了多少个类了。

接着大家用上述JVM参数来运行这个程序即可，可以看到如下所示的打印输出：

目前创建了263个Car类的子类了

Exception in thread "main" java.lang.IllegalStateException: Unable to load cache item

at net.sf.cglib.core.internal.LoadingCache.createEntry(LoadingCache.java:79)

at net.sf.cglib.core.internal.LoadingCache.get(LoadingCache.java:34)

at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData.get(AbstractClassGenerator.java:119)

at net.sf.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:294)

at net.sf.cglib.reflect.FastClass$Generator.create(FastClass.java:65)

at net.sf.cglib.proxy.MethodProxy.helper(MethodProxy.java:121)

at net.sf.cglib.proxy.MethodProxy.init(MethodProxy.java:75)

at net.sf.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:226)

at com.limao.demo.jvm.Demo1$1.intercept(Demo1.java:22)

at com.limao.demo.jvm.Demo1$Car$$EnhancerByCGLIB$$7e5aa3a5_264.run(<generated>)

at com.limao.demo.jvm.Demo1.main(Demo1.java:30)

Caused by: java.lang.OutOfMemoryError: Metaspace

at java.lang.Class.forName0(Native Method)

at java.lang.Class.forName(Class.java:348)

at net.sf.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:467)

at net.sf.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:339)

at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:96)

at net.sf.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:94)

at net.sf.cglib.core.internal.LoadingCache$2.call(LoadingCache.java:54)

at java.util.concurrent.FutureTask.run(FutureTask.java:266)

at net.sf.cglib.core.internal.LoadingCache.createEntry(LoadingCache.java:61)

... 10 more

大家注意一下上述异常日志的两个地方，一个是在创建了263个类之后，10M的Metaspace区域就被耗尽了，接着就会看到异常中有如下的一个：

Caused by: java.lang.OutOfMemoryError: Metaspace。

这个OutOfMemoryError就是经典的内存溢出的问题，而且他明确告诉你，是Metaspace这块区域内存溢出了。

而且大家可以看到，一旦内存溢出，本来在运行的JVM进程直接会崩溃掉，你的程序会退出，这就是真实的内存溢出的日志。

**6、本文总结**

本文带着大家用一段CGLIB动态生成类的代码演示了一下Metaspace区域内存溢出的场景，相信大家自己动手做一下实验，就可以切实感受到这个内存溢出的效果了。

**End**

### 077、动手实验：自己模拟出JVM栈内存溢出的场景体验一下！

**1、前文回顾**

上文我们已经给大家演示了如何通过在系统中动态生成大量的类来塞满Metaspace区域，进而引发Metaspace区域的内存溢出。

这篇文章我们就来带着大家通过代码来示范一下栈内存溢出的场景。

**2、重新分析一下JVM中的栈内存**

咱们先来简单的回顾一下JVM的整体运行原理，大家不要忘记下面这张图，必须牢牢印刻在自己的脑海里。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13042200_1578304055.cn/txdocpic/0/d503efe22946ae4bccce149013474264/0)       

既然大家已经对Metaspace这块区域的内存溢出理解的很深刻了，那么接着我们来回顾一下栈内存这块区域的内存溢出。

讲到这里我们先带着大家来思考一下，JVM进程到底会占用机器上多少内存？

先不考虑一些细小的其他内存区域，就仅仅考虑一下最核心的几块就可以了，包括了：Metaspace区域，堆内存区域，各个线程的栈内存区域。

Metaspace区域我们一般会设置为512MB左右的大小，这个大小只要你代码里没有自己胡乱生成类，一般都是绝对足够存放你一个系统运行时需要的类的。

堆内存大小，之前在分析GC的时候给大家大量的分析过，堆内存一般分配在机器内存的一半就差不多了，毕竟还要考虑其他人对内存的使用。

那么最后一块内存区域我们之前一直没怎么给大家说过，就是栈内存区域。

其实大家考虑一个最基本的线上机器配置，比如4核8G的线上机器，其中512MB给了Metaspace，4G给了堆内存（其中包括了年轻代和老年代），剩余就只有3G左右内存了，要考虑到操作系统自己也会用掉一些内存。

那么剩余你就认为有一两个GB的内存可以留给栈内存好了。

通常来说，我们会设置每个线程的栈内存就是1MB，假设你一个JVM进程内包括他自带的后台线程，你依赖的第三方组件的后台线程，加上你的核心工作线程（比如说你部署在Tomcat中，那就是Tomcat的工作线程），还有你自己可能额外创建的一些线程，可能你一共JVM中有1000个线程。

那么1000个线程就需要1GB的栈内存空间，每个线程有1MB的空间。

所以基本上这套内存模型是比较合理的，其实一般来说，4核8G机器上运行的JVM进程，比如一个Tomcat吧，他内部所有的线程数量加起来在几百个是比较合理的，也就占据几百MB的内存，线程太多了，4核CPU负载也会过高，也并不好。

所以Metaspace区域+堆内存+几百个线程的栈内存，就是JVM一共对机器上的内存资源的一个消耗。

所以大家这里也能理解一个道理，你要是给每个线程的栈内存分配过大的空间，那么会导致机器上能创建的线上数量变少，要是给每个线程的栈内存相对较小，能创建的线程就会比较多一些。

当然一般来说，现在都建议给栈内存在1MB就可以了。

**3、回顾一下栈内存溢出的原理**

接着我们来回顾一下栈内存溢出的原理，其实特别的简单，大家看下图中画红圈的地方，其实每个线程的栈内存是固定的，要是你一个线程不停的无限制的调用方法，每次方法调用都会有一个栈桢入栈，此时就会导致线程的栈内存被消耗殆尽。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36302700_1578304055.cn/txdocpic/0/2bef9a4b100fb118d9342d3f6386b58c/0)       

但是通常而言你的线程不至于连续调用几千次甚至几万次方法，对不对？一般发生这种情况，只有一个原因，就是你的代码有bug，出现了死循环调用，或者是无限制的递归调用，最后连续调用几万次之后，栈内存就溢出了，没法放入更多的方法栈桢了。

**4、用一段代码示范一下栈内存溢出**

下面大家先看一段代码：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51334100_1578304055.png)

上面的代码非常简单，就是work()方法调用自己，进入一个无限制的递归调用，陷入死循环，也就是说在main线程的栈中，会不停的压入work()方法调用的栈桢，直到1MB的内存空间耗尽。

另外大家需要设置这个程序的JVM参数如下：-XX:ThreadStackSize=1m，通过这个参数设置JVM的栈内存为1MB。

接着大家运行这段代码，会看到如下所示的打印输出：

目前是第5675次调用方法

java.lang.StackOverflowError

也就是说，当这个线程调用了5675次方法之后，他的栈里压入了5675个栈桢，最终把1MB的栈内存给塞满了，引发了栈内存的溢出。大家看到StackOverflowError，就知道是线程栈内存溢出了。

**5、本文总结**

本文带着大家用代码实验了一下栈内存的溢出，大家可以看到1MB的栈内存可以让你连续调用5000次以上的方法，其实这个数量已经很多了，除了递归方法以外，一般根本不可能出现方法连续调用几千次的。

所以大家就知道，一般这种栈内存溢出是极少在生产环境出现的，即使有，一般都是代码中的bug导致的，本周后续会用真实的生产案例告诉大家当时我们线上系统是什么情况下出现过偶发性的这种问题。

**End**

### 078、动手实验：自己模拟出JVM堆内存溢出的场景体验一下！

**1、前文回顾**

上篇文章我们已经分析过了栈内存溢出的场景，同时用示例代码带着大家做了实验，体验了一下栈内存溢出的时候是什么样子的。

不过说实话，Metaspace区域和栈内存的溢出，一般都是极个别情况下才会发生的，并不是一种普遍的现象。

但是今天要给大家讲的这个堆内存溢出的场景，就是非常普遍的现象了，一旦要是系统负载过高，比如并发量过大，或者是数据量过大，或者是出现了内存泄漏的情况，很容易就导致JVM内存不够用了，就会堆内存溢出，然后系统崩溃。

所以今天就带着大家来体验一下堆内存溢出的场景。

**2、回顾一下堆内存溢出的一个典型场景**

首先还是来一张完整的JVM运行原理图，里面包含了对象的分配，GC的触发，对象的转移，各个环节如何触发内存溢出的，大家一定要牢记这张图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97897200_1578304071.cn/txdocpic/0/3e360ee4c6a46c799e658200ec621d7e/0)       

接着我们就来回顾一下一个典型的堆内存溢出的场景：

假设现在系统负载很高，不停的运转和工作，不停的创建对象塞入内存里，刚开始是塞入哪里的？

当然是年轻代的Eden区了，如下图红圈所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15613000_1578304072.cn/txdocpic/0/e6aad65c5fcd66de41936a8af3a16df0/0)       

但是因为系统负载实在太高了，很快就把Eden区塞满了，这个时候触发ygc

但是ygc的时候发现不对劲，因为似乎Eden区里还有很多的对象都是存活的，而且survivor区域根本放不下，这个时候只能把存活下来的大批对象放入老年代中去，如下图红圈处。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/28536500_1578304072.cn/txdocpic/0/9c10c00d34d9e238e44a65cdffc184d9/0)       

就这么来几次ygc之后，每次ygc后都有大批对象进入老年代，老年代很快就会塞满了，而且最重要的是这里的对象还大多都是存活的。

所以接下来一次ygc后又要转移一大批对象进入老年代，先触发full gc，但是full gc之后老年代里还是塞满了对象，如下图红圈所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40669500_1578304072.cn/txdocpic/0/c58e6dc690f01f78d9b41dd2f23df1a7/0)       

这个时候ygc后存活下来的对象哪怕在full gc之后还是无法放入老年代中，此时就直接报出堆内存溢出了，如下图红圈所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51751600_1578304072.cn/txdocpic/0/3f12d5093186f7a5746b5df9eb98a229/0)       

所以堆内存溢出的场景就是这样子，非常的简单，后续我们会用真实的案例结合一些业务系统的实际情况给大家继续展示堆内存溢出的一些场景。

**3、用示例代码来演示堆内存溢出的场景**

我们来看下面这段代码： 

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66014200_1578304072.png)

代码很简单，就是在一个while循环里不停的创建对象，而且对象全部都是放在List里面被引用的，也就是不能被回收。

大家试想一下，如果你不停的创建对象，Eden区满了，他们全部存活会全部转移到老年代，反复几次之后老年代满了。

然后Eden区再次满了，ygc后存活对象再次进入老年代，此时老年代先full gc，但是回收不了任何对象，因此ygc后的存活对象就一定是无法进入老年代的。

所以我们用下面的JVM参数来运行一下代码：-Xms10m -Xmx10m，我们限制了堆内存大小总共就只有10m，这样可以尽快触发堆内存的溢出。

我们在控制台打印的信息中可以看到如下的信息：

当前创建了第360145个对象

Exception in thread "main" java.lang.OutOfMemoryError: Java heap space

所以从这里就可以看到，在10M的堆内存中，用最简单的Object对象搞到老年代被塞满大概需要36万个对象。然后堆内存实在放不下任何其他对象，此时就会OutOfMemory了，而且告诉了你是Java heap space，也就是堆空间发生了内存溢出的。

**4、本文总结**

从这篇文章我们可以看到堆内存是如何溢出的，以及溢出的时候回看到什么样的异常信息

接下来我们将会用两天的时间给大家讲几个真实的线上生产案例，让大家感受一下我们曾经线上系统是如何遇到内存溢出的。

**End**

### 079、案例实战：一个超大数据量处理系统是如何不堪重负OOM的？

**1、前文回顾**

之前我们已经用代码给大家都演示过几种不同的内存溢出的场景了，但是光看代码演示可能大家还是找不到感觉。因此，我们同样也会用曾经遇到过的真实线上系统运行场景来让大家看看是如何触发堆内存溢出的。

**2、还是那个超大数据量处理系统的案例**

大家是否还记得我们不止一次提过的一个超大数据量的计算引擎系统？这个系统是我们自己研发的一个非常复杂的PB级数据计算系统，远比很多流行的开源技术要强悍，架构上也非常复杂，同时处理的数据量也特别大。

但是因为这个专栏并不是给大家讲这种具体的系统，因此我们还是用简化的案例场景来给大家解释当时遇到的线上OOM问题。

之前就用这个系统案例给大家分析过GC问题，但是因为他处理的数据量实在是很大，负载也过高，所以除了GC问题以外，还有OOM问题。

首先用最最简化的一张图给大家解释系统的工作流程。简单来说，就是不停的从数据存储中加载大量的数据到内存里来进行复杂的计算，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26130000_1578304079.cn/txdocpic/0/244ef735c4e7f5e3f5c8140555c3482a/0)    

这个系统会不停的加载数据到内存里来计算，每次少则加载几十万条数据，多则加载上百万条数据，所以系统的内存负载压力是非常大的。

另外这里给大家多讲一些之前案例中没提到过的这个系统的一些运行流程，因为他跟我们这次讲解的OOM场景是有关系的。

这个系统每次加载数据到内存里计算完毕之后，就需要将计算好的数据推送给另外一个系统，两个系统之间的数据推送和交互，最适合的就是基于消息中间件来做

因此当时就选择了将数据推送到Kafka，然后另外一个系统从Kafka里取数据，如下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36905400_1578304079.cn/txdocpic/0/37b3000a1c5a78c4dd2e033bfa4a56f9/0)       这就是系统完整的一个运行流程，加载数据、计算数据、推送数据。

**3、针对Kafka故障设计的高可用场景**

既然系统架构如此，那么大家思考一下，数据计算系统要推送计算结果到Kafka去，万一Kafka挂了怎么办？此时就必须设计一个针对Kafka的故障高可用机制

就当时而言，刚开始负责这块的工程师选择了一个思考欠佳的技术方案。一旦发现Kafka故障，就会将数据都留存在内存里，不停的重试，直到Kafka恢复才可以，大家看下图的示意。     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/45937600_1578304079.cn/txdocpic/0/6dbc7a4b29b3db8f610dba83082a62bb/0)       

这个时候就有一个隐患了，万一真的遇上Kafka故障，那么一次计算对应的数据必须全部驻留内存，无法释放，一直重试等待Kafka恢复，这是绝对不合理的一个方案设计。

然后数据计算系统还在不停的加载数据到内存里来处理，每次计算完的数据还无法推送到Kafka，全部得留存在内存里等着，如此循环往复，必然导致内存里的数据越来越多。

**4、无法释放的内存最终导致OOM**

正是因为有这个机制的设计，所以有一次确实发生了Kafka的短暂临时故障，也因此导致了系统无法将计算后的数据推送给Kafka

然后所有数据全部驻留在内存里等待，并且还在不停的加载数据到内存里来计算。

内存里的数据必然越来越多，每次Eden区塞满之后，大量存活的对象必须转入老年代中，而且这些老年代里的对象还是无法释放掉的。

老年代最终一定会满，而且最终一定会有一次Eden区满之后，一大批对象要转移到老年代，结果老年代即使Full gc之后还是没有空间可以放的下，最终就会导致内存溢出。然后线上收到报警说内存溢出。

最后这个系统全线崩溃，无法正常运行。

如何对这个问题进行修复呢？

其实很简单，当时就临时直接取消了Kafka故障下的重试机制，一旦Kafka故障，直接丢弃掉本地计算结果，允许释放大量数据占用的内存。后续的话，将这个机制优化为一旦Kafka故障，则计算结果写本地磁盘，允许内存中的数据被回收。

这就是一个非常真实的线上系统设计不合理导致的内存溢出问题，想必大家看了这个案例后，一定对内存溢出问题感触更加深刻了。

后面我们还将有更多的各种各样奇形怪状的OOM案例带给大家，并且要给大家介绍很多解决OOM问题的技巧。

**End**

### 080、案例实战：两个新手工程师误写代码是如何导致OOM的？

**1、前文回顾**

上周五的文章给大家介绍了一个系统设计上的缺陷导致的极端场景下的OOM，让大家从较为真实的系统案例中感受一下OOM到底是如何发生的。

今天的文章就给大家说一说之前遇到过的团队里两个新手工程师误写代码导致的OOM的问题。

**2、第一个案例：一时迷糊写出了一个无限循环调用**

第一个案例是当时团队里招聘的一个实习生同学，写出了一个代码上的bug导致线上系统出现栈内存溢出的场景。

当时有一个非常重要的系统，我们设计了一个链路监控机制，也就是会在一个比较核心的链路节点，写一些重要的日志到Elasticsearch集群里去，事后会基于ELK进行核心链路日志的一些分析，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/22095200_1578304086.cn/txdocpic/0/00ee0812612810a7f946a17a8660930b/0)       

同时我们对这个机制做了规定，如果在某个节点写日志时发生了某些异常，此时也必须将这个链路节点的异常写入ES集群里去，因为我们在分析的时候，需要知道系统运行到这里有一个异常。

因此当时那个实习生同学写出来的伪代码大致如下：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/32577300_1578304086.png)



不知道大家看了上面的代码是作何感想？当时这个同学居然在log()方法中一旦ES集群出现故障的时候再次调用了自己，继续尝试将日志写入ES集群。

因此在线上系统中，有一次ES集群短暂故障了一会儿，结果直接就导致log()方法中写ES集群每次都是失败的，都会抛异常。

而一旦抛异常进入了catch语句中，就会再次重新回过头来调用log()方法。

然后log()方法再次写ES集群发现不行，继续抛异常进入catch中，再次循环调用自己。

线上系统本来在ES集群故障的时候不该有什么问题的，因为核心业务逻辑都是可以运行的，最多不过就是无法把核心日志写入ES集群罢了。

但是因为这个bug，导致在ES故障时，所有系统全部在写日志的时候，陷入了一个无限循环调用log()方法的困境中。

之前给大家演示过，一旦无限循环调用方法自己，一定会在一定时间导致线程的栈内存溢出的，此时直接会导致JVM进程的崩溃

系统居然因为这么一个小问题崩溃了！这就是一次非常真实的线上案例。

后来针对此类问题，我们都是通过严格的持续集成+严格的Code Review标准来避免的

每个人每天都会写一点代码，这个代码必须是配套单元测试可以运行的，然后全部提交到持续集成服务器上去，代码集成到整体代码中，自动运行全部单元测试+集成测试。

在单元测试+集成测试中，我们都是要求针对一些try catch中可能走到catch的分支写一些测试的，因此一旦有这类代码，每天只要工程提交到持续集成系统上去，立马就会自动运行测试触发这个问题，就可以立刻解决了。

而且每天这一点代码也必须交给指定的其他同事进行Code Review，别人会仔细看你今天写的每一行代码，做一个审查，一旦发现问题也会打回去重新修改代码。从此之后，这种低端的问题再也没有发生过。

**3、第二个案例：没有缓存的动态代理**

第二个案例同样是之前的一个新手工程师写的，这个并不是实习生，是一个校招生同学，在团队里工作了1年左右的时间。

但是确实因为经验不足，有一次在实现一块代码机制的时候，也是犯了一个很大的错误。

简单来说，当时这个同学想要实现一个动态代理机制，也就是说在系统运行的时候，针对已有的某个类，生成一个动态代理类，也就是动态生成类，然后对那个类的一些方法调用做一些额外的处理。

当时这个是一个我们自己研发的分布式事务框架，对于这个框架是有这位同学参与在里面的，因此在框架中有时候要对一些已经有的类实现动态代理，去实现分布式事务一些的复杂底层机制。

当时大概的一个伪代码其实跟之前给大家的代码是类似的：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/46663800_1578304086.png)

不知道大家发现类似这种代码里的一个问题没有？比如你用CGLIB的Enhancer针对某个类动态生成了一个子类，这个子类你完全可以缓存起来，下次直接用这个已经生成好的子类来创建对象就可以了

类似下面这样：

![blob.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/57945100_1578304086.png)

其实这个类只要生成一次就可以了，下次来直接用这个动态生成的类创建一个对象就可以了。

但是当时那个工程师没有缓存这个动态生成的类，就是每次调用方法都生成一个类，这就闯祸了。

有一次线上系统负载很高的时候，因为这个框架直接导致瞬间创建了一大堆的类，塞满了Metaspace区域无法回收，进而导致Metaspace区域直接内存溢出，系统也崩溃了，这也是一个很大的问题。

后来对于这类问题，是严格要求每次上线必须走严格的自动化压力测试，通过高并发压力下系统是否正常运行支撑24小时，来判断是否可以上线。

这样类似于这类代码在上线之前就会被压力测试露出马脚，因为压力一大，瞬间会引发这个问题。

**4、本文总结**

这周的文章，我们带着大家感受了一下各种内存溢出发生的场景，同时给出了几个真实的线上生产案例是如何导致各个内存区域溢出的

相信大家对内存溢出这个问题，有了一个更加深刻的理解。

接下来我们会带着大家一起来学习如何对线上的OOM进行监控，同时在OOM时如何让JVM自动保留现场，同时结合几个案例和工具学习，发生OOM之后如何快速排查和定位到底代码哪里出现了OOM，以及如何进行解决。

最后做一个说明，有同学反馈专栏的目录和最初的大纲写的略有出入。这是因为整个专栏的写作是一个很长的过程，在这过程中我会根据大家每天的提问反馈，对大家的掌握专栏的情况进行评估，然后在此基础上进行微调，以便于大家更好的掌握。专栏干货只会多，不会少，这个请大家放心。

**End**

### 081、如何对对线上系统的OOM异常进行监控和报警！

**1、前文回顾**

我们之前已经把OOM的一些核心原理以及事发现场和一些典型案例都给大家讲清楚了，相信大家现在对OOM已经比较了解了

那么现在的一个核心问题就是一旦发生了各种场景下的OOM，我们到底应该如何处理呢？

所以本周将会从OOM问题的监控开始，给大家讲OOM的排查、定位和解决的一系列思路

然后下周开始再给大家分析多个各种场景下的奇怪的OOM案例，让大家去积累丰富的OOM问题解决经验。

**2、最佳的解决方案**

我们先给大家说一种最佳的OOM监控方案，其实说白了也很简单，之前一直给大家强调，公司最好是应该有一种监控平台，比如Zabbix、Open-Falcon之类的监控平台。

如果有监控平台的话，就可以接入系统异常的一些监控和报警，你可以设置一旦系统出现了OOM异常，就发送报警给对应的开发人员，通过邮件、短信或者钉钉之类的IM工具。

这个是中大型公司里最常用的一种方案了，一般来说我们都对线上系统有以下几个层面的监控：

机器（CPU、磁盘、内存、网络）资源的负载情况，JVM的GC频率和内存使用率，系统自身的业务指标，系统的异常报错。

这些东西都会基于监控平台接入对应的监控项，同时设定关键监控项的一些报警阈值。

下面我来分层给大家解释一下这个所谓的监控体系的思路，我们这个专栏虽然不是专门教大家做监控的，但是因为说到了系统的JVM监控，因此可以顺带给大家说一下思路，大家再结合自己公司的监控系统去考虑一下怎么做。

**3、一个比较成熟的系统监控体系的建议**

首先通过监控平台是可以看到你的所有线上系统所在的机器资源的负载情况的，比如CPU负载，这个可以看到现在你的CPU目前的使用率有多高，比如你的CPU使用率都达到100%了，此时一定有问题了，你得检查一下为什么CPU负载那么高。

而且可以看到你的机器上磁盘IO的一些负载，包括磁盘上发生了多少数据量的IO，一些IO的耗时等等。

当然一般的业务系统本身不会直接读写自己本地的磁盘IO，最多就是写一些本地日志而已。

但是你应该关注的是你本地磁盘的使用量和剩余空间，因为有的系统因为一些代码bug，可能会一直往本地磁盘写东西，万一把你的磁盘空间写满了就麻烦了，这样也会导致你的系统无法运行。

其次可以看到你机器上的内存使用量，这个是从机器整体层面去看的，看看机器对内存使用的一些变化。

当然内存这块，比较核心的还是JVM这块的监控，我们是可以看到JVM各个内存区域的使用量的一个变化的。

最后就是机器上的网络负载，就是通过网络IO读写了多少数据，一些耗时，等等。

还有一个比较关键的，就是JVM的Full GC的频率，这个一般会用一段时间内的Full GC次数来监控，比如5分钟内发生了几次Full GC。

其实线上机器最容易出问题的主要三大块，一个是CPU，必须要对CPU的使用率做一个监控，如果CPU负载过高，比如长期使用率超过90%，就得报警了；

一个是内存，同样得监控内存的使用率，如果机器内存长期使用率超过了一定的阈值，比如长期使用率超过90%，那肯定是有问题的，随时机器内存可能就不够了；

一个是JVM的Full GC问题，假设5分钟内发生了10次Full GC，那一定是频繁Full GC了。

所以建议大家去看看自己公司是否有监控平台，同时是否建立起来基本的监控体系， 对CPU、内存、Full GC等核心问题进行了监控和自动报警。

另外比较常见的就是对系统的业务指标的监控，比如你可以在每次系统创建一个订单就上报一次监控，然后监控系统会收集你1分钟内的订单数量。然后你可以设定一个阈值，比如1分钟内要是订单数量超过了100就报警。

因为可能订单过多涉及到了一些刷单之类的行为，这就是业务系统的指标监控，这个都是你自己去进行指标上报的。

最后一个，就是系统所有的try catch中的异常报错，必须要接入报警，一旦有异常，都需要上报到监控平台，然后监控平台会告诉你，最近有一次异常，只要系统报错，你立马可以收到报警。

因此非常核心的一点，就是要对线上系统的异常进行监控，一旦JVM有OOM之类的问题可以立马感知到。

**4、一种比较Low的JVM OOM问题的被动发现方法**

如果大家实在没有上述那种监控和报警的体系，那就没办法主动感知到JVM OOM问题了，此时只能用最Low的土办法就发现OOM了。

被动发现OOM问题，主要靠两个：

第一个是线上系统假设因为OOM突然挂掉，此时一定会导致用户无法使用，然后迅速反馈到客服，客服反馈给你，你就知道自己的系统挂掉了。

第二个就是你必须去检查一下系统的线上日志，一般来说，系统有异常的时候，必须通过log4j之类的日志框架写入本地日志文件，如果这个都不做，那实在是没办法说什么了。只要你检查日志，就会发现之前给大家演示过的OOM问题。

**5、本文总结**

今天给大家总结了一下**发现OOM的两种办法：**一种是通过监控系统，一种是被动等待系统挂掉后客服来通知你

大家可以思考一下，在自己的公司，到底应该怎么来做这个OOM的监控，只有你监控到OOM问题了，接着才能去分析到底为什么会发生这个问题。

**End**

### 082、一个关键问题：如何在JVM内存溢出的时候自动dump内存快照？

**1、前文回顾**

上一篇文章已经给大家介绍了线上运行的系统如果发生OOM异常如何监控到

要不是主动监控，要不是被动发现，总之无论如何，你的系统在经过你的GC优化后，平时也许跑的很正常，结果突然某一天就OOM了，那你总是会知道的。

今天的文章就给大家讲一下，假设你知道了自己的系统发生OOM了，应该怎么来处理呢？

**2、解决OOM问题的一个初步思路**

首先第一个问题，假设发生OOM了，必然说明系统中某个区域的对象太多了，塞满了那个区域，而且一定是无法回收掉那些对象，最终才会导致内存溢出的。

既然是这个思路，要解决OOM的话，首先就得知道到底是什么对象太多了最终导致OOM的？

所以你想知道什么对象太多导致OOM的，就必须得有一份JVM发生OOM时的dump内存快照

只要有了那个dump内存快照，你就可以用之前介绍过的MAT之类的工具瞬间分析得到什么对象太多了。

那么现在一个关键问题来了，到底怎么做才可以在JVM内存溢出的时候自动dump出来一份内存快照呢？

**3、在OOM的时候自动dump内存快照**

看到这里，大家必须得对一个事情有个概念，大家可以思考一下，假设JVM发生OOM了，你觉得JVM是完全来不及处理然后突然进程就没了吗？也就是JVM是看起来非常突然的自己无法控制的就挂掉了吗？

其实不是的，只要之前对JVM OOM的各种情况原理有一定的了解，就会知道JVM本身在发生OOM之前都会尽可能的去进行GC腾出来一些内存空间

如果GC后还是没有空间，放不下对象， 才会触发内存溢出的。

所以JVM自己对OOM情况的发生是完全有把控权的，他知道什么时候会触发OOM，也是他自己感觉不行的时候才会去触发的

所以OOM的发生并不是大家想的那样，突然之间内存太多了，JVM自己都没反应过来就直接崩溃了，并非如此。

因此JVM如果知道要发生OOM了，此时完全可以让他做点事情

什么事情呢？

我们可以让他在OOM时dump一份内存快照，事后我们只要分析这个内存快照，一下就可以知道是哪些可恶的对象占用了所有的内存，并且还无法释放。

此时你就需要在JVM的启动参数中加入如下的一些参数：

-XX:+HeapDumpOnOutOfMemoryError 

-XX:HeapDumpPath=/usr/local/app/oom

第一个参数意思是在OOM的时候自动dump内存快照出来，第二个参数是说把内存快照放到哪儿去

只要你加入了这两个参数，在JVM OOM崩溃的时候，无论你是立马主动收到一个报警，还是被动让客服通知了你，立马就可以去找OOM时候的内存快照了

**4、迄今为止我们可以得出的一份JVM参数模板**

在学习完这篇文章之后，我们最常用的一些JVM参数已经全部学习完了，而且可以总结一份JVM参数模板出来供大家进行参考，大家未来对自己的系统只要根据自己的情况去调整就可以了：

“-Xms4096M -Xmx4096M -Xmn3072M -Xss1M  -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFaction=92 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -XX:+PrintGCDetails -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/app/oom”

这份JVM参数模板基本上涵盖了所有你需要的一些参数

首先是各个内存区域的大小分配，这个是需要你精心调优的

其次是两种垃圾回收器的指定，接着是一些常规性的CMS垃圾回收的参数，可以帮助优化偶尔发生的Full GC性能。

**最重要的**，就是平时要打印出来GC日志，GC日志可以配合你用jstat工具分析GC频率和性能的时候用，jstat可以分析出来GC的频率，但是对每次具体的GC情况，可以结合GC日志来看。

还有就是在OOM的时候需要自动dump内存快照，这样即使突然发生OOM，你只要得知了这个事，立马就可以去分析内存快照了。

接下来，**我们会用三篇文章来依次带着大家用MAT工具来分析Metaspace、栈内存、堆内存三种内存溢出下的内存快照文件**

同时结合GC日志，让大家更加清晰的理解每次发生内存溢出时候的具体过程和原理，以及解决内存溢出的排查过程。

**End**

### 083、动手实验：Metaspace区域内存溢出的时候，应该如何解决？

**1、前文回顾**

上一讲已经说了我们处理OOM需要的一些参数，今天我们来讲一下Metaspace区域内存溢出

我们先分析一下GC日志，然后再让JVM自动dump出来内存快照，最后用MAT来分析一下这份内存快照，从内存快照里去找到内存溢出的原因。

**2、示例代码**

首先我们先上之前的那段代码：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/34716200_1578304106.png)

我们还是用这段代码来说明，但是要记得需要在JVM参数中加入一些东西，因为我们想看一下GC日志和导出内存快照，如下所示：

-XX:+UseParNewGC

-XX:+UseConcMarkSweepGC

-XX:MetaspaceSize=10m

-XX:MaxMetaspaceSize=10m

-XX:+PrintGCDetails

-Xloggc:gc.log

-XX:+HeapDumpOnOutOfMemoryError

-XX:HeapDumpPath=./

大家要注意，上面那个HeapDumpPath参数我给调整为当前项目的根目录下了，这样我们看的时候方便一些。

**3、分析GC日志**

接着我们用上述JVM参数运行这段程序，会发现项目下面多了两个文件，一个是gc.log，还有一个是java_pid910.hprof

当然不同的机器运行这个hprof文件的名字是不太一样的，因为他会用你的PID进程id作为文件名字。

接着我们先来分析一下gc.log，也就是分析一下他是如何拼命往Metaspace区域里放入大量生成的类，然后触发Full GC，接着回收Metaspace区域，回收后还是无法放下更多的类，接着才会抛出内存溢出的异常。

然后我们再用MAT分析一下OOM的时候的内存快照，带着大家学习一下如何用MAT工具找到Metaspace内存溢出问题的原因。

先把我这里的GC日志给大家抛出来，同时我们就跟着GC日志一行一行分析，到底是怎么回事，大家紧紧跟着脚步来走。

0.716: [GC (Allocation Failure) 0.717: [ParNew: 139776K->2677K(157248K), 0.0038770 secs] 139776K->2677K(506816K), 0.0041376 secs] [Times: user=0.03 sys=0.01, real=0.00 secs]

大家看这行日志，这是第一次GC，他本身是一个Allocation Failure的问题

也就是说，他是在Eden区中分配对象时，发现Eden区内存不足了，于是就触发了一次ygc。

那么，这个对象到底是什么对象？

简单，还记得我们在代码里写的么？Enhancer本身是一个对象，他是用来生成类的，如下所示：Enhancer enhancer = new Enhancer()。

接着我们基于每次Enhancer生成的类还会生成那个类的对象，如下所示：Car car = (Car) enhancer.create()。

因此上述代码不光是动态生成类，本身他也是对应很多对象的，因此你在while(true)循环里不停的创建对象，当然会塞满Eden区了，大家看上述日志：[ParNew: 139776K->2677K(157248K), 0.0038770 secs] 

这就是说，在默认的内存分配策略下，年轻代一共可用空间是150MB左右，这里还包含了一点Survivor区域的大小

然后大概都用到140MB了，也就是Eden区都塞满了，此时就触发了Allocation Failure，没Eden区的空间分配对象了，此时就触发ygc。

这个倒没什么可说的，因为之前我们都讲过了。

0.771: [Full GC (Metadata GC Threshold) 0.771: [CMS: 0K->2161K(349568K), 0.0721349 secs] 20290K->2161K(506816K), [Metaspace: 9201K->9201K(1058816K)], 0.0722612 secs] [Times: user=0.12 sys=0.03, real=0.08 secs]

接着我们来看这次GC，这就是Full GC了，而且通过“Metadata GC Threshold”清晰看到，是Metaspace区域满了，所以触发了Full GC

这个时候看下面的日志，20290K->2161K(506816K)，这个就是说堆内存（年轻代+老年代）一共是500MB左右，然后有20MB左右的内存被使用了，这个必然是年轻代用的。

然后Full GC必然会带着一次Young GC，因此这次Full GC其实是执行了ygc了，所以回收了很多对象，剩下了2161KB的对象，这个大概就是JVM的一些内置对象了。

然后直接就把这些对象放入老年代，为什么呢，因为下面的日志：[CMS: 0K->2161K(349568K), 0.0721349 secs] 

这里明显说了，Full GC带着CMS进行了老年代的Old GC，结果人家本来是0KB，什么都没有，然后从年轻代转移来了2161KB的对象，所以老年代变成2161KB了。

接着看日志： [Metaspace: 9201K->9201K(1058816K)]

此时Metaspace区域已经使用了差不多9MB左右的内存了，此时明显是发现离我们限制的10MB内存很接近了，所以触发了Full GC，但是对Metaspace GC后发现类全部存活了，因此还是剩余9MB左右的类在Metaspace里。

0.843: [Full GC (Last ditch collection) 0.843: [CMS: 2161K->1217K(349568K), 0.0164047 secs] 2161K->1217K(506944K), [Metaspace: 9201K->9201K(1058816K)], 0.0165055 secs] [Times: user=0.03 sys=0.00, real=0.01 secs]

接着又是这次Full GC，人家也说的很清晰了，Last ditch collection

就是说，最后一次拯救的机会了，因为之前Metaspace回收了一次但是没有类可以回收，所以新的类无法放入Metaspace了。

所以再最后试一试Full GC，能不能回收掉一些

结果如下：[Metaspace: 9201K->9201K(1058816K)], 0.0165055 secs]

Metaspace区域还是无法回收掉任何的类，几乎还是占满了我们设置的10MB左右。

0.860: [GC (CMS Initial Mark) [1 CMS-initial-mark: 1217K(349568K)] 1217K(506944K), 0.0002251 secs] [Times: user=0.00 sys=0.00, real=0.00 secs]

0.860: [CMS-concurrent-mark-start]

0.878: [CMS-concurrent-mark: 0.003/0.018 secs] [Times: user=0.05 sys=0.01, real=0.02 secs]

0.878: [CMS-concurrent-preclean-start]

Heap

par new generation  total 157376K, used 6183K [0x00000005ffe00000, 0x000000060a8c0000, 0x0000000643790000)

 eden space 139904K,  4% used [0x00000005ffe00000, 0x0000000600409d48, 0x00000006086a0000)

 from space 17472K,  0% used [0x00000006086a0000, 0x00000006086a0000, 0x00000006097b0000)

 to  space 17472K,  0% used [0x00000006097b0000, 0x00000006097b0000, 0x000000060a8c0000)

concurrent mark-sweep generation total 349568K, used 1217K [0x0000000643790000, 0x0000000658cf0000, 0x00000007ffe00000)

Metaspace    used 9229K, capacity 10146K, committed 10240K, reserved 1058816K

 class space   used 794K, capacity 841K, committed 896K, reserved 1048576K

接着就直接JVM退出了，退出的时候就打印出了当前内存的一个情况，年轻代和老年代几乎没占用，但是Metaspace的capacity是10MB，使用了9MB左右，无法再继续使用了，所以触发了内存溢出。

此时就会在控制台打印出如下的一行东西：

Caused by: java.lang.OutOfMemoryError: Metaspace

at java.lang.ClassLoader.defineClass1(Native Method)

at java.lang.ClassLoader.defineClass(ClassLoader.java:763)

... 11 more

明确抛出异常，说OutOfMemoryError，原因就是Metaspace区域满了导致的。

因此假设是Metaspace内存溢出了，然后客服通知了我们，或者我们自己监控到了异常，此时直接去线上机器看一下GC日志和异常信息就可以了，通过上述分析立刻就知道了，系统是如何运行的，触发了几次GC之后引发了内存溢出。

**4、分析内存快照**

当我们知道是Metaspace引发的内存溢出之后，立马就可以把内存快照文件从线上机器拷回本地笔记本电脑，打开MAT工具进行分析，如下图所示：     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/53277400_1578304106.cn/txdocpic/0/b619d4bb76e8e2dd7be69ca0368c4421/0)       

从这里可以看到实例最多的就是AppClassLoader

为啥有这么多的ClassLoader呢？一看就是CGLIB之类的东西在动态生成类的时候搞出来的，我们可以点击上图的Details进去看看。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/73605100_1578304106.cn/txdocpic/0/f4c5c4871bff5c72f243b677ee22fbb8/0)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4130700_1578304107.cn/txdocpic/0/533f47a0dbec035cdc6d7a0099d5e6df/0)       

为什么这里有一大堆咱们自己的Demo1中动态生成出来的Car$$EnhancerByCGLIB的类呢？

看到这里就真相大白了，上图已经清晰告诉我们，是我们自己的哪个类里搞出来了一大堆的动态生成的类，所以填满了Metaspace区域。

所以此时直接去代码里排查动态生成类的代码即可。

**解决这个问题的办法也很简单，直接对Enhancer做一个缓存，只有一个，不要无限制的去生成类就可以了。**

**5、本文总结**

今天这篇文章，带着大家全程基于示例代码从GC日志到内存快照进行了一通分析

从GC日志我们知道系统是如何在多次GC之后无奈内存溢出的

从内存快照我们就知道到底是什么东西占据了太多的内存，然后代码里找到原因解决即可。

希望大家跟着文章，也在自己本地实战一把，这样才能真正消化这些内容，转化为自己的东西。

**End**

### 084、动手实验：JVM栈内存溢出的时候，应该如何解决？

**1、前文回顾**

之前的文章，我们分析了Metaspace区域是如何内存溢出的，同时还带着大家分析了一下内存快照。

今天这篇文章，我们就带大家分析一下JVM栈内存溢出的时候，怎么来解决。

**2、栈内存溢出能依托之前的办法解决吗？**

首先大家思考一个问题：栈内存溢出能按照之前的方法解决吗？

也就是说，GC日志、内存快照，这些东西对解决栈内存溢出有帮助吗？

首先明确一点，栈内存溢出跟堆内存是没有关系的，因为他的本质是一个线程的栈中压入了过多方法调用的栈桢，比如几千次方法调用的几千个栈桢。

此时就导致线程的栈内存不足，无法放入更多栈桢了。

所以GC日志对你有用吗？

**没用！**因为GC日志主要是分析堆内存和Metaspace区域的一些GC情况的，就线程的栈内存和栈桢而言，他们不存在所谓的GC。

如果大家还记得之前我们画的图，就应该知道，调用一个方法时在栈里压入栈桢，接着执行完整个方法，栈桢从栈里出来，然后一个线程运行完毕时，他的栈内存就没了。

所以本身这块内存不存在所谓的GC和回收，调用方法就给栈桢分配内存，执行完方法就回收掉那个栈桢的内存。

那么内存快照呢？

内存快照主要是分析一些内存占用的，同样是针对堆内存和Metaspace的，所以对线程的栈内存而言，也不需要借助这个东西。

**3、示例代码**

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/7551100_1578304115.png)



使用的JVM参数如下：

- -XX:ThreadStackSize=1m
- -XX:+PrintGCDetails
- -Xloggc:gc.log
- -XX:+HeapDumpOnOutOfMemoryError
- -XX:HeapDumpPath=./
- -XX:+UseParNewGC
- -XX:+UseConcMarkSweepGC

**4、运行代码后分析异常报错信息的调用栈**

接着我们运行代码让他产生栈内存溢出的报错，如下：

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

at com.limao.demo.jvm.Demo2.work(Demo2.java:13)

实际上我们会在这里看到大段大段的如上所示的异常，也就是说，他会直接告诉你这个栈内存溢出的问题，是因为你拼命的调用Demo2这个类的work()方法时发生的。

因此就栈内存溢出而言，我们定位和解决问题非常的简单，你只要把所有的异常都写入本地日志文件，那么当你发现系统崩溃了，第一步就去日志里定位一下异常信息就知道了。

比如，昨天我们通过异常信息直接定位出来是Metaspace区域出的异常，然后分析一下GC日志就完全知道发生溢出的全过程，接着再分析一下MAT的内存快照，就知道是哪个类太多导致的异常。

今天的栈内存溢出，我们直接去日志文件里看到是栈内存溢出：Exception in thread "main" java.lang.StackOverflowError。

此时心里就有数了，然后直接看一下对应报错的方法就可以了。知道是哪个方法，直接去代码中定位问题即可。

**5、本文总结**

今天的文章带着大家学习了一下栈内存溢出的解决办法，其实这是最容易定位和解决的一种异常了，大家只要有一个好习惯，就是把异常信息都写入本地日志文件，系统崩溃了直接看日志就知道怎么回事了。

**End**

### 085、动手实验：JVM堆内存溢出的时候，应该如何解决？

**1、前文回顾**

上一篇文章已经给大家分析了栈内存溢出是如何来解决的，这篇文章我们给大家分析一下最常见的堆内存溢出是如何来解决的。

**2、示例代码**

我们还是沿用之前的示例代码：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/49891500_1578304123.png)



采用的JVM参数如下：

-Xms10m

-Xmx10m

-XX:+PrintGCDetails

-Xloggc:gc.log

-XX:+HeapDumpOnOutOfMemoryError

-XX:HeapDumpPath=./

-XX:+UseParNewGC

-XX:+UseConcMarkSweepGC

接着我们运行上述程序。

**3、运行后的观察**

其实堆内存溢出的现象也是很简单的，在系统运行一段时间之后，直接会发现系统崩溃了，然后登录到线上机器检查日志文件

先看到底为什么崩溃：

java.lang.OutOfMemoryError: Java heap space

Dumping heap to ./java_pid1023.hprof ...

Heap dump file created [13409210 bytes in 0.033 secs]

Exception in thread "main" java.lang.OutOfMemoryError: Java heap space

这个就很明显告诉我们，是Java堆内存溢出了，而且他还给我们导出了一份内存快照。

所以此时我们GC日志都不用分析了，因为堆内存溢出往往对应着大量的GC日志，所以你分析起来是很麻烦的。

此时直接将线上自动导出的内存快照拷贝回本地笔记本电脑，然后用MAT分析即可。

**4、用MAT分析内存快照**

采用MAT打开内存快照之后会看到下图：     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69128700_1578304123.cn/txdocpic/0/9ec2faed059d171f940cc4fe363c7bbe/0)       

这次MAT非常简单，直接在内存泄漏报告中告诉我们，内存溢出原因只有一个！那就是这个问题，因为他没提示任何其他的问题。

我们这次来给大家仔细分析一下MAT给我们的分析报告。

首先看下面的句子：The thread java.lang.Thread @ 0x7bf6a9a98 main keeps local variables with total size 7,203,536 (92.03%) bytes。

这个意思就是main线程通过局部变量引用了7230536个字节的对象，大概就是7MB左右。

考虑到我们总共就给堆内存10MB，所以7MB基本上个已经到极限了，是差不多的。

我们接着看：The memory is accumulated in one instance of "java.lang.Object[]" loaded by "<system class loader>"。

这句话的意思就是内存都被一个实例对象占用了，就是java.lang.Object[]。

我们肯定不知道这个是什么东西，所以得往下看，点击**Details**      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83769900_1578304123.cn/txdocpic/0/02022b12672fb19d394c5ab7df3ba350/0)       



在Details里我们能看到这个东西，也就是占用了7MB内存的的java.lang.Object[]，他里面的每个元素在这里都有，我们看到是一大堆的java.lang.Object。那么这些java.lang.Object不就是我们在代码里创建的吗？

至此真相大白，我们已经知道，就是一大堆的Object对象占用了7MB的内存导致了内存溢出。

接着下一个任务就是知道这些对象是怎么创建出来的，那么我们怎么找呢？回到之前的上一级页面，各位看下图。    ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97658600_1578304123.cn/txdocpic/0/9714597f9bfab56c3bf58ef49d749909/0)       

这个是说可以看看创建那么多对象的线程，他的一个执行栈，这样我们就知道这个线程执行什么方法的时候创建了一大堆的对象。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15161300_1578304124.cn/txdocpic/0/6c1ff7ff329e0daadd076eeccd82931e/0)       

大家看上面的调用栈，其实说的很明显了，在Demo3.main()方法中，一直在调用ArrayList.add()方法，然后此时直接引发了内存溢出。所以我们只要在对应代码里看一下，立马就知道怎么回事了。

接下来优化对应的代码即可，就不会发生内存溢出了。

**5、本文总结**

今天的文章带着大家分析了一下，堆内存溢出的问题如何分析和定位

其实很简单，一个是必须在JVM参数中加入自动导出内存快照，一个是到线上看一下日志文件里的报错，如果是堆溢出，立马用MAT分析内存快照。

MAT分析的时候一些顺序和技巧，还有思路，也教给大家了，先看占用内存最多的对象是谁，然后分析那个线程的调用栈，接着就可以看到是哪个方法引发的内存溢出了。接着优化代码即可。

到这周为止，我们已经彻底学会了JVM OOM的原理以及现象，还有解决问题的思路和方法。

下周开始我们用最后两周给大家演示多个各种各样的奇怪的OOM问题，都是从我们之前经历过的大量真实案例，带着各种案例的业务背景给大家分析。

这样大家积累丰富的OOM处理经验后，就能彻底成为JVM优化实战的高手了。

**End**

### 086、案例实战：每秒仅仅上百请求的系统为什么会因为OOM而崩溃？

**1、案例背景介绍**

本文将会给大家分析一个线上的真实生产案例，在这个案例中，一个每秒仅仅只有100+请求的系统却频繁的因为OOM而崩溃。

在对这个案例进行深度分析的时候，大家会看到一个OOM问题是如何牵扯到Tomcat底层工作原理、Tomcat内核参数的设置、服务请求超时时间等问题的。

**2、系统发生OOM的生产现场**

我们从系统发生OOM的生产现场开始说起，某一天突然收到线上系统的一个报警，说这个系统发生了异常

一看异常信息立马让人大惊失色，居然是大名鼎鼎的OOM问题！这个问题可不得了，因为谁都知道，一旦JVM发生了OOM，将会导致系统直接崩溃掉，因为OOM代表的是JVM本身已经没办法继续运行了！

因此我们立马登录到系统的线上机器去查看对应的日志，在这里插一句，大家记住一点，一旦你收到系统OOM的报警，或者是有人突然反馈说你负责的线上系统崩溃了，**第一件事情：**一定是登录到线上机器去看日志，而不是做别的事情。

当时在机器的日志文件中看到类似下面的一句话：

Exception in thread "http-nio-8080-exec-1089" java.lang.OutOfMemoryError: Java heap space

但凡对Tomcat的底层工作原理有一定了解的朋友，应该立马就会反映过来了，这里的“http-nio-8080-exec-1089”说的其实就是Tomcat的工作线程。

而后面的“java.lang.OutOfMemoryError: Java heap space”大家如果认真学习了之前的内容，都很清楚，指的是堆内存溢出的问题。

所以连起来看，这段日志的意思，就是Tomcat的工作线程在处理请求的时候需要在堆内存里分配对象，但是发现堆内存塞满了，而且根本没办法回收任何一点多余的对象，此时实在没办法在堆内存里放下更多对象了，报了这个异常。

**3、初步看看Tomcat的底层原理**

说到这里，我们先通过一步一图的方式，带大家看看Tomcat的基本工作原理，以及发生这个OOM异常的基本的原因。

首先，相信每个学过Java Web的朋友应该都知道，我们写的系统都是部署在Tomcat中的。

最早我们会在Eclipse / Intellij IDEA这种开发工具里写一堆的Servlet，然后打包之后放入Tomcat，再启动Tomcat

接着我们访问Tomcat监听的一个端口号（一般是8080），然后系统的功能就可以运行起来了。

当然，后来随着技术发展，我们不再写Servlet这么原始的东西了，有一些类似Spring MVC之类的框架把Servlet封装起来了，我们就基于Spring MVC之类的框架去开发。

再到后来，越来越先进了，出现了Spring Boot，我们可以把Tomcat之类的Web容器都内嵌在系统里。再到后来甚至是基于Spring Cloud去开发分布式的系统。

当然，先别扯远了，先看一张图，这是一个最基本的Web系统部署在Tomcat中运行的一个图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86850700_1578304129.cn/txdocpic/0/c486eb6315aed076bd40af5bd3c37075/0)       

上面这个图相信大家都可以理解，我们基于Spring Cloud、Spring Boot、Spring Web MVC等技术，写好一套系统，给打包之后，放入线上服务器中部署的Tomcat目录下，然后启动Tomcat就可以了。

Tomcat会监听一个默认8080的端口号，然后接着我们我们就通过浏览器就可以对这个机器上的Tomcat发起请求了，类似下面这种请求：

http://192.168.31.109:8080/order?userid=100

接着Tomcat会监听8080端口收到这个请求，通常来说他会把请求交给Spring Web MVC之类的框架去处理。

这类框架一般底层都封装了Servlet/Filter之类的组件，他也是用这类组件去处理请求的，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97251000_1578304129.cn/txdocpic/0/8d6477277dc6833ba64b7fd86c635f7e/0)       

然后类似Spring MVC的框架的Servlet组件，就会根据你的请求路径，比如“/order”这种东西，去找到你代码中用来处理这个请求的Controller组件，这个相信学过Java Web的朋友都能理解。

那么现在我们来思考一个问题，稍微牵扯到一些Tomcat的底层工作原理：Tomcat是个什么东西？

不知道大家有没有思考过，如果我们是把写好的系统放入Tomcat目录中，然后启动Tomcat，此时我们启动的Tomcat本身就是一个JVM进程，因为Tomcat自己也是Java写的。

所以看下图，大家首先明确一个概念，就是**Tomcat自己就是一个JVM进程**，我们写好的系统只不过是一些代码而已

这些代码就是一个一个的类，这些类被Tomcat加载到内存里去，然后由Tomcat来执行我们写的类。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8525800_1578304130.cn/txdocpic/0/16f71aab35922e5853ee2f0b4004215c/0)       

既然如此，**Tomcat本身是如何去监听8080端口上收到的请求的？**

很简单，Tomcat有自己的工作线程，大家务必要对Tomcat的工作线程这个概念有一个认识，即Tomcat有很多自己的工作线程，少则一两百个，多则三四百个也是可以的。

然后从8080端口上收到的请求都会均匀的分配给这些工作线程去进行处理，而这些工作线程收到请求之后，就负责调用Spring MVC框架的代码，Spring MVC框架又负责调用我们自己写好的代码，比如一些Controller类之类的

所以最终运行起来的原理如下图所示，大家看一下。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/18891400_1578304130.cn/txdocpic/0/94d89a9d907c38d7fe14914b88eea3da/0)       

大家看上图中的箭头运行的顺序，是不是就上面我们说的那个顺序。

好了，关于Tomcat的底层原理就初步说到这里，毕竟这不是讲Tomcat的专栏。主要为了方便各位理解这个案例，因此简单阐述一下，不然大家很难理解透彻下面的内容。

当然后续如果有需要，也会考虑专门写专栏讲解Tomcat，毕竟搞 java 的，谁又离得开这只Tom猫呢？

**4、回头看看异常日志**

好！言归正传，接着我们回过头看看在当时的线上系统的日志中发现的异常：

Exception in thread "http-nio-8080-exec-1089" java.lang.OutOfMemoryError: Java heap space

这时候大家再来理解就很简单了，“http-nio-8080-exec-1089”

这个东西说白了就是上图中的Tomcat工作线程，因为是他负责调用Spring MVC以及我们写的Controller、Service、DAO等一大堆的代码的，所以他发现运行的时候堆内存不够了，就会抛出来堆内存溢出的异常了。

**5、不要忘了一个关键的JVM参数**

一旦我们发现线上系统发生了内存溢出的异常，第一步一定是看日志

具体是看什么？主要有两点：

1、看看到底是堆内存溢出？还是栈内存溢出？或者是Metaspace内存溢出？首先得确定一下具体的溢出类型

2、看看是哪个线程运行代码的时候内存溢出了。因为Tomcat运行的时候不光有自己的工作线程，我们写的代码也可能会创建一些线程出来

万一是我们自己启动的线程遇到了内存溢出呢？所以这两个要点是首先要关注的。

接着看完了这两个东西之后，就得记得**每个系统上线，务必得设置一个参数：**

-XX:+HeapDumpOnOutOfMemoryError

**这个参数会在系统内存溢出的时候导出来一份内存快照到我们指定的位置。**

接着排查和定位内存溢出问题，主要就得依托这个自动导出来的内存快照了。

**6、对内存快照进行分析**

相信大家还记得之前我们详细讲解的MAT分析内存快照的技巧，通常来说在内存溢出的时候分析内存快照也并没那么的复杂，主要其实就是通过MAT来找到那些占据内存最大的对象。

首先我们会发现占据内存最大的是大量的“byte[]”数组，一大堆的byte[]数组就占据了大约8G左右的内存空间。而我们当时线上机器给Tomcat的JVM堆内存分配的也就是8G左右的内存而已。

因此我们直接可以得出第一个结论：Tomcat工作线程在处理请求的时候会创建大量的byte[]数组，大概有8G左右，直接把JVM堆内存占满了。

此时导致继续处理新请求的时候，没办法继续在堆中分配新对象了，所以就内存溢出了。

大家看下面的图，我们在图中体现出来了这个意思。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29610100_1578304130.cn/txdocpic/0/7240d475442b089d22ea308aaae905a4/0)       

接着我们当然是想分析一下，到底是哪些byte[]数组在这里了，因此我们就继续通过MAT深入查看，发现大概是类似下面的一大堆byte[]数组：

- byte[10008192] @ 0x7aa800000 GET /order/v2 HTTP/1.0-forward...
- byte[10008192] @ 0x7aa800000 GET /order/v2 HTTP/1.0-forward...
- byte[10008192] @ 0x7aa800000 GET /order/v2 HTTP/1.0-forward...
- byte[10008192] @ 0x7aa800000 GET /order/v2 HTTP/1.0-forward...

上面只是写出来几个示例的，其实当时这里看到了很多类似这样的数组，而且数组的大小都是一致的10MB。

大概清点了一下，类似上面那样的数组，大概有800个左右，也就对应了8G的空间。

接着我们进一步思考，这种数组应该是谁创建的？

首先负责写代码的工程师一口咬定，绝对不是自己写出来的，那么在MAT上可以继续查看一下这个数组是被谁引用的，大致可以发现是Tomcat的类引用的，具体来说是类似下面的这个类：

org.apache.tomcat.util.threads.TaskThread

这个类一看就是Tomcat自己的线程类，因此可以认为是Tomcat的线程创建了大量的byte[]数组，占据了8G的内存空间。

之前的文章给大家提到过，MAT中可以查看具体有哪些线程存在，其实MAT使用很方便，也并不难，大家在实际使用的时候，自己多摸索，多点一点，多尝试，就会逐渐摸熟他的使用方式 。最关键的，是掌握分析的思路。

此时我们发现Tomcat的工作线程大致有400个左右，也就是说每个Tomcat的工作线程会创建2个byte[]数组，每个byte[]数组是10MB左右

最终就是400个Tomcat工作线程同时在处理请求，结果创建出来了8G内存的byte[]数组，进而导致了内存溢出。

大家看下面的图，我们把上述分析推断逻辑都反应在图里，大家看一下就一目了然了。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38955800_1578304130.cn/txdocpic/0/e766406eba183d1d48fbcb88923c555d/0)       

**7、系统每秒QPS才只有100？！！**

此时我们大致结合上图，脑海中已经可以出现一幅流动的图形了，一秒钟之内瞬间来了400个请求，导致Tomcat的400个工作线程全部上阵处理请求，每个工作线程在处理一个请求的时候，会创建2个数组，每个数组是10MB，结果导致瞬间就让8G的内存空间被占满了。

看起来生产系统的情况就是如此，那么，**这里有什么问题吗？**

当然，我们来具体分析一下，首先，我们检查了一下系统的监控，发现每秒请求并不是400，而是100！

也就是说明明每秒才100个请求，**怎么可能Tomcat的400个线程都处于工作状态？**

此时我们不禁会倒吸一口凉气，出现这种情况只有一种可能，那就是每个请求处理需要4秒钟的时间！

如果每秒来100个请求，但是每个请求处理完毕需要4秒的时间，那么在4秒内瞬间会导致有400个请求同时在处理，也就会导致Tomcat的400个工作线程都在工作！接着就会导致上述的情况。

好，第一个问题解决了，那么第二个问题来了，为什么Tomcat工作线程在处理一个请求的时候会创建2个10MB的数组？

这里我们可以到Tomcat的配置文件里搜索一下，发现了如下的一个配置：

max-http-header-size: 10000000

**有了，就是这个东西**，导致Tomcat工作线程在处理请求的时候会创建2个数组，每个数组的大小如上面配置就是10MB。

因此，在这里，我们来继续梳理一遍系统运行时候的场景：

每秒100个请求，每个请求处理需要4秒，导致4秒内有400个请求同时被400个线程在处理，每个线程会根据配置创建2个数组，每个数组是10MB，占满了8G的内存。

我们把上述结论在下面的图中继续反应出来，大家来看看。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/50346800_1578304130.cn/txdocpic/0/97f5226b5696f58c6fd0c938b2f0525f/0)       



**8、为什么处理一个请求需要4秒钟？**

问题全部分析清楚了，接着当然是需要解决问题了

**第一个问题**，为什么处理一个请求需要4秒钟？

这个问题绝对是偶发性的，不是平时每次处理请求都如此，因为负责这个系统的工程师说了，平时处理一个请求也就几百毫秒的时间而已。

好，那么既然如此，唯一的办法只能是在日志里去找问题了，继续翻看事故发生时的日志，发现日志中除了OOM以外，其实有大量的服务请求调用超时的异常，类似下面那样子：

Timeout Exception....

也就是说，我们的这个系统在通过RPC调用其他系统的时候突然出现了大量的请求超时，立马翻看一下系统的RPC调用超时的配置，惊讶的发现，负责这个系统的工程师居然将服务RPC调用超时时间设置为了刚好是4秒！

也就是说，一定是在这个时间里，远程服务自己故障了，导致我们的系统RPC调用他的时候是访问不通的，然后就会在配置好的4秒超时时间之后抛出异常，在这4秒内，工作线程会直接卡死在无效的网络访问上。

我们看下面的图，在图中我们表述出来了服务间调用超时的问题。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/62194800_1578304130.cn/txdocpic/0/2df16e80a0e4f1403f1a8f16a842ffd6/0)       

在上图中已经表述的很清楚了，之所以每个请求需要处理4秒钟，是因为下游服务故障了，网络请求都是失败的，此时会按照设置好的4秒超时时间一直卡住4秒钟之后才会抛出Timeout异常，然后请求处理结束。

**这就是一个请求处理需要4秒钟的根本原因**，进而导致每秒100个请求的压力下，4秒内积压400个请求同时在处理，导致400个工作线程创建了800个数组，每个数组10MB内存，耗尽了8G的内存，最终导致内存溢出！

**9、对系统进行优化**

其实要解决上述问题，只要分析清楚了原因，还是非常简单的，对症下药即可。

最核心的问题就是那个超时时间设置的实在太长了，因此立马将超时时间改为1秒即可。

这样的话，每秒100个请求过来，也就只有200个数组，占据2G内存，远远不会把堆内存塞满的，然后1秒内这100个请求会全部超时，请求就处理结束了。

下一秒再来100个请求又是新的一轮处理，不会每秒积压100个请求，4秒积压400个请求同时处理了。

另外一个，对Tomcat的那个参数，max-http-header-size，可以适当调节的小一些就可以了，这样Tomcat工作线程自身为请求创建的数组，不会占据太大的内存空间的。

**10、本文小结**

本文实际上是一个有一定复杂度的综合性的OOM故障排查的案例，但是跟后续大量的其他OOM案例一样，大家会发现，解决这类问题的思路都是一致的，需要你一步一步去分析，但是实际面对各种各样的千奇百怪的问题，需要你举一反三，能够对你系统涉及的种种技术本身有较为深入的了解才可以。

**End**

### 087、案例实战：Jetty 服务器的 NIO 机制是如何导致堆外内存溢出的？

**1、案例背景引入**

这个案例是我曾经带过的一个项目，使用Jetty作为Web服务器的时候在某个非常罕见的场景下发生的一次堆外内存溢出的场景。

这种场景其实并不多见，但还是准备用来作为一个案例，给大家介绍一下这种场景的排查方法。

**2、案例发生现场**

有一天突然收到线上的一个报警：某台机器部署的一个服务突然之间就不可以访问了。

此时第一反应当然是立马登录上机器去看一下日志，因为服务挂掉，很可能是OOM导致的崩溃，当然也可能是其他原因导致的问题。

这个时候在机器的日志中发现了如下的一些信息：

nio handle failed java.lang.OutOfMemoryError: Direct buffer memory

at org.eclipse.jetty.io.nio.xxxx

at org.eclipse.jetty.io.nio.xxxx

at org.eclipse.jetty.io.nio.xxxx

过多的日志信息给省略掉了，因为都是非常杂乱的一些信息，也没太大意义，大家关注比较核心的一些信息就可以

上述日志中，最主要的就是告诉我们有OOM异常，但是是哪个区域导致的呢？

居然是我们没见过的一块区域：**Direct buffer memory**，而且在下面我们还看到一大堆的jetty相关的方法调用栈

到此为止，仅仅看到这些日志，我们基本就可以分析出来这次OOM发生的原因了。

**3、初步分析事故发生的原因**

先给大家解释一个东西：Direct buffer memory

这个东西其实就是堆外内存，顾名思义，他是JVM堆内存之外的一块内存空间，这块内存空间不是JVM管理的，因此之前我们也没怎么多讲这块内容，但是你的Java代码确实是可以在JVM堆之外使用一些内存空间的。

这些空间就叫做Direct buffer memory，如果按英文字面理解，他的意思是直接内存，其实你要这么叫也没问题，但是从字面可以看出来，这块内存并不是JVM管理的，而是“直接”被操作系统管理。

正因为这样，所以其英文名叫做Direct buffer memory，就是直接内存的意思。但是如果我们叫他直接内存，又显得非常的奇怪，所以通常更好的称呼就是“堆外内存”。

另外再给大家解释一个东西：Jetty。这个是什么？

其实你大致可以理解为跟Tomcat一样的东西，就是Web容器

Jetty本身也是Java写的，我们如果写好了一个系统，可以打包后放入Jetty，启动Jetty即可。

Jetty启动之后，本身就是一个JVM进程，他会监听一个端口号，比如说9090

然后你就向Jetty监听的9090端口发送请求，Jetty会把请求转交给你用的Spring MVC之类的框架，Spring MVC之类的框架再去调用写好的Controller之类的代码。

我们先看下面一个图，简单看看Jetty作为一个JVM进程运行我们写好的系统的一个流程：      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6304000_1578304137.cn/txdocpic/0/c0077c57dac7f120d7483a65db6ac237/0)       

首先我们可以明确一点，这次OOM是Jetty这个Web服务器在使用堆外内存的时候导致的

也就是说，基本可以推测出来，Jetty服务器可能在不停的使用堆外内存，然后堆外内存空间不足了，没法使用更多的堆外内存了，此时就会抛出内存溢出的异常。

至于为什么Jetty要不停的使用堆外内存空间，大家就暂时先别管那么多了，那涉及到Jetty作为一个Web服务器的底层源码细节。

我们只要知道他肯定会不停的去使用堆外内存，然后用着用着堆外内存不够了，就内存溢出了。

我们接着看下面的图，下面的图里，就体现出来了Jetty不停使用堆外内存的场景。    ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/19982000_1578304137.cn/txdocpic/0/7c986bf6940fda9cd76de61fa032a343/0)       

**4、关于解决OOM问题的底层技术修为的一点建议**

讲到这里，我们肯定会有一点很疑惑了：Jetty既然是用Java写的，那么他是如何通过Java代码去在JVM堆之外申请一块堆外内存来使用的？然后这个堆外内存空间又是如何释放掉的呢？

这个东西其实涉及到Java的NIO的底层技术细节，如果大家之前对NIO没什么了解的话，突然看到这个异常，估计是没法继续分析下去了。

因此这里也给大家额外插一句话，其实JVM的性能优化相对还是较为容易一些的，而且基本上整个套路之前也已经给大家都说的很清楚了。

但是如果是解决OOM问题，那么除了一些特别弱智和简单的，比如有人在代码里不停的创建对象最后导致内存溢出这种。其他的很多生产环境下的OOM问题，都是有点技术难度的。

大家如果把整个专栏最后两周的内容都看完了，就会有一个很深的感触，那就是1000个工程师可能会遇到1000种不同的OOM问题。

可能排查的思路是类似的，或者解决问题的思路是类似的，但是如果你要解决各种OOM问题，是需要对各种技术都有一定的了解，换句话说，需要有较为扎实的技术内功修为。

比如昨天的那个案例，就需要你对Tomcat的一些工作原理有一定的了解，你才能分析清楚那个案例。

同样，今天的这个案例，就要求你对Java NIO技术的工作原理有一定的了解才能分析清楚。

因此这里也说句题外话：希望大家在学习JVM技术本身之余，多去对其他的核心主流技术做一些深层次的甚至源码级别的研究

这些底层技术积累将会在你的线上系统出现问题的时候，迅速帮助你分析和解决问题。

**5、堆外内存是如何申请的，又是如何释放的？**

接着我们继续来看看，这个堆外内存是如何申请和释放的？

简单来说，如果在Java代码里要申请使用一块堆外内存空间，是使用DirectByteBuffer这个类，你可以通过这个类构建一个DirectByteBuffer的对象，这个对象本身是在JVM堆内存里的。

但是你在构建这个对象的同时，就会在堆外内存中划出来一块内存空间跟这个对象关联起来，我们看看下面的图，你就对他们俩的关系很清楚了。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/32107000_1578304137.cn/txdocpic/0/f6cfe2c591b0b123c74d7dcf8d284ba1/0)       

因此在分配堆外内存的时候大致就是这个思路，那么堆外内存是如何释放的呢？

很简单，当你的DirectByteBuffer对象没人引用了，成了垃圾对象之后，自然会在某一次young gc或者是full gc的时候把DirectByteBuffer对象回收掉。

只要回收一个DirectByteBuffer对象，就会自然释放掉他关联的那块堆外内存，我们看看下面的图就知道了。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/47296000_1578304137.cn/txdocpic/0/a4383623e619a8b323cd670737da85f5/0)       

**6、为什么会出现堆外内存溢出的情况？**

那么大家现在应该很清楚了，一般什么情况下会出现堆外内存的溢出？

很简单，如果你创建了很多的DirectByteBuffer对象，占用了大量的堆外内存，然后这些DirectByteBuffer对象还没有GC线程来回收掉，那么就不会释放堆外内存！

久而久之，当堆外内存都被大量的DirectByteBuffer对象关联使用了，如果你再要使用更多的堆外内存，那么就会报内存溢出了！

那么，**什么情况下会出现大量的DirectByteBuffer对象一直存活着，导致大量的堆外内存无法释放呢？**

有一种可能，就是系统承载的是超高并发，复杂压力很高，瞬时大量请求过来，创建了过多的DirectByteBuffer占用了大量的堆外内存，此时再继续想要使用堆外内存，就会内存溢出了！

但是这个系统是这种情况吗？

明显不是！因为这个系统的负载其实没有想象中的那么高，不会有瞬时大量的请求过来。

**7、真正的堆外内存溢出原因分析**

这个时候你就得思路活跃起来了，我们完全可以去用jstat等工具观察一下线上系统的实际运行情况，同时根据日志看看一些请求的处理耗时，综合性的分析一下。

当时我们通过jstat工具分析jvm运行情况，同时分析了过往的gc日志，另外还看了一下系统各个接口的调用耗时之后，分析出了如下的思路。

首先看了一下接口的调用耗时，这个系统并发量不高，但是他每个请求处理较为耗时，平均在每个请求需要一秒多的时间去处理。

然后我们通过jstat发现，随着系统不停的被调用会一直创建各种对象，包括Jetty本身会不停的创建DirectByteBuffer对象去申请堆外内存空间，接着直到年轻代的Eden区满了，就会触发young gc，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66582000_1578304137.cn/txdocpic/0/6b747bb9016529308386863677d52a95/0)       

但是往往在进行垃圾回收的一瞬间，可能有的请求还没处理完毕，此时就会有不少DirectByteBuffer对象处于存活状态，不能被回收掉，当然之前不少DirectByteBuffer对象对应的请求可能处理完毕了，他们就可以被回收了。

此时肯定会有一些DirectByteBuffer对象以及一些其他的对象是处于存活状态的，那么就需要转入Survivor区域中。

但是大家注意了，这个系统当时在上线的时候，内存分配的极为不合理，在当时而言，大概就给了年轻代一两百MB的空间，老年代反而给了七八百MB的空间，进而导致年轻代中的Survivor区域只有10MB左右的空间。

因此往往在young gc过后，一些存活下来的对象（包括了一些DirectByteBuffer在内）会超过10MB，没法放入Survivor中，就会直接进入老年代，我们看下图就表现出了这个过程。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/77423300_1578304137.cn/txdocpic/0/41a0894df52eeca8bedd6dcdb6b49302/0)       

因此上述的过程就这么反复的执行，必然会慢慢的导致一些DirectByteBuffer对象慢慢的进入老年代中，老年代中的DirectByteBuffer对象会越来越多，而且这些DirectByteBuffer都是关联了很多堆外内存的，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97970500_1578304137.cn/txdocpic/0/a3074bb955474986939163d1f18bcff1/0)       



这些老年代里的DirectByteBuffer其实很多都是可以回收的状态了，但是因为老年代一直没塞满，所以没触发full gc，也就自然不会回收老年代里的这些DirectByteBuffer了！当然老年代里这些没有被回收的DirectByteBuffer就一直关联占据了大量的堆外内存空间了！

直到最后，当你要继续使用堆外内存的时候，结果所有堆外内存都被老年代里大量的DirectByteBuffer给占用了，虽然他们可以被回收，但是无奈因为始终没有触发老年代的full gc，所以堆外内存也始终无法被回收掉。

最后就会导致内存溢出问题的发生！

**8、难道Java NIO就没考虑过这个问题吗？**

所以这里我们先不说如何解决这个问题，先说一点，难道Java NIO就从没考虑过会有上述问题的产生过吗？

当然不是了，Java NIO是考虑到的！他知道可能很多DirectByteBuffer对象也许没人用了，但是因为没有触发gc就导致他们一直占据着堆外内存。

所以在**Java NIO的源码中会做如下处理**，他每次分配新的堆外内存的时候，都会调用System.gc()去提醒JVM去主动执行以下gc去回收掉一些垃圾没人引用的DirectByteBuffer对象，释放堆外内存空间。

只要能触发垃圾回收去回收掉一些没人引用的DirectByteBuffer，就会释放一些堆外内存，自然就可以分配更多的对象到堆外内存去了。

但是我们又在JVM中设置了如下参数：

-XX:+DisableExplicitGC

导致这个System.gc()是不生效的，因此就会导致上述的情况。

**9、最终对问题的优化**

其实项目问题有两个，一个是内存设置不合理，导致DirectByteBuffer对象一直慢慢进入老年代，导致堆外内存一直释放不掉

另外一个是设置了-XX:+DisableExplicitGC导致Java NIO没法主动提醒去回收掉一些垃圾DIrectByteBuffer对象，同样导致堆外内存无法释放。

因此最终对这个项目做的事情就是：

一个是合理分配内存，给年轻代更多内存，让Survivor区域有更大的空间

另外一个就是放开-XX:+DisableExplicitGC这个限制，让System.gc()生效。

做完优化之后，DirectByteBuffer一般就不会不断进入老年代了。只要他停留在年轻代，随着young gc就会正常回收释放堆外内存了。

另外一个，只要你放开-XX:+DisableExplicitGC的限制，Java NIO发现堆外内存不足了，自然会通过System.gc()提醒JVM去主动垃圾回收，可以回收掉一些DirectByteBuffer释放一些堆外内存。

**End**

### 088、案例实战：一次微服务架构下的RPC调用引发的OOM故障排查实践

**1、案例背景引入**

这个OOM的案例是发生在微服务架构下的一次RPC调用过程中的，也是一次非常奇怪的故障案例

大家应该还记得昨天的文章里讲的一个解决系统OOM故障的核心能力积累，你必须对你线上系统使用的各种技术，从服务框架，到第三方框架，到Tomcat/Jetty等Web服务器，再到各种底层的中间件系统，对他们的源码最好都要有深入的理解。

因为一般线上系统OOM，都不是简单的由你的代码导致的，可能是因为你系统使用的某个开源技术的内核代码有一定的故障和缺陷，这时你要解决OOM问题，就必须深入到开源技术的源码中去分析。

对于我们的专栏而言，最后两周大量的OOM案例，主要带给大家的就是各种真实的生产案例以及分析思路

主要帮助大家掌握思路，但是真正到了具体的OOM故障时，每个人的故障可能都是不同的，都需要你去分析对应的技术的底层，然后找到故障原因，最后解决故障。

**2、系统架构介绍**

这个系统是比较早的一个系统，在进行服务间的RPC通信时，采用的是基于Thrift框架自己封装出来的一个RPC框架，可能很多朋友对Thrift之类的东西都没概念，那就不要过多的关注这个了。

大家只需要知道当时我们自己封装了一个RPC框架就可以了，然后在一个系统中，就是基于这个RPC框架去进行通信。

我们看一下下面的图，图中就是一个最基本的服务间RPC调用的示例。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/35619400_1578304145.cn/txdocpic/0/ad9faba9fb20affb774aececa31d99e0/0)       

**3、故障发生现场**

就当时而言，平时是服务A通过RPC框架去调用服务B，但是有一天，负责服务A的工程师更新了一些代码，然后将服务A重新上线之后，服务A自己倒是没什么，结果反而是服务B却突然宕机了！

这可真是奇怪和诡异的场景，因为明明修改代码的是服务A，重新部署的也是服务A，结果怎么成了服务B却挂掉了？

别急，让我们一步一步慢慢分析。我们先看一下下面的图，里面反映出了当时的生产情况。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51682400_1578304145.cn/txdocpic/0/5b8a23806d52df7c80265b242627d37a/0)       当时我们立马登录到服务B的机器去查看服务B的日志，结果在里面赫然发现了OOM异常：

java.lang.OutOfMemoryError Java heap space

直接告诉你堆内存溢出了！当时我们就很奇怪，服务B为什么会OOM？

难道是服务B自己的问题？那不如重启一下服务B？

于是我们尝试重启了服务B，结果发现服务B重启过后很快又宕机了，而且原因还是OOM。

这个就奇怪了，因为在服务A修改代码部署之前，服务B从来没出现过这种情况！都是服务A修改了代码部署之后才导致服务B出现了这种情况的！

**4、初步查找内存溢出的故障发生点**

一般内存溢出的时候，大家务必要先找一下故障的发生点，其实看日志就可以了，因为在日志中都会有详细的异常栈信息

我们会发现，引发OutOfMemory异常的，居然就是我们自研的那个RPC框架！

当时大致来说你可以看到的一个异常信息如下所示：

java.lang.OutOfMemoryError: Java heap space

xx.xx.xx.rpc.xx.XXXClass.read()

xx.xx.xx.rpc.xx.XXXClass.xxMethod()

xx.xx.xx.rpc.xx.XXXClass.xxMethod()

这里就不给出当时详细的异常信息截图了，因为当时也没截图保存下来。但是去除掉大量无效的信息后，其实核心的日志大致就类似上述那样子。

这里我们初步可以确定，就是自研的RPC框架在接收请求的时候引发了OOM！

我们看看下图，在里面反映出来了RPC框架运行的时候引发OOM的场景。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65701900_1578304145.cn/txdocpic/0/f7a8645eed6f1f8e612e893d1cd229d1/0)   

**5、分析内存快照找到占用内存最大的对象**

既然已经定位到OOM故障发生的位置了，也就是谁引发的OOM，接下来肯定得用MAT分析一下发生OOM的时候，对内存占用最大的是哪个对象了。

此时把OOM的时候自动导出的内存快照打开分析，发现占用内存最大的是一个超大的byte[]数组！

当时我们一台机器给的堆内存也不过就是4GB而已，而在内存快照中发现，居然一个超大的byte[]数组就占据了差不多4GB的空间

这个byte[]数组是哪儿来的？

通过分析这个byte[]数组的引用者（这个在MAT里有对应的功能，大家右击一个对象会出现一个菜单，里面会有很多选项，自己多尝试尝试），会发现这个数组就是RPC框架内部的类引用的。

我们在下面的图里给大家反映出来当前的一个场景。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/76077800_1578304145.cn/txdocpic/0/8535e89497996ff96d0ded010c739dcb/0)       

**6、通过分析源代码找出原因**

一般分析到这一步，答案几乎就快要揭晓了，因为我们通过日志第一步已经定位到是谁导致的OOM，往往可能就是某个技术，比如Tomcat？Jetty？或者是RPC框架？

第一步我们先得定位到这个主体人

第二步一般就是用MAT之类的工具去分析内存快照，找到当时占用内存最大的对象是谁，可以找找都是谁在引用他，当然一般第一步通过看日志就大概知道导致内存溢出的类是谁，日志的异常栈里都会告诉你的。

第三步，很简单，你得对那个技术的源代码进行分析，比如对Tomcat、Jetty、RPC框架的源代码去进行追踪分析

通过代码分析找到故障发生的原因，如果可能的话，最好是在本地搭建一个类似的环境，把线上问题给复现出来。

我们这里当时就结合日志里的异常栈分析了一下自己写的RPC框架的源代码，先给大家简单说一下，这个框架在接收请求的时候的一个流程。

首先在服务A发送请求的时候，会对你传输过来的对象进行序列化。这个序列化，简单来说，就是把你的类似Request的对象变成一个byte[]数组，大概可以理解为这个意思，我们在下图里给大家反映出来。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/90039500_1578304145.cn/txdocpic/0/b912abc555a7fc9ef39f80e282709f0c/0)    然后对服务B而言，他首先会根据我们自定义的序列化协议（当时用的是Protobuf，很多人可能不了解这个，先别管这个了），对发送过来的数据进行反序列化

接着把请求数据读取到一个byte[]缓存中去，然后调用业务逻辑代码处理请求，最后请求处理完毕，清理byte[]缓存。

我们也在下面的图中反映出来服务B的处理流程。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4847400_1578304146.cn/txdocpic/0/d3bb80059713dcf4b783dc54f99b8cea/0)       

想必大家都已经看明白上面RPC框架运行的原理了，接着我们自然在源码中要寻找一下，为什么用来缓冲请求的byte[]数组会搞成几个GB那么大？正常情况下，这个数组应该最多不超过1MB的。

**7、铺垫一个关键知识点：RPC框架的类定义**

通过源码的分析，我们最终总算搞清楚了，原来当时有这么一个特殊的情况，因为RPC框架要进行对象传输，就必须得让服务A和服务B都知道有这么一个对象

给大家举个例子，比如服务A要把一个Request对象传输给服务B，那么首先需要使用一种特定的语法定义一个对象文件，当时用的是ProtoBuf支持的语法，大家不理解的可以直接忽略，直接看看大概类似下面这样的一个文件就可以了：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26764300_1578304146.png)



然后会通过上面那个特殊语法写的文件反向生成一个对应的Java类出来，此时会生成一个Java语法的Request类，类似下面这样：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38851900_1578304146.png)



接着这个Request类你需要在服务A和服务B的工程里都要引入，他们俩就知道，把Request交给服务A，他会自己进行序列化成字节流，然后到服务B的时候，他会把字节流反序列化成一个Request对象。

我们看下面的图，里面就引入了Request类的概念。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52974400_1578304146.cn/txdocpic/0/3cce287bdbbc9aa92ca8f9ed40c23b75/0)       

服务A和服务B都必须知道有Request类的存在，然后才能把Request对象序列化成字节流，也才能从字节流反序列化出来一个Request类的对象。

这是继续分析案例之前，大家务必清楚理解的一个概念。

**8、RPC框架的一个bug：过大的默认值！**

明白了这个，咱们就可以来继续看了，在上面的图中，服务B在接收到请求之后，会先反序列化，接着把请求读出来放入一个byte[]数组。

但是这里RPC框架我们有一个bug，就是一旦发现对方发送过来的字节流反序列化的时候失败了，这个往往是因为服务A对Request类做了修改，但是服务B不知道这次修改，Request还是以前的版本。

结果比如服务A的Request类有15个字段，序列化成字节流给你发送过来了，服务B的Request类只有10个字段，有的字段名字还不一样，那么反序列化的时候就会失败。

当时代码中写的逻辑是，一旦反序列化失败了，此时就会开辟一个byte[]数组，默认大小是4GB，然后把对方的字节流原封不动的放进去。

所以最终的问题就出在这里了，当时服务A的工程师修改了很多Request类的字段，结果没告诉服务B的工程师。

所以服务A上线之后，序列化的Request对象到服务B那里是没办法反序列化的，此时服务B就会直接开辟一个默认4GB的byte[]数组。

然后就会直接导致OOM了。

我们在下面的图中，把两边Request字段不一致的情况，反序列化失败的情况，以及开辟大数组的情况，都反应出来了，大家结合图可以看一下。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75174900_1578304146.cn/txdocpic/0/1714661a8f9d5b62d46c02c5431e8b41/0)       

**9、最终的解决方案**

肯定很多人会疑惑，当时那个工程师为什么要把异常情况下的数组默认大小设置为几个GB那么大？

这个其实也没办法，因为当时写这段代码的刚好是才毕业的应届工程师，当时他考虑的是万一反序列化失败了，那么就原封不动的封装字节流到数组里去，让我们来自行处理。

但是他又不知道对方字节流里数据到底有多少，所以就干脆开辟一个特别大的数组，保证一定能放下字节流就可以了。

而且一般测试的时候都不会测到这种异常情况，所以之前一直没发现这个问题。

其实解决这个问题的办法很简单，把RPC框架中那个数组的默认值从4GB调整为4MB即可，一般请求都不会超过4MB，不需要开辟那么大的数组。

另外就是让服务A和服务B的Request类定义保持一致即可。上述问题就解决了。

**End**

### 089、案例实战：一次没有WHERE条件的SQL语句引发的OOM问题排查实践！

**1、案例背景引入**

这个案例也是我们线上曾经出现过的一个真实的生产案例，是一个年轻的工程师在使用mybatis写SQL语句的时候在某些情况下允许不加where条件就可以执行，结果导致一下子查询出来上百万条数据引发了系统的OOM。

这个案例本身是属于比较简单的那种，不涉及太多其他的技术问题，的确就是纯系统代码自身的问题导致的

所以我们拿这个案例来给大家再深入讲讲在使用MAT进行线上内存快照分析时候的一些技巧，相信对大家也是很有用的。

**2、关于MAT工具对OOM故障的实践意义**

这里给说一句，如果你的系统触发OOM是由于Tomcat、Jetty、RPC框架之类的底层技术，那么MAT对你来说用处并不是那么大

因为你最多就是用MAT找一找占用内存过多的对象，然后结合异常日志调用栈和MAT中的对象引用情况，初步定位一下是底层技术中的哪部分代码导致的内存溢出。

而如果要真正解决内存溢出问题，还得去仔细研究Tomcat、Jetty、RPC框架之类技术的底层源码，结合线上系统的负载情况、访问压力以及GC情况，以及底层技术的源码细节，真正分析清楚发生OOM的原因，然后才能解决。

但是如果OOM主要是由于你自己的系统代码的问题导致的，那么就容易解决的多了

只要依托MAT层层分析，瞬间就可以定位到你代码的问题所在，毕竟你自己写的代码你是最熟悉的，所以你很快就可以解决问题。

**3、故障发生现场**

我们先来看一下故障发生的现场，某一天突然我们收到反馈说线上一个系统崩溃不可用了，此时当然是立即登录到线上机器去查看日志了，在日志中果然发现了OOM的异常：java.lang.OutOfMemoryError，java heap space。

堆内存溢出了，那我们下一步肯定是把自动导出的内存快照拷贝到自己电脑上，用MAT去分析对应的内存快照了

今天就给大家详细说说真实的生产案例中，如何巧妙的用MAT工具迅速定位问题代码。

**4、第一步：检查内存中到底是什么对象太多了**

第一步，我们可以用MAT中的一个Histogram功能，去检查一下占用内存最多的对象有哪些

之前给大家介绍过内存泄漏报告的使用，那个功能也是没问题的，也很好用，但是今天给大家介绍另外一套分析的工具和思路，其实原理都是类似的。

分析内存快照，说白了，无非就是找到占用内存最大的对象，然后就是找到谁在引用这个对象，是哪个线程在引用，接着找到创建这些对象的相关代码和方法，然后你就可以一头扎到对应的源码里去分析问题了。

我们先看下面的图，在这个图里，可以点击下图中红圈处的一个按钮，那个就是Histogram按钮，点击过后就会进入另外一个界面。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/85030800_1578304152.cn/txdocpic/0/751a90c3ef977e6143c10c4f0618e1a6/0)       

接着我们进入Histogram界面，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21702000_1578304153.cn/txdocpic/0/f2009c8ddc34f7f933ecbdf031e4bcca/0)       



在这个界面中，其实就可以瞬间看到是谁占用内存过多了，比如这里明显是Demo1$Data这个内部类占用了过多的内存。

这里说了，Demo1$Data对象有10000个，此时你有没有一种很好奇的冲动，想要看看这些Demo1$Data对象都是什么东西？

没问题，我们继续接着分析。

**5、第二步：深入看看占用内存过多的对象**

此时我们可以进入第二步，你可以深入的看看占用内存过多的对象是被谁引用的，哪个线程引用的，他们里面都是什么东西

此时我们可以按照下图，点击红圈处的按钮。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61613300_1578304153.cn/txdocpic/0/f1b76d19dec381859062bdc7fb359606/0)       



点击上图的那个红圈处的按钮之后，就会进入一个dominator_tree的界面，他会展示出来当前你JVM中所有的线程，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/82607900_1578304153.cn/txdocpic/0/0a1dbe13d315e9171e829875f4dc1993/0)       



在这里，你可以清晰的看到哪些线程创建了过多的对象，比如我们这里排名第一的线程就是：

java.lang.Thread @ 0x5c0020838 main Thread

就是说一个main线程创建了过多的对象

那我们就直接可以展开这个线程，到底他创建了哪些对象？看看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6805000_1578304154.cn/txdocpic/0/467730d9b7dd024c093d03e6b5bd7fc8/0)       



我们展开main Thread之后，发现里面有一个java.util.ArrayList @ 0x5c00206a8

说明线程创建了一个巨大的ArrayList，我们继续展开这个ArrayList，里面是一个java.lang.Object[]数组，继续展开，就会看到大量的Demo1$Data对象了。

真相大白，其实从dominator_tree界面，我们很快就能找到是哪个线程创建了过多的对象，而且层层展开，就可以看到这个线程创建了哪些对象太多了，就可以跟之前的Histogram界面中占用内存最多的Demo1$Data对上了。

而且这里我们可以轻易看到每个Demo1$Data对象的详细的情况，你可以展开任何一个对象看看。

**6、生产案例的追踪**

当时对于我们那个线上生产案例而言，到这一步为止，大家猜猜追踪到的是什么东西？

我们当时追踪到这里，发现某个Tomcat的工作线程创建了一大堆的java.lang.HashMap，那么这些java.lang.HashMap中是什么？

我们发现全都是各种从数据库里查出来的字段，你只要展开那个HashMap就能看到你出来放入内存的所有数据。

所以看到这一步基本就很明确了，就是Tomcat的工作线程处理一个请求的时候，发起了一个SQL语句，查出来了大量的数据，每条数据是一个HashMap，就是这大量的数据导致了系统的OOM。

**7、第三步：到底是哪一行代码创建了这么多的对象？**

找到占用内存最大的对象之后，最后一步就是要定位一下是哪一行代码，或者是哪个方法创建了那么多的对象？

这个又需要另外一个视图了，大家看下图的红圈处。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/33113200_1578304154.cn/txdocpic/0/7790482e738abdff6501f85e97b99530/0)       



点击上图红圈的按钮，会进入一个thread_overview界面，如下图所示，这里会展示出来JVM中所有的线程以及每个线程当时的方法方法调用栈，以及每个方法中创建了哪些对象：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55339700_1578304154.cn/txdocpic/0/8ced4023b841a8ff60400063535037cf/0)       



比如上图，我们会直接看到一个main Thread，他先是执行了一个Demo1.java类中的第12行处的一个Demo1.main()方法，接着这个main方法又执行了一个java.lang.Thread类的sleep()方法，一清二楚。

任何一个线程在此时此刻都执行和调用了哪些方法，都会在这里显示出来。

我们接着展开上图中的Demo1.main()方法，你就可以看到线程调用每个方法的时候创建和引用了哪些对象，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/81694900_1578304154.cn/txdocpic/0/a3cedea3b40cde8296e173cbcb5f366f/0)       



在上图中，我们发现Demo1.main方法执行的时候创建了一个ArrayList，展开发现是一个java.lang.Object[]数组，再次展开发现就是一大堆的Demo1$Data对象，到此为止，真相大白，一清二楚。

通过上述步骤，你可以快速的定位出来占用内存过多的对象，以及到底是哪个线程创建了这些对象，到底是线程执行哪个方法的时候创建了这些对象，每个对象的细节你都可以看到是什么东西。

**8、继续对生产案例进行追踪**

采用上述方法，当时我们对生产案例进行追踪，立马就定位到了是系统中的一个业务方法，在执行查询操作的时候，因为没有带上WHERE条件，直接查询出来了全部的上百万的数据，导致了内存的溢出。

此时就直接对那个方法对应的SQL语句进行修改即可，要求他必须每次都带上WHERE条件。

**9、经典的MAT步骤：可以套用到全部案例中去**

大家应该还记得我们之前的几个案例都直接告诉大家，当时用MAT进行分析的时候，定位到了哪些对象是在哪里被引用的，其实用的都是类似这篇文章里讲解的步骤。

大家可以把这套MAT定位步骤套用到全部案例中去，也可以用到自己自己实际的工作中去。

**End**



**1、案例背景**

线上有一个数据同步系统，是专门负责从另外一个系统去同步数据的，简单来说，另外一个系统会不停的发布自己的数据到Kafka中去，然后我们有一个数据同步系统就专门从Kafka里消费数据，接着保存到自己的数据库中去，大概就是这样的一个流程。

我们看下图，就是这个系统运行的一个流程。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/14971400_1578304155.cn/txdocpic/0/89b88af606a4cfd8b4705900ae9f3032/0)       

结果就这么一个非常简单的系统，居然时不时就报一个内存溢出的错误，然后就得重启系统，过了一段时间又会再次内存溢出一下。而且这个系统处理的数据量是越来越大，因此我们发现他内存溢出的频率越来越高，到这个情况，就必须要处理一下了。

**2、经验丰富的工程师：从现象看到本质**

其实一般遇到这种现象，只要是经验丰富的工程师，应该已经可以具备从现象看到本质的能力了。我们可以来分析和思考一下，他既然是每次重启过后都会在一段时间以后出现内存溢出的问题，说明肯定是每次重启过后，内存都会不断的上涨。

而且一般要搞到JVM出现内存溢出，通常的就是两种情况，要不然是并发太高，瞬间大量并发创建过多的对象，导致系统直接崩溃了。要不就是有内存泄漏之类的问题，就是很多对象都赖在内存里，无论你如何GC就是回收不掉。

那么这个场景可能是怎么回事呢？我们当时分析了一下，这个系统的负载并不是很高，随着数据量不少，但是并不是那种瞬时高并发的场景。那么很可能就是随着时间推移，有某种对象越来越多，赖在内存里了。

然后不断的触发gc，结果每次gc都回收不掉这些对象。

一直到最后，内存实在不足了，就会内存溢出，我们看看下面的图，在下图里就画出了这个问题。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27354700_1578304155.cn/txdocpic/0/502e536bca2169695756a2b3c7cd5354/0)       

**3、通过jstat来确认我们的推断**

接着直接在一次重启系统之后，用jstat观察了一下JVM运行的情况：

发现老年代的对象一直在增长，不停的在增长，每次Young GC过后，老年代的对象就会增长不少，而且当老年代的使用率达到100%之后，我们发现会正常触发Full GC，但是Full GC根本回收不掉任何对象。

导致老年代使用率还是100%！

然后老年代使用率维持100%一段时间过后，就会爆出内存溢出的问题，因为再有新的对象进入老年代，实在是没有空间放他了！

所以基本就确认了我们的判断，每次系统启动，不知道什么对象会一直进入堆内存，而且随着Young GC执行，对象会一直进入老年代，最后触发Full GC都无法回收老年代的对象，最终就是内存溢出。

**4、通过MAT找到占用内存最大的对象！**

关于MAT分析内存快照的方法，之前已经讲解的很详细了，其实在这些案例中就不用重复一些截图了，直接说出过程和结论就好！在内存快照中，我们发现了一个问题，那就是有一个队列数据结构，直接引用了大量的数据，就是这个队列数据结构占满了内存！

那么这个队列是干什么用的？简单来说，从Kafka消费出来的数据会先写入这个队列，接着从这个队列再慢慢写入数据库中，这个主要是要额外做一些中间的数据处理和转换，所以自己在中间又加了一个队列。

我们看下面的图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44559400_1578304155.cn/txdocpic/0/6b9c6da2db6017b95be7d0f91ba54734/0)       

那么这个队列是怎么用的？问题就出在这里了！

大家都知道，从Kafka消费数据，是可以一下子消费一批出来的，比如消费几百条数据粗来。因此当时这个写代码的同学，直接就是每次消费几百条数据出来给做成一个List，然后把这个List放入到队列里去！

最后就搞成了，一个队列比如有1000个元素，每个元素都是一个List，每个List里都有几百条数据！这种做法怎么行？会导致内存中的队列里积压几十万条，甚至百万条数据！最终一定会导致内存溢出！

而且只要你数据还停留在队列中，就是没有办法被回收的。

我们看下面的图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/58774200_1578304155.cn/txdocpic/0/7dc9ae7f730819a5b5849d6369562b22/0)       



其实上图就是一个典型的对生产和消费的速率没控制好的例子。从Kafka里消费出来数据放入队列的速度很快，但是从队列里消费数据进行处理然后写入存储的速度较慢，最终会导致内存队列快速积压数据，导致内存溢出。

而且这种队列每个元素都是一个List的做法，会导致内存队列能容纳的数据量大幅度膨胀。

最终解决这个问题也很简单，把上述内存队列的使用修改了一下，做成了定长的阻塞队列，比如最多1024个元素，然后每次从Kafka消费出来数据，一条一条数据写入队列，而不是做成一个List放入队列作为一个元素。

因此这样内存中最多就是1024个数据，一旦内存队列满了，此时Kafka消费线程就会停止工作，因为被队列给阻塞住了。不会说让内存队列中的数据过多。

我们看下面的图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69421800_1578304155.cn/txdocpic/0/aedcdb05017167a39bc16181aa093b04/0)       

**5、本文小结**

本文是我们整个专栏的最后一个案例，其实相信大家认真学习完这个专栏之后，就会感受到我们设计这个专栏的思路。专栏的核心是通过一步一图和大白话的方式，让大家学会JVM的核心运行原理，接着学习了JVM GC优化的核心原理和OOM问题的核心原理。

接着我们给大家讲解了JVM的GC问题以及OOM的常见发生场景和解决方法。

同时我们带给了大家数十个来源于我们真实生产环境的JVM优化案例，包括GC优化案例和OOM优化案例，大量的优化案例让大家可以对各种不同场景的问题有一个了解，同时积累起来了对不同问题进行分析、排查和解决的一个思路。

在这个过程中，如果大家反复去把这些案例看几遍，吸收透彻了，本质上就会积累起来较为丰富的JVM优化实践的经验积累，当你日后真的在工作中需要解决JVM问题的时候，就会发现这些知识全部都可以派上用场了。

**End**

### 090、本周思考题

以下三道思考题，大家可以复习一下前几周的专栏内容，然后结合自己公司的实际情况思考一下，同时检验自己的学习效果：

1. 结合自己公司系统的业务来分析，遇到内存溢出该怎么处理？
2. 如果你的线上系统数据量猛增100倍，会出现OOM问题吗？
3. 对自己的线上系统，导出一份内存快照分析一下！

### 091、本周答疑问题汇总！

**问题：**

请问老师，文中 [Metaspace: 9201->9201(1058816K)] 括号中的1058816K是什么意思？也不是10M啊？

**回答：**那个一般不用管他，他主要根据你设置的参数触发垃圾回收

------

**问题：**

老师 我默认得 新生代 和 堆内存大小和你得不一致呢， 不过通过配置参数模拟成功了。，，不过还是想问问，这是eclips配置不同得原因吗

**回答：**是的，ide开发工具会有一定的影响

**问题：**

元数据区在1.8版本已经移到了本地内存，此时还会有gc？

**回答：**还是会的

**问题：**

如果元数据空间存放类的信息满了，然后没有进行垃圾回收的情况下， 元数据空间就会产生oom吗？

**回答：**是的

------

**问题：**

有个疑问，就是g1将以前gc区域化整为零，这样时间可控，更精细化，但region回收算法还是按照之前的吗？

**回答：**回收每个region的时候是按照复制算法来的

------

**问题：**

一个新的小对象（不会因为过大而分配到老年代的对象）是只分配到eden，还是会分配到eden+一个survivor钟

**回答：**仅仅分配到eden区

------

**问题：**

老师好，这课太棒了！！！这钱花得值！！！ 冒昧请教个问题，有点糊涂，就是您004课图中“内存区域运行方法使用”这块内存区域和“JAVA虚拟机栈”区别是啥呢？ 

您说“内存区域运行方法使用”这个内存区域是用来保存方法中的变量的，但是“JAVA虚拟机栈”不也是保存方法中变量的吗？ 

比如我有个m1方法，里面有个int a = 1,那么这个a是在“内存区域运行方法使用”和执行m1方法的线程所属栈帧里各存一份儿？ 这两块关系是啥呢？欺盼您的回复，谢谢🙏

**回答：**多谢支持，其实这两个东西就是一个意思

------

**问题：**

老师好。这里有个小问题，文中说基于动态年龄判断，年龄n以及之上的对象进入老年代，我理解此时最大也就是n了呢，为啥还有及以上？

**回答：**举个例子，年龄为1234的对象加起来就50%了，那么年龄5678的不是也会可能存在吗？

------

**问题：**

老师，有个疑问。在Minor GC之前一通检查时，假如发生了Full GC，Full GC已经包含了Minor GC，那么Full GC之后是否还会单独再执行一次Minor GC？

**回答：**那样一般就不会了

------

**问题：**

为什么通过cglib创建的这些代理类不能被回收？

**回答：**因为一直在cglib内部被引用了

------

**问题：**

老师，eden区满了，s1没有满，新来一个对象，是出发gc还是分配到s1

**回答：**eden区满，直接触发young gc了

------

**问题：**

老师，我在idea上执行总是会多几次分配失败的日志，这个有办法忽略吗？有时候分析会造成很大的干扰

**回答：**这个没事的，每个人不同的jdk版本以及idea的环境，会导致实验结果有差异，你主要关注背后的本质就可以了

------

**问题：**

第一次young gc后发现触发了动态年龄担保规则不是就会直接进入老年代了么，为啥是第二次gc后才进去的

**回答：**这是jvm的运行管理，第一次不会直接触发动态年龄判定，要到第二次才会触发

------

**问题：**

老师，我的jdk是1.8.0_131 版本，window环境，1.126: [GC (Allocation Failure) 1.126: [ParNew: 3326K->512K(4608K), 0.0021687 secs] 3326K->988K(9728K), 0.0023565 secs] [Times: user=0.00 sys=0.00, real=0.02 secs] 1.146: [GC (Allocation Failure) 1.146: [ParNew: 3662K->0K(4608K), 0.0012292 secs] 4138K->985K(9728K), 0.0013099 secs] [Times: user=0.06 sys=0.00, real=0.00 secs] 实验结果出现了两次gc，这个和jdk版本有关吗？

**回答：**是的，跟jdk版本有关系，不过没事，你重点关注本质的运行原理就可以

------

**问题：**

为什么在处理请求时会创建2个数组，这个没有太明白

**回答：**这个就得分析tomcat源码了，就没必要了，你只要在这里知道他对每个请求tomcat内核就是会创建2个数组存放请求和响应就可以了

------

**问题：**

需要做资源隔离（超时熔断处理，配合请求队列异步，只给几个固定的线程资源），很经典的微服务问题👍👍👍

**回答：**是的，这个是很经典的一个问题

------

**问题：**

永久代OOM的时候异常是什么？

**回答：**Metaspace相关的异常

------

**问题：**

那么生产上到底要不要禁止System.gc（）？

**回答：**原则上要禁止，但是这个案例中特殊情况，Java NIO需要用System.gc()，所以只能放开了，只要保证你代码里别写System.gc()就可以了

------

**问题：**

线上这样的问题应该避免，rpc调用都有熔断降级的，下游系统故障不应该影响上游系统业务，否则就会引发雪崩事故。

**回答：**对的，就是这样的，一般应该做熔断，但是不是每个系统都会上来就做熔断的

**问题：**

很好奇，8g的dump是怎么打开的

**回答：**之前讲解MAT的时候其实说过的，你得把MAT的启动参数调整为8g，就可以本地打开8g的内存快照了

------

**问题：**

DirectByteBuffer在堆内存是引用对象吧？真正内容在堆外内存，这样堆内可以新建很多引用对象。写代码的时候要预估好，并手动释放

**回答：是的**

------

**问题：**

谢谢老师对于压测环境的回复，在预发布只压测一台机器得到的结果，然后做线性关联得出线上的最终效果吗？

这里仅对一台预发布机器进行压测对于最终发布后线上的性能表现有多大的参考价值，或者需要做一些什么样的措施来达到通过对预发布一台机器的压测得出最终线上的实际性能表现呢？不知道我说清楚没有，谢谢老师

**回答：**压测的时候一般就是一台机器，或者几台机器，然后直接线性预估线上的就可以了

------

**问题：**

老师，你之前说过复制算法是先把对象复制到另外一个空白区域，然后将这块区域回收，现在在进行Minor Gc之后发现存活区放不下存活的对象了

那么如果使用的是复制算法，在回收之前就会将存活对象放入存活区，此时jvm发现存活区放不下对象，会将存活的对象转移到老年代，我的问题是这个存活对象到底是回收之前转移到存活区还是回收之后转移到存活区

**回答：**回收之前，他要先把存活对象放入survivor区域，再回收eden区的垃圾对象

------

**问题：**

老师写的很好，我有个问题想问下，比如启动一个tomcat下的web应用，第一次访问页面的时候比较慢，后面就快多了，是不是就因为第一次访问时，程序中需要加载很多类的实例，这个时候这个类的.Class对象还没被加载到常量池中，堆内存中也没有写个对象，所以会关联很多类对象和实例对象，所以比较慢

第二次访问是类对象以经有了，如果是单机模式的话，那么堆内存中实例对象也创建好了，这时只需要把实例对象的引用给虚拟机栈，所以后续访问会非常快，这么理解对吗？

**回答：**是的，理解的基本没问题

------

**问题：**

我觉得应该是根据项目情况而定，大部分情况下，应该是要禁止掉的。 但是如果系统的类似那种文件存储系统，IM系统，大量用到NIO的场景下，就要开放System.gc() 。同时要求写代码的同学不要写System.gc() ;)

**回答：**是的，就是这个意思

------

**问题：**

很好奇一个问题，就是输出gc.log的时候，究竟会不会有性能问题？看日志也只有分配失败或者关键事件才会有日志输出

**回答：**一般是不会有性能问题的

------

**问题：**

如果一个tomcat部署两个应用，两个应用之间通过hessian调用，这两个应用就是两个进程吗？

**回答：**一个Tomcat就是一个进程，两个应用都在一个进程里

------

**问题：**

老师，请问你这里讲的都是基于JDK8来讲的吧，JDK8以前的JVM，堆是包含永久带，而从8开始永久带变成元数据区，并且不在堆范围内？ 另外，JDK8开始，一个什么native memory规避了内存泄漏？这个是怎么理解，谢谢

**回答：**都是JDK 8来讲的，其实你说的堆外内存泄漏，可以看后面的案例，后面有案例讲了

------

**问题：**

老师为什么我MAT工具分析出来的跟您不一样啊？ 我看到的是这个，并不是Demo1里面动态生成的类 java.lang.String @ 0x6e9a08a58 org.springframework.cglib.core.AbstractClassGenerator$ClassLoaderData$1

**回答：**这个是JDK自己的类，你没看到其他的类吗？jvm参数设置呢？

**问题：**

老师为什么我的是这样的 main at java.lang.OutOfMemoryError.<init>()V (OutOfMemoryError.java:48) at java.util.Arrays.copyOfRange([CII)[C (Arrays.java:3664) at java.lang.String.<init>([CII)V (String.java:201) at java.lang.StringBuilder.toString()Ljava/lang/String; (StringBuilder.java:407) at com.zhss.demo.zuul.gateway.jvm.内存泄漏.SpaceDemo.main([Ljava/lang/String;)V (SpaceDemo.java:18)

**回答：**这个也没问题，显示出了内存溢出的异常了

------

**问题：**老师，你们查看接口的调用耗时是怎么做的？

**回答：**这个一般可以用一些监控系统去做

------

**问题：**

protobuf协议里Request类的定义，如果是B定义，A有需求再通知B改，然后jar版本更新一下，今天的问题应该不会出现了吧

**回答：**是的，如果更新了jar就不会有问题了

------

**问题：**

老师如何知道tomcat现在有多少线程在处理 如果没有监控如何知道现在每秒多少请求

**回答：**之前的文章里其实讲过的，可以自己写一个简单的metrics统计框架，给一些接口加入注解，然后自动统计每个接口每秒钟内的访问

------

**问题：**

老年代不是不超过40%吗，那怎么会有超过45%的时候？

**回答：**年轻代和老年代的占比是动态变化的，老年代最多是可以超过40%的

------

**问题：**

如果4M的数组存满了会发生什么情况？

**回答：**那就只能丢弃部分数据了

------

**问题：**

如果minor gc和 full gc 都是遍历线程栈和方法区的gc roots去追踪存活对象，那么full gc的时候岂不是做了minor gc的一些活，清理了一些年轻代的短暂对象？这与full GC只负责老年代有点冲突了，还是说在遍历GC roots的时候是可以区分新生代还是老年代的？

**回答：**gc roots可能会遍历到年轻代和老年代，但是回收老年代的时候只会回收老年代里的垃圾对象，遍历和回收是两回事儿

------

**问题：**

请问一下线上的JVM监控是怎样实现的, 是每秒执行jstat然后进行分析和聚合得到结果在输出的方式吗?

**回答：**

这个你可以自己做，比如用jstat + shell脚本，自己写一个，也可以用现成的监控系统，比如zabbix，都是开源的，可以去看一下

------

**问题：**

请问一下我看Protobuf的文档上说Protobuf是对变化友好的, 也就是如果A这边的Request变更了,B那边还是原来的, 那在做反序列化的时候, 就只对B这边有的字段进行反序列化, 不会处理B不知道的字段, 这样看的话好像不会出本文的问题, 请问是这样吗?

**回答：**

这个还不完全如此，因为我们哪怕反序列化成功了，但是在取用字段的时候会出问题报错。比如说，服务A的Request有一个字段和服务B的Request字段名不一样，那么会导致服务B的Request反序列化完了，取不到某个字段

------

**问题：**

这个问题我们之前公司遇到过，虽然加了where条件，但是因为数据很大，每次load出来还是有几百兆，而且这块数据随着时间还会增长（当然频率很低），当时系统没做缓存，JVM也没有调优，后续就没有后续了😂

**回答：**

是的，数据量太大会进入老年代，导致频繁full gc的

------

**问题：**

我喜欢图文，如果视频，有的内容比较冗余，而且你还不敢快进。图文更有利于阅读，我就属于那种jvm理论还算丰富，但是没什么实践，希望老师的可能能让我收获很多，辛苦了！

**回答：**

加油，好好学习，一定会收获很多

------

**问题：**

如果你是在idea 里面直接跑出现的问题会跟老师有很大的差异，就运行空的main 方法都会出现gc 的情况。最好的方式去打成jia 包，然后用 java -XX: ... -classpath jar包 class 这种方式去运行，出来的结果是差不多的。

**回答：**

是的，自己用jar包运行

------

**学员实践反馈：**

然后这是我的测试效果，机器是 mac ,jdk 版本 1.8.0_201： CommandLine flags: -XX:InitialHeapSize=10485760 -XX:MaxHeapSize=10485760 -XX:MaxNewSize=5242880 -XX:NewSize=5242880 -XX:OldPLABSize=16 -XX:PretenureSizeThreshold=10485760 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:SurvivorRatio=8 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC 0.065: [GC (Allocation Failure) 0.065: [ParNew: 3596K->321K(4608K), 0.0005020 secs] 3596K->321K(9728K), 0.0005608 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 4608K, used 2451K [0x00000007bf600000, 0x00000007bfb00000, 0x00000007bfb00000) eden space 4096K, 52% used [0x00000007bf600000, 0x00000007bf814930, 0x00000007bfa00000) from space 512K, 62% used [0x00000007bfa80000, 0x00000007bfad06b8, 0x00000007bfb00000) to space 512K, 0% used [0x00000007bfa00000, 0x00000007bfa00000, 0x00000007bfa80000) concurrent mark-sweep generation total 5120K, used 0K [0x00000007bfb

**回答：**

非常好

------

**学员实践反馈：**

在没有最后放入最后一个2M 的对象之前，前面的3个 1M 的对像，没有超过eden 的大小，从日志中可以看出来，eden 4096K, 89% used 是可以匹配的。 CommandLine flags: -XX:InitialHeapSize=10485760 -XX:MaxHeapSize=10485760 -XX:MaxNewSize=5242880 -XX:NewSize=5242880 -XX:OldPLABSize=16 -XX:PretenureSizeThreshold=10485760 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:SurvivorRatio=8 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC Heap par new generation total 4608K, used 3678K [0x00000007bf600000, 0x00000007bfb00000, 0x00000007bfb00000) eden space 4096K, 89% used [0x00000007bf600000, 0x00000007bf997928, 0x00000007bfa00000) from space 512K, 0% used [0x00000007bfa00000, 0x00000007bfa00000, 0x00000007bfa80000) to space 512K, 0% used [0x00000007bfa80000, 0x00000007bfa80000, 0x00000007bfb00000) concurrent mark-sweep generation total 5120K, used 0K [0x00000007bfb00000, 0x00000007c0000000, 0x00000007c0000000) Metaspace used 2698K, capacity 4486K, committed 4864K, reserved

**回答：非常好**

------

**问题：**

为什么没有Parallel，难道很少用吗？

**回答：**

很少用，一般生产不用parallel回收器 

### 092、案例实战：每天10亿数据的日志分析系统的OOM问题排查实践！

**1、案例背景引入**

今天的案例背景是一个每天10亿数据量的日志清洗系统，这个系统做的事情其实非常的简单，他主要就是从Kafka中不停的消费各种日志数据，然后对日志的格式进行很多清洗，比如对一些涉及到用户敏感信息的字段（姓名、手机号、身份证号）进行脱敏处理，然后把清洗后的数据交付给其他的系统去使用。

比如推荐系统、广告系统、分析系统都会去使用这些清洗好的数据，我们先看下面的图，大致就知道这个系统的运行情况了。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/41584100_1578304264.cn/txdocpic/0/27ed23c04be27b69db82d43d7c385dc1/0)       

**2、事故发生现场**

某天我们也是突然收到线上的报警，发现日志清洗系统发生了OOM的异常！

我们登陆到线上机器查看日志之后，发现还是那么经典的java.lang.OutOfMemoryError: java heap space的问题，又是堆内存溢出。

此时我们当然就会来分析一下问题到底出在哪里了，大家应该还记得我们分析OOM问题的套路

首先先看看异常日志，去定位一下到底是谁导致的这个问题，当时我们在日志里大致看到了类似如下的一些信息：

java.lang.OutOfMemoryError: java heap space

xx.xx.xx.log.clean.XXClass.process()

xx.xx.xx.log.clean.XXClass.xx()

xx.xx.xx.log.clean.XXClass.xx()

xx.xx.xx.log.clean.XXClass.process()

xx.xx.xx.log.clean.XXClass.xx()

xx.xx.xx.log.clean.XXClass.xx()

xx.xx.xx.log.clean.XXClass.process()

xx.xx.xx.log.clean.XXClass.xx()

xx.xx.xx.log.clean.XXClass.xx()

当然大量无关紧要的日志信息可以直接忽略掉了，毕竟当时也没有截图，直接看上面最关键的一些信息

大家可以很明显的发现，似乎同样的一个方法（XXClass.process()）反复出现了多次，最终导致了堆内存溢出的问题。

这个时候通过日志，有经验的朋友可能已经可以发现一个问题了，那就是在某一处代码出现了大量的递归操作。正是大量的递归操作之后，也就是反复调用一个方法之后，导致了堆内存溢出的问题。

初步是大致定位出来问题所在了，接着当然我们就得去用MAT分析一下内存快照了。

**3、初步分析内存快照**

接着我们开始分析生产现场的内存快照，之前我们已经详细讲解了如何通过MAT去分析内存快照，快速定位创建大量对象的代码和方法，其实在日志中我们是可以看到是哪个方法导致的内存溢出，但是我们通过日志不知道到底是哪个方法调用创建了大量的对象。

因此最终无论如何，还是得通过MAT去分析一下，在分析的时候，我们就发现了一个问题，因为有大量的XXClass.process()方法的递归执行，每个XXClass.process()中都创建了大量的char数组！

最后因为XXClass.process()方法又多次递归调用，也就导致了大量的char[]数组耗尽了内存。

先看看下图，在图里我们表示出来了方法递归调用，每次调用都创建大量char[]数组导致的内存溢出问题。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56122100_1578304264.cn/txdocpic/0/04e166ae152941f7bb6ca469de88af25/0)      

**4、功夫在诗外：问题在JVM参数上**

基本定位出了问题所在了，但是先别着急直接去代码中检查问题所在，因为我们当时发现了一个比较大的问题。

虽然XXClass.process()方法递归调用了多次，但是实际上我们在MAT中发现递归调用的次数也并不是很多，大概也就是十几次递归调用到最多几十次递归调用而已

而且我们观察了一下，所有递归调用加起来创建的char[]数组对象总和其实也就最多1G而已。

如果是这样的话，其实我们应该先注意一个问题，那就是可能这次OOM的发生不一定是代码就写的有多么的烂，可能就是我们的JVM的内存参数设置的不对，给堆内存分配的空间太小了！

如果要是给JVM堆内存分配更大的空间呢？一切都要尝试一下，所以先别着急，慢慢来。

先看下面的图，在里面我们表示出来了这个内存过小的问题。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66795700_1578304264.cn/txdocpic/0/79872af6705f5786f384d5fc3c350076/0)       

**5、分析一下JVM的GC日志**

如果你要知道这个堆内存到底是不是设置太小了，就得先分析一下JVM运行时的内存使用模型。

现在系统已经宕机了，我们唯一可以看到的，就是当时在JVM启动参数中加入的自动记录的GC日志了。

从GC日志中，我们是可以看到JVM启动时的完整参数设置的，核心的内容如下所示：

-Xmx1024m -Xms1024m  -XX:+PrintGCDetails -XX:+PrintGC()  -XX:+HeapDumpOnOutOfMemoryError -Xloggc:/opt/logs/gc.log -XX:HeapDumpPath=/opt/logs/dump

大家可以看到，这里主要是把gc日志详细记录在了一个日志文件里，另外指定了内存溢出的时候要导出内存快照，另外就是堆内存给的是1GB大小，但是要知道这台机器可是4核8G的！

接着我们看一下当时记录下来的gc.log日志。

[Full GC (Allocation Failure) 866M->654M(1024M)]

[Full GC (Allocation Failure) 843M->633M(1024M)]

[Full GC (Allocation Failure) 855M->621M(1024M)]

[Full GC (Allocation Failure) 878M->612M(1024M)]

我把GC日志中大量的无关紧要的信息省略掉了，因为跟我们分析关系不大

但是大家可以发现一点，因为Allocation Failure触发的Full GC很多，也就是堆内存无法分配新的对象了，然后触发GC，结果触发的时候肯定是先触发Full GC了，这个关于Full GC触发的原因和时机之前大量的分析过，这里不多说了。

而且你会发现每次Full GC都只能回收掉一点点对象，发现堆内存几乎都是占满了。

另外我们这里没有显示时间，当时日志里显示的是每秒钟都会执行一次Full GC，这个就很可怕了。基本上我们可以明确一点，应该是在系统运行的时候，因为XXClass.process()方法不停递归创建了大量的char[]数组，导致堆内存几乎是塞满的。

我们先看下面的图，表示出了这一点。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/77324700_1578304264.cn/txdocpic/0/0fb3cec66d610587fbcb56f3e82dac00/0)      

然后这就导致了连续一段时间，每秒触发一次Full GC，因为内存都满了，特别是老年代可能几乎都满了，所以可能是每秒钟执行young gc之前，发现老年代可用空间不足，就会提前触发full gc

也可能是young gc过后存活对象太多无法放入Survivor中，都要进入老年代，放不下了，只能进行full gc。

我们看下图，表示出来了每秒钟执行一次full gc的场景。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/87760200_1578304264.cn/txdocpic/0/53a475969b46ee282d6a5d9fd6c1e0e4/0)       

但是每次full gc只能回收少量对象，直到最后可能某一次full gc回收不掉任何对象了，然后新的对象无法放入堆内存了，此时就会触发OOM内存溢出的异常。

我们看下面的图，表示出来了这个过程。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/3825600_1578304265.cn/txdocpic/0/c91f0848d74d18f85456935350e21f13/0)       

分析到这里不知道大家有什么感觉？其实很明显一点，就是堆内存肯定是偏小了，这个就导致频繁的full gc。

**6、分析一下JVM运行时内存使用模型**

接着我们再用jstat分析一下当时JVM运行时的内存模型，当时我们重启了系统，每秒钟打印一次jstat的统计信息，就看到了下面的情况：

S0 S1 E O YGC FGC

0 100 57 69 36 0

0 100 57 69 36 0

0 100 65 69 37 0

0 100 0 99 37 0

0 100 0 87 37 1

我就给出部分信息大家就可以看出来问题所在了，刚开始都是年轻代的Eden区在涨，接着YGC从36到37，就是发生了一次YGC，接着Old区直接从占比69%到99%

说明什么？

说明YGC后存活对象太多，Survivor放不下，直接进老年代了！

接着老年代都占了99%了，直接就触发了一次Full GC，但是也仅仅让老年代从占比99%到87%而已，回收了少量的对象。

上面的那个过程反复循环几次，大家思考一下，年轻代的对象反复进入老年代，不停的触发Full GC，但是还回收不了多少对象，几次循环过后，老年代满了，可能Full GC没回收多少对象，新的对象一大批放不下了，就触发OOM了。

**7、优化第一步：增加堆内存大小**

所以这个OOM的问题，说白了不能直接说是代码问题，从JVM运行情况以及内存大小来看，就是内存分配不足的问题。

因此这里第一步，直接在4核8G的机器上，给堆内存加大空间，直接给了堆内存5G的内存。

接着运行系统，通过jstat观察，就可以发现，每次Young GC过后存活对象都落入Survivor区域了，不会随便进入老年代，而且因为堆内存很大，基本上运行一段时间不会发生OOM问题了。

**8、优化第二步：改写代码**

另外就是改写代码，让他不要占用过多的内存。当时代码之所以递归，就是因为在一条日志中，可能会出现很多用户的信息，一条日志也许会合并包含了十几个到几十个用户的信息。

这个时候代码中就是会递归十几次到几十次去处理这个日志，每次递归都会产生大量的char[]数组，是切割了日志用来处理的。

其实这个代码写的完全没有必要，因为对每一条日志，如果发现包含了多个用户的信息，其实就对这一条日志切割出来进行处理就可以了，完全没有必要递归调用，每次调用都切割一次日志，生成大量的char[]数组。

所以把这一步代码优化了之后，一下子发现线上系统的内存使用情况降低了10倍以上。

**9、案例总结**

今天这个案例，大家会发现，我们先是通过OOM的排查方法去分析，发现主要是内存太小导致的问题

然后用gc日志和jstat分析，明显发现是内存不够用了，最后加大系统内存，并且优化代码就可以了。

**End**

### 093、案例实战：一次服务类加载器过多引发的OOM问题排查实践！

**1、案例背景引入**

公司里有一个非常正常的线上的服务，采用的是Web系统部署在Tomcat中的方式来进行启动的。

但是有一段时间，我们突然收到一些反馈，说是这个服务非常的不稳定，经常会出现访问这个服务的接口的时候出现服务的假死问题。

一旦出现这种接口调用时服务假死的情况，相当于我们的这个服务就完全不可用了，因此收到了不少上游服务的反馈。

但是上游服务反馈了另外一个非常关键的情况，就是经常一段时间内无法访问这个服务的接口，但是过了一会儿又可以访问了！

也就是说，似乎每次系统假死都只是一段时间而已！

因此我们着手进行了下面一系列的排查，最终解决了这个问题。

**2、使用top命令检查机器资源使用**

这里先给大家介绍一个技巧，因为当时的生产情况是服务假死，接口无法调用，并不是直接就抛出了OOM之类的异常。

因此其实你也很难直接去看他的线上日志，说根据日志立马就可以定位问题。

因此针对服务假死这个问题，我们首先可以先用linus的top命令去检查一下机器的资源使用量，通过这个命令可以看到机器上运行的各个进程对CPU和内存两种资源的使用量。

为什么要看这个呢？

因为如果服务出现无法调用接口假死的情况，首先要考虑的是两种问题。

**第一种问题：**这个服务可能使用了大量的内存，内存始终无法释放，因此导致了频繁GC问题。

也许每秒都执行一次Full GC，结果每次都回收不了多少，最终导致系统因为频繁GC，频繁Stop the World，接口调用出现频繁假死的问题。

**第二种问题：**可能是这台机器的CPU负载太高了，也许是某个进程耗尽了CPU资源，导致你这个服务的线程始终无法得到CPU资源去执行，也就无法响应接口调用的请求。这也是一种情况。

因此针对服务假死的问题，通过top命令先看一下，立马心里就有数了。

针对线上这台机器使用top命令检查之后，就发现了一个问题，这个服务的进程对CPU耗费很少，仅仅耗费了1%的CPU资源，但是他耗费了50%以上的内存资源。这个就引起了我们的注意。

因为这台机器是4核8G的标准线上虚拟机，针对这种机器通常我们会给部署的服务的JVM总内存在5G~6G，刨除掉Metaspace区域之类的，堆内存大概会给到4G~5G的样子，毕竟还得给创建大量的线程留下一部分的内存。

之前给大家介绍过，JVM使用的内存主要是三类，栈内存、堆内存和Metaspace区域，现在一般会给Metaspace区域512MB以上的空间，堆内存假设有4G，然后栈内存呢？每个线程一般给1MB的内存，那么如果你JVM进程中有几百上千个线程，也会有将近1G的内存消耗。

此时JVM进程其实耗费的总内存就接近6G了，另外你还得给操作系统内核以及其他的进程留出一部分的内存空间去使用。

因此最终让你的JVM可以使用的堆内存大概也就是机器上一半的内存而已。

大家到这里就知道了，为什么我们看到这个服务进程对内存耗费超过50%感到有点惊讶，因为这说明他几乎快要把分配给他的内存消耗殆尽了！

而且最关键的是，他长期保持对内存资源的消耗在50%以上，甚至达到更高，说明他GC的时候并没有把内存回收掉！

**3、在内存使用这么高的情况下会发生什么？**

此时我们开始思考一个问题，既然这个服务的进程对内存使用率这么高，可能发生的问题也就三种。

**第一种**是内存使用率居高不下，导致频繁的进行full gc，gc带来的stop the world问题影响了服务。

**第二种**是内存使用率过多，导致JVM自己发生OOM。

**第三种**是内存使用率过高，也许有的时候会导致这个进程因为申请内存不足，直接被操作系统把这个进程给杀掉了！

所以此时我们就开始分析，到底是哪种问题呢？

首先我们先使用jstat分析了一下JVM运行的情况，确实内存使用率很高，也确实经常发生gc，但是实际上gc耗时每次也就几百毫秒，并没有耗费过多的时间

也就是说虽然gc频率高，但是其实是个常规现象。

而且我们发现这个服务经常频繁gc，但是有的时候频繁gc的时候，也没听见上游服务反馈说服务假死！因此第一种情况其实直接可以过滤掉了。

接着我们分析第二种情况，难道是JVM自己有时候发生OOM挂掉了？挂掉的时候必然导致服务无法访问，上游服务肯定会反馈说我们服务死掉的！

但是我们检查了一下应用服务自身的日志，并没有看到任何日志输出了OOM异常！

接着我们猜测，也许是第三种问题，就是JVM运行的时候要申请的内存过多，结果内存不足了，有时候os会直接杀掉这个进程！

可能在进程被杀掉的过程中，就出现了上游服务无法访问的情况，但是我们的进程都是有监控脚本的，一旦进程被杀掉，会有脚本自动把进程重新启动拉起来。

所以也许其他服务过一会就会发现，服务又可以访问了！

**4、到底是谁占用了过多的内存？**

既然如此，按照我们的思路继续分析下去，如果要解决这个问题，就必须要找出来，到底是什么对象占用我们的内存过多，进而申请过多的内存，最后导致进程被杀掉了？

很简单，直接从线上导出一份内存快照即可。

我们在线上系统运行一段时间过后，用top命令和jstat命令观察了一段时间，发现jvm已经耗费了超过50%的内存了，此时迅速导出了一份内存快照进行分析。

此时用MAT进行内存快照分析的时候，我们发现，居然是一大堆的ClassLoader也就是类加载器，有几千个，而且这些类加载器加载了的东西，都是大量的byte[]数组，所有这些一共占用了超过50%的内存。

**看起来元凶就是他了！**

那这些ClassLoader是哪儿来的？为什么会加载那么多的byte[]数组？

具体原因就不说了，但是大家应该知道一点，我们除了用类加载加载类以外，其实还可以用类加载器去加载一些其他的资源，比如说一些外部配置文件什么的。

当时写这个系统代码的工程师做了自定义类加载器，而且在代码里没有限制的创建了大量的自定义类加载器，去重复加载了大量的数据，结果经常一下子就把内存耗尽了，进程就被杀掉了！

因此解决这个问题非常的简单，直接就是修改代码，避免重复创建几千个自定义类加载器，避免重复加载大量的数据到内存里来，就可以了。

**5、本文小结**

其实所谓的案例实战，他们背后的原理都是一套东西，归根结底从原理层面都是类似的

但是用不同的案例，可以告诉大家各种不同的故障场景以及不同问题的分析思路，这个是案例对大家最有用的一个地方。

因此希望大家好好吸收各种不同的案例，去体会不同场景的问题和解决思路。

**End**

### 094、案例实战：一个数据同步系统频繁OOM内存溢出的排查实践

**1、案例背景**

首先说一下案例背景，线上有一个数据同步系统，是专门负责从另外一个系统去同步数据的，简单来说，另外一个系统会不停的发布自己的数据到Kafka中去，然后我们有一个数据同步系统就专门从Kafka里消费数据，接着保存到自己的数据库中去，大概就是这样的一个流程。

我们看下图，就是这个系统运行的一个流程。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26784100_1578304272.cn/txdocpic/0/89b88af606a4cfd8b4705900ae9f3032/0)       

结果就这么一个非常简单的系统，居然时不时就报一个内存溢出的错误，然后就得重启系统，过了一段时间又会再次内存溢出一下。

而且这个系统处理的数据量是越来越大，因此我们发现他内存溢出的频率越来越高，到这个情况，就必须要处理一下了。

**2、经验丰富的工程师：从现象看到本质**

一般遇到这种现象，只要是经验丰富的工程师，应该已经可以具备从现象看到本质的能力了。我们可以来分析和思考一下，既然每次重启过后都会在一段时间以后出现内存溢出的问题，说明肯定是每次重启过后，内存都会不断的上涨。

而且一般要高到JVM出现内存溢出，通常就是两种情况，要不然是并发太高，瞬间大量并发创建过多的对象，导致系统直接崩溃了。要不就是有内存泄漏之类的问题，就是很多对象都赖在内存里，无论你如何GC就是回收不掉。

那么这个场景是怎么回事呢？我们当时分析了一下，这个系统的负载并不是很高，虽然数据量不少，但并不是那种瞬时高并发的场景。

这么看来，很可能就是随着时间推移，有某种对象越来越多，赖在内存里了。然后不断的触发gc，结果每次gc都回收不掉这些对象。

一直到最后，内存实在不足了，就会内存溢出

我们看看下面的图，在下图里就画出了这个问题。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37729500_1578304272.cn/txdocpic/0/502e536bca2169695756a2b3c7cd5354/0)       

**3、通过jstat来确认我们的推断**

接着直接在一次重启系统之后，用jstat观察了一下JVM运行的情况：

我们发现，老年代的对象一直在增长，不停的在增长。每次Young GC过后，老年代的对象就会增长不少。

而且当老年代的使用率达到100%之后，我们发现会正常触发Full GC，但是Full GC根本回收不掉任何对象，导致老年代使用率还是100%！

然后老年代使用率维持100%一段时间过后，就会报内存溢出的问题，因为再有新的对象进入老年代，实在没有空间放他了！

所以这就基本确认了我们的判断，每次系统启动，不知道什么对象会一直进入堆内存，而且随着Young GC执行，对象会一直进入老年代，最后触发Full GC都无法回收老年代的对象，最终就是内存溢出。

**4、通过MAT找到占用内存最大的对象！**

关于MAT分析内存快照的方法，之前已经讲解的很详细了，在这个案例中就不用重复截图了，直接说出过程和结论就好！

在内存快照中，我们发现了一个问题，那就是有一个队列数据结构，直接引用了大量的数据，就是这个队列数据结构占满了内存！

那这个队列是干什么用的？

简单来说，从Kafka消费出来的数据会先写入这个队列，接着从这个队列再慢慢写入数据库中，主要是要额外做一些中间的数据处理和转换，所以自己在中间又加了一个队列。

我们看下面的图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/45852300_1578304272.cn/txdocpic/0/6b9c6da2db6017b95be7d0f91ba54734/0)       

那么这个队列是怎么用的？问题就出在这里了！

大家都知道，从Kafka消费数据，是可以一下子消费一批出来的，比如消费几百条数据出来。

因此当时这个写代码的工程师，直接就是每次消费几百条数据出来给做成一个List，然后把这个List放入到队列里去！

最后就搞成了这种情况：比如一个队列有1000个元素，每个元素都是一个List，每个List里都有几百条数据！

这种做法怎么行？会导致内存中的队列里积压几十万条，甚至百万条数据！最终一定会导致内存溢出！

而且只要你数据还停留在队列中，就是没有办法被回收的。

我们看下面的图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/57833300_1578304272.cn/txdocpic/0/7dc9ae7f730819a5b5849d6369562b22/0)       

上图就是一个典型的对生产和消费的速率没控制好的例子。

从Kafka里消费出来数据放入队列的速度很快，但是从队列里消费数据进行处理，然后写入存储的速度较慢，最终会导致内存队列快速积压数据，导致内存溢出。

而且这种队列每个元素都是一个List的做法，会导致内存队列能容纳的数据量大幅度膨胀。

最终解决这个问题也很简单，把上述内存队列的使用修改了一下，做成了定长的阻塞队列。

比如最多1024个元素，然后每次从Kafka消费出来数据，一条一条数据写入队列，而不是做成一个List放入队列作为一个元素。

因此这样内存中最多就是1024个数据，一旦内存队列满了，此时Kafka消费线程就会停止工作，因为被队列给阻塞住了。不会让内存队列中的数据过多。

我们看下面解决问题之后的图：      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/67592600_1578304272.cn/txdocpic/0/aedcdb05017167a39bc16181aa093b04/0)       

**5、本文小结**

本文是我们整个专栏的最后一个案例，相信大家认真学完这个专栏后，就会感受到我们设计这个专栏的思路。

专栏的核心是通过一步一图和大白话的方式，让大家学会JVM的核心运行原理，接着学习了JVM GC优化的核心原理和OOM问题的核心原理。

接着我们给大家讲解了JVM的GC问题以及OOM的常见发生场景和解决方法。

同时我们带给了大家数十个来源于我们真实生产环境的JVM优化案例，包括GC优化案例和OOM优化案例

大量的优化案例让大家可以对各种不同场景的问题有一个了解，同时积累起来了对不同问题进行分析、排查和解决的思路。

在这个过程中，如果大家反复去把这些案例看几遍，吸收透彻了，本质上就会积累起来较为丰富的JVM优化实践的经验积累

当你日后真的在工作中需要解决JVM问题的时候，就会发现这些知识全部都可以派上用场了。

**End**

### 095、总复习：线上系统的JVM参数优化、GC问题定位排查、OOM分析解决

今天这篇文章基本到了我们这个专栏的尾声了，最后我们整体梳理一下这个专栏教给大家的知识。

相信大家应该还记得我们最早做这个JVM优化实战专栏的初衷，因为国内绝大多数的Java工程师对JVM这块知识的学习，总是停留在一些理论的层面

从没有人系统化的告诉你，JVM到底应该如何做优化。

网上也只有一些看不太明白的博客分享少数人自己的JVM优化经验，但是很多新手面对这些优化博客往往只能看到表面，而无法看到其背后的本质。

因此我们才决定了做这个JVM优化实战专栏，希望能够帮助到国内大部分的Java工程师掌握对线上生产系统进行JVM性能优化和OOM问题解决的思路。

而且我们在专栏讲解的过程中，抛弃掉了一些跟大家平时生产环境实战不太相关的内容，比如class文件结构、编译器、字节码执行引擎，等等。

这个专栏带给大家的东西其实主要就是三个部分：

**第一个**是JVM运行我们写好的系统最根本的工作原理，包括：

- 内存各个部分的划分
- 代码在执行的过程中，各个内存区域是如何配合协调工作的
- 对象是如何分配的
- GC如何触发
- GC执行的原理是什么
- 常见的用于控制JVM工作行为的一些核心参数都有哪些

**第二个**是对于一个写好的系统，我们应该如何通过预估的方法给他设置一些相对合理的JVM参数，然后在测试的时候如何合理的优化调整JVM的参数

另外在线上部署的系统如何对JVM进行监控，如何对线上出现GC性能问题的JVM进行合理的优化。

除此以外，我们提供了大量的来自生产一线的实战案例，向大家展示了各种奇怪场景下的JVM GC问题，通过案例来给大家展示解决这类问题的思路和方法。

**第三个**是对于一个生产运行的系统，如果出现了OOM内存溢出问题，我们应该采用一种什么样的方式来进行分析、定位和解决？

同样我们也提供了大量的一线生产案例，给大家展示了各种千奇百怪的OOM问题的场景，同时展示了对这类问题的分析、定位和解决的思路。

我们相信，如果大家认认真真的学习完这个专栏，首先对JVM的运行原理一定有了一个较为深刻的理解

同时对自己负责的线上系统，能给出比较合理的JVM参数的设置

另外，对线上系统可能发生的频繁GC和OOM两种问题，都能有一定的经验和能力，去采用正确的思路分析解决生产问题。

所以在这里，我们希望大家在学习完这个专栏之后，有时间可以经常回过头去复习复习，看看专栏里的内容，然后自己做一些笔记，梳理出来JVM的工作原理，梳理出来合理设置JVM参数的方法，线上频繁GC和OOM问题的处理方法。

同时，希望大家好好的理解专栏中提供的非常宝贵的几十个实战案例的问题场景、分析和解决问题的思路

吃透这几十个案例背后的本质，你一定会积累出非常宝贵的JVM优化实战经验的！

**End**

### 096、专栏彩蛋：面试中如何展现自己的JVM实战经验？

这是我们专栏的最后一篇文章，也是我最后留给大家的一个 **彩蛋**

这篇文章给大家讲讲出去面试的时候，在JVM这块应该如何表现。

其实网上有很多的JVM相关的一些面试问题，平时常见的在面试里面试官问的也是雷同的一些问题

比如JVM的内存划分、GC算法、垃圾回收器，诸如此类的一些东西，有的人也许会问到一些偏JVM底层实现，跟实战相关不大的东西，比如JIT编译等等。

但是其实大家学完这个专栏之后，对常规的JVM面试问题基本都问题不大了，而且即使是面试时碰到少数冷门问题，自己上网搜索一下资料，基于你在这里学习到的知识来看，基本都能解决。

但是在面试的时候，在我们的这个专栏推出之前，往往有**三个问题**在面试的时候是几乎所有人都回答的非常不好的。

第一个是你们生产环境的系统的JVM参数怎么设置的？为什么要这么设置？

还有一个是你在生产环境中的JVM优化经验可以聊聊？

另外一个是说说你在生产环境解决过的JVM OOM问题？

大部分人往往都负责一些没太大技术挑战的系统，因此很多时候都是用的默认的JVM参数

在默认的JVM参数下，可能堆内存就几百M，新生代可能就一两百MB，老年代有两三百MB。

但是很多系统往往就几十个人使用，使用频率很低，业务逻辑虽然很复杂，但是其实访问量很少。

因此JVM虽然内存小，但是系统运行基本都不会有太大的问题。

这就导致大部分人遇到上述三种问题，基本都是哑口无言，即使有人能勉强说出个一二来，说的也非常的不系统，而且无法解释背后的根本原理。

但是在这个专栏学习完之后，相信每个人都能在面试的时候，就JVM实战这块有很好的表现。

大家完全可以把专栏中学习到的几十个案例，都结合自己负责的系统思考一下，看看在你的系统中是否有可能会遇到类似的问题？

如果没有，那么想一下，假设你的系统压力增长10倍或者100倍，会不会让你的系统产生案例中的问题？

然后你就可以拿着这些思考出来的东西到面试里去说了，可以结合你们自己的系统的情况，说说系统发生哪些生产问题？如何优化和解决的？这么做背后的本质是什么？为什么要这么做？

当你把这套东西说出来之后，基本上你已经比绝大多数没学过这个专栏的工程师，在JVM实战经验这块，表现起码好10倍以上。

好了，这就算是最后我留给大家的一个作业，也是希望每个同学都用心去做的一件事情。

大家务必把学到的知识紧密的跟自己负责的系统结合起来，出去面试的时候表现出让面试官惊讶的JVM实战经验和能力！

最后，说说文首给大家说到的**彩蛋**！在这里做一点预告！

很多朋友都特别希望我出一个《从0开始带你成为MySQL优化实战高手》的专栏

因为大家都特别喜欢这种大白话 + 一步一图，把原理和生产实战案例结合起来的讲解方式。

首先在这里非常感谢大家的支持和厚爱，我会在工作之余尽快规划下一个专栏。希望在未来几个月内可以推出《从0开始带你成为MySQL优化实战高手》

我将会用一如既往的风格，丰富的实战案例，带大家成为MySQL的优化实战高手！

**End**

