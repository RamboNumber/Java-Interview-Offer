### 61 探秘黑科技：基于mmap内存映射实现磁盘文件的高性能读写

**1、mmap：Broker读写磁盘文件的核心技术**

今天我们要给大家介绍一个非常关键的黑科技，很多人可能都不太熟悉，这个技术就是mmap技术，而Broker中就是大量的使用mmap技术去实现CommitLog这种大磁盘文件的高性能读写优化的。

通过之前的学习，我们知道了一点，就是Broker对磁盘文件的写入主要是借助直接写入os cache来实现性能优化的，因为直接写入os cache，相当于就是写入内存一样的性能，后续等os内核中的线程异步把cache中的数据刷入磁盘文件即可。

那么今天我们就要对这个过程中涉及到的mmap技术进行一定的分析。

**2、传统文件IO操作的多次数据拷贝问题**

首先我们先来给大家分析一下，假设RocketMQ没有使用mmap技术，就是使用最传统和基本的普通文件IO操作去进行磁盘文件的读写，那么会存在什么样的性能问题？

**答案是：多次数据拷贝问题**

首先，假设我们有一个程序，这个程序需要对磁盘文件发起IO操作读取他里面的数据到自己这儿来，那么会经过以下一个顺序：

首先从磁盘上把数据读取到内核IO缓冲区里去，然后再从内核IO缓存区里读取到用户进程私有空间里去，然后我们才能拿到这个文件里的数据

大家看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29346700_1578390216.cn/txdocpic/0/1aad67e234f26d970d6cc8e198bbf6ad/0)       

为了读取磁盘文件里的数据，是不是发生了两次数据拷贝？

没错，所以这个就是普通的IO操作的一个弊端，必然涉及到两次数据拷贝操作，对磁盘读写性能是有影响的。

那么如果我们要将一些数据写入到磁盘文件里去呢？

那这个就是一样的过程了，必须先把数据写入到用户进程私有空间里去，然后从这里再进入内核IO缓冲区，最后进入磁盘文件里去

我们看下面的图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/39510200_1578390216.cn/txdocpic/0/01447d0f2d19fedc386681567434def8/0)       

在数据进入磁盘文件的过程中，是不是再一次发生了两次数据拷贝？没错，所以这就是传统普通IO的问题，有两次数据拷贝问题。

**3、RocketMQ是如何基于mmap技术+page cache技术优化的？**

接着我们来看一下，RocketMQ如何利用mmap技术配合page cache技术进行文件读写优化的？

首先，RocketMQ底层对CommitLog、ConsumeQueue之类的磁盘文件的读写操作，基本上都会采用mmap技术来实现。

如果具体到代码层面，就是基于JDK NIO包下的MappedByteBuffer的map()函数，来先将一个磁盘文件（比如一个CommitLog文件，或者是一个ConsumeQueue文件）映射到内存里来

这里我必须给大家解释一下，这个所谓的内存映射是什么意思

其实有的人可能会误以为是直接把那些磁盘文件里的数据给读取到内存里来了，类似这个意思，但是并不完全是对的。

**因为刚开始你建立映射的时候，并没有任何的数据拷贝操作，其实磁盘文件还是停留在那里**，只不过他把物理上的磁盘文件的一些地址和用户进程私有空间的一些虚拟内存地址进行了一个映射

我们看下面的图



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/48167800_1578390216.cn/txdocpic/0/ea021100aeab1bc8d34a0aae72cc15c7/0)       

这个地址映射的过程，就是JDK NIO包下的MappedByteBuffer.map()函数干的事情，底层就是基于mmap技术实现的。

另外这里给大家说明白的一点是，这个mmap技术在进行文件映射的时候，一般有大小限制，在1.5GB~2GB之间

所以RocketMQ才让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大。

这样限制了RocketMQ底层文件的大小，就可以在进行文件读写的时候，很方便的进行内存映射了。

然后接下来要给大家讲的一个概念，就是之前给大家说的PageCache，实际上在这里就是对应于虚拟内存

所以我们在下面的图里就给大家画出了这个示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56481300_1578390216.cn/txdocpic/0/cd8de66c5a30d2358d4511676cd6a2dd/0)       

**4、基于mmap技术+pagecache技术实现高性能的文件读写**

接下来就可以对这个已经映射到内存里的磁盘文件进行读写操作了，比如要写入消息到CommitLog文件，你先把一个CommitLog文件通过MappedByteBuffer的map()函数映射其地址到你的虚拟内存地址。

接着就可以对这个MappedByteBuffer执行写入操作了，写入的时候他会直接进入PageCache中，然后过一段时间之后，由os的线程异步刷入磁盘中，如下图我们可以看到这个示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65440200_1578390216.cn/txdocpic/0/a8f9e215b29fe434a766949ca69635d1/0)      

看到这里我们有没有发现什么问题？

对了！就是上面的图里，似乎只有一次数据拷贝的过程，他就是从PageCache里拷贝到磁盘文件里而已！这个就是你使用mmap技术之后，相比于传统磁盘IO的一个性能优化。

接着如果我们要从磁盘文件里读取数据呢？

那么此时就会判断一下，当前你要读取的数据是否在PageCache里？如果在的话，就可以直接从PageCache里读取了！

比如刚写入CommitLog的数据还在PageCache里，此时你Consumer来消费肯定是从PageCache里读取数据的。

但是如果PageCache里没有你要的数据，那么此时就会从磁盘文件里加载数据到PageCache中去，如下图

而且PageCache技术在加载数据的时候**，**还会将**你加载的数据块的临近的其他数据块也一起加载到PageCache里去。**

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/73989000_1578390216.cn/txdocpic/0/fbf492b3a936b192477ec4edd3d6fbe9/0)    大家可以看到，在你读取数据的时候，其实也仅仅发生了一次拷贝，而不是两次拷贝，所以这个性能相较于传统IO来说，肯定又是提高了。

**5、预映射机制 + 文件预热机制**

接着给大家说几个Broker针对上述的磁盘文件高性能读写机制做的一些优化：

**（1）内存预映射机制**：Broker会针对磁盘上的各种CommitLog、ConsumeQueue文件预先分配好MappedFile，也就是提前对一些可能接下来要读写的磁盘文件，提前使用MappedByteBuffer执行map()函数完成映射，这样后续读写文件的时候，就可以直接执行了。

**（2）文件预热**：在提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取尤其是CommitLog、ConsumeQueue的时候，其实有可能会频繁的从磁盘里加载数据到内存中去。

所以其实在执行完map()函数之后，会进行madvise系统调用，就是提前尽可能多的把磁盘文件加载到内存里去。

通过上述优化，才真正能实现一个效果，就是写磁盘文件的时候都是进入PageCache的，保证写入高性能；同时尽可能多的通过map + madvise的映射后预热机制，把磁盘文件里的数据尽可能多的加载到PageCache里来，后续对CosumeQueue、CommitLog进行读取的时候，才能尽可能从内存里读取数据。

**6、对今天文章的一点总结**

今天我们在之前给大家讲解的PageCache技术基础之上，引入了Broker底层大量采用的mmap技术

实际上在Broker读写磁盘的时候，是大量把mmap技术和pagecache技术结合起来使用的，通过mmap技术减少数据拷贝次数，然后利用pagecache技术实现尽可能优先读写内存，而不是物理磁盘。 

**End**

### 62 授人以渔：思考一个小问题，Java工程师真的只会Java就可以了吗？

今天的授人以渔想让大家思考一个小问题，大家觉得对于搞Java的工程师而言，是不是真的就只要会Java编程语言就可以了呢？

相信很多人看到这个问题，一定会说，肯定不是！

如果放在10多年以前，国内互联网还不是很发达的时候，基于Java也就是开发一些普通的系统，或者是软件而已，往往用户数量不是很多，也不涉及什么真正的高并发之类的问题。

其实当时而言，搞Java真的就是主要是把Java语言的语法学好，然后会一些框架，比如Struts2、Hibernate、Spring之类的东西，最多深入研究一下Oracle数据库，研究一下Tomcat、WebLogic之类的Web服务器就可以了。

但是放在即将到来的2020年，在这个时代而言，行业对Java工程师的要求已经大大提高了，行业要求你要精通Java语言本身，还得要精通分布式架构设计、微服务架构设计、高并发架构设计、各种分布式中间件系统（比如Redis、RocketMQ之类的）、海量数据存储，等等。

所以相信大家已经感受到时代的发展对Java工程师极高的要求了！

那么说这个跟我们这个专栏有什么关系呢？

其实借着最近给大家加餐讲的两个所谓“黑科技”，想跟大家说一点建议。

大家会发现，最近讲的两个所谓黑科技，也不是什么真正的黑科技！说白了，本质上他们都是利用底层的操作系统级别的一些技术，来实现了网络通信、磁盘读写上的高度性能优化和并发优化！

所以对即将到来的2020年以及今后的几年，行业对Java工程师的期望会非常高，大家除了java本身以外，还需要精通各种上层的架构设计，另外还得精研底层操作系统内核、计算机底层这些技术，才能真正成为行业内的优秀工程师！

对于今天讨论的这个话题：**Java工程师真的只会Java就可以了吗？**大家如果有什么自己的想法可以发在评论区中跟别人一起分享自己的观点和思考。

**End**

### 63 抛砖引玉：通过本专栏的大白话讲解之后，再去深入阅读一些书籍和源码

今天给大家讲一个抛砖引玉的话题，就是在本专栏内学习了我们用大白话和一步一图的模式讲解的RocketMQ底层原理之后，大家其实可以在这个基础之上，再根据自己的时间和情况深入去对RocketMQ进行一些研究。

很多人往往在学习一个技术的时候，最大的一个痛苦就是，仅仅只能通过书籍、官网、博客了解到一个技术最基本的原理和一些demo案例的使用，但这样的话，对大家在工作中实践这个技术和出去面试的帮助是很小的！

所以我们几个朋友做狸猫技术窝的一个初衷，其实就是用一些小专栏的模式，用大白话的语言风格还有一步一图的方式，去给大家讲清楚一个一个的技术

对每个技术希望大家学完之后都能上手进行实战，对常见的技术方案、项目案例有一定的了解，对一些底层原理能有一定的了解。

这样大家无论是自己进行项目实战，还是出去面试，其实都会有很好的效果。

但我们也知道，对于少部分特别有技术探索欲望的朋友而言，他们可能往往不会满足于这个程度，还希望自己对某个技术做进一步深入的研究，这个时候我们建议大家，完全可以在理解本专栏的基础之上，可以对以下一些资料进行深入的研究：

**（1）官方文档**：官方文档的作用其实往往是参考，也就是你在实践的时候可能会想要擦和与一些参数的含义，此时可以去官网查阅

**（2）源码剖析的博客**：网上有不少某个技术领域的高手，都会系列的源码剖析博客，看起来虽然比较高深难懂，但是如果大家有了本专栏的一个原理基础之后再去看，相信能更好理解一些

**（3）源码剖析的书籍**：大家还可以去网上搜一些某个技术的源码剖析的书籍，这些书籍往往看起来也是很高深难懂的，但是同理，大家理解了我们的专栏之后，再去看这些书籍，往往会更容易一些

**（4）直接阅读源码**：在有了上述技术的积累之后，有些朋友甚至可以尝试直接把RocketMQ的源码拉取到本地，进行本地的运行和调试，自己慢慢的逐步逐步的分析他的源码

**End**

### 64 授人以渔：一个学习方法的探讨，如何深入研究一个技术？

今天的授人以渔的环节，希望跟大家一起探讨一个话题，如何学好一个新的技术？

我们这里给大家总结一个比较好的学习方法和路径：

**（1）入门和了解**：官网文档大致看一下，网上一些入门博客看一下，然后写几个demo出来，对基本原理有一定的了解，这个就是入门的水平了

**（2）熟悉**：当了解一个技术之后，接着要做的事情，务必是实践和做项目，必须由项目或者场景驱动，去思考一个技术到底应该怎么来使用

如果手头有一个项目可以用这个技术，那是最好的，如果没有的话，那么可以关注一些实战类的专栏或者课程，像狸猫技术窝的专栏，我们一般都会从案例场景驱动，去讲解技术如何实践

当你对一个技术有了一定的实践经验，或者从一些资料中吸收了一些实践经验之后，往往这个时候你会对这个技术的实践方案、部署架构、生产方案以及优化方式，有了一定的了解了，这个时候就是熟悉的水平了。

**（3）精通**：当你对一个技术有了较多的实践之后，可以选择对这个技术的源码相关的博客、书籍进行阅读，甚至直接阅读其源码，当你精通一个技术的源码之后，可以称之为对这个技术掌握到了精通

**（4）顶尖专家**：如果你对某个技术有很深的研究，在开源社区里是核心的commitor，在一个大公司里是这个技术的最高负责人，基于这个技术抗下了超高的负载，有了行业里第一手的顶尖实践经验，而且对底层有深入的研究，此时你可以称之为是这个技术在国内的顶尖专家

因此对任何一个技术，建议大家都参照这个路径去学习，不可能每个技术都达到第4个等级，但是尽量让自己对每个技术都达到第二个熟悉的水平，甚至是第三个精通的水平，当你对多个技术都达到精通水平之后，你会发现自己就是很多人仰慕的大牛。

大家对技术学习的路径和方法有什么自己的思考吗？可以在评论区提出来，跟别人一起分享和交流。

**End**

### 65 阶段性复习：一张思维导图带你梳理 RocketMQ 的底层实现原理

今天我们对本阶段进行一个复习

大家目前已经一点点学习了RocketMQ的一些底层原理，包括他的MessageQueue的概念以及在Broker上的分布式存储，以及Producer写入消息的底层原理，还有Broker的数据存储机制，以及Broker高可用架构的实现原理，包括Consumer的底层原理，以及基于Broker读写分离架构读取消息的原理。

下面给大家一个思维导图，让大家可以针对性的快速回忆和巩固知识体系：

​    ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/99641300_1578390344.cn/txdocpic/0/9ecd28d889f24e9696ee6b242b1e1a3d/0)      

**End**

### 66 阶段性复习：在深度了解RocketMQ底层原理的基础之上，多一些主动思考

今天同时希望大家在掌握MQ的底层原理之后再次回顾一下我们之前留的深度思考的作业，希望大家查漏补缺，把没有完成的思考再去反复拷问一下自己：

（1）Kafka、RabbitMQ他们有类似的数据分片机制吗？他们是如何把一个逻辑上的数据集合概念（比如一个Topic）给在物理上拆分为多个数据分片的？然后拆分后的多个数据分片又是如何在物理的多台机器上分布式存储的？

（2）为什么一定要让MQ实现数据分片的机制？如果不实现数据分片机制，让你来设计MQ中一个数据集合的分布式存储，你觉得好设计吗？

（3）同步刷盘和异步刷盘两种策略，分别适用于什么不同的场景呢？

（4）异步刷盘可以提供超高的写入吞吐量，但是有丢失数据的风险，这个适用于什么业务场景？在你所知道的业务场景，或者工作接触过的业务场景中，有哪些场景需要超高的写入吞吐量，但是可以适度接受数据丢失？

（5）同步刷盘会大幅度降低写入吞吐量，但是可以让你的数据不丢失，你接触哪些场景，是严格要求数据务必不能丢失任何一条，但是吞吐量并没有那么高的呢？

（6）Kafka、RabbitMQ他们的broker收到消息之后是如何写入磁盘的？采用的是同步刷盘还是异步刷盘的策略？为什么？

（7）每次写入都必须有超过半数的Follower Broker都写入消息才可以算做一次写入成功，那么大家思考一个问题，这样做是不是会对Leader Broker的写入性能产生影响？是不是会降低TPS？是不是必须要在所有的场景都这么做？为什么呢？

（8）一般我们获取到一批消息之后，什么时候才可以认为是处理完这批消息了？是刚拿到这批消息就算处理完吗？还是说要对这批消息执行完一大堆的数据库之类的操作，才算是处理完了？

（9）如果获取到了一批消息，还没处理完呢，结果机器就宕机了，此时会怎么样？这些消息会丢失，再也无法处理了吗？如果获取到了一批消息，已经处理完了，还买来得及提交消费进度，此时机器宕机了，会怎么样呢？

（10）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是我想这里大家肯定都有自己的答案了。

（11）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？

（12）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？

（13）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些要点需要注意的？

**End**

### 67 生产案例：从 RocketMQ 全链路分析一下为什么用户支付后没收到红包？

**1、客服反馈的一个奇怪问题：支付之后没有收到红包**

小猛在前段时间把MQ技术引入到自己的团队之后，花了一段时间去研究RocketMQ比较底层的原理，希望未来遇到一些MQ使用中的技术难题的时候，可以有足够的RocketMQ技术功底去解决。

果不其然，这一天客服给小猛的技术团队反馈了一个问题，有用户反馈说，按照规则应该是在支付之后可以拿到一个现金红包的，但是他在支付了一个订单之后，却并没有收到这个现金红包，于是就反馈给了客服。

这个问题可就奇怪了，大家都不知道问题是出在哪儿了。

经过一通排查，找了系统中打印的很多日志之后，发现了一个奇怪的现象

按理来说，订单系统在完成支付之后，会推送一条消息到RocketMQ里去，然后红包系统会从RocketMQ里接收那条消息去给用户发现金红包，我们看下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/34527500_1578390358.cn/txdocpic/0/05a93f3a8b47ec3239ede24e6dbef9e2/0)     

但是从订单系统和红包系统当天那个时间段的日志来看，居然只看到了订单系统有推送消息到RocketMQ的日志，但是并没有看到红包系统从RocketMQ中接收消息以及发现金红包的日志。

于是大家推测，问题可能就出在这儿了，是不是支付订单消息在传输的过程中丢失了？导致现金红包没有派发出去！

那么接着我们就来一步一步分析一下，对于MQ的使用过程中，到底有哪些地方会导致消息丢失？

**2、订单系统推送消息到MQ的过程会丢失消息吗？**

我们先看第一个问题，订单系统在接收到订单支付成功的通知之后，必然会去推送一条订单支付成功的消息到MQ的，那么在这个过程中，会出现丢失消息的问题吗？

其实答案是显而易的，有可能会丢失

举一个比较常见的例子，比如订单系统在推送消息到RocketMQ的过程中，是通过网络去进行传输的，但是这个时候恰巧可能网络发生了抖动，也就是网络突然有点问题，导致这次网络通信失败了。

于是这个消息必然就没有成功投递给MQ

我们看下面的图里的示意，我用红圈标志出来了订单系统投递消息到MQ可能因为网络问题导致失败的情况。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44281300_1578390358.cn/txdocpic/0/35a8f5d92be466d1b5b2a7d867dba3d8/0)    除此之外，大家觉得还有没有其他的原因可能会导致订单系统推送消息到MQ失败的？

那是相当的多了，比如MQ确实是收到消息了，但是他的网络通信模块的代码出现了异常，可能是他内部的网络通信的bug，导致消息没成功处理。

或者是你在写消息到RocketMQ的过程中，刚好遇到了某个Leader Broker自身故障，其他的Follower Broker正在尝试切换为Leader Broker，这个过程中也可能会有异常。类似的问题可能还有其他的。

所以首先我们在使用任何一个MQ的时候，无论是RocketMQ，还是RabbitMQ，或者是Kafka，大家都要明确一点：不一定你发送消息出去就一定会成功，有可能就会失败，此时你的代码里可能会抛出异常，也可能不会抛出异常，这都不好说，具体要看到底什么原因导致的消息推送失败。

**3、消息到达MQ了，MQ自己会导致消息丢失吗？**

接着我们来看下一个问题，即使我们的订单系统成功的把消息写入了MQ，此时我们就可以想当然的认为你写成功了，消息就一定不会丢失了吗？

这个也是未必的，我们来分析一下为什么

因为通过之前的RocketMQ的底层原理的分析，我们现在都明确了一点，就是你的消息写入MQ之后，其实MQ可能仅仅是把这个消息给写入到page cache里，也就是操作系统自己管理的一个缓冲区，这本质也是内存

我们看下面的图示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56017200_1578390358.cn/txdocpic/0/42be410edf9cccbea4a527dc8a9fc9ef/0)       

大家注意图里的示意，可能你认为写成功了一个消息，但是此时仅仅进入了os cache，还没写入磁盘呢。

然后这个时候，假如要是出现了Broker机器的崩溃，大家思考一下，机器一旦宕机，是不是os cache内存中的数据就没了？

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/67496600_1578390358.cn/txdocpic/0/9880ccf273d103d70d5e595478808435/0)       

看到这里，想必每个人都会倒吸一口气，还真是，要是没有研究过MQ底层原理的人，可能还真不知道数据要是进入os cache的时候，碰上机器宕机，内存里的数据必然就会丢失了，你机器即使重启了，然后重启broker进程，此时这个数据也没了。

**4、就算消息进入磁盘了，你以为真的万无一失吗？**

我们接着来看下一个问题，我们通过之前的学习也都知道一点，那就是Broker把消息写入os cache之后，其实操作系统自己在一段不太确定的时间之后，他自己是会把数据从内存刷入磁盘文件里去的

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78207300_1578390358.cn/txdocpic/0/2f61cdcf52123c86846559f4cd9cef28/0)       

好，假设现在我们写入MQ的一条消息已经稳稳进入Broker所在机器的磁盘文件里了，大家觉得这个时候数据一定不会丢失吗？

显然不能想的那么简单，因为如果你的磁盘出现故障，比如磁盘坏了，你上面存储的数据还是会丢失。

如果大家注意留意近两年的一些技术行业的不起眼的新闻，就会知道，有的互联网公司就是把数据存储在服务器的磁盘上，但是因为没有做完善的冗余备份，结果机器磁盘故障就导致那个互联网公司运营几年的核心数据都没了，找不回来了，大量的投资都打了水漂，血淋淋的教训。

所以如果消息进入了broker机器的磁盘之后，万一你实在是点儿背，赶上机器刚好磁盘坏了，可能上面的消息也就都丢失了

我们看下图，清晰的标识出来，磁盘坏了也会导致上面存储的数据丢失。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/89049400_1578390358.cn/txdocpic/0/2ec4050aa2d37ef75c3c3224174c3f63/0)       

**5、即使红包系统拿到了消息，就一定不会丢失了吗？**

好，我们接着往后看，即使红包系统这个时候顺利从MQ里拿到了一条消息，然后他就一定能安稳的把现金红包发出去吗？

这也是未必的。要解释这个问题，我们就要牵扯到消息的offset这个概念了。

之前其实我们已经给大家在底层原理分析的部分详细解释了MQ底层的存储结构，包括消息的offset的概念

说白了，offset就是代表了一个消息的标识，代表了他的位置

我们给大家举个例子，看下图，假设现在有两个消息，offset分别为1和2。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/99410400_1578390358.cn/txdocpic/0/e225c96664c1a20c7d6348c971f492a4/0)       

现在我们假设红包系统已经获取到了消息1了，然后消息1此时就在他的内存里，正准备运行代码去派发现金红包呢，但是要注意，此时还没派发现金红包

我们先看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10962500_1578390359.cn/txdocpic/0/33863fe0e9cd4e5cf6c7a437f3df7b96/0)       

我们都知道，默认情况下，MQ的消费者有可能会自动提交已经消费的offset，那么如果此时你还没处理这个消息派发红包的情况下，MQ的消费者可能直接自动给你提交这个消息1的offset到broker去了，标识为你已经成功处理了这个消息，我们看下图。



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23322900_1578390359.cn/txdocpic/0/6b872b005e624f602e4d13aada33c7a1/0)       

接着恰巧在这个时候，我们的红包系统突然重启了，或者是宕机了，或者是可能在派发红包的时候更新数据库失败了，总之就是他突然故障了，红包系统的机器重启了一下，然后此时内存里的消息1必然就丢失了，而且红包也没发出去

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36711000_1578390359.cn/txdocpic/0/72fe2e45fd78cd9e4d3bf276483082e9/0)       



**6、用户支付之后红包到底为什么没发送出去呢？**

我们学习了今天的文章就可以来解答这个问题了，为什么用户支付了，但是红包没发出去呢？

其实原因有多种可能，比如订单系统推送消息到MQ就失败了，压根儿就没推送过去；

或者是消息确实推送到MQ了，但是结果MQ自己机器故障，把消息搞丢了；

或者是红包系统拿到了消息，但是他把消息搞丢了，结果红包还没来得及发。

如果真的在生产环境里要搞明白这个问题，就必须要打更多的日志去一点点分析消息到底是在哪个环节丢失了？

如果订单系统推送了消息，结果红包系统连消息都没收到，那可能消息根本就没发到MQ去，或者MQ自己搞丢了消息。

如果红包系统收到了消息，结果红包没派发，那么就是红包系统搞丢了消息。

**7、授人以渔：一个小小的思考题**

今天给大家留的一个小小的思考题，是跟每个人的工作环境相关的，希望大家想想如下一些问题：

- 你的系统里用到了MQ吗？
- 如果用到了，有没有可能会出现消息丢失的问题？
- 你们之前是否出现过消息丢失的问题？
- 如果没有遇到过消息丢失问题，你想想你们有没有可能遇到？
- 如果遇到过，你们是怎么解决的？

这个是跟大家工作相关的问题，没有标准答案，大家踊跃在评论区交流。

**End**

### 68 发送消息零丢失方案：RocketMQ事务消息的实现流程分析

**1、解决消息丢失的第一个问题：订单系统推送消息丢失**

既然我们已经明确了消息在基于MQ传输的过程中可能丢失的几个地方，那么我们接着就得一步一步考虑如何去解决各个环节丢失消息的问题

首先要解决的第一个问题，就是订单系统推送消息到MQ的过程中，可能消息就丢失了。

之前我们也说过了，可能在订单系统推送消息到MQ的过程中，就因为常见的网络故障之类的问题，导致消息就丢失了

这里我们可以看一下下图中的示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/3479500_1578390368.cn/txdocpic/0/cd30bc30f7ebff85ae0c1386e1533249/0)       

在RocketMQ中，有一个非常强悍有力的功能，就是事务消息的功能，凭借这个事务级的消息机制，就可以让我们确保订单系统推送给出去的消息一定会成功写入MQ里，绝对不会半路就搞丢了。

今天我们就来系统的分析一下RocketMQ的事务消息机制的原理。

**2、发送half消息到MQ去，试探一下MQ是否正常**

首先作为我们的订单系统而言，假设他收到了一个订单支付成功的通知之后，他必然是需要在自己的订单数据库里做一些增删改操作的，比如更新订单状态之类的。

可能有的朋友会觉得，订单系统不就是先在自己数据库里做一些增删改操作，然后就直接发个消息到MQ去，让其他关注这个订单支付成功消息的系统去从MQ获取消息做对应的处理就可以了么？

事实上还真不是这么简单。

在基于RocketMQ的事务消息机制中，我们首先要让订单系统去发送一条half消息到MQ去，这个half消息本质就是一个订单支付成功的消息，只不过你可以理解为他这个消息的状态是half状态，这个时候红包系统是看不见这个half消息的

然后我们去等待接收这个half消息写入成功的响应通知，我们看下面的图

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13391800_1578390368.cn/txdocpic/0/06247daf28691a3f8069f774f723b89f/0)   看到这儿可能有的朋友就开始有点郁闷了，可能会觉得你没事儿先发个half消息给MQ干什么？

大家先别着急，你可以想一下，假设你二话不说让订单系统直接做了本地的数据库操作，比如订单状态都更新为了已完成，然后你再发送消息给MQ，结果报出一堆异常，发现MQ挂了。

这个时候，必然导致你没法通过消息通知到红包系统去派发红包，那用户一定会发现自己订单支付了，结果红包没收到。

所以，在这里我们首先第一件事，不是先让订单系统做一些增删改操作，而是先发一个half消息给MQ以及收到他的成功的响应，初步先跟MQ做个联系和沟通

大概这个意思就是说，确认一下MQ还活着，MQ也知道你后续可能想发送一条很关键的不希望丢失的消息给他了！

**3、万一要是half消息写入失败了呢？**

这里我们先来分析第一种情况，万一要是你订单系统写half消息给MQ就失败了呢？

可能你发现报错了，可能MQ就挂了，或者这个时候网络就是故障了，所以导致你的half消息都没发送成功，总之你现在肯定没法跟MQ通信了。

这个时候你的订单系统就应该执行一系列的回滚操作，比如对订单状态做一个更新，让状态变成“关闭交易”，同时通知支付系统自动进行退款，这才是正确的做法。

因为你订单虽然支付了，但是包括派发红包、发送优惠券之类的后续操作是无法执行的，所以此时必然应该把钱款退还给用户，说交易失败了。

这里给大家插播一个我曾经亲身经历过的事情，曾经有一次在一家便利店购物的时候，我这里都已经显示扫码支付成功了，但是店员那边说在等待他们系统确认，结果等了一会儿，系统显示后台系统有异常，交易失败了，然后过了一会儿就让支付宝自动退款给我了。

其实这就是类似的例子。

**4、half消息成功之后，订单系统完成自己的任务**

接着我们来考虑第二种情况，你的half消息写成功了，这个时候你应该干什么呢？

这个时候你的订单系统就应该在自己本地的数据库里执行一些增删改操作了，因为一旦half消息写成功了，就说明MQ肯定已经收到这条消息了，MQ还活着，而且目前你是可以跟MQ正常沟通的。

我们看下面的图，示意了下一步是订单系统执行自己的增删改操作。

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21937000_1578390368.cn/txdocpic/0/510e1f995294a612fc22c87d69421eb8/0)       

**5、如果订单系统的本地事务执行失败了怎么办？**

接着我们继续看下一种情况，万一要是订单系统更新自己的数据库失败了怎么办？

比如订单系统的数据库当时也有网络异常，或者数据库挂了，总而言之，就是你想把订单更新为“已完成”这个状态，是干不成了。

这个时候其实也很简单，直接就是让订单系统发送一个rollback请求给MQ就可以了。这个意思就是说，你可以把之前我发给你的half消息给删除掉了，因为我自己这里都出问题了，已经无力跟你继续后续的流程了。

我们看下面的图，我给出了这个示意。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37346600_1578390368.cn/txdocpic/0/1c3832a462e03db6d0bbe499e15ec896/0)   当然你发送rollback请求给MQ删除那个half消息之后，你的订单系统就必须走后续的回退流程了，就是通知支付系统退款。

当然这里可能还有一些订单系统自己的高可用降级的机制需要考虑，比如数据库无法更新了，此时你可能需要在机器本地磁盘文件里写入订单支付失败的记录。

然后你可以开一个后台线程在MySQL数据库恢复之后 ，再把订单状态更新为“已关闭”。不过这个不在我们讨专栏的范围之内。

**6、如果订单系统完成了本地事务之后，接着干什么？**

如果订单系统成功完成了本地的事务操作，比如把订单状态都更新为“已完成”了，此时你就可以发送一个commit请求给MQ，要求让MQ对之前的half消息进行commit操作，让红包系统可以看见这个订单支付成功消息

我们看下面的图。



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/47398900_1578390368.cn/txdocpic/0/f681987c09cb96109297e61f32375439/0)       

之前我们也提到过了，所谓的half消息实际就是订单支付成功的消息，只不过他的状态是half

也就是他是half状态的时候，红包系统是看不见他的，没法获取到这条消息，必须等到订单系统执行commit请求，消息被commit之后，红包系统才可以看到和获取这条消息进行后续处理。

**7、让流程严谨一些：如果发送half消息成功了，但是没收到响应呢？**

大致的事务消息的流程是讲完了，但是接着让我们来进行比较严谨的分析，如果我们把half消息发送给MQ了，MQ给保存下来了，但是MQ返回给我们的响应我们没收到呢？此时会发生什么事情？

这个时候我们没收到响应，可能就会网络超时报错，也可能直接有其他的异常错误，这个时候订单系统会误以为是发送half消息到MQ失败了，订单系统就直接会执行退款流程了，订单状态也会标记为“已关闭”。

我们看下面的图的示意。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/58503400_1578390368.cn/txdocpic/0/940973c9b0ccdaa2146319761d4257b9/0)       

但这个时候MQ已经存储下来一条half消息了，那对这个消息怎么处理？

其实RocketMQ这里有一个补偿流程，他会去扫描自己处于half状态的消息，如果我们一直没有对这个消息执行commit/rollback操作，超过了一定的时间，他就会回调你的订单系统的一个接口

他会问问你：这个消息到底怎么回事？你到底是打算commit这个消息还是要rollback这个消息？

我们看下图示意      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66333600_1578390368.cn/txdocpic/0/cf3750226d1bc4265d9bf99a2946a37a/0)   这个时候我们的订单系统就得去查一下数据库，看看这个订单当前的状态，一下发现订单状态是“已关闭”，此时就知道，你必然得发送rollback请求给MQ去删除之前那个half消息了！

我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75629300_1578390368.cn/txdocpic/0/83dded8396d61b61472c4fa572a80193/0)       

**8、如果rollback或者commit发送失败了呢？**

我们再假设一种场景，如果订单系统是收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了rollback或者commit请求，发送给MQ了，结果因为网络故障，导致rollback或者commit请求发送失败了呢？

这个时候其实也很简单，因为MQ里的消息一直是half状态，所以说他过了一定的超时时间会发现这个half消息有问题，他会回调你的订单系统的接口

你此时要判断一下，这个订单的状态如果更新为了“已完成”，那你就得再次执行commit请求，反之则再次执行rollback请求。

本质这个MQ的回调就是一个补偿机制，如果你的half消息响应没收到，或者rollback、commit请求没发送成功，他都会来找你问问对half消息后续如何处理。

再假设一种场景，如果订单系统收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了rollback或者commit请求，发送给MQ了。很不巧，mq在这个时候挂掉了，导致rollback或者commit请求发送失败，怎么办？

如果是这种情况的话，那就等mq自己重启了，重启之后他会扫描half消息，然后还是通过上面说到的补偿机制，去回调你的接口

**9、停一下脚步，想想上面这个流程的意义在哪里？**

看到这里我们来停下脚步想想，上面这个流程的意义在哪里呢？

其实很简单，如果你的MQ有问题或者网络有问题，half消息根本都发不出去，此时half消息肯定是失败的，那么订单系统就不会执行后续流程了！

如果要是half消息发送出去了，但是half消息的响应都没收到，然后执行了退款流程，那MQ会有补偿机制来回调找你询问要commit还是rollback，此时你选择rollback删除消息就可以了，不会执行后续流程！

如果要是订单系统收到half消息了，结果订单系统自己更新数据库失败了，那么他也会进行回滚，不会执行后续流程了！

如果要是订单系统收到half消息了，然后还更新自己数据库成功了，订单状态是“已完成”了，此时就必然会发送commit请求给MQ，一旦消息commit了，那么必然保证红包系统可以收到这个消息！

而且即使你commit请求发送失败了，MQ也会有补偿机制，回调你接口让你判断是否重新发送commit请求

总之，就是你的订单系统只要成功了，那么必然要保证MQ里的消息是commit了可以让红包系统看到他！

所以大家可以结合我们的图思考一下上述流程，通过这套事务消息的机制，是不是就可以保证我们的订单系统一旦成功执行了数据库操作，就一定会通知到红包系统去派发红包？至少订单系统到MQ之间的消息传输是不会有丢失的问题了！

**End**

### 69 RocketMQ黑科技解密：事务消息机制的底层实现原理

**1、half 消息是如何对消费者不可见的？**

我们之前已经说过了RocketMQ事务消息的全流程，在这个流程中，第一步就是要由订单系统去发送一个half消息给MQ

大家看下面的图，应该还记得这个东西。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29083700_1578390374.cn/txdocpic/0/b238b17f40fdb46b5fbefd9ce81661a6/0)     

然后当时我们给大家说过，对于这个half消息，红包系统这个时候是看不到他的，没法消费这条消息去处理，那这个half消息是如何做到不给人家红包系统看到的呢？

其实RocketMQ底层采取了一个巧妙的设计。

咱们先举个例子，订单系统发送了一个half状态的订单支付消息到“OrderPaySuccessTopic”里去，这是一个Topic

然后呢，红包系统也是订阅了这个“OrderPaySuccessTopic”从里面获取消息的，我们看下图示意。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40534800_1578390374.cn/txdocpic/0/ea7efedf32b445a322ff4cee06bd7050/0)       

当然我们从之前的底层原理剖析的环节都知道，其实你写入一个Topic，最终是定位到这个Topic的某个MessageQueue，然后定位到一台Broker机器上去，然后写入的是Broker上的CommitLog文件，同时将消费索引写入MessageQueue对应的ConsumeQueue文件

这个如果大家遗忘了，回头去看一下，我们看下图示意。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52997100_1578390374.cn/txdocpic/0/bead89a405ea5637b8dd9562865d1b1c/0)       

所以通过上面的图我们知道，如果你写入一条half消息到OrderPaySuccessTopic里去，会定位到这个Topic的一个MessageQueue，然后定位到上图RocketMQ的一台机器上去，接着按理说，消息会写入CommitLog。

同时消息的offset会写入MessageQueue对应的ConsumeQueue，这个ConsumeQueue是属于OrderPaySuccuessTopic的，然后红包系统按理说会从这个ConsumeQueue里获取到你写入的这个half消息。

但是实际上红包系统却没法看到这条消息，其本质原因就是RocketMQ一旦发现你发送的是一个half消息，他不会把这个half消息的offset写入OrderPaySuccessTopic的ConsumeQueue里去。

他会把这条half消息写入到自己内部的“RMQ_SYS_TRANS_HALF_TOPIC”这个Topic对应的一个ConsumeQueue里去

我们看下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61891200_1578390374.cn/txdocpic/0/1be92dbd2d8b189b4c161d86a0197a5c/0)       

真相大白了，所以对于事务消息机制之下的half消息，RocketMQ是写入内部Topic的ConsumeQueue的，不是写入你指定的OrderPaySuccessTopic的ConsumeQueue的

所以你的红包系统自然无法从OrderPaySuccessTopic的ConsumeQueue中看到这条half消息了

**2、在什么情况下订单系统会收到half消息成功的响应？**

下一个问题来了，那么在什么情况下订单系统会收到half消息成功的响应呢？

简单来说，结合上面的内容，大家就可以清晰判断出，必须要half消息进入到RocketMQ内部的RMQ_SYS_TRANS_HALF_TOPIC的ConsumeQueue文件了，此时就会认为half消息写入成功了，然后就会返回响应给订单系统。

所以这个时候，一旦你的订单系统收到这个half消息写入成功的响应，必然就知道这个half消息已经在RocketMQ内部了。

**3、假如因为各种问题，没有执行rollback或者commit会怎么样？**

下一个问题，假如因为网络故障，订单系统没收到half消息的响应，或者说自己发送的rollback/commit请求失败了，那么RocketMQ会干什么？

其实这个时候他会在后台有定时任务，定时任务会去扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息，如果你超过一定时间还是half消息，他会回调订单系统的接口，让你判断这个half消息是要rollback还是commit

如下图      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/73116900_1578390374.cn/txdocpic/0/5545cac7fe05610ce319dd75f1ab47e8/0)       

**4、如果执行rollback操作的话，如何标记消息回滚？**

假设我们的订单系统执行了rollback请求，那么此时就需要对消息进行回滚。

之前我们说，RocketMQ会把这个half消息给删除，但是大家觉得删除消息是真的会在磁盘文件里删除吗？

显示不是的

因为RocketMQ都是顺序把消息写入磁盘文件的，所以在这里如果你执行rollback，他的本质就是用一个OP操作来标记half消息的状态

RocketMQ内部有一个OP_TOPIC，此时可以写一条rollback OP记录到这个Topic里，标记某个half消息是rollback了，如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86019500_1578390374.cn/txdocpic/0/342e0cb8f8e28d35a83582c58271e1e1/0)       

另外给大家说一下，假设你一直没有执行commit/rollback，RocketMQ会回调订单系统的接口去判断half消息的状态，但是他最多就是回调15次，如果15次之后你都没法告知他half消息的状态，就自动把消息标记为rollback。

**5、如果执行commit操作，如何让消息对红包系统可见？**

最后一个问题，如果订单系统执行了commit操作，如何让消息对红包系统可见呢？

其实也很简单，你执行commit操作之后，RocketMQ就会在OP_TOPIC里写入一条记录，标记half消息已经是commit状态了。

接着需要把放在RMQ_SYS_TRANS_HALF_TOPIC中的half消息给写入到OrderPaySuccessTopic的ConsumeQueue里去，然后我们的红包系统可以就可以看到这条消息进行消费了，如下图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/1567600_1578390375.cn/txdocpic/0/4aab1876230902f8c3c4cd6079a183aa/0)      

**6、之前研究RocketMQ底层原理的意义**

看到这里，大家应该对事务消息机制的底层原理比较了解了，其实他的本质都是基于CommitLog、ConsumeQueue这套存储机制来做的，只不过中间有一些Topic的变换，half消息可能就是写入内部Topic的。

所以通过这里的学习，大家也会逐渐明白，在研究一些中间件技术的高阶功能之前，最好是先对他的底层原理有一个学习，这样才能更好的理解

**End**

### 70 为什么解决发送消息零丢失方案，一定要使用事务消息方案？

**1、停下脚步，做一点深入思考**

我们已经把RocketMQ事务消息的实现流程和底层原理都进行了研究了，接着我们需要思考一个问题:

我们最早给大家说在发送消息的时候，可能存在消息的丢失，也就是说，可能消息根本就没有进入到MQ就丢了，我们看下面的图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55052700_1578390381.cn/txdocpic/0/3a28af953bf8c514793c6d7578a2a41e/0)       

然后我们没解释过多的东西直接就切入了RocketMQ事务消息的讲解，其实通过RocketMQ事务消息机制的研究，我相信每个人现在都可以确信一点，如果你使用事务消息机制去发送消息到MQ，一定是可以保证消息必然发送到MQ的，不会丢！

但是我们现在回过头来想想事务消息机制的原理图，大家可以看到下面这个图

这个图的流程可真是复杂啊，先得发送half消息，完了你还得发送rollback or commit的请求，要是中间有点什么问题，MQ还得回调你的接口

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/74008800_1578390381.cn/txdocpic/0/2be12c732620e9231c934778224e6850/0)       

我们真的有必要使用这么复杂的机制去确保消息到达MQ，而且绝对不会丢吗？

毕竟这么复杂的机制完全有可能导致整体性能比较差，而且吞吐量比较低，是否有更加简单的方法来确保消息一定可以到达MQ呢？

**2、一个小思考：能不能基于重试机制来确保消息到达MQ？**

想到这里，我觉得很多朋友可能内心已经闪现出一个想法了，就是我们之前觉得发消息到MQ，无非就是觉得可能半路上消息给搞丢了，然后消息根本没进去MQ内部，我们也没做什么额外的措施，这就导致消息找不回来了。

那么我们先搞清楚一个问题，我们发送消息到MQ，然后我们可以等待MQ返回响应给我们，在什么样的情况下，MQ会返回响应给我们呢？

答案显而易见，就是MQ收到消息之后写入本地磁盘文件了，当然这个时候可能仅仅是写入os cache而已，但是只要他写入自己本地存储了，就会返回响应给我们。

我们看下面的图里的示意      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91857600_1578390381.cn/txdocpic/0/4ea9697c53a47610ae99995e6b304fff/0)       

那么只要我们在代码中发送消息到MQ之后，同步等待MQ返回响应给我们，一直等待，如果半路中有网络异常或者MQ内部异常，我们肯定会收到一个异常，比如网络错误，或者请求超时之类的。

如果我们在收到异常之后，就认为消息到MQ发送失败了，然后再次重试尝试发送消息到MQ，接着再次同步等待MQ返回响应给我们，这样反复重试，是否可以确保消息一定会到达MQ？

我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/625600_1578390382.cn/txdocpic/0/fbf78c19304196ddbc74fa8a92a37f62/0)       

理论上似乎在一些短暂网络异常的场景下，我们是可以通过不停的重试去保证消息到达MQ的，因为如果短时间网络异常了消息一直没法发送，我们只要不停的重试，网络一旦恢复了，消息就可以发送到MQ了。

如果要是反复重试多次发现一直没法把消息投递到MQ，此时我们就可以直接让订单系统回滚之前的流程，比如发起退款流程，判定本次订单支付交易失败了。

看起来这个简单的同步发送消息 + 反复重试的方案，也可以做到保证消息一定可以投递到MQ中，大家想想是不是？

确实如此，而且在基于Kafka作为消息中间件的消息零丢失方案中，对于发送消息这块，因为Kafka本身不具备RocketMQ这种事务消息的高级功能，所以一般我们都是对Kafka会采用同步发消息 + 反复重试多次的方案，去保证消息成功投递到Kafka的。

但是如果是在类似我们目前这个较为复杂的订单业务场景中，仅仅采用同步发消息 + 反复重试多次的方案去确保消息绝对投递到MQ中，似乎还是不够的，接下来我们分析一下在复杂业务场景下，这里有什么问题。

**3、先执行订单本地事务，还是先发消息到MQ？**

要分析这里的问题，我们就需要考虑一个事情：我们应该是先执行订单本地事务，还是先发消息到MQ去？

如果我们先执行订单本地事务，接着再发送消息到MQ的话，看起来伪代码可能是这样的：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10183600_1578390382.png)

上面那段伪代码看着似乎天衣无缝，先执行订单本地事务，接着发送消息到MQ，如果订单本地事务执行失败了，则不会继续发送消息到MQ了；

如果订单事务执行成功了，发送MQ失败了，自动进行几次重试，重试如果一直失败，就回滚订单事务。

但是这里有一个问题，假设你刚执行完成了订单本地事务了，结果还没等到你发送消息到MQ，结果你的订单系统突然崩溃了！

这就导致你的订单状态可能已经修改为了“已完成”，但是消息却没发送到MQ去！这就是这个方案最大的隐患。

我们看下面的图      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/25808300_1578390382.cn/txdocpic/0/cf0e94288daea4dd00a3d7166b193536/0)       

如果出现这种场景，那你的多次重试发送MQ之类的代码根本没机会执行！而且订单本地事务还已经执行成功了，你的消息没发送出去，红包系统没机会派发红包，必然导致用户支付成功了，结果看不到自己的红包！

**4、把订单本地事务和重试发送MQ消息放到一个事务代码中**

我们接着来考虑下一个问题，这个时候有人会想到一个新的想法，如果把订单本地事务代码和发送MQ消息的代码放到一个事务代码中呢？

我们看下面的伪代码示例：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/34831100_1578390382.png)



上面这个代码看起来似乎解决了我们的问题，就是在这个方法上加入事务，在这个事务方法中，我们哪怕执行了orderService.finishOrderPay()，但是其实也仅仅执行了一些增删改SQL语句，还没提交订单本地事务。

如果发送MQ消息失败了，而且多次重试还不奏效，则我们抛出异常会自动回滚订单本地事务；

如果你刚执行了orderService.finishOrderPay()，结果订单系统直接崩溃了，此时订单本地事务会回滚，因为根本没提交过。

但是对于这个方案，还是非常的不理想，原因就出在那个MQ多次重试的地方

假设用户支付成功了，然后支付系统回调通知你的订单系统说，有一笔订单已经支付成功了，这个时候你的订单系统卡在多次重试MQ的代码那里，可能耗时了好几秒种，此时回调通知你的系统早就等不及可能都超时异常了。

而且你把重试MQ的代码放在这个逻辑里，可能会导致订单系统的这个接口性能很差

我们看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43873600_1578390382.cn/txdocpic/0/843d822bda470d16f34b1046ae8fbf4b/0)       

**5、再说了，你就一定可以依靠本地事务回滚吗？**

除了我们上面说的那个问题之外，我可能还不得不给很多寄希望于订单事务和发送MQ消息包裹在一个事务代码中的朋友，泼一盆冷水，大家觉得我们一定可以依靠本地事务回滚吗？

这还真的未必，我们看下面的代码：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/53456100_1578390382.png)



大家着重在里面看，我们虽然在方法上加了事务注解，但是代码里还有更新Redis缓存和Elasticsearch数据的代码逻辑，如果你要是已经完成了订单数据库更新、Redis缓存更新、ES数据更新了，结果没法送MQ呢订单系统崩溃了。

虽然订单数据库的操作会回滚，但是Redis、Elasticsearch中的数据更新会自动回滚吗？

不会的，因为他们根本没法自动回滚，此时数据还是会不一致的。所以说，完全寄希望于本地事务自动回滚是不现实的。

**6、保证业务系统一致性的最佳方案：基于RocketMQ的事务消息机制**

所以分析完了这个同步发送消息 + 反复多次重试的方案之后，我们会发现他实际落地的时候是可以的，但是里面存在一些问题

比如可能会让订单事务执行成功，结果消息没发送出去，或者是订单事务执行成功了，但是反复多次重试发送消息到MQ极为耗时，导致调用你接口的人频繁超时异常。

所以真正要保证消息一定投递到MQ，同时保证业务系统之间的数据完全一致，业内最佳的方案还是用基于RocketMQ的事务消息机制。

因为这个方案落地之后，他可以保证你的订单系统的本地事务一旦成功，那么必然会投递消息到MQ去，通知红包系统去派发红包，保证业务系统的数据是一致的。而且整个流程中，你没必要进行长时间的阻塞和重试。

如果half消息发送就失败了，你就直接回滚整个流程。如果half消息发送成功了，后续的rollback或者commit发送失败了，你不需要自己去卡在那里反复重试，你直接让代码结束即可，因为后续MQ会过来回调你的接口让你判断再次rollback or commit的。

希望大家在学习一个技术的时候，反复给自己多问几个为什么，从各个角度去思考。

**End**

### 71 用支付后发红包的案例场景，分析RocketMQ事物消息的代码实现细节

**1、对RocketMQ的事务消息分析代码落地实现**

今天我们来分析一下RocketMQ的事务消息的代码落地实现

既然已经对事务消息的流程、原理以及使用场景都进行了分析，那下一步就是真正看看如何基于RocketMQ提供的Java API进行编码实现了。

我们将会直接基于官方文档提供的事务消息API使用的例子来给大家进行分析，同时我们会把订单系统的业务场景放在里面，加入一些伪代码让大家来参考一下。

对代码的分析我们全部基于注释写在代码里了，大家通过看注释就完全可以理解代码的使用。

**2、发送half事务消息出去**

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98433300_1578390388.png)

**3、假如half消息发送失败，或者没收到half消息响应怎么办？**

我们已经看到如何发送half消息了，但是假如发送half消息失败了怎么办呢？

此时我们其实会在执行“producer.sendMessageInTransaction(msg, null)”的时候，收到一个异常，发现消息发送失败了。

所以我们可以用下面的代码去关注half消息发送失败的问题：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/7616500_1578390389.png)

那如果一直没有收到half消息发送成功的通知呢？

针对这个问题，我们可以把发送出去的half消息放在内存里，或者写入本地磁盘文件，后台开启一个线程去检查，如果一个half消息超过比如10分钟都没有收到响应，那就自动触发回滚逻辑。

**4、如果half消息成功了，如何执行订单本地事务？**

刚才代码里有一个TransactionListener，这个类也是我们自己定义的，如下所示：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/16933600_1578390389.png)



**5、如果没有返回commit或者rollback，如何进行回调？**



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/24960600_1578390389.png)



**6、给大家留下一个小作业**

大家在看完今天对RocketMQ的事务消息代码的解释之后，建议大家可以去基于之前自己部署好的RocketMQ，去实验一下这个事务消息的代码

然后请大家自己反复思考一下，在这段代码运行的过程中，各个地方如果出现网络异常，或者是系统突然崩溃了，这套机制是如何确保消息投递稳定运行的。

**End**

### 72 Broker消息零丢失方案：同步刷盘 + Raft协议主从同步

**1、用了事务消息机制，消息就一定不会丢了吗？**

之前我们花了很多的篇幅去讲RocketMQ的事务消息机制，原因无他，就是这个机制是RocketMQ非常核心以及重要的一个功能，可以让我们实现生产消息环节的消息不丢失，而且最重要的是，他可以保证两个业务系统的数据一致性。

但是现在我们接着来思考，如果我们在生产消息的时候用了事务消息之后，就真的可以保证数据就不会丢失了吗？

那还真是未必

假设咱们现在订单系统已经通过事务消息的机制，通过half消息 + commit的方式，把消息在MQ里提交了

也就是说，现在对于MQ而言，那条消息已经进入到他的存储层了，可以被红包系统看到了

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83537200_1578390395.cn/txdocpic/0/8e95d8c9bb81adf923b20b55e195d076/0)       

但是我们先稍微等一下，你的这条消息在commit之后，会从half topic里进入OrderPaySuccessTopic中，但是此时仅仅是消息进入了这个你预定的Topic而已，仅仅是可以被红包系统看到而已，此时可能你的红包系统还没来得及去获取这条消息。

然后恰巧在此时，你的这条消息又仅仅停留在os cache中，还没进入到ConsumeQueue磁盘文件里去，然后此时这台机器突然宕机了，os cache中的数据全部丢失，此时必然会导致你的消息丢失，红包系统再没机会读到这条消息了。

我们看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98867000_1578390395.cn/txdocpic/0/a29ed3050b91b7ced11b9a2e37c94014/0)       

**2、就算你走运，消息进了磁盘就不会丢了吗？**

那我们接着看，就算我们很走运，比如你的消息已经进入了OrderPaySuccessTopic的ConsumeQueue磁盘文件了，不是停留在os cache里了，此时消息就一定不会丢失了吗？

这也未必

即使消息已经进入磁盘文件了，但是这个时候红包系统还没来得及消费这条消息，然后此时这台机器的磁盘突然就坏了，就会一样导致消息丢失，而且可能消息再也找不回来了，同样会丢失数据。

我们看下图的示意。

![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/19814000_1578390396.cn/txdocpic/0/849528f47bef45eda652d5990ef512ed/0)      

**3、明确一个前提：保证消息写入MQ不代表不丢失**

所以看到这里，我们需要明确一个前提，我们无论是通过比较简单的同步发送消息 + 反复多次重试的方案，还是事务消息的方案，哪怕我们确保消息已经写入MQ成功了，此时也未必消息就不会丢失了。

因为即使你写入MQ成功了，这条消息也大概率是仅仅停留在MQ机器的os cache中，一旦机器宕机内存里的数据都会丢失，或者哪怕消息已经进入了MQ机器的磁盘文件里，但是磁盘一旦坏了，消息也会丢失。

如果消息丢失了，你的红包系统还没来得及消费，那么他就永远没机会消费和派发红包了，所以对于你而言，如果你仅仅只是使用MQ的话，可能不清楚MQ集群内部发生过的一些机器故障，也就不清楚数据丢失的具体原因了。

**4、异步刷盘 vs 同步刷盘**

说到这里，我们终于可以进入正题了，到底怎么去确保消息写入MQ之后，MQ自己不要随便丢失数据呢？

解决这个问题的第一个关键点，就是将异步刷盘调整为同步刷盘。

所谓的异步刷盘，就是之前我们一直说的那种模式。

也就是说，你的消息即使成功写入了MQ，他也就在机器的os cache中，没有进入磁盘里，要过一会儿等操作系统自己把os cache里的数据实际刷入磁盘文件中去

我们看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37492300_1578390396.cn/txdocpic/0/b58c2939bd5f761243edc5459e985956/0)       

所以在异步刷盘的模式下，我们的写入消息的吞吐量肯定是极高的，毕竟消息只要进入os cache这个内存就可以了，写消息的性能就是写内存的性能，那每秒钟可以写入的消息数量肯定更多了，但是这个情况下，可能就会导致数据的丢失。

所以如果一定要确保数据零丢失的话，可以调整MQ的刷盘策略，我们需要调整broker的配置文件，将其中的flushDiskType配置设置为：SYNC_FLUSH，默认他的值是ASYNC_FLUSH，即默认是异步刷盘的。

如果调整为同步刷盘之后，我们写入MQ的每条消息，只要MQ告诉我们写入成功了，那么他们就是已经进入了磁盘文件了！

比如我们发送half消息的时候，只要MQ返回响应是half消息发送成功了，那么就说明消息已经进入磁盘文件了，不会停留在os cache里。

我们看下图的示意，如果我们使用同步刷盘的策略，那么可以确保写入MQ的消息一定是已经进入磁盘文件了

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51811900_1578390396.cn/txdocpic/0/9ee6605daf692d4c46c7c3e11a747ca4/0)       

**5、如何通过主从架构模式避免磁盘故障导致的数据丢失？**

接着我们解决下一个问题，如何避免磁盘故障导致的数据丢失？

其实道理也很简单，我们必须要对Broker使用主从架构的模式

也就是说，必须让一个Master Broker有一个Slave Broker去同步他的数据，而且你一条消息写入成功，必须是让Slave Broker也写入成功，保证数据有多个副本的冗余。

我们看下图的一个示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/72625700_1578390396.cn/txdocpic/0/7c53d596643a9d4e540a28bb860f496f/0)       

这样一来，你一条消息但凡写入成功了，此时主从两个Broker上都有这条数据了，此时如果你的Master Broker的磁盘坏了，但是Slave Broker上至少还是有数据的，数据是不会因为磁盘故障而丢失的。

对于主从同步的架构，我们本来就是讲解了基于DLedger技术和Raft协议的主从同步架构，你如果采用了这套架构，对于你所有的消息写入，只要他写入成功，那就一定会通过Raft协议同步给其他的Broker机器，这里的原理我们之前都已经讲解过了，大家有遗忘的回去看看即可。

**6、MQ确保数据零丢失的方案总结**

所以通过今天的分析，我们知道了，只要你把Broker的刷盘策略调整为同步刷盘，那么绝对不会因为机器宕机而丢失数据；

只要你采用了主从架构的Broker集群，那么一条消息写入成功，就意味着多个Broker机器都写入了，此时任何一台机器的磁盘故障，数据也是不会丢失的。

最起码只要Broker层面保证写入的数据不丢失，那就一定可以让红包系统消费到这条消息了！

**End**

### 73 Consumer消息零丢失方案：手动提交offset + 自动故障转移

**1、红包系统拿到了消息就一定会派发红包吗？**

通过前面的学习，我们现在已经知道了如何确保订单系统发送出去的消息一定会到达MQ中，而且也能确保了如果消息到达了MQ如何确保一定不会丢失

我们看下面的图示意了这个场景

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/2247700_1578390402.cn/txdocpic/0/20f39086b2cc18ec35c51bf86b7faf32/0)       

只要我们能做到这一点，那么我们必然可以保证红包系统可以获取到一条订单支付成功的消息，然后一定可以去尝试把红包派发出去。

但是现在的问题在于，即使红包系统拿到了这条消息，就一定可以成功的派发红包吗？

**答案是未必**

我们之前也给大家分析过这个问题，如果红包系统已经拿到了这条消息，但是消息目前还在他的内存里，还没执行派发红包的逻辑，此时他就直接提交了这条消息的offset到broker去说自己已经处理过了，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13607500_1578390402.cn/txdocpic/0/225cd423060d8794e1e8b445aceb00c0/0)       

接着红包系统在上图这个状态的时候就直接崩溃了，内存里的消息就没了，红包也没派发出去，结果Broker已经收到他提交的消息offset了，还以为他已经处理完这条消息了。

等红包系统重启的时候，就不会再次消费这条消息了。

关于这个问题，我们之前通过画图的方式，已经清晰的展示了消息offset错误提交之后，导致红包系统可能无法再次获取到这条消息的问题。

所以我们在这里，首先要明确一点，那就是即使你保证发送消息到MQ的时候绝对不会丢失，而且MQ收到消息之后一定不会把消息搞丢失，但是你的红包系统在获取到消息之后还是可能会搞丢。

**2、Kafka消费者的数据丢失问题**

虽然我们这个专栏主要是依托RocketMQ来讲解消息中间件技术的原理、生产架构以及技术方案的，但是其实我们这里涉及到的各种技术思想，包括MQ数据丢失问题以及解决方案，在Kafka、RabbitMQ等其他中间件里也是完全适用的。

所以对我们目前讲解的这个消费者数据丢失的问题，其实完全可以套用到Kafka中去，因为Kafka的消费者采用的消费的方式跟RocketMQ是有些不一样的，如果按照Kafka的消费模式，就是会产生数据丢失的风险。

也就是说Kafka消费者可能会出现上图说的，拿到一批消息，还没来得及处理呢，结果就提交offset到broker去了，完了消费者系统就挂掉了，这批消息就再也没机会处理了，因为他重启之后已经不会再次获取提交过offset的消息了。

**3、RocketMQ消费者的与众不同的地方**

但是对于RocketMQ的消费者而言，他是有一些与众不同的地方的，至少跟Kafka的消费者是有较大不同的

我们再来回顾一下之前我们写过的RocketMQ消费者的代码，如下所示。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26134900_1578390402.png)



我们再回顾一下这里的具体的一小块代码：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36297700_1578390402.png)



大家会发现，RocketMQ的消费者中会注册一个监听器，就是上面小块代码中的MessageListenerConcurrently这个东西，当你的消费者获取到一批消息之后，就会回调你的这个监听器函数，让你来处理这一批消息。

然后当你处理完毕之后，你才会返ConsumeConcurrentlyStatus.CONSUME_SUCCESS作为消费成功的示意，告诉RocketMQ，这批消息我已经处理完毕了。

所以对于RocketMQ而言，其实只要你的红包系统是在这个监听器的函数中先处理一批消息，基于这批消息都派发完了红包，然后返回了那个消费成功的状态，接着才会去提交这批消息的offset到broker去。

所以在这个情况下，如果你对一批消息都处理完毕了，然后再提交消息的offset给broker，接着红包系统崩溃了，此时是不会丢失消息的

我们看下图的示意      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/50993500_1578390402.cn/txdocpic/0/a7ddf302a28eada7b9df0971e825559b/0)       

那么如果是红包系统获取到一批消息之后，还没处理完，也就没返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS这个状态呢，自然没提交这批消息的offset给broker呢，此时红包系统突然挂了，会怎么样？

我们看下图示意的这个场景。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/63936000_1578390402.cn/txdocpic/0/39c5ab205adf4f71ba5dca4536370196/0)       

其实在这种情况下，你对一批消息都没提交他的offset给broker的话，broker不会认为你已经处理完了这批消息，此时你突然红包系统的一台机器宕机了，他其实会感知到你的红包系统的一台机器作为一个Consumer挂了。

接着他会把你没处理完的那批消息交给红包系统的其他机器去进行处理，所以在这种情况下，消息也绝对是不会丢失的

我们看下图的示意      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78068100_1578390402.cn/txdocpic/0/781acfaf5d4ec80b0099a9ca5abbfd42/0)       

**4、需要警惕的地方：不能异步消费消息**

所以大家也看到了，在默认的Consumer的消费模式之下，必须是你处理完一批消息了，才会返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS这个状态标识消息都处理结束了，去提交offset到broker去。

在这种情况下，正常来说是不会丢失消息的，即使你一个Consumer宕机了，他会把你没处理完的消息交给其他Consumer去处理。

但是这里我们要警惕一点，就是**我们不能在代码中对消息进行异步的处理**，如下错误的示范，我们开启了一个子线程去处理这批消息，然后启动线程之后，就直接返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS状态了



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/90636400_1578390402.png)



如果要是用这种方式来处理消息的话，那可能就会出现你开启的子线程还没处理完消息呢，你已经返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS状态了，就可能提交这批消息的offset给broker了，认为已经处理结束了。

然后此时你红包系统突然宕机，必然会导致你的消息丢失了！

因此在RocketMQ的场景下，我们如果要保证消费数据的时候别丢消息，你就老老实实的在回调函数里处理消息，处理完了你再返回ConsumeConcurrentlyStatus.CONSUME_SUCCESS状态表明你处理完毕了！

**End**

### 74 基于 RocketMQ 设计的全链路消息零丢失方案总结

**1、对全链路消息零丢失方案进行总结**

基于我们之前讲解的内容，我们现在可以对全链路的消息零丢失方案进行一个总结了：

1. **发送消息到MQ的零丢失**：

2. 1. 方案一（同步发送消息 + 反复多次重试）
   2. 方案二（事务消息机制），两者都有保证消息发送零丢失的效果，但是经过分析，事务消息方案整体会更好一些

3. **MQ收到消息之后的零丢失**：开启同步刷盘策略 + 主从架构同步机制，只要让一个Broker收到消息之后同步写入磁盘，同时同步复制给其他Broker，然后再返回响应给生产者说写入成功，此时就可以保证MQ自己不会弄丢消息

4. **消费消息的零丢失**：采用RocketMQ的消费者天然就可以保证你处理完消息之后，才会提交消息的offset到broker去，只要记住别采用多线程异步处理消息的方式即可

如果大家想要保证在一个消息基于MQ流转的时候绝对不会无缘无故的丢失，那么可以采取上述一整套的方案，包括落地的代码演示都已经给大家看到了。

但是今天我们除了总结这个方案之外，我们还需要对消息零丢失方案进行一些优劣分析。

**2、消息零丢失方案的优势与劣势**

如果在系统中落地一套消息零丢失方案，不管是哪个系统，不管是哪个场景，都可以确保消息流转的过程中不会丢失，看起来似乎很有吸引力，这也是消息零丢失方案的优势所在，可以让系统的数据都是正确的，不会有丢失的。

但是他的劣势在哪里呢？

显而易见的是，你用了这套方案之后，会让你整个从头到尾的消息流转链路的性能大幅度下降，让你的MQ的吞吐量大幅度的下降

比如本身你的系统和MQ配合起来，每秒可以处理几万条消息的，结果当你落地消息零丢失方案之后，可能每秒只能处理几千条消息了。

为什么会这样呢？我们接下来一步一步分析一下。

**3、为什么消息零丢失方案会导致吞吐量大幅度下降？**

我们先来看这个发送消息到MQ的环节，如果我们仅仅只是简单的把消息发送到MQ，那么不过就是一次普通的网络请求罢了，我们就是发送请求到MQ然后接收响应回来，这个性能自然很高，吞吐量也是很高的

我们看下面的图示

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/62383100_1578367146.cn/txdocpic/0/745751ae6a8fba7c55810960dd5d5e39/0)       

但是如果你改成了基于事务消息的机制之后呢？

那么此时这里的实现原理图如下所示，这里涉及到half消息、commit or rollback、写入内部topic、回调机制，等诸多复杂的环节

不说别的，光是你成功发送一条消息，都至少要half + commit两次请求。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5043100_1578367147.cn/txdocpic/0/b8bb5f0c920ca47cd2487db00b611ea5/0)       

所以当你一旦上了如此复杂的方案之后，势必会导致你的发送消息的性能大幅度下降，同时发送消息到MQ的吞吐量大幅度下降。

接着我们再看MQ收到消息之后的行为，在MQ收到消息之后，一样会让性能大幅度下降。

首先MQ的一台broker机器收到了消息之后，必然直接把消息刷入磁盘里，这个性能就远远低于你写入os cache了，完全不是一个数量级的，比如你写入os cache相当于是内存，可能仅仅需要0.1ms，但是你写入磁盘文件可能就需要10ms！如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44628300_1578367147.cn/txdocpic/0/6ffd5e9a5fc7a880d677980bbe6640c2/0)       

接着你的这台broker机器还必须直接把消息复制给其他的broker，完成多副本的冗余，这个过程涉及到两台broker机器之间的网络通信，另外一台broker机器写数据到自己本地磁盘去，同样会比较慢，如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83436800_1578367147.cn/txdocpic/0/5b97cbb091360c56f6b26d4929ebf260/0)       

在broker完成了上述两个步骤之后，接着才能返回响应告诉你说这次消息写入已经成功了，大家试想一下，写入一条消息需要强制同步刷磁盘，而且还需要同步复制消息给其他的broker机器

这两个步骤一加入，可能原本10ms的事儿就会变成100ms了！所以这里也势必会导致性能大幅度下降，MQ的broker的吞吐量会大幅度下降。

最后看你的消费者，当你的消费者拿到消息之后，比如他直接开启一个子线程去处理这批消息，然后他就直接返回CONSUME_SUCCESS状态了，接着他就可以去处理下一批消息了！如果这样的话，你消费消息的速度会很快，吞吐量会很高！

但是如果为了保证数据不丢失，你必须是处理完一批消息再返回CONSUME_SUCCESS状态，那么此时你消费者处理消息的速度会降低，吞吐量 自然也会下降了！

我们看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/25668000_1578367148.cn/txdocpic/0/019404717b09cc71b19a9c8f20723631/0)       

**4、消息零丢失方案到底适合什么场景？**

所以简单一句话，如果你一定要上消息零丢失方案，那么必然导致从头到尾的性能下降以及MQ的吞吐量下降。

所以一般大家不要轻易在随便一个业务里就上如此重的一套方案，要明白这背后的成本！

那么消息零丢失方案到底适用于什么场景呢？

一般我们建议，对于跟金钱、交易以及核心数据相关的系统和核心链路，可以上这套消息零丢失方案。

比如支付系统，他是绝对不能丢失任何一条消息的，你的性能可以低一些，但是不能有任何一笔支付记录丢失。

比如订单系统，公司一般是不能轻易丢失一个订单的，毕竟一个订单就对应一笔交易，如果订单丢失，用户还支付成功了，你轻则要给用户赔付损失，重则弄不好要经受官司，特别是一些B2B领域的电商，一笔线上交易可能多大几万几十万。

所以对这种非常非常核心的场景和少数几条核心链路，才会建议大家上这套复杂的消息0丢失方案。

而对于其他大部分没那么核心的场景和系统，其实即使丢失一些数据，也不会导致太大的问题，此时可以不采取这些方案，或者说你可以在其他的场景里做一些简化。

比如你可以把事务消息方案退化成“**同步发送消息 + 反复重试几次**”的方案，如果发送消息失败，就重试几次，但是大部分时候可能不需要重试，那么也不会轻易的丢失消息的！最多在这个方案里，可能会出现一些数据不一致的问题。

或者你把broker的刷盘策略改为异步刷盘，但是上一套主从架构，即使一台机器挂了，os cache里的数据丢失了，但是其他机器上还有数据。但是大部分时候broker不会随便宕机，那么异步刷盘策略下性能还是很高的。

所以说，对于非核心的链路，非金钱交易的链路，大家可以适当简化这套方案，用一些方法避免数据轻易丢失，但是同时性能整体很高，即使有极个别的数据丢失，对非核心的场景，也不会有太大的影响。

**5、小作业：给你自己的项目设计一套消息0丢失方案**

今天给大家留一个小作业，是跟每个人自己的系统相关的

大家思考一下自己负责的项目，你是否需要上消息零丢失方案？如果不需要上完整的复杂方案的话，是否可以上一些简化的方案尽量避免数据丢失？

**End**

### 75 生产案例：从 RocketMQ 底层原理分析为什么会重复发优惠券？

**1、客服反馈的奇怪问题：有用户重复收到了多个优惠券**

话说小猛的团队前段时间对MQ的消息丢失问题进行了系统的研究，同时对系统的核心流程引入了一套消息零丢失的方案，保证系统的核心消息数据不会丢失，上线之后效果非常好，再也没出现过用户支付后没发红包的问题了。

但是今天客服突然又给技术团队反馈了一个问题，说是有用户在支付一个订单之后，一下子收到了多个优惠券，本来按照规则只应该有一个优惠券的！

也就是说，我们给用户重复发放了一个优惠券！

于是针对这个问题，小猛的团队又展开了一番研究。

**2、问题的定位：优惠券系统重复消费了一条消息**

从之前介绍的项目背景中，我们都可以知道一点，那就是订单系统已经跟各个系统进行了解耦，也就是说当订单支付成功之后，会发送一条消息到MQ里去，然后红包系统从里面获取消息派发红包，优惠券系统从里面获取消息派发优惠券，其他系统也是同理。

所以当小猛的团队对线上系统的日志进行分析的时候，发现了一个奇怪的问题，那就是优惠券系统似乎对同一条消息重复的处理了两次，导致他给一个用户重复的派发了两个优惠券

我们看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/28408100_1578466575.cn/txdocpic/0/2c4939d63b258d698bdbaaa8a7cc75e6/0)       

优惠券重复派发两次的问题已经定位到了，就是优惠券系统对同一个订单支付成功的消息处理了两次，就导致给用户重复发放了两张优惠券。

那么接下来我们需要思考的问题就是，为什么优惠券会对同一个消息重复处理两次呢？

**3、订单系统发送消息到MQ的时候会重复吗？**

要搞明白为什么优惠券系统对同一个消息重复处理了两次，我们先来研究第一个问题，我们的订单系统收到一个支付成功的通知之后，他在发送消息到MQ的时候，会重发把一个消息发送两次吗？

可能有的朋友乍一看觉得应该不可能，但是其实在生产环境中运行的系统，显然是有可能把一个消息重复发两次的！

首先，假设用户在支付成功之后，我们的订单系统收到了一个支付成功的通知，接着他就向MQ发送了一条订单支付成功的消息，这个大家都知道没有什么问题。

但是偏偏可能因为不知道什么原因，你的订单系统处理的速度有点慢，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61685600_1578466575.cn/txdocpic/0/2bd5db172b1bdaacf783145cbe7944f1/0)       

然后可能就因为你的订单系统处理的速度有点慢了，这就导致支付系统跟你订单系统之间的请求出现了超时，此时有可能支付系统再次重试调用了你订单系统的接口去通知你，这个订单支付成功了，然后你的订单系统这个时候可能又一次推送了一条消息到MQ里去，相当于是一个订单支付成功的消息，你重复推送了两次到MQ！

此时相当于是MQ里就会对一个订单的支付成功消息，总共有两条，我们看下图的示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92791000_1578466575.cn/txdocpic/0/a66f9cf4ac171eeaeaf6f859e0b90391/0)       

那如果你订单系统对一个订单重复推送了两次支付成功消息到MQ，MQ里对一个订单有两条重复的支付成功消息，优惠券系统必然会消费到一个订单的两条重复的支付成功消息，也必然会针对这个订单给用户重复的派发两个优惠券，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/28180400_1578466576.cn/txdocpic/0/28311f1191c5a72b3dd0fe72e561aa26/0)       

所以大家看到这里，通过一步一图的方式，可以很清晰的看到我们用于发送消息到MQ的订单系统，如果出现了接口超时等问题，可能会导致上游的支付系统重试调用订单系统的接口，进而导致订单系统对一个消息重复发送两条到MQ里去！

**4、重试是一把双刃剑：订单系统自己重复发送消息**

接着我们来考虑第二种情况，假设支付系统没有对一个订单重复调用你的订单系统的接口，而是你订单系统自己可能就重复发送消息到MQ里去

那这是一个什么情况呢？我们来分析一下。

假设我们的订单系统为了保证消息一定能投递到MQ里去，因此采用了重试的代码，我们之前也讲过这个伪代码的示意

我们看下面的代码片段，如果发现MQ发送有异常，则会进行几次重试。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43670600_1578466774.png)



但是这种重试的方式，其实是一把双刃剑，因为正是这个重试就可能导致消息重复发送

我们来考虑一个情况，假设你发送了一条消息到MQ了，其实MQ是已经接收到这条消息了，结果MQ返回响应给你的时候，网络有问题超时了，就是你没能及时收到MQ返回给你的响应。

但是大家一定要明确一点，此时MQ里其实是已经有你发送过去的消息了，只不过他返回给你的响应没能给到你而已！

我们看下面的图示

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70821600_1578466576.cn/txdocpic/0/470a53ea06bdcc9982c37d77e054b300/0)       

这个时候，你的代码里可能会发现一个网络超时的异常，然后你就会进行重试再次发送这个消息到MQ去，然后MQ必然会收到一条一模一样的消息，进而导致你的消息重复发送了！

大家看下图的示意

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5002900_1578466577.cn/txdocpic/0/be351953fdec52a95a78cc582efedbf5/0)       

所以这种重试代码大家在使用的时候一定要小心！因为他还是有一定的概率会导致你重发消息的！

**5、优惠券系统重复消费一条消息**

接着我们继续来看，即使你没有重复发送消息到MQ，哪怕MQ里就一条消息，优惠券系统也有可能会重复进行消费

这是为什么呢？我们一步一步来分析一下。

假设你的优惠券系统拿到了一条订单成功支付的消息，然后都已经进行处理了，也就是说都已经对这个订单给你发了一张优惠券了，本来我们之前讲过，这个时候他应该返回一个CONSUME_SUCCESS的状态，然后提交消费进度offset到broker的。

但是不巧的是，你刚刚发完优惠券，还没来得及提交消息offset到broker呢！优惠券系统就进行了一次重启！比如可能优惠券系统的代码更新了，需要重启进行重新部署。

我们看下面的图示

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44147500_1578466577.cn/txdocpic/0/6ac5f93ef029635cdcde172e05396999/0)       

这时因为你没提交这条消息的offset给broker，broker并不知道你已经处理完了这条消息，然后优惠券系统重启之后，broker就会再次把这条消息交给你，让你再一次进行处理，然后你会再一次发送一张优惠券，导致重复发送了两次优惠券！

这就是对同一条消息，优惠券系统重复处理两次的原因，我们看下面的图示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/71936400_1578466577.cn/txdocpic/0/3fb533f8287f88a4c90ebb95b61bca86/0)       

**6、消息重复问题应该是一种家常便饭**

实际上大家要知道，对类似优惠券系统这样的业务系统，我们肯定是会频繁的更新代码的，可能每隔几天就需要重启一次系统进行代码的更新

所以其实你重启优惠券系统的时候，可能有一批消息刚处理完，还没来得及提交offset给broker呢，然后你重启之后就会再一次重复处理这批消息，这种情况可能是家常便饭！

另外就是对于系统之间的调用，有的时候出现超时和重试的情况也是很常见的，所以你负责发消息到MQ的系统，很可能时不时的出现一次超时，然后被别人重试调用你的接口，你可能会重复发送一条消息到MQ里去，这可能也是家常便饭！

因此在使用MQ的时候，大家应该对消息重复问题习惯他，当做必须处理的一个问题。

**7、小作业：你的系统里出现过消息重复问题吗？**

今天给大家留一个小的思考题，希望每个朋友都思考一下：

- 自己负责的系统中如果使用MQ的话，是否出现过消息重复的问题吗？
- 如果有的话，是一个什么场景？
- 对你们有什么影响？
- 你们是如何发现的？然后是如何解决的？

大家可以踊跃的把自己的经验分享到专栏的评论区里去。

**End**

### 76 对订单系统核心流程引入 幂等性机制，保证数据不会重复

**1、到底什么是幂等性机制？**

上一次我们详细的分析了MQ的消息出现重复的问题，那么这一次我们就来分析一下到底应该如何去避免MQ中的消息进行重复处理。

要解决这个问题，我们就要先给大家讲一个概念，叫做幂等性机制。

这个幂等性机制，其实就是用来避免对同一个请求或者同一条消息进行重复处理的机制，所谓的幂等，他的意思就是，比如你有一个接口，然后如果别人对一次请求重试了多次，来调用你的接口，你必须保证自己系统的数据是正常的，不能多出来一些重复的数据，这就是幂等性的意思。

那么对于我们的MQ而言，就是你从MQ里获取消息的时候，要保证对同一个消息只能处理一次，不能重复处理多次，导致出现重复的数据。

因此要解决MQ的消息重复问题，关键就是要引入幂等性机制。

**2、发送消息到MQ的时候如何保证幂等性？**

现在我们先来看第一个问题，当我们的订单系统发送消息到MQ的时候需要保证幂等性吗？

我们都知道，订单系统的接口可能会被重复调用导致发送重复的消息到MQ去，也可能自己有重试机制导致发送重复的消息到MQ，如下图所示

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70628500_1578466826.cn/txdocpic/0/8b113467e73611e8587ea6c4c4c3accc/0)       

那么我们如果想要让订单系统别发送重复的消息到MQ去，应该怎么做呢？

大体上来说，常见的方案有两种。

第一个方案就是业务判断法，也就是说你的订单系统必须要知道自己到底是否发送过消息到MQ去，消息到底是否已经在MQ里了。

我们举个例子，当支付系统重试调用你的订单系统的接口时，你需要发送一个请求到MQ去，查询一下当前MQ里是否存在针对这个订单的支付消息？

如果MQ告诉你，针对id=1100这个订单的支付成功消息，在我这里已经有了，你之前已经写入进来了，那么订单系统就可以不要再次发送这条消息到MQ去了，我们看下图的示意。



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17557900_1578466827.cn/txdocpic/0/5193776253921581037aa0b08d522765/0)       

这个业务判断法的核心就在于，你的消息肯定是存在于MQ里的，到底发没发送过，只有MQ知道。如果没发送过这个消息，MQ里肯定没有这个消息，如果发送过这个消息，MQ里肯定给有这个消息。

所以当你的订单系统的接口被重试调用的时候，你这个接口上来就应该发送请求到MQ里去查询一下，比如对订单id=1100这个订单的支付成功消息，在你MQ那里有没有？如果有的话，我就不再重复发送消息了！

**3、基于Redis缓存的幂等性机制**

接着我们来讲第二种方法，就是状态判断法

这个方法的核心在于，你需要引入一个Redis缓存来存储你是否发送过消息的状态，如果你成功发送了一个消息到MQ里去，你得在Redis缓存里写一条数据，标记这个消息已经发送过，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/54332100_1578466827.cn/txdocpic/0/e9019f64e9b7b8b1d4279a915e97f8f0/0)       

那么当你的订单接口被重复调用的时候，你只要根据订单id去Redis缓存里查询一下，这个订单的支付消息是否已经发送给MQ了，如果发送过了，你就别再次发送了！

我们看下图的示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86029100_1578466827.cn/txdocpic/0/3a797eaaa671d4bd3f9b62795e7782da/0)       

其实两种幂等性机制都是很常用的，但是大家这里一定要知道一个事情，那就是对于基于Redis的状态判断法，有可能没办法完全做到幂等性。

举个例子，你的支付系统发送请求给订单系统，然后已经发送消息到MQ去了，但是此时订单系统突然崩溃了，没来得及把消息发送的状态写入Redis

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27145400_1578466828.cn/txdocpic/0/dce66633ed49d2136b7ce6516a191d6b/0)       

这个时候如果你的订单系统在其他机器上部署了，或者他重启了，那么这个时候订单系统被重试调用的时候，他去找Redis查询消息发送状态，会以为消息没发送过，然后会再次发送重复消息到MQ去

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/63090400_1578466828.cn/txdocpic/0/751dfeac87e67e65ee464372dd4d84b1/0)       

所以这种方案一般情况下是可以做到幂等性的，但是如果有时候你刚发送了消息到MQ，还没来得及写Redis，系统就挂了，之后你的接口被重试调用的时候，你查Redis还以为消息没发过，就会发送重复的消息到MQ去。

**4、有没有必要在订单系统环节保证消息不重复发送？**

所以在我们这个场景中，如果在订单系统环节要保证消息不重复发送，要不然就是直接通过查询MQ来判断消息是否发过，要不然就是通过引入Redis来保存消息发送状态。其实这两种方案都不是太好。

因为RocketMQ虽然是支持你查询某个消息是否存在的，这个功能我们后面的案例会讲他的功能使用和底层原理，但是在这个环节你直接从MQ查询消息是没这个必要的，他的性能也不是太好，会影响你的接口的性能。

另外基于Redis的消息发送状态的方案，在极端情况下还是没法100%保证幂等性，所以也不是特别好的一个方案。

所以对于我们而言，在这里建议是不用在这个环节保证幂等性，也就是我们可以默许他可能会发送重复的消息到MQ里去。

**5、优惠券系统如何保证消息处理的幂等性？**

接着我们来看优惠券系统假设会拿到重复的消息，那么如何保证消息处理的幂等性？

其实这里就比较简单了，直接基于业务判断法就可以了，因为优惠券系统每次拿到一条消息后给用户发一张优惠券，实际上核心就是在数据库里给用户插入一条优惠券记录

我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4691700_1578466829.cn/txdocpic/0/b435885dd4d8f5f6bbebacbc8f94747a/0)       

那么如果优惠券系统从MQ那里拿到一个订单的两条重复的支付成功消息，这个时候其实很简单，他只要先去优惠券数据库中查询一下，比如对订单id=1100的订单，是否已经发放过优惠券了，是否有优惠券记录，如果有的话，就不要重复发券了！

通过这个业务判断的方法，就可以简单高效的避免消息的重复处理了，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/42732500_1578466829.cn/txdocpic/0/374793588eb6a551a72989668b3cf3ad/0)      

**6、MQ消息幂等性的方案总结**

一般来说，对于MQ的重复消息问题而言，我们往MQ里重复发送一样的消息其实是还可以接收的，因为MQ里有多条重复消息，他不会对系统的核心数据直接造成影响，但是我们关键要保证的，是你从MQ里获取消息进行处理的时候，必须要保证消息不能重复处理。

这里的话，要保证消息的幂等性，我们优先推荐的其实还是业务判断法，直接根据你的数据存储中的记录来判断这个消息是否处理过，如果处理过了，那就别再次处理了。因为我们要知道，基于Redis的消息发送状态的方案，在一些极端情况下还是没法完全保证幂等性的。

**7、小作业：如果给你们的系统设计消息幂等性方案，如何做？**

上次给大家留了一个小作业，让大家思考一下自己的系统中是否会出现消息重复的问题。

那么今天给大家留一个小作业，结合你自己的业务思考一下，如果你的消息有重复问题，如何基于业务判断法去保证消息处理的幂等性？

大家可以踊跃在评论区留言自己的方案设计，跟大家一起交流。

**End**

### 77 如果优惠券系统的数据库宕机，如何用死信队列解决这种异常场景？

**目录**

1. 如果优惠券系统的数据库宕机，会怎么样？
2. 数据库宕机的时候，你还可以返回CONSUME_SUCCESS吗？
3. 如果对消息的处理有异常，可以返回RECONSUME_LATER状态
4. RocketMQ是如何让你进行消费重试的？
5. 如果连续重试16次还是无法处理消息，然后怎么办？
6. 消息处理失败场景下的方案总结

**1、如果优惠券系统的数据库宕机，会怎么样？**

之前我们已经分析和解决了MQ实践使用过程中可能存在的消息丢失问题和消息重复问题，现在假设我们可以基本确保MQ的消息不丢失，同时不会对消息进行重复处理，在正常流程下，基本没什么问题了。

那么接着我们来看下一个问题，假设我们的MQ使用都没问题，但是如果我们的优惠券系统的数据库宕机了呢？

因为我们一直都是假设了一个场景，就是订单支付成功之后会推送消息到MQ，然后优惠券系统、红包系统会从MQ里获取消息去执行后续的处理，比如发红包或者发优惠券。

那么如果这个时候，优惠券系统的数据库宕机了，就必然会导致我们从MQ里获取到消息之后是没办法进行处理的，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4671500_1578878207.cn/txdocpic/0/04ffcdb78d6a4e697e92474fdfb56b16/0)       

所以针对这样的一个坑爹的异常场景我们应该怎么处理？优惠券系统应该怎么对消息进行重试？重试多少次才行？万一反复重试都没法成功，这个时候消息应该放哪儿去？直接给扔了吗？

我们今天就对这个实际的生产场景进行分析。

**2、数据库宕机的时候，你还可以返回CONSUME_SUCCESS吗？**

先让我们回顾一下你的优惠券系统使用RocketMQ的Consumer是如何从MQ中获取到消息的，我们看下面的代码片段

在下面的代码片段中，清晰可以看到，我们注册了一个监听器回调函数，当Consumer获取到消息之后，就会交给我们的函数来处理。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15901900_1578878207.png)



而且我们之前还对这个方法进行了分析，我们可以在这个回调函数中对消息进行处理，比如发红包、发优惠券之类的，处理完成之后，就可以返回一个状态告诉RocketMQ Consumer这批消息的处理结果。

比如，如果返回的是CONSUME_SUCCESS，那么Consumer就知道这批消息处理完成了，就会对提交这批消息的Offset到broker去，然后下次就会继续从broker获取下一批消息来处理了。

但是如果此时我们在上面的回调函数中，对一批消息发优惠券的时候，因为数据库宕机了，导致优惠券发放逻辑无法完成，此时我们还能返回CONSUME_SUCCESS状态吗？

如果你返回的话，下一次就会处理下一批消息，但是这批消息其实没处理成功，此时必然导致这批消息就丢失了。

肯定会导致有一批用户没法收到优惠券的！

**3、如果对消息的处理有异常，可以返回RECONSUME_LATER状态**

所以实际上如果我们因为数据库宕机等问题，对这批消息的处理是异常的，此时没法处理这批消息，我们就应该返回一个RECONSUME_LATER状态

他的意思是，我现在没法完成这批消息的处理，麻烦你稍后过段时间再次给我这批消息让我重新试一下！

所以我们看下面的代码，应该改成如下的方式：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29391700_1578878207.png)



大家可以在上面的代码中看到，我们已经做出了相应的修改，如果消息处理失败了，就返回RECONSUME_LATER状态，让RocketMQ稍后再重新把这批消息给我，让我重试对这批消息进行处理！

**4、RocketMQ是如何让你进行消费重试的？**

那么RocketMQ在收到你返回的RECONSUME_LATER状态之后，是如何让你进行消费重试的呢？

简单来说，RocketMQ会有一个针对你这个ConsumerGroup的重试队列。如果遗忘了ConsumerGroup消费组概念的朋友可以再回过头去复习一下。

如果你返回了RECONSUME_LATER状态，他会把你这批消息放到你这个消费组的重试队列中去

比如你的消费组的名称是“VoucherConsumerGroup”，意思是优惠券系统的消费组，那么他会有一个“%RETRY%VoucherConsumerGroup”这个名字的重试队列，我们看下图的示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44941600_1578878207.cn/txdocpic/0/3755586b1ca7b796ef6afa0cc66e7ae0/0)       

然后过一段时间之后，重试队列中的消息会再次给我们，让我们进行处理。如果再次失败，又返回了RECONSUME_LATER，那么会再过一段时间让我们来进行处理，默认最多是重试16次！每次重试之间的间隔时间是不一样的，这个间隔时间可以如下进行配置：

messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h

上面这段配置的意思是，第一次重试是1秒后，第二次重试是5秒后，第三次重试是10秒后，第四次重试是30秒后，第五次重试是1分钟后，以此类推，最多重试16次！

**5、如果连续重试16次还是无法处理消息，然后怎么办？**

那么如果在16次重试范围内消息处理成功了，自然就没问题了，但是如果你对一批消息重试了16次还是无法成功处理呢？

这个时候就需要另外一个队列了，叫做**死信队列**，所谓的死信队列，顾名思义，就是死掉的消息就放这个队列里。

那么什么叫死掉的消息呢？

其实就是一批消息交给你处理，你重试了16次还一直没处理成功，就不要继续重试这批消息了，你就认为他们死掉了就可以了。然后这批消息会自动进入死信队列。

死信队列的名字是“%DLQ%VoucherConsumerGroup”，我们其实在RocketMQ的管理后台上都是可以看到的。

如下图所示

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65537300_1578878207.cn/txdocpic/0/b538d8fb0df1b5619222888e3771f917/0)       

那么对死信队列中的消息我们怎么处理？

其实这个就看你的使用场景了，比如我们可以专门开一个后台线程，就是订阅“%DLQ%VoucherConsumerGroup”这个死信队列，对死信队列中的消息，还是一直不停的重试。

**6、消息处理失败场景下的方案总结**

这一次我们就搞清楚了另外一个生产环境下的问题，就是消费者底层的一些依赖可能有故障了，比如数据库宕机，缓存宕机之类的，此时你就没办法完成消息的处理了，那么可以通过一些返回状态去让消息进入RocketMQ自带的重试队列，同时如果反复重试还是不行，可以让消息进入RocketMQ自带的死信队列，后续针对死信队列中的消息进行单独的处理就可以了。

**End**

### 78 生产案例：为什么基于 RocketMQ 进行订单库数据同步时会消息乱序？

**1、大数据团队同步订单数据库的技术方案回顾**

在讲完了常规性使用MQ技术的过程中可能遇到的三大问题（消息丢失、消息重复、处理失败）以及其对应的解决方案之后，我们来看一些特殊场景下的MQ的使用问题。

首先我们来看看消息乱序的问题

之前我们在讲业务背景的时候，就分析过一个案例，就是大数据团队需要获取订单数据库中的全部数据，然后将订单数据保存一份在自己的大数据存储系统中，比如HDFS、Hive、HBase等

接着基于大数据技术对这些数据进行计算，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/90264800_1578535996.cn/txdocpic/0/956e0eee2d6110d00b13e08a1441f136/0)然后我们之前讲过，如果让大数据系统自己直接跑复杂的大SQL在订单系统的数据库上来出一些数据报表，是会严重影响订单系统的性能的，所以后来这个方案优化为了，基于Canal这样的中间件去监听订单数据库的binlog，就是一些增删改操作的日志，然后把这些binlog发送到MQ里去。

接着大数据系统自己从MQ里获取binlog，落地到自己的大数据存储中去，然后对自己的存储中的数据进行计算得到数据报表即可，我们看下图。 ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/12459100_1578535997.cn/txdocpic/0/b109c9219c1e3cd09a7e4d76d3a5e7e3/0)       

**2、大数据团队遇到的奇怪问题：数据指标错误**

这个技术方案原本大家都以为会运行的很良好，结果没想到大数据团队在上了这个技术方案一段时间之后，遇到了一些奇怪的问题。他们通过这个方案计算出来的数据报表，被公司的管理层和运营同事发现，很多数据指标都是错误的！

于是他们就展开了排查，在对自己的大数据存储中的订单数据与订单数据库中的订单数据进行了一次比对之后，发现他们那儿的一些订单数据是不对的。比如在订单数据库中一个订单的字段A的值是100，结果在大数据存储中的一个订单的字段A的值是0。

那如果两边的订单数据的字段值都不一致的话，必然会导致最终计算出来的数据报表的指标是错误的！

**3、严密排查之后发现的原因：订单数据库的binlog消息乱序**

因此大数据团队针对这个问题，在系统中打印了很多的日志，然后观察了几天，发现了一个惊人的问题，那就是订单数据库的binlog在通过MQ同步的过程中，出现了奇怪的消息乱序的现象！

简单来说，比如订单系统在更新订单数据库的时候，有两条SQL语句：

insert into order values(xx, 0)

update order set xxvalue=100 where id=xxx

就是先插入了一条订单数据，刚开始他一个字段的值是0，接着更新他的一个字段的值是100。

然后这两条SQL语句是对应着两个binlog的，也就是两个更新日志，一个binlog是insert语句的，一个binlog是update语句的，这个binlog会进入到MQ中去。

然后大数据系统从MQ获取出来binlog的时候，居然是先获取出来了update语句的binlog，然后再获取了insert语句的binlog

也就是说，这个时候会先执行更新操作，但是此时数据根本不存在，没法进行更新，接着执行插入操作，也就是插入一条字段值为0的订单数据进去，最后大数据存储中的订单记录的字段值就是0。

我们看下图，有一个清晰的过程展示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/54505900_1578535997.cn/txdocpic/0/7599fd7316fa2d58edf677191995d871/0)       

正是这个消息乱序的原因，导致了大数据存储中的数据都错乱了。

**4、为什么基于MQ来传输数据会出现消息乱序？**

接着我们来分析一下，为什么基于MQ传输数据的时候默认会出现消息乱序的问题呢？

其实非常简单，我们之前都学习过，可以给每个Topic指定多个MessageQueue，然后你写入消息的时候，其实是会把消息均匀分发给不同的MessageQueue的。

比如我们这里在写入binlog到MQ的时候，可能会把insert binlog写入到一个MessageQueue里去，update binlog写入到另外一个MessageQueue里去

我们看下面的图示     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/87944600_1578535997.cn/txdocpic/0/9eedf9c792ff62c119222fa88ea8b611/0)       

接着大数据系统在获取binlog的时候，可能会部署多台机器组成一个Consumer Group，对于Consumer Group中的每台机器都会负责消费一部分MessageQueue的消息，所以可能一台机器从上图的ConsumeQueue01中获取到了insert binlog，一台机器从上图的ConsumeQueue02中获取到了update binlog

如下图所示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/19643700_1578535998.cn/txdocpic/0/794160f52a8a3e91b5a9eef15935b231/0)       

因为我们看到上图中，是两台机器上的大数据系统并行的去获取binlog，所以完全有可能是其中一个大数据系统先获取到了update binlog去执行了更新操作，此时存储中没有数据，自然是没法更新的。

然后另外一个大数据系统再获取到insert binlog去执行插入操作，最终导致只有一个字段值为0的订单数据，如下图：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/57968400_1578535998.cn/txdocpic/0/b6d480a2a4bc297780e828e2d579fa9b/0)       

**5、消息乱序：必须要正视的一个问题**

所以经过本文的分析，我们完全可以清晰的看到，在使用MQ的时候出现消息乱序是非常正常的一个问题，因为我们原本有顺序的消息，完全可能会分发到不同的MessageQueue中去，然后不同机器上部署的Consumer可能会用混乱的顺序从不同的MessageQueue里获取消息然后处理。

所以在实际使用MQ的时候，我们必须要考虑到这个问题。

**6、小作业：你们的系统遇到过消息乱序的问题吗？**

今天给大家留一个小作业，希望大家思考一下：

1. 你们系统如果使用过MQ的话，到底有没有消息乱序的问题？
2. 如果消息乱序对你们的业务有没有影响？
3. 如果遇到过的话，你们当时是怎么处理的？

希望大家在评论区踊跃的分享自己的经验。

**End**

### 79 在RocketMQ中，如何解决订单数据库同步的消息乱序问题？

**1、RocketMQ中消息乱序的原因回顾**

上一次我们已经分析过订单数据库同步过程中的消息乱序问题产生的根本原因了，其实最关键的是，属于同一个订单的binlog进入了不同的MessageQueue，进而导致一个订单的binlog被不同机器上的Consumer来获取和处理，那么如果是这样的话，必然会导致这一个订单的binlog会乱序执行。

我们再看一下下面的图，来回顾一下这个消息乱序的原因。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51832000_1578536412.cn/txdocpic/0/4e571345b05affac72a807bee61433f1/0)       

**2、让属于同一个订单的binlog进入一个MessageQueue**

所以要解决这个消息的乱序问题，最根本的方法其实非常简单，就是得想办法让一个订单的binlog进入到一个MessageQueue里去。

给大家举个例子，比如对一个订单，我们先后执行了insert、update两条SQL语句，也就对应了2个binlog。

那么我们现在就必须要想办法让这个订单的2个binlog都直接进入到Topic下的一个MessageQueue里去。

那么我们这个时候应该怎么做呢？我们完全可以根据订单id来进行判断，我们可以往MQ里发送binlog的时候，根据订单id来判断一下，如果订单id相同，你必须保证他进入同一个MessageQueue。

我们这里可以采用取模的方法，比如有一个订单id是1100，那么他可能有2个binlog，对这两个binlog，我们必须要用订单id=1100对MessageQueue的数量进行取模，比如MessageQueue一共有15个，那么此时订单id=1100对15取模，就是5

也就是说，凡是订单id=1100的binlog，都应该进入位置为5的MessageQueue中去！

通过这个方法，我们就可以让一个订单的binlog都按照顺序进入到一个MessageQueue中去，看下面的图：      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/95743500_1578536412.cn/txdocpic/0/958973c9bac1ff21bd76f3230b27ca17/0)       

**3、真的这么简单吗？获取binlog的时候也得有序！**

我们来思考一下，真的就上面说的那么简单，只要一个订单的binlog都进入一个MessageQueue就搞定这个问题了吗？

显然不是！

我们要考虑到一个问题，首先，我们的MySQL数据库的binlog一定都是有顺序的。

比如，订单系统对订单数据库执行了两条SQL，先是insert语句，然后是update语句。那么此时MySQL数据库自己必然是在磁盘文件里按照顺序先写入insert语句的binlog，然后写入update语句的binlog，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/34252400_1578536413.cn/txdocpic/0/7b43e13627c7e399970ca1afa3afa7c5/0)       

当我们从MySQL数据库中获取他的binlog的时候，此时也必须是按照binlog的顺序来获取的，也就是说比如Canal作为一个中间件从MySQL那里监听和获取binlog，那么当binlog传输到Canal的时候，也必然是有先后顺序的，先是insert binlog，然后是update binlog，如下图所示。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/68321100_1578536413.cn/txdocpic/0/876f90d2d5e57ab0ccdbdfd8aca90324/0)       

接着我们将binlog发送给MQ的时候，必须将一个订单的binlog都发送到一个MessageQueue里去，而且发送过去的时候，也必须是严格按照顺序来发送的

只有这样，最终才能让一个订单的binlog进入同一个MessageQueue，而且还是有序的，如下图所示：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6057000_1578536414.cn/txdocpic/0/4981046dc55fe9b104adfa7dca761ac9/0)       

所以我们必须要严格做到以上几点，才能保证一个订单的binlog绝对有序的进入一个MessageQueue中。

**4、Consumer有序处理一个订单的binlog**

接着我们可以想一下，一个Consumer可以处理多个MessageQueue的消息，但是一个MessageQueue只能交给一个Consumer来进行处理，所以一个订单的binlog只会有序的交给一个Consumer来进行处理！

我们看下图，这样的话一个大数据系统就可以获取到一个订单的有序的binlog，然后有序的根据binlog把数据还原到自己的存储中去。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43380300_1578536414.cn/txdocpic/0/4f3c989b7b639afa98f3e442532f9d96/0)       

**5、这就完了吗？没有，万一消息处理失败了可以走重试队列吗？**

那么大家觉得这样就完了吗？

绝对不是，我之前给大家讲过，在Consumer处理消息的时候，可能会因为底层存储挂了导致消息处理失败，之前我们说过，此时可以返回RECONSUME_LATER状态，然后broker会过一会儿自动给我们重试。

但是这个方案用在我们的有序消息中可以吗？

那绝对是不行的，因为如果你的consumer获取到订单的一个insert binlog，结果处理失败了，此时返回了RECONSUME_LATER，那么这条消息会进入重试队列，过一会儿才会交给你重试。

但是此时broker会直接把下一条消息，也就是这个订单的update binlog交给你来处理，此时万一你执行成功了，就根本没有数据可以更新！又会出现消息乱序的问题，我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/84288800_1578536414.cn/txdocpic/0/be8be4eac1cd23e7e34da2c56ebfebd8/0)       

所以对于有序消息的方案中，如果你遇到消息处理失败的场景，就必须返回SUSPEND_CURRENT_QUEUE_A_MOMENT这个状态，意思是先等一会儿，一会儿再继续处理这批消息，而不能把这批消息放入重试队列去，然后直接处理下一批消息。

**6、有序消息方案与其他消息方案的结合**

如果你一定要求消息是有序的，那么必须得用上述的有序消息方案，同时对这个方案，如果你要确保消息不丢失，那么可以和消息零丢失方案结合起来，如果你要避免消息重复处理，还需要在消费者那里处理消息的时候，去看一下，消息如果已经存在就不能重复插入，等等。

同时还需要设计自己的消息处理失败的方案，也就是不能进入重试队列，而是暂停等待一会儿，继续处理这批消息。

**7、作业：为你的系统设计一个多种方案结合的顺序消息方案**

假设你的系统里有需要保证消息顺序性的场景，那么此时如果你要保证消息的顺序性，就必然要上消息顺序方案，此时可能还需要结合其他的消息方案

所以你可以思考一下，在你的业务场景中，应该如何针对你的业务设计多种方案结合的消息方案出来。

希望大家踊跃分享在评论区跟其他朋友一起交流。

**End**

### 80 基于订单数据库同步场景，来分析RocketMQ的顺序消息机制的代码实现

**1、如何让一个订单的binlog进入一个MessageQueue？**

我们先来看第一个代码落地的分析，首先要实现消息顺序，必须让一个订单的binlog都进入一个MessageQueue中，此时我们可以写如下的代码：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55857300_1578537984.png)



在上面的代码片段中，我们可以看到，关键因素就是两个，一个是发送消息的时候传入一个MessageQueueSelector，在里面你要根据订单id和MessageQueue数量去选择这个订单id的数据进入哪个MessageQueue。

同时在发送消息的时候除了带上消息自己以外，还要带上订单id，然后MessageQueueSelector就会根据订单id去选择一个MessageQueue发送过去，这样的话，就可以保证一个订单的多个binlog都会进入一个MessageQueue中去。

**2、消费者如何保证按照顺序来获取一个MessageQueue中的消息？**

接着我们来看第二块，就是消费者如何按照顺序，来获取一个MessageQueue中的消息？

我们看下面的代码：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/53322500_1578537984.png)



在上面的代码中，大家可以注意一下，我们使用的是MessageListenerOrderly这个东西，他里面有Orderly这个名称

也就是说，Consumer会对每一个ConsumeQueue，都仅仅用一个线程来处理其中的消息。

比如对ConsumeQueue01中的订单id=1100的多个binlog，会交给一个线程来按照binlog顺序来依次处理。否则如果ConsumeQueue01中的订单id=1100的多个binlog交给Consumer中的多个线程来处理的话，那还是会有消息乱序的问题。

**3、作业：大家自己去实验一下消息的顺序性**

今天给大家留的一个作业就是，大家自己部署一个MQ，然后自己构造不同订单id下的有序的binlog数据，然后用上述的方法把他们发送到一个MessageQueue里去，然后在Consumer端观察一下。

是不是你可以拿到每个订单id下的有序的binlog，可以完全按照顺序拉处理？

如果处理失败了，是不是可以返回特殊状态，暂停一会儿再继续处理这批binlog，而不会跳过他们去处理下一批binlog？

希望大家都去测试一下，然后把自己的实验结果分享到评论区里去。

**End**

### 81 如何基于RocketMQ的数据过滤机制，提升订单数据库同步的处理效率

**1、混杂在一起的订单数据库的binlog**

我们已经花费了一些篇幅讲完了消息顺序方案了，那么我们现在就接着订单数据库同步的这个场景，来简单的看一下如何对混杂在一起的数据进行过滤的方案。

首先我们都知道，一个数据库中可能会包含很多表的数据，比如订单数据库，他里面除了订单信息表以外，可能还包含很多其他的表。

所以我们在进行数据库binlog同步的时候，很可能是把一个数据库里所有表的binlog都推送到MQ里去的！

我们看下面的图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69544000_1578538019.cn/txdocpic/0/6cb549cf3f9d1d98c0e02306010ec809/0)       



所以在MQ的某个Topic中，可能是混杂了订单数据库里几个甚至十几个表的binlog数据的，不一定仅仅包含我们想要的表的binlog数据！

**2、处理不关注的表的binlog，有多么浪费时间！**

那么此时假设我们的大数据系统仅仅关注订单数据库中的表A的binlog，并不关注其他表的binlog，那么大数据系统可能需要在获取到所有表的binlog之后，对每条binlog判断一下，是否是表A的binlog？

如果不是表A的binlog，那么就直接丢弃不要处理；如果是表A的binlog，才会去进行处理！

但是这样的话，必然会导致大数据系统处理很多不关注的表的binlog，也会很浪费时间，降低消息的效率，我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11751200_1578538020.cn/txdocpic/0/b77c4e28e8de6884561723573f4b64f1/0)       

**3、在发送消息的时候，给消息设置tag和属性**

针对这个问题，我们可以采用RocketMQ支持的数据过滤机制，来让大数据系统仅仅关注他想要的表的binlog数据即可。

首先，我们在发送消息的时候，可以给消息设置tag和属性，我们看下面的代码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91865300_1578538115.png)

上面的代码清晰的展示了我们发送消息的时候，其实是可以给消息设置tag、属性等多个附加的信息的。

**4、在消费数据的时候根据tag和属性进行过滤**

接着我们可以在消费的时候根据tag和属性进行过滤，比如我们可以通过下面的代码去指定，我们只要tag=TableA和tag=TableB的数据。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13116600_1578538150.png)

或者我们也可以通过下面的语法去指定，我们要根据每条消息的属性的值进行过滤，此时可以支持一些语法，比如：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/7634200_1578538150.png)

RocketMQ还是支持比较丰富的数据过滤语法的，如下所示：

（1）数值比较，比如：>，>=，<，<=，BETWEEN，=；

（2）字符比较，比如：=，<>，IN；

（3）IS NULL 或者 IS NOT NULL；

（4）逻辑符号 AND，OR，NOT；

（5）数值，比如：123，3.1415；

（6）字符，比如：'abc'，必须用单引号包裹起来；

（7）NULL，特殊的常量

（8）布尔值，TRUE 或 FALSE

**5、基于数据过滤减轻Consumer负担**

今天学习了这块知识后，我们以后就知道在使用MQ的时候，如果MQ里混杂了大量的数据，可能Consumer仅仅对其中一部分数据感兴趣，此时可以在Consumer端使用tag等数据过滤语法，过滤出自己感兴趣的数据来消费。

**End**

### 82 生产案例：基于延迟消息机制优化大量订单的定时退款扫描问题！

这篇文章以及接下来的几篇文章，将会是2020年春节前的最后几篇文章，考虑到很多人在看文章的时候都回老家准备过年了，可能也没太多精力在学习上，所以我们这几篇文章将会写的短小精干一些，避免耗费大家过多的时间。

在2020年春节过后，我还会讲解更多的MQ在实际生产环境下的一些技术问题以及对应的技术方案，比如MQ中百万消息积压解决方案、MQ消息轨迹、MQ监控报警、MQ中间件双写迁移方案、MQ权限方案，等等。

先来看看之前提到的一个订单退款扫描的问题。

我们先考虑一个正常的电商购物流程，一般来说我们作为用户在一个电商APP上都会选择一些商品加入购物车，然后对购物车里选择的一些商品统一下一个订单，此时后台的订单系统必然会在订单数据库中创建一个订单。

我们看下面的图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94569700_1579452596.cn/txdocpic/0/b884c80882754372a7d9a195208a8461/0)      

但是我们下了一个订单之后，虽然订单数据库里会有一个订单，订单的状态却是“待支付”状态，因为此时你其实还没有支付这个订单，我们的订单系统其实也在等待订单用户完成这个订单的支付。

这里就有两种可能了，一种可能是用户下单之后立马就支付掉了，那么接着订单系统可以走后续的流程，比如通过MQ发送消息通知优惠券系统给用户发优惠券，通知仓储系统进行调度发货，等等。

另外一种可能就是用户下单之后，一直在犹豫，迟迟没有下订单。

因此在实际情况中，其实APP的大量用户每天会下很多订单，但是不少订单可能是一直没有进行支付的，可能他下单之后犹豫了，可能是他忘了支付了！

我们看下图示意，订单数据库中有很多订单是没有支付的。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8723700_1579452597.cn/txdocpic/0/a27f18301f297ea0aace4b36ae1a87e3/0)       

所以一般订单系统都必须设置一个规则，当一个订单下单之后，超过比如30分钟没有支付，那么就必须订单系统自动关闭这个订单，后续你如果要购买这个订单里的商品，就得重新下订单了。

我们看下图，可能你的订单系统就需要有一个后台线程，不停的扫描订单数据库里所有的未支付状态的订单，看他如果超过30分钟了还没支付，那么就必须自动把订单状态 更新为“已关闭”      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/39993500_1579452597.cn/txdocpic/0/56bd70730401a5c94c54db3b83610328/0)       

但是这里就引入了一个问题，就是订单系统的后台线程必须要不停的扫描各种未支付的订单，这种实现方式实际上并不是很好。

一个原因是未支付状态的订单可能是比较多的，然后你需要不停的扫描他们，可能每个未支付状态的订单要被扫描N多遍，才会发现他已经超过30分钟没支付了。

另外一个是很难去分布式并行扫描你的订单。因为假设你的订单数据量特别的多，然后你要是打算用多台机器部署订单扫描服务，但是每台机器扫描哪些订单？怎么扫描？什么时候扫描？这都是一系列的麻烦问题。

因此针对类似这种场景，MQ里的**延迟消息**往往就会出场了，他是特别适合在这种场景里使用的，而且在实际项目中，MQ的延迟消息使用的往往是很多的。

所谓延迟消息，意思就是说，我们订单系统在创建了一个订单之后，可以发送一条消息到MQ里去，我们指定这条消息是延迟消息，比如要等待30分钟之后，才能被订单扫描服务给消费到

我们如下图所示      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/53777000_1579452597.cn/txdocpic/0/81df9797da0c5b298a86f90d1fb6fcd5/0)      

这样当订单扫描服务在30分钟后消费到了一条消息之后，就可以针对这条消息的信息，去订单数据库里查询这个订单，看看他在创建过后都过了30分钟了，此时他是否还是未支付状态？

如果此时订单还是未支付状态，那么就可以关闭他，否则订单如果已经支付了，就什么都不用做了，我们看下图

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/64989100_1579452597.cn/txdocpic/0/9bc3e376c484d39e337fdad207984af3/0)       

这种方式就比你用后台线程扫描订单的方式要好的多了，一个是对每个订单你只会在他创建30分钟后查询他一次而已，不会反复扫描订单多次。

另外就是如果你的订单数量很多，你完全可以让订单扫描服务多部署几台机器，然后对于MQ中的Topic可以多指定一个MessageQueue，这样每个订单扫描服务的机器作为一个Consumer都会处理一部分订单的查询任务。

所以MQ的延迟消息，是非常常用并且非常有用的一个功能。

**End**

### 83 基于订单定时退款场景，来分析RocketMQ的延迟消息的代码实现

上一篇文章我们分析了延迟消息的使用场景，这篇文章我们分析一下RocketMQ中对延迟消息的代码实现

其实RocketMQ对延迟消息的支持是很好的，实现起来也非常的容易，我们先看发送延迟消息的代码示例。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/25770700_1579569390.png)



大家看上面的代码，其实发送延迟消息的核心，就是设置消息的**delayTimeLevel**，也就是延迟级别

RocketMQ默认支持一些延迟级别如下：1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h

所以上面代码中设置延迟级别为3，意思就是延迟10s，你发送出去的消息，会过10s被消费者获取到。那么如果是订单延迟扫描场景，可以设置延迟级别为16，也就是对应上面的30分钟。

接着我们看看一个消费者的代码示例，比如订单扫描服务，正常他会对每个订单创建的消息，在30分钟以后才获取到，然后去查询订单状态，判断如果是未支付的订单，就自动关闭这个订单



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92243600_1579569389.png)



把延迟消息的使用搞明白了之后，想必大家以后在自己的系统中就可以使用延迟消息去支持一些特殊的业务场景了。

**End**

### 84 在RocketMQ的生产实践中积累的各种一手经验总结

到这篇文章为止，实际上MQ技术适用的一些业务场景，集群架构的原理，生产集群的部署与压测，业务场景的落地实践，MQ中间件的底层架构原理，在生产实践中MQ可能遇到的一些问题以及解决方案，我们几乎都已经讲完了。

其实大家如果对MQ技术掌握到目前这个程度，你在实际生产项目中落地使用MQ技术的时候已经可以比较好的基于你对MQ的深入理解，去使用MQ技术解决业务问题，而且对生产环境可能遇到的一些问题你都有对应的解决方案了。

因此在这篇文章里，我们给大家总结一下MQ在生产使用的时候，一些经验总结。

**（1）灵活的运用 tags来过滤数据**

之前我们讲解过基于tags来过滤数据的功能，其实在真正的生产项目中，建议大家合理的规划Topic和里面的tags，一个Topic代表了一类业务消息数据，然后对于这类业务消息数据，如果你希望继续划分一些类别的话，可以在发送消息的时候设置tags。

举个例子，比如我们都知道现在常见的外卖平台有美团外卖、饿了么外卖还有别的一些外卖，那么假设你现在一个系统要发送外卖订单数据到MQ里去，就可以针对性的设置tags，比如不同的外卖数据都到一个“WaimaiOrderTopic”里去。

但是不同类型的外卖可以有不同的tags：“meituan_waimai”，“eleme_waimai”，“other_waimai”，等等。

然后对你消费“WaimaiOrderTopic”的系统，可以根据tags来筛选，可能你就需要某一种类别的外卖数据罢了。

**（2）基于消息key来定位消息是否丢失**

之前我们给大家讲过，在消息0丢失方案中，可能要解决的是消息是否丢失的问题，那么如果消息真的丢失了，我们是不是要排查？此时是不是要从MQ里查一下，这个消息是否丢失了？

那么怎么从MQ里查消息是否丢失呢？

可以基于消息key来实现，比如通过下面的方式设置一个消息的key为订单id：message.setKeys(orderId)，这样这个消息就具备一个key了。

接着这个消息到broker上，会基于key构建hash索引，这个hash索引就存放在IndexFile索引文件里。

然后后续我们可以通过MQ提供的命令去根据key查询这个消息，类似下面这样：mqadmin queryMsgByKey -n 127.0.0.1:9876 -t SCANRECORD -k orderId

具体的命令，大家可以去查官方手册。

**（3）消息零丢失方案的补充**

之前我们给大家分析过消息零丢失方案，其实在消息零丢失方案中还有一个问题，那就是MQ集群彻底故障了，此时就是不可用了，那么怎么办呢？

其实对于一些金融级的系统，或者跟钱相关的支付系统，或者是广告系统，类似这样的系统，都必须有超高级别的高可用保障机制。

一般假设MQ集群彻底崩溃了，你生产者就应该把消息写入到本地磁盘文件里去进行持久化，或者是写入数据库里去暂存起来，等待MQ恢复之后，然后再把持久化的消息继续投递到MQ里去。

**（4）提高消费者的吞吐量**

如果消费的时候发现消费的比较慢，那么可以提高消费者的并行度，常见的就是部署更多的consumer机器

但是这里要注意，你的Topic的MessageQueue得是有对应的增加，因为如果你的consumer机器有5台，然后MessageQueue只有4个，那么意味着有一个consumer机器是获取不到消息的。

然后就是可以增加consumer的线程数量，可以设置consumer端的参数：consumeThreadMin、consumeThreadMax，这样一台consumer机器上的消费线程越多，消费的速度就越快。

此外，还可以开启消费者的批量消费功能，就是设置consumeMessageBatchMaxSize参数，他默认是1，但是你可以设置的多一些，那么一次就会交给你的回调函数一批消息给你来处理了，此时你可以通过SQL语句一次性批量处理一些数据，比如：update xxx set xxx where id in (xx,xx,xx)。

通过批量处理消息的方式，也可以大幅度提升消息消费的速度。

**（5）要不要消费历史消息**

其实consumer是支持设置从哪里开始消费消息的，常见的有两种：一个是从Topic的第一条数据开始消费，一个是从最后一次消费过的消息之后开始消费。对应的是：CONSUME_FROM_LAST_OFFSET，CONSUME_FROM_FIRST_OFFSET

一般来说，我们都会选择CONSUME_FROM_FIRST_OFFSET，这样你刚开始就从Topic的第一条消息开始消费，但是以后每次重启，你都是从上一次消费到的位置继续往后进行消费的。

**End**

### 85 企业级的RocketMQ集群如何进行权限机制的控制？

今天是春节之后我们重新开始更新专栏的第一天，今天要更新的两篇文章要讲解的内容都比较简单一些，因为考虑到很多人刚刚从老家回到工作所在地，可能状态还有点没调整过来，所以我们今天先讲两个简单轻松的话题，内容篇幅也不会很长。

另外，我们这个专栏已经重新制定了新版本的大纲，增加了30讲的内容，也就是增加了30%的篇幅，会引入一个RocketMQ源码分析的环节，深入RocketMQ底层，去慢慢的分析他里面的一些核心源码和原理，我们加量不加价，新版本的大纲过两天很快会公布出来，大家敬请期待。

今天先简单聊一下RocketMQ的权限控制的功能，简单来说，如果一个公司有很多技术团队，每个技术团队都会使用RocketMQ集群中的部分Topic，那么此时可能就会有一个问题了，如果订单团队使用的Topic，被商品团队不小心写入了错误的脏数据，那怎么办呢？可能会导致订单团队的Topic里的数据出错。

所以此时就需要在RocketMQ中引入权限功能，也就是说规定好订单团队的用户，只能使用“OrderTopic”，然后商品团队的用户只能使用“ProductTopic”，大家互相之间不能混乱的使用别人的Topic。

要在RocketMQ中实现权限控制也不难，首先我们需要在broker端放一个额外的ACL权限控制配置文件，里面需要规定好权限，包括什么用户对哪些Topic有什么操作权限，这样的话，各个Broker才知道你每个用户的权限。

首先在每个Broker的配置文件里需要设置aclEnable=true这个配置，开启权限控制

其次，在每个Broker部署机器的${ROCKETMQ_HOME}/store/config目录下，可以放一个plain_acl.yml的配置文件，这个里面就可以进行权限配置，类似下面这样子：

\# 这个参数就是全局性的白名单

\# 这里定义的ip地址，都是可以访问Topic的

globalWhiteRemoteAddresses:

\- 13.21.33.*

\- 192.168.0.*

\# 这个accounts就是说，你在这里可以定义很多账号

\# 每个账号都可以在这里配置对哪些Topic具有一些操作权限

accounts:

\# 这个accessKey其实就是用户名的意思，比如我们这里叫做“订单技术团队”

\- accessKey: OrderTeam

\# 这个secretKey其实就是这个用户名的密码

 secretKey: 123456

\# 下面这个是当前这个用户名下哪些机器要加入白名单的

 whiteRemoteAddress:

\# admin指的是这个账号是不是管理员账号

 admin: false

\# 这个指的是默认情况下这个账号的Topic权限和ConsumerGroup权限

 defaultTopicPerm: DENY

 defaultGroupPerm: SUB

\# 这个就是这个账号具体的堆一些账号的权限

\# 下面就是说当前这个账号对两个Topic，都具备PUB|SUB权限，就是发布和订阅的权限

\# PUB就是发布消息的权限，SUB就是订阅消息的权限

\# DENY就是拒绝你这个账号访问这个Topic

 topicPerms:

 \- CreateOrderInformTopic=PUB|SUB

 \- PaySuccessInformTopic=PUB|SUB

\# 下面就是对ConsumerGroup的权限，也是同理的

 groupPerms:

 \- groupA=DENY

 \- groupB=PUB|SUB

 \- groupC=SUB

\# 下面就是另外一个账号了，比如是商品技术团队的账号

\- accessKey: ProductTeam

 secretKey: 12345678

 whiteRemoteAddress: 192.168.1.*

 \# 如果admin设置为true，就是具备一切权限

 admin: true

上面的配置中，大家注意一点，就是如果你一个账号没有对某个Topic显式的指定权限，那么就是会采用默认Topic权限。

接着我们看看在你的生产者和消费者里，如何指定你的团队分配到的RocketMQ的账号，当你使用一个账号的时候，就只能访问你有权限的Topic。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/20917100_1580638136.png)



上面的代码中就是在创建Producer的时候后，传入进去一个AclClientRPCHook，里面就可以设置你这个Producer的账号密码，对于创建Consumer也是同理的。通过这样的方式，就可以在Broker端设置好每个账号对Topic的访问权限，然后你不同的技术团队就用不同的账号就可以了。

**End**

### 86 如何对线上生产环境的RocketMQ集群进行消息轨迹的追踪？

上一篇文章大家想必都了解到了RocketMQ集群如何实现指定账户对Topic访问的权限控制，这篇文章我们就来看下一个比较简单的话题，如何在生产环境里查询一条消息的轨迹？

也就是说，对于一个消息，我想要知道，这个消息是什么时候从哪个Producer发送出来的？他在Broker端是进入到了哪个Topic里去的？他在消费者层面是被哪个Consumer什么时候消费出来的？

我们有时候对于一条消息的丢失，可能就想要了解到这样的一个消息轨迹，协助我们去进行线上问题的排查，所以此时就可以使用RocketMQ支持的消息轨迹功能，我们看下面的配置过程。

首先需要在broker的配置文件里开启**traceTopicEnable=true**这个选项，此时就会开启消息轨迹追踪的功能。

接着当我们开启了上述的选项之后，我们启动这个Broker的时候会自动创建出来一个内部的Topic，就是**RMQ_SYS_TRACE_TOPIC**，这个Topic就是用来存储所有的消息轨迹追踪的数据的。

接着做好上述这一切事情之后，我们需要在发送消息的时候开启消息轨迹，此时创建Producer的时候要用如下的方式，下面构造函数中的第二个参数，就是enableMsgTrace参数，他设置为true，就是说可以对消息开启轨迹追踪。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97143900_1580687762.png)

在订阅消息的时候，对于Consumer也是同理的，在构造函数的第二个参数设置为true，就是开启了消费时候的轨迹追踪。

其实大家可以思考一下，一旦当我们在Broker、Producer、Consumer都配置好了轨迹追踪之后，其实Producer在发送消息的时候，就会上报这个消息的一些数据到内置的RMQ_SYS_TRACE_TOPIC里去

此时会上报如下的一些数据：Producer的信息、发送消息的时间、消息是否发送成功、发送消息的耗时。

接着消息到Broker端之后，Broker端也会记录消息的轨迹数据，包括如下：消息存储的Topic、消息存储的位置、消息的key、消息的tags。

然后消息被消费到Consumer端之后，他也会上报一些轨迹数据到内置的RMQ_SYS_TRACE_TOPIC里去，包括如下一些东西：Consumer的信息、投递消息的时间、这是第几轮投递消息、消息消费是否成功、消费这条消息的耗时。

接着如果我们想要查询消息轨迹，也很简单，在RocketMQ控制台里，在导航栏里就有一个消息轨迹，在里面可以创建查询任务，你可以根据messageId、message key或者Topic来查询，查询任务执行完毕之后，就可以看到消息轨迹的界面了。

在消息轨迹的界面里就会展示出来刚才上面说的Producer、Broker、Consumer上报的一些轨迹数据了。

**End**

### 087 由于消费系统故障导致的RocketMQ百万消息积压问题，应该如何处理？

今天同样是给大家讲解两个比较轻松的技术话题，因为最近几天的内容处于本专栏的一个新旧过渡期，对于原先大纲的90讲的内容，我们基本都已经快要讲完了，但是我们的新版大纲里又新增了30讲的RocketMQ源码分析的内容，所以这两天我们老大纲里最后几讲都是比较简单轻松的话题，很快我们将要进入非常硬核的30讲源码分析的环节。

先给大家说一个RocketMQ如果有百万消息积压了，应该怎么来处理呢？

说到这里，先给大家推荐一下石杉老师的《互联网Java工程师面试突击（第一季）》，在里面就有分析到MQ中间件的百万消息积压的问题以及常见的解决方案。大家可以在狸猫技术窝公众号的知识店铺里免费观看

我这里主要是基于RocketMQ的技术原理给大家分析一下，如果使用RocketMQ遇到百万消息积压的时候，我们应该怎么处理和解决呢？

我们先来思考一下，遇到百万消息积压大概是个什么场景。

先来一个比较真实的生产场景，我们曾经有一个系统，他就是由生产者系统和消费者系统两个环节组成的，生产者系统会负责不停的把消息写入RocketMQ里去，然后消费者系统就是负责从RocketMQ里消费消息。

这个系统在生产环境是有高峰和低谷的，在晚上几个小时的高峰期内，大概就会有100多万条消息进入RocketMQ。然后消费者系统从RocketMQ里获取到消息之后，会依赖一些NoSQL数据库去进行一些业务逻辑的实现。

然后有一天晚上就出现了一个问题，消费者系统依赖的NoSQL数据库就挂掉了，导致消费者系统自己也没法运作了，此时就没法继续从RocketMQ里消费数据和处理了，消费者系统几乎就处于停滞不动的状态。

然后生产者系统在晚上几个小时的高峰期内，就往MQ里写入了100多万的消息，此时都积压在MQ里了，根本没人消费和处理。

针对这种紧急的线上事故，一般来说有几种方案可以快速搞定他，如果这些消息你是允许丢失的，那么此时你就可以紧急修改消费者系统的代码，在代码里对所有的消息都获取到就直接丢弃，不做任何的处理，这样可以迅速的让积压在MQ里的百万消息被处理掉，只不过处理方式就是全部丢弃而已。

但是往往对很多系统而言，不能简单粗暴的丢弃这些消息，所以最常见的办法，还是先等待消费者系统底层依赖的NoSQL数据库先恢复了，恢复之后，就可以根据你的线上Topic的MessageQueue的数量来看看如何后续处理。

假如你的Topic有20个MessageQueue，然后你只有4个消费者系统在消费，那么每个消费者系统会从5个MessageQueue里获取消息，所以此时如果你仅仅依靠4个消费者系统是肯定不够的，毕竟MQ里积压了百万消息了。

所以此时你可以临时申请16台机器多部署16个消费者系统的实例，然后20个消费者系统同时消费，每个人消费一个MessageQueue的消息，此时你会发现你消费的速度提高了5倍，很快积压的百万消息都会被处理完毕。

但是这里你同时要考虑到你的消费者系统底层依赖的NoSQL数据库必须要能抗住临时增加了5倍的读写压力，因为原来就4个消费者系统在读写NoSQL，现在临时变成了20个消费者系统了。

当你处理完百万积压的消息之后，就可以下线多余的16台机器了。

这是一个最最常见的处理百万消息积压的办法，大体思路跟石杉老师在《互联网Java工程师面试突击（第一季）》里讲解的方案是一样的，只不过这里细化到根据RocketMQ的技术原理来讲解。

那么如果你的Topic总共就只有4个MessageQueue，然后你就只有4个消费者系统呢？

这个时候就没办法扩容消费者系统了，因为你加再多的消费者系统，还是只有4个MessageQueue，没法并行消费。

所以此时往往是临时修改那4个消费者系统的代码，让他们获取到消息然后不写入NoSQL，而是直接把消息写入一个新的Topic，这个速度是很快的，因为仅仅是读写MQ而已。

然后新的Topic有20个MessageQueue，然后再部署20台临时增加的消费者系统，去消费新的Topic后写入数据到NoSQL里去，这样子也可以迅速的增加消费者系统的并行处理能力，使用一个新的Topic来允许更多的消费者系统并行处理。

**End**

### 088 金融级的系统如何针对RocketMQ集群崩溃设计高可用方案？

今天要给大家讲解的第二个技术话题也比较轻松一些，这两三天讲解的内容基本上都不需要大量的画图，因为大家只要理解了之前讲解的大量的RocketMQ的技术原理，直接看文字就可以轻松理解这几天讲解的技术内容。

第二个技术话题：我们来说说金融级的系统中如果依赖了RocketMQ集群，那么在RocketMQ集群彻底崩溃的时候，我们应该如何设计他的高可用方案呢？

比如跟金钱相关的一些系统，他可能需要依赖MQ去传递消息，如果你MQ突然崩溃了，可能导致很多跟钱相关的东西就会出问题。

其实这种场景我们之前也是碰到过很多，类似这种跟钱相关的场景。

针对这种场景，我们通常都会在你发送消息到MQ的那个系统中设计高可用的降级方案，**这个降级方案通常的思路是，你需要在你发送消息到MQ代码里去try catch捕获异常，如果你发现发送消息到MQ有异常，此时你需要进行重试。**

如果你发现连续重试了比如超过3次还是失败，说明此时可能就是你的MQ集群彻底崩溃了，此时你必须把这条重要的消息写入到本地存储中去，可以是写入数据库里，也可以是写入到机器的本地磁盘文件里去，或者是NoSQL存储中去，几种方式我们都做过，具体要根据你们的具体情况来决定。

之后你要不停的尝试发送消息到MQ去，一旦发现MQ集群恢复了，你必须有一个后台线程可以把之前持久化存储的消息都查询出来，然后依次按照顺序发送到MQ集群里去，这样才能保证你的消息不会因为MQ彻底崩溃会丢失。

这里要有一个很关键的注意点，就是你把消息写入存储中暂存时，一定要保证他的顺序，比如按照顺序一条一条的写入本地磁盘文件去暂存消息。

而且一旦MQ集群故障了，你后续的所有写消息的代码必须严格的按照顺序把消息写入到本地磁盘文件里去暂存，这个顺序性是要严格保证的。

只要有这个方案在，那么哪怕你的MQ集群突然崩溃了，你的系统也是不会丢失消息的，对于一些跟金钱相关的金融系统、广告系统来说，这种高可用的方案设计，是非常的有必要的。

**End**

### 89 为什么要给RocketMQ增加消息限流功能保证其高可用性？

今天是我们这个专栏老版本大纲的最后两篇文章，我们依然讲点轻松点的，内容也不会太多，让大家有一个缓冲，明天开始我们就要进入专栏全新增加的30讲的RocketMQ源码分析的部分，到时候全称都是源码分析的硬干货，比较烧脑，大家做好准备。

今天先给大家讲第一个内容，就是RocketMQ的限流，这个功能我们文章里不会带着大家去做出来，但是会给大家讲一下为什么要给RocketMQ增加限流功能保证其高可用性呢？其实本质上来说，限流功能就是对系统的一个保护功能。

大家可以思考一个场景，假如有一个新来的工程师，因为没搞明白RocketMQ到底应该怎么使用，结果代码里写出了一个大bug，导致他负责的系统拼命的往MQ里不停的写入消息，可能一下子流量激增会导致MQ出现故障。

下面的代码就是曾经真实我们见过的一个工程师写出来的代码，大家可以看看：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/47092400_1580828599.png)

上面是简化以后的代码，实际上当时那段代码里混杂了很多的业务逻辑，但是当时出现的一个问题，就是业务代码报错了然后进入了catch代码块，结果那个工程师居然在catch代码块里写了一个while死循环，不停的发送消息。

而且上述系统当时是部署在10多台机器上的一个系统，所以相当于10多台机器都频繁的开足CPU的马力，拼命的往MQ里写消息，瞬间就导致MQ集群的TPS飙升，里面混入了大量的重复消息，而且MQ集群都快挂了。

所以针对这种场景，其实站在MQ的角度而言，你是没有办法去避免各种系统用上述的白痴方法来使用你的，毕竟公司大了，什么样的人都有，什么样的情况都可能出现，所以对MQ而言，你就必须去改造一下开源MQ的内核源码。

在接收消息这块，必须引入一个限流机制，也就是说要限制好，你这台机器每秒钟最多就只能处理比如3万条消息，根据你的MQ集群的压测结果来，你可以通过压测看看你的MQ最多可以抗多少QPS，然后就做好限流。

一般来说，限流算法可以采取**令牌桶算法**，也就是说你每秒钟就发放多少个令牌，然后只能允许多少个请求通过。关于限流算法的实现，不在我们的讨论范围内，大家可以自己查阅一下资料，也并不是很难。

我们这里主要是给大家讲一下，很多互联网大厂其实都会改造开源MQ的内核源码，引入限流机制，然后只能允许指定范围内的消息被在一秒内被处理，避免因为一些异常的情况，导致MQ集群挂掉。

**End**

### 90 设计一套Kafka到RocketMQ的双写+双读技术方案，实现无缝迁移！

今天的第二个技术问题，假设你们公司本来线上的MQ用的主要是Kafka，现在要从Kafka迁移到RocketMQ去，那么这个迁移的过程应该怎么做呢？应该采用什么样的技术方案来做迁移呢？

这里我们给大家介绍一个MQ集群迁移过程中的双写+双读技术方案。

简单来说，如果你要做MQ集群迁移，是不可能那么的简单粗暴的，因为你不可能说在某一个时间点突然之间就说把所有的Producer系统都停机，然后更新他的代码，接着全部重新上线，然后所有Producer系统都把消息写入到RocketMQ去了

一般来说，首先你要做到双写，也就是说，在你所有的Producer系统中，要引入一个双写的代码，让他同时往Kafka和RocketMQ中去写入消息，然后多写几天，起码双写要持续个1周左右，因为MQ一般都是实时数据，里面数据也就最多保留一周。

当你的双写持续一周过后，你会发现你的Kafka和RocketMQ里的数据看起来是几乎一模一样了，因为MQ反正也就保留最近几天的数据，当你双写持续超过一周过后，你会发现Kafka和RocketMQ里的数据几乎一模一样了。

但是光是双写还是不够的，还需要同时进行双读，也就是说在你双写的同时，你所有的Consumer系统都需要同时从Kafka和RocketMQ里获取消息，分别都用一模一样的逻辑处理一遍。

只不过从Kafka里获取到的消息还是走核心逻辑去处理，然后可以落入数据库或者是别的存储什么的，但是对于RocketMQ里获取到的消息，你可以用一样的逻辑处理，但是不能把处理结果具体的落入数据库之类的地方。

你的Consumer系统在同时从Kafka和RocketMQ进行消息读取的时候，你需要统计每个MQ当日读取和处理的消息的数量，这点非常的重要，同时对于RocketMQ读取到的消息处理之后的结果，可以写入一个临时的存储中。

同时你要观察一段时间，当你发现持续双写和双读一段时间之后，如果所有的Consumer系统通过对比发现，从Kafka和RocketMQ读取和处理的消息数量一致，同时处理之后得到的结果也都是一致的，此时就可以判断说当前Kafka和RocketMQ里的消息是一致的，而且计算出来的结果也都是一致的。

这个时候就可以实施正式的切换了，你可以停机Producer系统，再重新修改后上线，全部修改为仅仅写RocketMQ，这个时候他数据不会丢，因为之前已经双写了一段时间了，然后所有的Consumer系统可以全部下线后修改代码再上线，全部基于RocketMQ来获取消息，计算和处理，结果写入存储中。

基本上对于类似的一些重要中间件的迁移，往往都会采取双写的方法，双写一段时间，然后观察两个方案的结果都一致了，你再正式 下线旧的一套东西。

**End**

### 91 如何从Github拉取RocketMQ源码以及导入Intellij IDEA中？

**1、从Github下载RocketMQ源码**

今天开始我们进入本专栏全新增加的一部分内容，就是RocketMQ源码分析，首先我们这一讲会教大家，如果你要分析RocketMQ的源码，应该如何从Github上拉取他的源码下来，然后导入到Intellij IDEA里去，同时在本地进行源码的调试。

首先，我建议大家直接在浏览器中进入RocketMQ的github页面，直接在里面下载他的源码到本地，他的地址为：https://github.com/apache/rocketmq

大家进入这个地址之后，可以看到一个“clone or download”的按钮，在里面点击“Download ZIP”可以下载RocketMQ的源码，我们看下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/57464000_1580884687.cn/txdocpic/0/a218cd8b8ecd42c349d1a3ea56adbf13/0)       

这个直接用浏览器进行下载，可能速度是有点慢的，所以建议大家在浏览器下载界面里选择拷贝链接，然后打开迅雷之类的工具去下载，速度会非常的快。

接着在你本地会发现一个master.zip包，解压缩之后是rocketmq-master目录，这里就存放了RocketMQ最新的master分支的源码了。

**2、将RocketMQ源码导入到Intellij IDEA里去**

接着我们需要将RocketMQ源码导入到Intellij IDEA里去，大家打开Intellij IDEA比较新的版本的启动界面，会看到下面的界面：

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91223700_1580884687.cn/txdocpic/0/81ffaca291320bcff5611ea92d950fb2/0)       

接着点击上图中的Import Project，就是导入一个项目，因为RocketMQ源码本身下载下来就是一个已有的项目了，所以直接导入就可以了，此时会看到一个本地目录选择框，你就选择你的rocketmq-master目录就可以了，接着进入下面的界面。

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15962700_1580884688.cn/txdocpic/0/8d3542eaec87509146fea0ee1ac34823/0)       

在这个界面里，你就选择Import project from external model，然后选择里面的Maven就可以了，因为RocketMQ是基于maven来进行构建的，接着会进入下面的界面。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/53639300_1580884688.cn/txdocpic/0/1804399916455cdfa082008e81a8907a/0)       

这里你什么都不用管，你就直接点击Next就可以了，后续会出现一系列的界面，你都直接点击Next、Next就可以了，下面我给出你接下来会看到的一些界面的截图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/84532800_1580884688.cn/txdocpic/0/6c8f322b25f972166c7fd1cf6e52d380/0)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13646800_1580884689.cn/txdocpic/0/15df8052fdfc5c3d5ce93867eb147e61/0)       

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/63354000_1580884689.cn/txdocpic/0/5d575b364eeeb5b60a2cc3e9048fa680/0)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96179000_1580884689.cn/txdocpic/0/313edf6e0e0c9a6ca5db9e877ea3923c/0)       

从上图中可以看到我们拉取的是RocketMQ最新的稳定版本，4.6.0的版本的源码，一直到最后一个图，大家就直接点击Finish按钮就可以了，这就初步完成了源码的导入，接着会进入到Intellij IDEA的项目界面里去。

这个时候，你会看到有一个进度条会显示他在加载RocketMQ源码的所有依赖包的进度，然后你需要等待很长时间，根据每个人网速的不同，可能需要等待几十分钟到几个小时都有可能，接着你会看到如下所示的项目界面。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/34967000_1580884690.cn/txdocpic/0/b3153a4a7ea697b68d13e70602d5c068/0)       

但是你之前下载依赖包的过程可能会有一些失败的情况，比如因为网络超时导致jar包没下载下来，此时你可以进入命令行，无论是windows或者是mac都可以进入命令行的，然后进入到rocketmq-master项目的目录中，接着执行mvn clean install。

此时就是说用maven对项目执行一下清理、编译和部署到本地仓库，接着maven会自动下载一些之前下载失败的依赖包，然后你会看到maven的BUILD SUCCESS的提示，就说明这个项目彻底弄好了。

然后我们就搞定了RocketMQ的源码项目了，这个项目就导入到Intellij IDEA里去了。

**3、RocketMQ的源码目录结构**

接着我们简单给大家说一下RocketMQ的源码目录结构：

1. **broker**：顾名思义，这个里面存放的就是RocketMQ的Broker相关的代码，这里的代码可以用来启动Broker进程
2. **client**：顾名思义，这个里面就是RocketMQ的Producer、Consumer这些客户端的代码，生产消息、消费消息的代码都在里面
3. **common**：这里放的是一些公共的代码
4. **dev**：这里放的是开发相关的一些信息
5. **distribution**：这里放的就是用来部署RocketMQ的一些东西，比如bin目录 ，conf目录，等等
6. **example**：这里放的是RocketMQ的一些例子
7. **filter**：这里放的是RocketMQ的一些过滤器的东西
8. **logappender和logging**：这里放的是RocketMQ的日志打印相关的东西
9. **namesvr**：这里放的就是NameServer的源码
10. **openmessaging**：这是开放消息标准，这个可以先忽略
11. **remoting**：这个很重要，这里放的是RocketMQ的远程网络通信模块的代码，基于netty实现的
12. **srvutil**：这里放的是一些工具类
13. **store**：这个也很重要，这里放的是消息在Broker上进行存储相关的一些源码
14. **style、test、tools**：这里放的是checkstyle代码检查的东西，一些测试相关的类，还有就是tools里放的一些命令行监控工具类

**4、边干边学**

希望大家看完这篇文章就可以跟着把RocketMQ的源码下载下来，然后导入到Intellij IDEA里去，然后先看一下他整体的源码结构，跟着动手做一下，下一篇文章我们来讲解如何在Intellij IDEA中启动RocketMQ来本地调试他的源码。

**End**

### 92 如何在Intellij IDEA中启动NameServer以及本地调试源码？

**1、为什么要在Intellij IDEA中启动RocketMQ？**

上一篇文章我们说到已经把RocketMQ的源码从Github上下载下来，并且导入到Intellij IDEA中去了，而且还熟悉了一下RocketMQ的源码结构。

接着我们来考虑一个问题了，如果要分析RocketMQ源码的话，我们是直接就没头没脑的去翻看里面的源码吗？

这种分析源码的方式肯定是错误的，正确的做法，应该是尝试在Intellij IDEA中去启动RocketMQ，然后你就可以在源码中打一些断点，去观察RocketMQ源码的运行过程，而且在这个过程中，还需要从RocketMQ实际运行和使用的角度，去观察他的源码运行的流程。

什么意思呢？比如RocketMQ的使用的时候，刚开始肯定是先启动NameServer，那么我们是不是可以在NameServer的源码中打入断点，然后在Intellij IDEA中启动NameServer，接着观察他启动时候的源码运行流程？

接着下一步肯定是启动Broker，那么是不是可以在Broker源码中打入断点，然后在Intellij IDEA中启动Broker，去观察他启动时候的源码运行流程？

包括我们的客户端发送消息到Broker，Broker的主从同步，实际上我们都可以用这种方式在源码里打断点，然后在Intellij IDEA中启动和运行RocketMQ，来观察各种场景下的源码运行流程。

所以我们首先肯定要先能在Intellij IDEA中启动和调试RocketMQ的源码，接着才能进一步继续去分析他的源码运行流程。

**2、在Intellij IDEA中对NameServer启动类进行配置**

首先，我们需要在Intellij IDEA中启动NameServer，所以我们先在RocketMQ源码目录中找到namesvr这个工程，然后展开他的目录，找到NamesvrStartup.java这个类，我们看下面的图示。



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/24669400_1582196577.cn/txdocpic/0/ff9485f9c3ebc8e7e69ae2113f2f4dba/0)       



然后我们要选中NamesrvStartup这个类，接着在Intellij IDEA中的上面可以看到一个Edit Configuration的按钮，我们点击这个按钮

大家看下面的图示，接着会进入一个编辑NamesvrStartup这个启动类的界面。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/39638000_1582196577.cn/txdocpic/0/b93f5d67a501b6d32c5d4ac558432a84/0)      

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/48673500_1582196577.cn/txdocpic/0/e41a57ae06a8d93b7948263a1216f344/0)       

接着我们需要配置一个环境变量，就是ROCKETMQ_HOME，因为NameServer启动的时候他就是要求要有这个环境变量的，所以说我们需要在NamesvrStartup的配置编辑界面里给他加入一个环境变量的配置。

大家其实可以看到，上面的界面中，我们可以给一个类配置很多东西，包括他启动时候的JVM虚拟机的参数（VM options），包括我们要传递给他的main()方法的参数（Program options），这都是很实用的。

而我们要配置的是Environment Variables，就是环境变量，我们在上图中找到这个东西，他右边有一个按钮，大家可以点击一下，就会进入到一个添加环境变量的界面中去，大家看下面的图示。

​      ![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5648300_1582196577.png)       

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70936100_1582196577.cn/txdocpic/0/f858a765229cfc54dabd8b2b16467572/0)       

在上面那个图里有一个+号，你点击就可以添加环境变量，我们添加一个ROCKETMQ_HOME的环境变量，他的值你就输入一个新建的目录就好了

比如：xxx/rocketmq-nameserver，你填入你自己本地的目录，这个不要跟rocketmq-master源码目录一样，因为他是运行目录。

然后回到Edit Configuration的目录，就是点击下面的Apply按钮和OK按钮，就好了，我们就给NamesvrStartup这个启动类设置好了ROCKETMQ_HOME的环境变量了，他启动的时候是可以找到这个环境变量的。

**3、在rocketmq运行目录中创建需要的目录结构以及拷贝配置文件**

接着我们就需要在rocketmq-nameserver运行目录中创建我们需要的目录结构，此时我们需要创建conf、logs、store三个文件夹，因为后续NameServer运行是需要使用一些目录的。

然后我们把RocketMQ源码目录中的distrbution目录下的broker.conf、logback_namesvr.xml两个配置文件拷贝到刚才新建的conf目录中去，接着就需要修改这两个配置文件。

首先修改logback_namesvr.xml这个文件，修改里面的日志的目录，修改为你的rocketmq运行目录中的logs目录。里面有很多的${user.home}，你直接把这些${user.home}全部替换为你的rocketmq运行目录就可以了。

接着就是修改broker.conf文件，改成如下所示：

brokerClusterName = DefaultCluster

brokerName = broker-a

brokerId = 0

\# 这是nameserver的地址

namesrvAddr=127.0.0.1:9876

deleteWhen = 04

fileReservedTime = 48

brokerRole = ASYNC_MASTER

flushDiskType = ASYNC_FLUSH

\# 这是存储路径，你设置为你的rocketmq运行目录的store子目录

storePathRootDir=你的rocketmq运行目录的store子目录

\# 这是commitLog的存储路径

storePathCommitLog=你的rocketmq运行目录的store子目录/commitlog

\# consume queue文件的存储路径

storePathConsumeQueue=你的rocketmq运行目录的store子目录/consumequeue

\# 消息索引文件的存储路径

storePathIndex=你的rocketmq运行目录的store子目录/index

\# checkpoint文件的存储路径

storeCheckpoint=你的rocketmq运行目录的store子目录/checkpoint

\# abort文件的存储路径

abortFile=你的rocketmq运行目录/abort

**4、启动NameServer**

上面的东西都搞定之后，接着就可以右击NamesvrStartup类，选择Debug NamesvrStartup.main()了，就可以用debug模式去启动NameServer了，他会自动找到ROCKETMQ_HOME环境变量，这个目录就是你的运行目录，里面有conf、logs、store几个目录。

他会读取conf里的配置文件，所有的日志都会打印在logs目录里，然后数据都会写在store目录里，启动成功之后，在Intellij IDEA的命令行里就会看到下面的提示。

Connected to the target VM, address: '127.0.0.1:54473', transport: 'socket'

The Name Server boot success. serializeType=JSON

**End**

### 93 如何在Intellij IDEA中启动Broker以及本地调试源码？

**1、对Intellij IDEA中的broker模块进行配置**

上一讲我们已经成功的堆Intellij IDEA中的NameServer配置了环境变量，而且设置好了运行目录，配置文件等等，而且已经成功的在Intellij IDEA中启动了NameServer。

接下来有了NameServer，我们就可以在Intellij IDEA中启动一个Broke了。

首先我们依然是在Intellij IDEA中找到broker模块，然后展开他的目录，就可以找到一个BrokerStartup类，这个类是用来启动Broker进程的

我们看下面的图



​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/12413400_1580998939.cn/txdocpic/0/b744e19b217ab89c8be9f6e45ba8ccfa/0)       



然后我们选中这个BrokerStartup类，接着依然在Intellij IDEA的上面选择Edit Configuration，进入这个启动类的配置编辑界面，但是刚开始他会直接显示出来NamesvrStartup的配置编辑界面，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/50869300_1580998939.cn/txdocpic/0/6f76106417ac59041d31ff5799f584d4/0)       

我们发现他的Main class指定的是：org.apache.rocketmq.namesrv.NamesrvStartup，这不是我们要的类，所以这个时候，我们得重新为BrokerStartup类新建一个配置模板

在上面那个界面的左上角有一个+号，我们可以点一下，然后选择Application，此时会出现一个新的配置模板，如下图。

​     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/84199700_1580998939.cn/txdocpic/0/5ed2820f81fe0aaac8b0ee687beb2b6f/0)       

这个配置模板此时是没有名字的，我们在Name中输入BrokerStartup，Main class可以选择broker模块下的BrokerStartup类，Use classpath of module中可以选择broker这个module，然后会看到如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/22593100_1580998940.cn/txdocpic/0/2a67df4ed0572b86b41035bd1d635fa6/0)       

接着我们就可以对他进行配置了，首先在Program arguments里，我们需要输入下面的内容，给Broker启动的时候指定一个配置文件存放地址：-c 你的rocketmq运行目录/conf/broker.conf，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61953600_1580998940.cn/txdocpic/0/017a74932633b2e04c779c8b0f483070/0)       

接着我们需要配置环境变量，也就是ROCKETMQ_HOME，此时我们可以在Environment Variables里面添加一个ROCKETMQ_HOME环境变量，他的值就是我们的rocketmq运行目录就可以了，就是里面有conf、store、logs几个目录的，如下图所示。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98840500_1580998940.cn/txdocpic/0/3dc7939df81410b7f3ebfd7b0940940d/0)       

这些都配置好了之后，那么我们的BrokerStartup启动类就配置好了，因为这个时候Broker启动会收到一个-c以及配置文件的参数，而且他知道环境变量ROCKETMQ_HOME，知道运行目录是哪个，接着他就会基于这个配置文件来启动，同时在这个运行目录中存储数据，包括写入日志。

**2、再看broker的配置文件的内容**

接着我们再来看一次broker配置文件的 内容，就是broker.conf里的内容，是我们上一篇文章编辑的，如下所示。

大家会发现，这里主要是配置了NameServer的地址，然后配置了Broker的数据存储路径，包括commitlog文件、consume queue文件、index文件、checkpoint文件的存储路径，这些文件之前我们都给大家讲解过是干什么用的了，大家应该还记得。

brokerClusterName = DefaultCluster

brokerName = broker-a

brokerId = 0

\# 这是nameserver的地址

namesrvAddr=127.0.0.1:9876

deleteWhen = 04

fileReservedTime = 48

brokerRole = ASYNC_MASTER

flushDiskType = ASYNC_FLUSH

\# 这是存储路径，你设置为你的rocketmq运行目录的store子目录

storePathRootDir=你的rocketmq运行目录/store

\# 这是commitLog的存储路径

storePathCommitLog=你的rocketmq运行目录/store/commitlog

\# consume queue文件的存储路径

storePathConsumeQueue=你的rocketmq运行目录/store/consumequeue

\# 消息索引文件的存储路径

storePathIndex=你的rocketmq运行目录/store/index

\# checkpoint文件的存储路径

storeCheckpoint=你的rocketmq运行目录/store/checkpoint

\# abort文件的存储路径

abortFile=你的rocketmq运行目录/abort

\# 设置topic会自动创建

autoCreateTopicEnable=true

所以只要我们基于上述的broker配置文件来启动broker，那么他就会跟指定的nameserver来进行通信，然后在指定的目录里存放各种数据文件，包括在运行目录的logs目录里写入他自己的日志。

同时我们别忘了，在rocketmq-master源码目录下的distribution里，有一个logback-broker.xml，需要把这个拷贝到运行目录的conf目录中去，然后修改里面的地址，把${user.hom}都修改为你的rocketmq运行目录。

**3、使用debug模式启动Broker**

接着我们就可以使用debug模式启动BrokerStartup类了，右击他点击Debug BrokerStartup.main()，就可以启动他。

接着我们会看到如下的一段提示，就说明broker启动成功了：

Connected to the target VM, address: '127.0.0.1:55275', transport: 'socket'

The broker[broker-a, 192.168.3.9:10911] boot success. serializeType=JSON

然后我们在rocketmq运行目录下的logs中，会找到一个子目录是rocketmqlogs，里面有一个broker.log，就可以看到Broker的启动日志了，如下所示。

2020-02-05 14:19:55 INFO main - Try to start service thread:AllocateMappedFileService started:false lastThread:null

2020-02-05 14:19:56 WARN main - Load default transaction message hook service: TransactionalMessageServiceImpl

2020-02-05 14:19:56 WARN main - Load default discard message hook service: DefaultTransactionalMessageCheckListener

2020-02-05 14:19:56 INFO main - The broker dose not enable acl

2020-02-05 14:19:56 INFO main - Try to start service thread:ReputMessageService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:AcceptSocketService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:GroupTransferService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:HAClient started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:FlushConsumeQueueService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:FlushRealTimeService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:StoreStatsService started:false lastThread:null

2020-02-05 14:19:56 INFO main - Try to start service thread:FileWatchService started:false lastThread:null

2020-02-05 14:19:56 INFO FileWatchService - FileWatchService service started

2020-02-05 14:19:56 INFO main - Try to start service thread:PullRequestHoldService started:false lastThread:null

2020-02-05 14:19:56 INFO PullRequestHoldService - PullRequestHoldService service started

2020-02-05 14:19:56 INFO main - Try to start service thread:TransactionalMessageCheckService started:false lastThread:null

2020-02-05 14:19:56 INFO main - The broker[broker-a, 192.168.3.9:10911] boot success. serializeType=JSON

2020-02-05 14:20:06 INFO BrokerControllerScheduledThread1 - dispatch behind commit log 0 bytes

2020-02-05 14:20:06 INFO BrokerControllerScheduledThread1 - Slave fall behind master: 0 bytes

这就说明Broker已经启动成功了，下一篇文章我们就可以测试下本地启动的BrokerMQ可不可以进行消息的发送和消费了。

**End**

### 94 如何基于本地运行的RocketMQ进行消息的生产与消费？

**1、使用RocketMQ自带的例子程序测试消息的发送和消费**

接着我们可以使用RocketMQ自带的例子程序来测试消息的发送和消费，就在example模块下面就有，我们看下面的图。

在图中我们可以在quickstart包下找到Producer和Consumer两个例子类，而且大家可以看到，在这里包括事务消息、顺序消息，等等，很多高阶功能的例子在这里都有。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/577800_1581038324.cn/txdocpic/0/84e86b2e5e2d181e7226ee5243a7c835/0)       

**2、创建一个测试用的Topic出来**

首先我们需要启动rocketmq-console工程，之前在29讲里教过大家启动这个rocketmq控制台的步骤，大家把这个工程启动即可，在启动之后，我们就可以看到集群里有一台broker机器，如下图。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/16558800_1581038324.cn/txdocpic/0/35e5a98c6244fdde344cb48a23c72a99/0)       



接着我们就进入Topic菜单，新建一个名称为TopicTest的Topic即可，新建完之后在Topic列表就可以看到下面的内容了。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36259700_1581038324.cn/txdocpic/0/bf2fa386c4ba6b1d81e7ffc4b28cc74d/0)       

**3、修改和运行RocketMQ自带的Producer示例程序**

接着我们来修改一下RocketMQ自带的Producer示例程序，如下所示：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52022700_1581038357.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/60411500_1581038521.png)

接着我们执行运行上面的程序就可以了，他会发送1条消息到Broker里去，我们观察一下控制台的日志打印，可以看到下面的内容，就说明我们已经成功的把消息发送到Broker里去了。

SendResult [sendStatus=SEND_OK, msgId=240E03A24CD1B7A0B066027402ACC71F000018B4AAC217E3F1580000, offsetMsgId=C0A8030900002A9F0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-a, queueId=2], queueOffset=0]

**4、修改和运行RocketMQ自带的Consume示例程序**

接着修改RocketMQ自带的Consumer示例程序：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44244300_1581038542.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/35121500_1581038542.png)



接着运行上述程序，可以看到消费到了1条消息，如下所示：

ConsumeMessageThread_1 Receive New Messages: [MessageExt [queueId=2, storeSize=225, queueOffset=0, sysFlag=0, bornTimestamp=1580887214424, bornHost=/192.168.3.9:56600, storeTimestamp=1580887214434, storeHost=/192.168.3.9:10911, msgId=C0A8030900002A9F0000000000000000, commitLogOffset=0, bodyCRC=613185359, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=1, CONSUME_START_TIME=1580887519080, UNIQ_KEY=240E03A24CD1B7A0B066027402ACC71F000018B4AAC217E3F1580000, CLUSTER=DefaultCluster, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 48], transactionId='null'}]]

到此为止，我们的RocketMQ的源码环境彻底搭建完毕了，而且可以在本地启动以及收发消息，其实到这里为止，我们就可以去调试和分析RocketMQ的源码了。

**End**

### 95 源码分析的起点：从NameServer的启动脚本开始讲起

**1、看源码到底难不难？**

今天这一讲开始，我们就要正式开始带着大家一步一步的来分析RocketMQ的源码了，这个时候有很多的朋友会感到有点担心，因为大家可能都觉得看源码是一件非常困难的事。

为什么呢？因为很多人平时尝试过自己去看源码，结果看的一头雾水，没法继续看下去。大家也知道，网上其实很多技术都有对应的一些人写了一些源码解析的书籍，有很多人也去买了源码解析的书籍回来看。

结果发现他们写的源码解析的书籍看来看去，似乎还是看不懂，里面的内容极为的晦涩难懂，看了几次就是没法看下去，于是最后又是退缩了。网上也有一些人写的专门讲解源码分析的技术博客，其实有的内容写的还不错，但是很多人发现自己也是看来看去，还是看不懂！

所以因为上述的原因，大部分的Java工程师其实目前都没有能力自己去对一些技术进行源码级别的分析，没有能力去钻研一个技术较为底层的原理。而且很多人对源码这个东西，感觉有一种畏惧感，甚至是恐惧感，觉得自己根本看不懂。

看到这里，大家应该都发现了，看源码难不难？

难！

那么是不是真的大部分人此生都没法看懂一些开源技术的源码了？

未必！我们专栏接下来的这个部分，就是要带着大家一步一步的去看懂RocketMQ的核心源码，我们来换一种方式讲解源码。

**2、我们要如何去讲解RocketMQ的源码？**

为什么你会看不懂一些书籍或者博客的源码？因为很多人写的源码解析的书籍或者技术博客，往往是站在自己已经理解源码之后的角度去写的。

也就是说，他分析源码的时候，是先分析一个模块的源码，再分析一个模块的源码，接着分析下一个模块的源码。而且分析源码的过程中，以大量的源码和文字描述居多一些，很少有图在里面。

所以这个情况的话，如果是已经读懂这个技术的源码的人，是能看懂这本书的，但是如果是初次看这个技术源码的大多数人，按照这种顺序来，是很难理解的！

写书或者博客的目的，不是应该给那些不懂这个技术的源码的朋友分析吗？因此站在自己已经理解的角度来写文章分析源码，出发点可能就错了

所以我们这个专栏将要采用完全不同于传统的方式来分析RocketMQ的源码，简单来说，可以概括为三点：**场景驱动 + 通俗语言 + 大量画图**

首先，我们分析源码的顺序，不会是先讲NameServer全部的源码，然后是讲Producer全部的源码，接着讲Consumer全部的源码，最后讲Broker全部的源码，我们不会是这种一个模块一个模块的进行源码分析的。

我们会用场景来驱动源码的分析，也就是说，RocketMQ使用的时候，第一个步骤一定是先启动NameServer，那么我们就先来分析NameServer启动这块的源码，然后第二个步骤一定是启动Broker，那么我们再来分析Broker启动的流程。

接着Broker启动之后，必然会把自己注册到NameServer上去，那我们接着分析Broker注册到NameServer这部分源码，然后Broker必然会跟NameServer保持一个心跳，那我们继续分析Broker的心跳的源码。

所以实际上来说，我们分析源码，将会完全按照我们平时使用RocketMQ的各种场景来进行源码的分析，在一个场景中把各种源码串联起来分析，我觉得大家会觉得容易理解的多。

同时在分析的过程中，我们还是尽量用通俗易懂的语言去表达，而且尽量多画一些图给大家，用图来帮助大家理解源码中蕴含的复杂逻辑。

这就是我们将要采取的源码分析思路，那么接下来，我们就一起来开始RocketMQ的源码分析之旅吧。

**3、从NameServer的启动开始说起**

大家学习了之前的RocketMQ的知识体系之后，肯定都知道一点，那就是RocketMQ要玩儿起来的话，必须是先启动他的NameServer，因为后续Broker启动的时候，都是要向NameServer注册的，然后Producer发送消息的时候，需要从NameServer获取Broker机器信息，才能发送消息到Broker去。

所以我们第一个源码分析的场景，就是NameServer启动这个场景。

那么NameServer启动的时候，是通过哪个脚本来启动的呢？

其实我们之前都给大家讲过了，就是基于rocketmq-master源码中的distribution/bin目录中的mqnamesrv这个脚本来启动的，在这个脚本中有极为关键的一行命令用于启动NameServer进程，如下。

sh ${ROCKETMQ_HOME}/bin/runserver.sh org.apache.rocketmq.namesrv.NamesrvStartup $@

大家都看到，上面那行命令中用sh命令执行了runserver.sh这个脚本，然后通过这个脚本去启动了NamesrvStartup这个Java类，那么runserver.sh这个脚本中最为关键的启动NamesrvStartup类的命令是什么呢，如下

JAVA_OPT="${JAVA_OPT} -server -Xms4g -Xmx4g -Xmn2g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"

JAVA_OPT="${JAVA_OPT} -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:SurvivorRatio=8  -XX:-UseParNewGC"

JAVA_OPT="${JAVA_OPT} -verbose:gc -Xloggc:${GC_LOG_DIR}/rmq_srv_gc_%p_%t.log -XX:+PrintGCDetails"

JAVA_OPT="${JAVA_OPT} -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=30m"

JAVA_OPT="${JAVA_OPT} -XX:-OmitStackTraceInFastThrow"

JAVA_OPT="${JAVA_OPT} -XX:-UseLargePages"

JAVA_OPT="${JAVA_OPT} -Djava.ext.dirs=${JAVA_HOME}/jre/lib/ext:${BASE_DIR}/lib"

\#JAVA_OPT="${JAVA_OPT} -Xdebug -Xrunjdwp:transport=dt_socket,address=9555,server=y,suspend=n"

JAVA_OPT="${JAVA_OPT} ${JAVA_OPT_EXT}"

JAVA_OPT="${JAVA_OPT} -cp ${CLASSPATH}"

$JAVA ${JAVA_OPT} $@

大家可以看到，说白了，上述命令大致简化一下就是类似这样的一行命令：

java -server -Xms4g -Xmx4g -Xmn2g org.apache.rocketmq.namesrv.NamesrvStartup

这行命令只要是学习过Java基础的人应该都能理解。

通过java命令 + 一个有main()方法的类，就是会启动一个JVM进程，通过这个JVM进程来执行NamesrvStartup类中的main()方法，这个main()方法里就完成了NameServer启动的所有流程和工作，那么既然NameServer是一个JVM进程，肯定可以设置JVM参数了，所以上面你看到的一大堆-Xms4g之类的东西，都是JVM的参数。

这里给我的好朋友救火队长打个广告，如果大家看不懂上述一大堆JVM参数的话，推荐学习狸猫技术窝里的专栏《从0开始带你成为JVM实战高手》，很高质量的一个专栏

所以说白了，你使用mqnamesrv脚本启动NameServer的时候，本质就是基于java命令启动了一个JVM进程，执行NamesrvStartup类中的main()方法，完成NameServer启动的全部流程和逻辑，同时启动NameServer这个JVM进程的时候，有一大堆的默认JVM参数，你当然可以在这里修改这些JVM参数，甚至进行优化。

这边我也用下面的一张图来展示一下这个启动NameServer的过程，相信大家对照图来看，会理解的更加透彻一些。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91560100_1581289950.cn/txdocpic/0/2efc1616e04f39ad0ca400de160b78c5/0)      

**4、初步看一眼NamesrvStartup的main()方法**

今天要讲的内容其实到这里就结束了，我们先给大家讲一下NameServer是如何通过脚本来启动的，往往源码分析都是从他的启动脚本开始分析的。

但是我们在结束之前，也先来看一眼NamesrvStartup的main()方法，因为明天我们要开始分析他的源码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75443500_1581289980.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65183300_1581289980.png)



好了，我们先大致看一眼上面的那个类，初步的注释和解释我都写在上面了，其实初步看一眼就行，明天我们来继续分析这里面的NameServer启动逻辑。

**5、今天给大家留的小作业**

今天我给大家留一个课后小作业，希望大家认真去做，大家可以去自己仔细分析一下mqnamesrv脚本中的每一行逻辑，以及runserver.sh中的每一行shell脚本代码的逻辑，透彻的理解一个中间件系统的进程是如何启动起来的。

**End**

### 96 NameServer在启动的时候都会解析哪些配置信息？

**1、猜猜NamesrvController到底是个什么东西？**

我们现在来正式开始看NameServer的启动流程的源码，首先我们昨天已经讲到，NamesrvStartup这个类的main()方法会被执行，然后执行的时候实际上会执行一个main0()这么个方法，如下所示。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/33672400_1581347520.png)



在上面的源码中，我们会注意到这么一行代码：

NamesrvController controller = createNamesrvController(args); 

这行代码很明显，就是在创建一个NamesrvController类，这个类似乎是NameServer中的一个核心组件。

那么大家觉得这个类可能会是用来干什么的呢？

我们可以大胆的推测一下，NameServer启动之后，是不是需要接受Broker的请求？因为Broker都要把自己注册到NameServer上去。

然后Producer这些客户端是不是也要从NameServer拉取元数据？因为他们需要知道一个Topic的MessageQueue都在哪些Broker上。

所以我们完全可以猜想一下，NamesrvController这个组件，很可能就是NameServer中专门用来接受Broker和客户端的网络请求的一个组件！因为平时我们写Java Web系统的时候，大家都喜欢用Spring MVC框架，在Spring MVC框架中，用于接受HTTP请求的，就是Controlller组件！

所以我们看下面的图，大家可以先推测一下，NamesrvController组件，实际上就是NameServer中的核心组件，用来负责接受网络请求的！     ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55294400_1581347520.cn/txdocpic/0/23ad2c8f08d3b9bc983cc486512d6337/0)      

**2、NamesrvController是如何被创建出来的？**

接着我们来看一下，NamesrvController是如何被创建出来的？还是回到那行代码：

NamesrvController controller = createNamesrvController(args)

这里明显调用了一个createNamesrvController()方法，创建出来了NamesrvController这个关键组件！

所以我们可以初步看一下，createNamesrvController()这个方法中大概是在干什么呢？我们继续往下看。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/79335900_1581347520.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/3991100_1581347521.png)



上面那段代码是不是看着让人感觉特别的痛苦？是不是大家开始初步的感觉到阅读源码的痛苦了？往往看一些开源项目源码的时候，很多人就是初步看一看，看到类似上面这种代码的时候，就感觉看不下去了，因为实在是看不懂他在干什么！

这个时候大家不要着急，我们来慢慢的给大家解释一下，分一个一个小的代码片段，来给大家拆解一下上面的代码在干什么。

**3、阅读源码的一个技巧：哪些需要细看，哪些可以暂时先跳过**

这里我们结合上面的源码，来给大家讲解一下阅读源码的一个小技巧，简单来说，就是在阅读源码的时候，有些源码是要细看的，但是有些源码你可以大致猜测一下他的作用，就直接略过去了，抓住真正的重点去看！

比如说上面的createNamesrvController()方法，进入之后，刚开始就有一段让人看不太懂的代码，我们看看下面。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26260500_1581347521.png)

上面这段源码，大家看了有什么感受？其实估计大部分人都没什么感受，就是不知道这段代码是在干什么！

如果这个时候，有的人喜欢钻牛角尖的，直接去分析上面代码中的一些细节，比如看看ServerUtil.buildCommandlineOptions(new Options())是在干什么，或者看看ServerUtil.parseCmdLine()是在干什么，那你就误入迷途了

因为很明显上面的代码并不存在什么核心逻辑，你从他的代码的字面意思就可以大致猜测出来，他里面包含了很多CommandLine相关的字眼，那么顾名思义，这就是一段跟命令行参数相关的代码！

你其实大致推测一下都知道，我们在启动NameServer的时候，是使用mqnamesrv命令来启动的，启动的时候可能会在命令行里给他带入一些参数，所以很可能就是在这个地方，上面那块代码，就是解析一下我们传递进去的一些命令行参数而已！

所以这个地方你大致猜测一下，就可以直接略过去了，其实并没有必要陷入解析命令行参数的各种细节里去。

**4、非常核心的两个NameServer的配置类**

接着我们继续分析上述的代码片段，你略过刚才那段一看就是在解析命令行参数的代码，继续往下走，可以看到很关键的三行代码：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/85314000_1581347521.png)

上面三行代码才是你真正要关注的，你会看到他创建了NamesrvConfig和NettyServerConfig两个关键的配置类！

从他的类名，我们就可以推测出来，NamesrvConfig包含的是NameServer自身运行的一些配置参数，NettyServerConfig包含的是用于接收网络请求的Netty服务器的配置参数。

在这里也能明确感觉到，NameServer对外接收Broker和客户端的网络请求的时候，底层应该是基于Netty实现的网络服务器！

如果有朋友不知道Netty是什么，建议可以上网查一些Netty入门的博客和资料看看。

而且我们通过nettyServerConfig.setListenPort(9876)这行代码就可以发现，NameServer他默认固定的监听请求的端口号就是9876，因为他直接在代码里写死了这个端口号了，所以NettyServer应该就是监听了9876这个端口号，来接收Broker和客户端的请求的！

我们看下面的图，在图里我示意了基于Netty实现的服务器用于接收网络请求。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11317600_1581347522.cn/txdocpic/0/facc32f56df4c3a0919df92acdcec008/0)

**5、看看NameServer的两个核心配置类里都包含了什么？**

接着我们看看NameServer的那两个核心配置类里都包含了什么东西，我们直接看下面的两个类的代码片段以及我写的注释就可以了。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/35265300_1581347522.png)

其实看完了上面的NamesrvConfig，你会发现里面并没有什么特别关键的NameServer的配置信息。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/57128200_1581347522.png)

其实上面的NettyServerConfig一看就很明确了，那里的参数就是用来配置NettyServer的，配置好NettyServer之后，就可以监听9876端口号，然后Broker和客户端有请求过来，他就可以处理了。

**6、NameServer的核心配置到底是如何进行解析的？**

看明白上面两个核心配置类之后，接着我们就可以继续往下看代码，看看那两个核心配置类的配置都是如何解析的。

 ![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/76506400_1581347522.png)

上面的代码如果看懂了，我来给大家举个例子，比如说你在启动NameServer的时候，用-c选项带上了一个配置文件的地址，然后此时他启动的时候，运行到上面的代码，就会把你配置文件里的配置，放入两个核心配置类里去。

比如你有一个配置文件是：nameserver.properties，里面有一个配置是serverWorkerThreads=16，那么上面的代码就会读取出来这个配置，然后覆盖到NettyServerConfig里去！

接着我们来解释剩余的配置相关的代码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/71468300_1581347617.png)



其实在下面的图里，我直接展示出来了，NameServer启动的时候后，刚开始就是在初始化和解析NameServerConfig、NettyServerConfig相关的配置信息，但是一般情况下，我们其实不会特意设置什么配置，所以他这里一般都是用默认配置的！      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/98681700_1581347522.cn/txdocpic/0/7c4fd5fff199a3652c64f03df6e457a7/0)

**7、跟NameServer启动日志配合起来看**

其实我们知道NameServer刚启动就会初始化和解析一些核心配置信息，尤其是NettyServer的一些网络配置信息，然后初始化完毕配置信息之后，他就会打印这些配置信息，我们此时可以看一下之前讲解源码环境搭建的时候，不是指定了NameServer的启动日志么？

实际上翻看一下NameServer的启动日志，会看到如下的内容：

2020-02-05 15:10:05 INFO main - rocketmqHome=rocketmq-nameserver

2020-02-05 15:10:05 INFO main - kvConfigPath=namesrv/kvConfig.json

2020-02-05 15:10:05 INFO main - configStorePath=namesrv/namesrv.properties

2020-02-05 15:10:05 INFO main - productEnvName=center

2020-02-05 15:10:05 INFO main - clusterTest=false

2020-02-05 15:10:05 INFO main - orderMessageEnable=false

2020-02-05 15:10:05 INFO main - listenPort=9876

2020-02-05 15:10:05 INFO main - serverWorkerThreads=8

2020-02-05 15:10:05 INFO main - serverCallbackExecutorThreads=0

2020-02-05 15:10:05 INFO main - serverSelectorThreads=3

2020-02-05 15:10:05 INFO main - serverOnewaySemaphoreValue=256

2020-02-05 15:10:05 INFO main - serverAsyncSemaphoreValue=64

2020-02-05 15:10:05 INFO main - serverChannelMaxIdleTimeSeconds=120

2020-02-05 15:10:05 INFO main - serverSocketSndBufSize=65535

2020-02-05 15:10:05 INFO main - serverSocketRcvBufSize=65535

2020-02-05 15:10:05 INFO main - serverPooledByteBufAllocatorEnable=true

2020-02-05 15:10:05 INFO main - useEpollNativeSelector=false

不知道大家有何感觉？是不是感觉通过分析源码以及其中的日志打印，可以初步把源码运行的过程和日志文件的打印结合起来了？

**8、完成NamesrvController组件的创建**

在今天最后要讲解的内容，就是初步看一下NamesrvController是如何创建出来的，继续看下面的代码。

这里非常明确，就是直接构造了NamesrvController这个组件，同时传递了NamesrvConfig和NettyServerConfig两个核心配置类给他。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/58733300_1581347617.png)



我们看下面的图示，我们可以看到箭头的指向，两个核心配置类在初始化完毕之后，都是交给了NamesrvController这个核心的组件的。

 ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23979000_1581347523.cn/txdocpic/0/d8fd228bde59cfcd69bc3e05ba7d5f3b/0)  

**9、今天的源码分析作业**

今天我们其实着重给大家分析了NameServer启动过程中的createNamesrvController()方法的流程，讲解了他是如何初始化化两个核心配置类，然后基于核心配置类构造了NamesrvController这个核心组件的。

同时在源码分析的过程中还给大家讲解了一些小技巧，所以希望大家可以今天自己在RocketMQ源码环境中，自己阅读和分析一下createNamesrvController()这个方法，去体会一下里面的源码逻辑。

**End**

### 97 NameServer是如何初始化基于Netty的网络通信架构的？

**1、简单回顾：NamesrvController是如何被创建出来的？**

我们先来简单的回顾一下，NamesrvController这个非常核心的组件是个什么东西以及是如何被创建出来的？

简单来说，大家先看一眼下面的图，是上一讲我们画出来的一个初步的NameServer架构图。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75304800_1581601407.cn/txdocpic/0/e7ea06ec4a7e43dfc89f317a38819a13/0)    

我们上一篇文章分析到，NameServer启动的时候，实际上会解析配置文件，然后初始化NamesrvConfig和NettyServerConfig两个核心配置类

然后我们进一步知道NamesrvConfig里其实没什么关键的东西，最主要的还是NettyServerConfig里包含的一些网络通信的参数，他们基本都有自己的默认值。

然后基于这两个核心配置类，实际上最后我们在源码里看到，他初步的构建出来了NamesrvController这个核心组件，如下面的代码片段，在createNamesrvController()这个方法中，最终是创建出了这个核心组件的。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/20848000_1581601516.png)

而且我们还初步推测了一下，看到Netty相关的字眼，那么NamesrvController内部肯定包含基于Netty实现的网络通信组件了

所以大家在上面的图里可以看到，我们画了一个Netty网络服务器，负责监听和处理Broker以及客户端发送过来的网络请求。

**2、NamesrvController被创建出来了，Netty服务器就能启动？**

我们都知道，你这个NamesrvController被创建之后，我们最关心的其实就是他里面的Netty服务器得启动，这样NameServer才能在默认的9876这个端口上接收Broker和客户端的网络请求，比如Broker注册自己，客户端拉取Broker路由数据，等等。

那你觉得NamesrvController被创建出来了，然后就万事大吉了？

当然不可能那么简单的，我们看下面的NamesrvController的构造函数，他里面其实就是保存了一些实例变量的值而已，根本没干什么实质性的事儿。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/64275000_1581601600.png)



上面构造函数的代码你看一眼就行了，其实我根本都懒得去解释，因为大家看到这里基本也就只能知道他里面给一堆实例变量赋值了，构造了一些对象。但是比如KVConfigManager、RouteInfoManager、BrokerHousekeepingService这些东西是什么，你现在也看不出来，你暂时也不需要知道。

但是我们有一点是可以肯定的，仅仅创建出来一个NamesrvController，那绝对是不够的，肯定后续还有一些关键的代码，必须要启动他里面的Netty服务器才是核心的工作，这样他才能接受网络请求！

**3、NamesrvController是如何被启动的？**

因此我们回到NameServer启动时候最核心的main0()方法里去，看看你构造了NamesrvController之后，接着应该是要干什么事情。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/88008200_1581601677.png)



在上面的代码里，我省略了下面一些无关紧要的代码，其实大家会发现，启动NameServer的逻辑是最清晰明了的，因为看下来他就两个步骤：

一个是我们之前分析过的创建NamesrvController，这个过程中会有一些解析配置文件的工作，之前讲过了。

另外下面就是最关键的一个步骤，就是start(controller)这个代码，他就是启动了NamesrvController这个核心的组件！

**4、NamesrvController在启动时会干什么？**

接着我们来看start(controller)这个方法里的代码逻辑，看看下面，我省略了一些代码，就看下start()方法开头的一些源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92275700_1581601712.png)



在上面我们可以看到，最为关键的一行代码就是boolean initResult = controller.initialize()这个地方，他其实就是对NamesrvController执行了initialize初始化的操作。

既然是初始化，那么我们可以大胆的推测下，NamesrvController里我们最为关注的，不就是Netty服务器么，那么这个初始化的地方，是不是就是把他内部的Netty服务器给初始化构造出来了呢？

**5、Netty服务器是如何初始化的？**

进入到controller.initilize()方法内部，我省略了一些代码，就看开始进入的代码片段：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13209200_1581601791.png)

看到这里，大家是不是就恍然大悟了，啥都不用多说了，刚开始有一个kvConfigManager.load()，我们大致推测可能就是在里面有一些kv配置数据，是这个组件管理的，然后这里可能就是从磁盘上加载了kv配置吧。

但是我们一定不要陷入一些无关紧要的源码流程里去，因为我们现在分析NameServer启动的源码，不就是想知道他是如何初始化网络通信架构的么？我们就想知道后续Broker和客户端给他发送请求的时候，他是怎么处理的！

所以此时你只要对kvConfigManager有个印象就可以了，然后可以把他抛之脑后了，我们接着看下一行代码

this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService)

这个就非常不得了了，很明显，他就是构造了一个NettyRemotingServer，也就是Netty网络服务器。

其实我们一直在找的就是这块代码，大家看到这里，先别太激动，也别太着急往下看，我们捋一捋思路，再看一眼下面那幅图，我把里面的NettyRemotingServer给写进去了，大家会发现核心的网络通信组件已经出来了。

在NamesrvController组件被构造好之后，接着进行初始化的时候，首先就是把核心的NettyRemotingServer网络服务器组件给构造了出来。      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91140600_1581601407.cn/txdocpic/0/ff128fc901231e9d7629039946347fca/0)

**6、NettyRemotingServer是如何初始化的？**

接着我们当然是应该看看NettyRemotingServer中的Netty服务器是如何初始化的了，这就得看看他的构造函数了

但是他的构造函数里代码很多，其实我就截取最为关键的一行代码给大家看就可以了。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8270100_1581601791.png)



上面的代码中，其实最为关键的一行代码就是：

this.serverBootstrap = new ServerBootstrap()

有的朋友可能没学习过Netty，不过不要紧，我直接告诉你就行了，这个ServerBootstrap，就是Netty里的一个核心的类，他就是代表了一个Netty网络服务器，通过这个东西，最终可以让Netty监听一个端口号上的网络请求。

讲到这里，我在图里又加入了一点东西，NettyRemotingServer是一个RocketMQ自己开发的网络服务器组件，但是其实底层就是基于Netty的原始API实现的一个ServerBootstrap，是用作真正的网络服务器的。   ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4941500_1581601408.cn/txdocpic/0/7f3cb75439aff1e7a6494fac42582ff3/0)       

**7、今日作业**

相信大家一边跟着我的文章看一些核心源码片段，同时看一下我对源码的一些用最通俗的语言分析的思路，同时再加上我画的一些图，应该能够很容易理解RocketMQ的核心源码逻辑思路，同时结合图形可以快速的对源码执行过程中对应的架构原理有一个深刻的印象，最终你源码分析文章看完了，应该脑子里记住的就是我画的这些图。

同时只要你看懂了源码分析的文章，再自己认真的去完成我布置的一些源码分析的作业，自己复习和巩固源码思路，自己对照画好的图去会议源码执行流程和思路，那么你一定可以对RocketMQ的源码有一个较为深刻的理解的。

今日布置给大家的作业，就是结合我讲解的源码分析思路，自己去看一下NamesrvStratup中的main0()方法中的start(controller)这块启动NamesrvController的初步的一些源码，找其中关键的地方仔细看一下，理解NettyRemotingServer这个网络通信组件初始化的过程。

**End**

### 98 NameServer最终是如何启动Netty网络通信服务器的？

**1、NamesrvController初始化过程的一些遗留代码**

上次我们其实整体源码是分析到NamesrvController的initialize()方法，他在进行初始化，然后讲到他初始化了NettyRemotingServer，其中包含了一个Netty API开发的ServerBootstrap，说白了，就是一个网络服务器

我们看下下面的图，已经讲解的很明白了。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/33295600_1581901767.cn/txdocpic/0/e5766267300ff2b26c6db7319bc112f8/0)

接着我们简单看下NamesrvController.initialize()方法遗留下来的还没讲的一些源码，那些源码暂时其实对我们来说都不是太重要，我稍微给大家讲一下就行了。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10787400_1581901806.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/33631400_1581901867.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91902400_1581901970.png)



通过上面的源码分析，你会发现NamesrvController.initialize()方法，最核心的还是初始化Netty网络服务器，其他的就是启动了后台线程执行定时任务还重要一些，但是暂时我们还不用关注他，其他的代码你给忽略了也是可以的。

希望大家在跟着我逐步的分析RocketMQ核心源码的时候，也能够慢慢的掌握我分析源码的思路，你要明白，哪些是需要你重点关注的，哪些你可以暂时放着后续再来看，哪些是你可以干脆给忽略掉了。

**2、回到start(controller)方法里看看**

接着我们回到start(controller)方法里看看，大家看一下。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86163700_1581901970.png)



上面的controller.initialize()初始化这块代码我们实际上已经看完了，知道他已经初始化了Netty服务器出来，然后接着我们往下看他通过Runtime类注册了一个JVM关闭时候的shutdown钩子，就是JVM关闭的时候会执行上述注册的回调函数。

那个回调函数里执行了NamesrvController.shutdown()方法，其实我们都不用看里面的代码，都会知道，这里无非都是一些关闭Netty服务器的释放网络资源和线程资源的一些代码，如果大家一定要看，那我们看一下下面的代码。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69748800_1581901767.png)

感觉如何？是不是发现他就是在关闭NettyRemotingServer释放网络资源，然后关闭RemotingExecutor就是释放Netty服务器的工作线程池的资源，还有关闭ScheduledExecutorService就是释放执行定时任务的后台线程资源。

其实这里最关键的一行代码是：controller.start()。说白了，他已经初始化了Netty服务器了，但是现在还没启动，没启动的话，Netty服务器就不会监听9876这个默认的端口号，那么NameServer就什么也干不了。

所以此时，他必须要对NamesrvController组件做一个启动操作，这样的话，就可以把他内部的Netty服务器给启动了。

**3、Netty服务器是如何启动的？**

接着我们进入controller.start()方法内部看看，如下。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86558800_1581901767.png)

其实这里就很清晰了，这个NamesrvContorller启动，核心就是在启动NettyRemotingServer，也就是Netty服务器。在remotingServer.start()方法里，有很多的代码，我给大家逐步的分析各个片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6165500_1581901768.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/31816300_1581901768.png)

接着回看上面的一行代码：.localAddress(new InetSocketAddress(this.nettyServerConfig.getListenPort()))。这行代码，其实就是设置了Netty服务器要监听的端口号，默认就是9876

因此到此为止，你可以理解为Netty服务器启动了，开始监听端口号9876了，此时我们看下面的图，图里就展示出了Netty服务器监听端口号的这个示意。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51042100_1581901768.cn/txdocpic/0/e7baf2a07ca2a00c9b72239d5a645e8e/0)       

**4、总结**

到此为止，我们已经初步了解了NameServer是如何启动的了，了解到他最核心的就是基于Netty实现了一个网络服务器，然后监听默认的9876端口号，可以接收Broker和客户端的网络请求。

接着明天开始我们就要研究一下NameServer启动好之后，Broker是如何启动的，如何向NameServer进行注册，如何进行心跳，NameServer是如何管理Broker的。

**5、今天作业**

今天给大家留的小作业，就是仔细看看NettyRemotingServer的start()方法，仔细看一下里面是如何基于Netty API实现一个网络服务器的配置和启动的。请大家认真完成作业，有什么问题或者心得，欢迎在评论区留言

**End**

### 99 Broker启动的时候是如何初始化自己的核心配置的？

**1、NameServer已经启动后的示意图**

之前我们已经用了几讲的内容分析了一下NameServer的启动过程，从他的启动脚本开始讲起，然后一路讲解了他的配置的初始化，以及核心的NamesrvController组件的初始化和启动，最后通过源码一步一步发现，居然底层是基于Netty构建了一个网络服务器，然后监听了9876端口号。

于是我们可以看到下图，当我们的NameServer启动之后，其实他就是有一个Netty服务器监听了9876端口号，此时Broker、客户端这些就可以跟NameServer建立长连接和进行网络通信了！

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37804200_1581940116.cn/txdocpic/0/a549ee1e776dc5af322e8999bffed5bf/0)       

既然NameServer已经启动了，而且我们知道他已经有一个Netty服务器在监听端口号，等待接收别人的连接和请求了，接着我们就应该看看Broker是如何启动的了！

**2、BrokerStartup的入口源码分析**

其实大家之前看过我们的RocketMQ集群搭建和部署的实操内容，就应该都知道，启动Broker的时候也是通过mqbroker这种脚本来实现的，最终脚本里一定会启动一个JVM进程，开始执行一个main class的代码。

之前我们已经教过大家如何从启动脚本开始分析了，这里就不再重复了，我们在最后的今日源码分析作业里给大家留了作业，让大家自己去从broker的启动脚本开始分析，我们这里就直接从他的main class开始讲起了。

实际上Broker的JVM进程启动之后，会执行BrokerStartup的main()方法，这个BrokerStartup类，就在rocketmq源码中的broker模块里，大家看下图的源码截图，就会看到这个类。

​    ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/50696300_1581940116.cn/txdocpic/0/295dbe34a49148d750e6b21937730962/0)       

我们进入这个BrokerStartup类，在里面可以看到一个main()方法，如下所示：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/59216100_1581940116.png)

不知道大家看到这段源码有什么感觉没有？是不是发现跟NamesrvStratup里的一段代码是很类似的？同样都是先创建了一个Controller核心组件，然后用start()方法去启动这个Controller组件！

因此，我们今天就重点分析这个createBrokerController()方法是如何创建broker的核心组件BrokerController出来的就可以了。

**3、开始分析BrokerController的创建过程**

进入了createBrokerContorller()方法之后，首先你会看到下面的一堆代码，很多人看到这里又会出现非常痛苦的心情，因为感觉看不懂啊！

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/67943400_1581940116.png)

很多人肯定会不知道上面的源码是干什么的，但是其实非常简单，你即使不知道也没什么大不了！因为这些都不是最核心的一些代码，你即使看不懂，也不要有畏难心理，就是接着往下看就是了！

当然，这里可以教大家一些看源码的技巧，当你不停得看各种开源项目的源码，看的多了之后，慢慢的就会摸索出很多看源码的技巧，当你的技巧积累的很多了之后，你会发现你慢慢的什么源码自己都看得懂了！

比如一开始你看到有一个System.setProperty()，他这个明显是在设置一个系统级的变量，至于设置了这个变量干什么用的，你这里留个印象就行了，你不需要知道，暂时也没法知道，毕竟这源码又不是你写的！

然后后续的

if (null == System.getProperty()) { 

  NettySystemConfig.socketSndbufSize = 131072 

}

这种代码，很多人可能就有点眉目了，他意思就是，如果某个系统级的变量没有设置，那么就在这里设置，而且明显发现，他设置的是Netty网络通信相关的变量，就是socket的发送缓冲大小。

看到这里，很多人还是一头雾水，为什么要在这里设置Netty的网络通信的参数？

其实要我说，你管他呢！毕竟你又不是RocketMQ的源码作者，人家就是喜欢在这里干这么个事，后续可能你看了别的代码，突然在某个地方会跟这里联想起来。但是你要在这个地方，就直接领悟出来写这段源码的含义，那是不可能的！

毕竟第一你不是RocketMQ源码作者肚子里的蛔虫，第二你又不可能跟人家源码作者有心灵感应！很多源码为什么要在这个地方写，为什么要这么写，写了之后后续派什么用场的，实际上需要你综合看后面很多其他源码，才能综合起来考虑明白。

很多时候，一段源码写在这里，可能只有源码的作者自己才知道是为什么！因为当时就是他这么想的，他才这么写的！

**4、又见Broker的核心配置类**

我们接着往后看，会发现如下一段奇怪的代码：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/80395800_1581940116.png)

这个代码说奇怪，其实也不奇怪，因为之前在NameServer里看到类似的了，其实说白了，他就是用来解析你通过命令行给broker传递的一些参数的，这些参数在main()方法里通过上面的args传递进来，然后在这里他就是通过ServerUtil.parseCmdLine()方法，在解析这些命令行参数罢了！

接着我们继续往后看，会看到一段极为关键的源代码，大家请看我在代码里的注释。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92141200_1581940116.png)



大家看完了上面那段源码的分析，有什么感觉？

很明显，套路是一样的，broker在这里启动的时候也是先搞了几个核心的配置组件，包括了broker自己的配置、broker作为一个netty服务器的配置、broker作为一个netty客户端的配置、broker的消息存储的配置。

那么为什么broker自己又是netty服务器，又是netty客户端呢？

很简单了，当你的客户端连接到broker上发送消息的时候，那么broker就是一个netty服务器，负责监听客户端的连接请求。

但是当你的broker跟nameserver建立连接的时候，你的broker又是一个netty客户端，他要跟nameserver的netty服务器建立连接。

所以通过上述分析，我们画出了下面的图，包含了Broker的几个核心配置组件。

​      ![img](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/1838000_1581940117.cn/txdocpic/0/7b3741086ccd44448e04456b261c7183/0) 

**5、为核心配置类解析和填充信息**

接着我们看看他是如何为自己的核心配置类，解析和填充信息的，继续看下面的代码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13971200_1581940117.png)

上面代码其实清晰明了，假设说你在启动broker的时候，用了-c选项带了一个配置文件的地址，此时他会读取配置文件里的你自定义的一些配置的信息，然后读取出来覆盖到那4个核心配置类里去。

大家都记得我们之前启动broker的时候，其实都是要自定义一个broker配置文件的，然后用mqbroker启动的时候，都是要用-c选项带上自己的配置文件地址的，就是在上面的代码中，他会读取我们自定义的配置文件，填充到他的配置类里去。

接着我们往后看下面的源代码，我都在注释里写了他是如何解析和填充配置的。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/68287100_1581940142.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/64000_1581940164.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94940300_1581940163.png)

基本上上面的配置解析和填充的代码，一路看下来就到这里了，大家重点不是去理解他的代码怎么写的，而是去尝试积累这种看源码的经验和技巧，你要明白，任何其他的开源项目，可能都有类似的代码，就是构建配置类，读取配置文件的配置，解析命令行的配置参数，然后做各种配置的校验和设置。

最终他就会在这里得到4个填充完整的配置类了！明天我们就该讲解他有了这些配置之后，是如何构建出来BrokerController的！

**6、今日源码分析作业**

今天给大家留一个源码分析的小作业，大家可以去看看broker的启动脚本，然后分析一下启动脚本里干的事情，他是如何一步一步的启动Broker的JVM进程的，然后执行的main class是谁，他的默认的JVM参数都是什么。

然后大家顺着main class的源代码，参考本文的源码分析思路，自己再把初始化配置的源码过程自己看一下，找找那种逐步逐步看懂源码的感觉，掌握分析源码的技巧

如果大家看的时候有什么心得体会，可以在评论区里发出来，跟别人一起交流！

**End**

### 100 BrokerController是如何构建出来的，以及他包含了哪些组件？

**1、BrokerController是在哪里创建出来的？**

上回我们讲到Broker在启动的时候，他首先会执行一个关键的方法，就是createBrokerController()

在这个里面，我们之前分析了他已经初始化以及解析了一些核心配置组件，我们看看下面的图，你会发现Broker里已经有一些核心配置组件了。

​      ![01.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/54223200_1582037335.jpg)       

上述的核心配置组件，上一次我们也分析过了，其实这类配置初始化的代码就是看着麻烦，并没什么难度的，他本质上就是用默认的配置参数值以及我们配置文件里的配置参数值，包括命令行传递的配置参数值，去填充到这些配置组件中去。

然后后续你Broker运行的过程中，各种行为自然都是根据这些配置组件里的配置参数值来走的，大概就是这个意思，所以大家千万别对源码感到恐慌。

我们接着看，那么在准备好了上述核心配置组件之后，接下来下一步是干什么呢？

其实就是要创建最核心的Broker组件了，这个Broker组件是如何创建的呢？

我们看下面的源码，下面的源码就是在createBrokerController()方法中的。

final BrokerController controller = new BrokerController(

  brokerConfig,

  nettyServerConfig,

  nettyClientConfig,

  messageStoreConfig); 

controller.getConfiguration().registerConfig(properties);

上述代码的意思已经非常明显了，他就是创建了一个核心的BrokerController组件，这个BrokerController组件，你大致可以认为就是代表了Broker他自己好了。我们可以来梳理一下BrokerStartup和BrokerController两个代码组件的关系，以及为什么要这么设计。

**2、为什么要叫做“BrokerController”呢？**

首先我们来思考一下，Broker启动的时候，他的main class是谁？是不是BrokerStartup这个类？

那么这个类名，顾名思义，就是用来启动Broker的一个类，他里面包含的是把Broker给进行初始化和完成全部启动工作的逻辑。

所以，大家觉得BrokerStartup自己可以代表成是一个Broker吗？明显是不对的。

所以其实最核心的组件，就是BrokerController，这个BrokerController可能有的朋友会以为跟我们平时在Java Web开发中，用Spring MVC框架开发的一系列Controller是一个意思，负责接收和处理请求。

其实这么想，也有一定道理，但是也不完全是对的。

因为毕竟中间件系统的架构设计思想和普通的Java Web业务系统还是不一样的。虽然两种Contorller的含义是相近的，但还是有区别。

BrokerController，如果一定要用中文来表达出这个组件的含义的话，你应该把他叫做是“Broker管理控制组件”

什么意思呢？他这个组件其实被创建出来以及初始化完毕之后，就是用来控制当前在运行的这个Broker的！

所以他英文叫做BrokerController，中文叫做“Broker管理控制组件”。

也正是因为如此，你大致可以理解一下他们这里的关系，我们用mqbroker脚本启动的JVM进程，实际上你可以认为就是一个Broker，这里Broker实际上应该是代表了一个JVM进程的概念，而不是任何一个代码组件！

然后BrokerStartup作为一个main class，其实是属于一个代码组件，他的作用是准备好核心配置组件，然后就是创建、初始化以及启动BrokerController这个核心组件，也就是启动一个Broker管理控制组件，让BrokerController去控制和管理Broker这个JVM进程运行过程中的一切行为，包括接收网络请求、包括管理磁盘上的消息数据，以及一大堆的后台线程的运行。

所以讲到这里，我又完善了一下下面的图，大家在里面可以清晰的看到Broker、BrokerStartup、BrokerController之间的关系。

![02.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/66102500_1582037335.jpg)

上面那个图里，其实就把这几者之间的关系，说的很清晰了，Broker这个概念本身代表的不是一个代码组件，他就是你用mqbroker脚本启动的JVM进程。然后JVM进程的main class是BrokerStartup，他是一个启动组件，负责初始化核心配置组件，然后基于核心配置组件去启动BrokerControler这个管控组件。

然后在Broker这个JVM进程运行期间，都是由BrokerController这个管控组件去管理Broker的请求处理、后台线程以及磁盘数据。

**3、初步看一下BrokerController的构造函数**

接着我们初步的看一下BrokerController的构造函数，如下所示，我在里面简单给大家写了一些注释。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/304800_1582037379.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/63692500_1582037408.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/92996600_1582037408.png)



简单来说，看完了上述的源码之后，其实大家没必要立马搞定每一个代码组件的含义，以及他们是做什么的，主要知道BrokerController内部是有一系列的功能性组件的，还有一大堆的后台线程池，知道这两点就可以了，如下图。

​      ![03.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/76520900_1582037335.jpg)       

大家在上图可以看到，我又在图里进行了完善，让你能感受到，BrokerController里包含了一大堆核心功能组件和后台线程池，至于这些功能组件和后台线程池都是干什么的，不是你现在就要去探究的，后续我们要基于一个一个的场景出发，去研发他里面的各种代码组件是如何工作的。

**4、今日源码分析作业**

今天给大家留一个源码分析的小作业，希望每个人都可以去看一下BrokerController的构造函数里的内容，自己感受一下，这个里面包含了很多功能组件以及后台线程池，感受一下他的源码是怎么来写的，另外去感受一下Broker这个JVM进程，BrokerStartup启动组件，BrokerController管控组件之间的关系。

如果在分析源码的时候有什么心得，可以发布在评论区跟大家一起交流。

**End**

### 101 在初始化BrokerController的时候，都干了哪些事情？

**1、BrokerController创建完之后是在哪里初始化的？**

接着上一讲，我们继续说，现在大家已经了解到了Broker作为一个JVM进程启动之后，是BrokerStartup这个启动组件，负责初始化核心配置组件，然后启动了BrokerController这个管控组件。然后在BrokerController管控组件中，包含了一大堆的核心功能组件和后台线程池组件。

现在我们来看一下下面的图，已经表达了上面的意思。

![01.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5500800_1582162577.jpg)            

接着我们来看一下，那现在BrokerController都创建好了，里面的一大堆核心功能组件和后台线程池都创建好了，接下来他还要做一些初始化的工作，这个触发BrokerController初始化的代码在哪里呢？

其实还是在createBrokerController()方法里，在你创建完了BrokerController之后，就有一个初始化的代码，看下面的代码和注释。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29728500_1582163225.png)



**2、一步一步分析BrokerController初始化的过程**

接着我们一步一步的分析BrokerController初始化的过程，大家看下面的源码和注释就可以了，其实很多东西你现在看一下我写的注释有个了解就行了，真的不用过于的深究，有时候刚开始你深究过多了，就会导致你发现大脑一片混乱，最后就直接放弃看源码了，所以这里大致有一个BrokerController初始化的过程就行了。

下面就是BrokerController.initialize()方法的完整的源码分析，大家重点看注释。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/33531900_1582163251.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/87311500_1582163409.png)

看完上面那一大坨代码，大家有什么感觉？

要我说，感觉就是没感觉，其实很多人平时自己看源码，看到这里就开始痛苦了，觉得真的看不懂，整个人陷入极大的挫败感和痛苦之中。其实完全没必要，上述代码你其实有一个印象就可以了，不用现在过于较真。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51287900_1582164928.png)

讲到这里，我们先在下面的图里补充一下Netty服务器的概念，让大家看到，BrokerController里其实也会包含核心的Netty服务器，用来接收和处理Producer以及Consumer的请求。

​      ![02.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/25395700_1582162577.jpg) 

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/99481400_1582164947.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11254700_1582164976.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/59577400_1582165013.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/81541800_1582165038.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11673200_1582165068.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65702700_1582165092.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/22108000_1582165119.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/32001800_1582165590.png)

估计很多人看完了上面一大堆的处理请求的线程池的初始化和启动后台定时调度任务的代码，都一脸的懵逼，但是稍微找到点感觉了。

毕竟每个人都知道，后续Broker要处理一大堆的各种请求，那么不同的请求是不是要用不同的线程池里的线程来处理？

然后Broker要执行一大堆的后台定时调度执行的任务，这些后台定时任务是不是要通过线程池来调度定时任务？

所以其实你只要理解到这个程度就可以了，所以此时我们对下面的图又做了一些改动，在里面引入了两种线程池的概念，一种线程池是用来处理别人发送过来的请求的，一种线程池是执行后台定时调度任务的。

​      ![03.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/37754300_1582162577.jpg)      

我们接着往后看剩余的代码，很多代码可能大家未必立马就能理解，但是没关系，我们继续往下看。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/8711200_1582165611.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/87537400_1582165638.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/65051400_1582165662.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52388700_1582165662.png)

**3、今天的一点总结**

其实如果一定要我说，今天大家看完这些源码，跟着我的注释来走，一方面是对BrokerController初始化的过程有一个大致的印象，另外一方面其实最核心的，你要知道，BrokerController一旦初始化完成过后，他其实就准备好了Netty服务器，可以用于接收网络请求，然后准备好了处理各种请求的线程池，准备好了各种执行后台定时调度任务的线程池。

这些都准备好之后，明天我们就要来讲解BrokerController的启动了，他的启动，必然会正式完成Netty服务器的启动，他于是可以接收请求了，同时Broker必然会在完成启动的过程中去向NameServer进行注册以及保持心跳的。

只有这样，Producer才能从NameServer上找到你这个Broker，同时发送消息给你。

**4、今日源码分析作业**

今天就请大家自己去把BrokerController的initialize()方法看一下，把里面的源码流程和逻辑过一下，自己也去理解一下，抓住里面的重点，有什么心得体会，都可以发布在评论区跟大家一起交流。

**End**

### 102 BrokerContorller在启动的时候，都干了哪些事儿？

今天我们来给大家继续讲BrokerController的启动这块的源码。

现在BrokerController已经完成了初始化，他的用于实现各种功能的核心组件都已经初始化完毕了，然后负责接收请求的Netty服务器也初始化完毕了，同时负责处理请求的线程池以及执行定时调度任务的线程池，也都初始化完毕了，可以说是，万事俱备只欠东风了

我们看下图

​      ![001.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/95400_1582196827.jpg)

这个时候，就是最终要对BrokerContorller执行一下启动的逻辑，让他里面的一些功能组件完成启动时候需要执行的一些工作，同时最核心的，其实就是完成Netty服务器的启动，让他去监听一个端口号，可以接收别人的请求。

我们先回到BrokerStartup启动组件的main()方法中去，可以看看里面的内容：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/60439800_1582196994.png)

大家会发现上面的main()方法中，已经完成了BrokerContorller的初始化，接着就是执行了start()方法，于是我们进入start()方法可以去看看。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/95542400_1582197515.png)

大家自己看看上面的start()方法，其实别的业务逻辑倒没什么，最主要就是执行了BrokerContorller的start()方法，也就是去自动了他

我们继续看，下面就是BrokerContorller.start()方法的源码了，大家仔细看里面我写的注释，都解释了每一个步骤是干什么的。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43375300_1582197533.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/76139100_1582197548.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69338800_1582197548.png)

看完上述源码，大家其实从中只要提取一些核心的东西，知道说Netty服务器启动了，可以接收网络请求了，然后还有一个BrokerOuterAPI组件是基于Netty客户端发送请求给别人的，同时还启动一个线程去向NameServer注册，知道这几点就可以了。

在这里，我在下面的图里，就给大家展示出来了，BrokerOuterAPI和向NameServer注册这两个东西。

​      ![002.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/14531200_1582196827.jpg)       

大家其实只要看完本篇文章，能理解到上图中的架构，就足够了，因为其实阅读和理解其他人写出来的复杂的源码，是一件很困难的事情，很多时候自己写的代码过两年都看不懂了，何况是看别人写了几年的代码呢！

所以其实看源码的时候，很重要的一个技巧，就是一定要有耐心，而且要抓住这个开源系统运行的主要流程和逻辑，从源码里重点抓住主要的一些组件和主要的流程，而不是在看源码的时候陷入各种组件的细节里去。

源码又不是你写的，你说你假设这个时候去看BrokerOuterAPI、RemotingServer、FileWatchService、MessageStore这些核心组件的源码细节，你觉得你这个时候看得懂吗？

只能说在看到现在这个程度的时候，你大致脑子里有个印象，你知道Broker里有这么一些核心组件，都进行了初始化以及完成了启动，但是你应该最主要关注的事情是这么几个：

（1）Broker启动了，必然要去注册自己到NameServer去，所以BrokerOuterAPI这个组件必须要画到自己的图里去，这是一个核心组件

（2）Broker启动之后，必然要有一个网络服务器去接收别人的请求，此时NettyServer这个组件是必须要知道的

（3）当你的NettyServer接收到网络请求之后，需要有线程池来处理，你需要知道这里应该有一个处理各种请求的线程池

（4）你处理请求的线程池在处理每个请求的时候，是不是需要各种核心功能组件的协调？比如写入消息到commitlog，然后写入索引到indexfile和consumer queue文件里去，此时你是不是需要对应的一些MessageStore之类的组件来配合你？

（5）除此之外，你是不是需要一些后台定时调度运行的线程来工作？比如定时发送心跳到NameServer去，类似这种事情。

所以当你从一个很高的角度去思考了Broker的运行之后，再切入到他的源码里，你会发现，其实你可以很轻松的从源码运行流程里提取出来一些核心组件，画到你的图里去，然后你的脑子里会轻易记住一个图。

迄今为止，上面那幅图，就是你对Broker源码的一个了解。

接着再往后走，一定要从各种场景驱动，去理解RocketMQ的源码，包括Broker的注册和心跳，客户端Producer的启动和初始化，Producer从NameServer拉取路由信息，Producer根据负载均衡算法选择一个Broker机器，Producer跟Broker建立网络连接，Producer发送消息到Broker，Broker把消息存储到磁盘。

上面我说的那些东西，每一个都是RocketMQ这个中间件运行的时候一个场景，一定要从这些场景出发，一点点去理解在每一个场景下，RocketMQ的各个源码中的组件是如何配合运行的。

千万不要在看源码的时候，就傻乎乎的一个类一个类的看，那样绝对是会放弃阅读一个源码的！

**End**

### 103 第三个场景驱动：Broker是如何把自己注册到NameServer去的？

**1、Broker将自己注册到NameServer的入口**

上回我们讲到了BrokerController启动的过程，其实他本质就是启动了Netty服务器去接收网络请求，然后启动了一堆核心功能组件，启动了一些处理请求的线程池，启动了一些执行定时调度任务的后台线程，如下图所示，我们回顾一下。

​      ![0.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15982400_1582537660.jpg)       

当然，最为关键的一点，就是他执行了将自己注册到NameServer的一个过程，我们看一下这个注册自己到NameServer的源码入口，下面这行代码就是在BrokerController.start()方法中

BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());

因此如果我们要继续研究RocketMQ源码的话，当然应该场景驱动来研究，之前已经研究完了NameServer和Broker两个核心系统的启动场景，现在来研究第三个场景，就是Broker往NameServer进行注册的场景。

因为只有完成了注册，NameServer才能知道集群里有哪些Broker，然后Producer和Consumer才能找NameServer去拉取路由数据，他们才知道集群里有哪些Broker，才能去跟Broker进行通信！

**2、进入registerBrokerAll()方法去初步看一看**

接着我们就进入到registerBrokerAll()方法中初步的去看一看，大家看下面的源码片段，就是registerBrokerAll()方法的源码，我都写了详细的注释了，大家仔细看一下。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96002900_1582533610.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/58319100_1582533774.png)

**3、继续探索真正的进行Broker注册的方法**

接着我们继续探索真正进行Broker注册的方法，也就是下面的doRegisterBrokerAll()方法，我们进去可以先初步看一下方法的整体情况，我都写了详细的注释，大家也仔细看一看就行。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/25310300_1582533796.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/12192100_1582533996.png)

其实大家看完上面的代码，再看一下下面的图中，我用红圈圈出来的部分，你就会发现，在这里实际上就是通过BrokerOuterAPI去发送网络请求给所有的NameServer，把这个Broker注册了上去。

​      ![1.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96959000_1582537667.jpg)   

**4、深入到网络请求级别的Broker注册逻辑**

接着我们继续去看BrokerOuterAPI中的registerBrokerAll()方法，就是深入到了网络请求级别的Broker注册了，我给代码写了详细的注释，大家也是仔细看一看。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/22322300_1582534014.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6423500_1582534031.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/87665300_1582534030.png)

上面这段代码里，大家最主要的，是要提取出来RequestHeader和RequestBody两个概念，就是通过请求头和请求体构成了一个请求，然后会通过底层的NettyClient把这个请求发送到NameServer去进行注册

我们看下图，我加入了这个概念。

​      ![2.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/13397900_1582537676.jpg)       

**5、今日源码作业**

今天给大家布置一个源码小作业，就是希望大家能够自己在Intellij IDEA里，把今天给大家分析的Broker注册的初步的一些流程都自己看一下，尝试跟我一样，去从乱七八糟的源码里提取出来最重要和关键的一些概念。

比如你应该注意到的是Broker注册的时候，最为关键的BrokerOuterAPI这个组件，然后注意到他里面是对每个NameServer都执行了注册，包括他还构造了RequestHeader和RequestBody组成的请求去进行注册。

如果大家有什么分析源码的心得，可以在评论区里发出来。

**End**

### 104 深入探索BrokerOuter API是如何发送注册请求的？

**1、进入真正的注册请求方法去看看**

现在我们进入到真正的注册Broker的网络请求方法里去看看，其实入口就是下面这行代码：

RegisterBrokerResult result = registerBroker(

namesrvAddr,oneway, timeoutMills,requestHeader,body);

进入这个方法之后，会看到下面的一段代码，我们可以看看，我写了详细的注释，大家仔细看看我写的注释。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10941800_1582547752.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38169000_1582547752.png)

看到这里，大家先看看下面的图，有没有发现最终的请求是基于NettyClient这个组件给发送出去的？大家看下面的红圈处。

![001.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/62211300_1582547752.jpg)

**2、进入到NettyClient的网络请求方法中去看看**

接着我们进入到NettyClient的网络请求方法中去看看，大家仔细看下面的代码，我都写了详细的注释了。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/42658800_1582547773.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91122800_1582547862.png)



通过上面代码的分析，我现在在下面的图里，给大家再次加入一些东西，我通过Channel这个概念，表示出了Broker和NameServer之间的一个网络连接的概念，然后通过这个Channel就可以发送实际的网络请求出去！

​      ![002.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86769900_1582547752.jpg)       

**3、如何跟NameServer建立网络连接？**

接着我们进入上面的this.getAndCreateChannel(addr)这行代码看看，他是如何跟NameServer之间建立实际的网络连接的？

大家看下面的代码，下面的代码就是先从缓存里尝试获取连接，如果没有缓存的话，就创建一个连接。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91007600_1582548021.png)



那我们接着看下面的this.createChannel(addr)方法是如何实际通过一个NameServer的地址创建出来一个网络连接的吧。

我们看下面的代码，我写了详细的注释，大家仔细看里面的注释。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/60630600_1582548042.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/42774000_1582548054.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91208000_1582548207.png)

大家看下图，只要上面的Channel网络连接建立起来之后，我下面画红圈的地方，其实Broker和NameServer都会有一个Channel用来进行网络通信。

​      ![003.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/18972200_1582547753.jpg)       

**4、如何通过Channel网络连接发送请求？**

接着我们看看，如何通过Channel网络连接发送请求出去？

其实核心入口就是下面的方法，之前讲过了。

RemotingCommand response = this.invokeSyncImpl(channel, request, timeoutMillis - costTime);

我们进入这个方法去看看，他是如何发送网络请求出去的？我同样写了详细的注释，大家注意看我的注释就行，一些乱七八糟的代码如果暂时看不明白也没关系的，关键是抓住重点的逻辑。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/89705300_1582548224.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/70619400_1582548224.png)

其实上面的代码只要关注我写的几行注释就可以了，抓住重点，就知道，最终底层其实就是基于Netty的Channel API，把注册的请求给发送到了NameServer就可以了。

我们看下面的图，里面的红圈就展示了通过Channel发送网络请求出去的示意。

​      ![004.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/41387300_1582547753.jpg)       

**5、今日源码分析作业**

希望大家参考今天的文章，把Broker注册的时候，在NettyClient底层进行Channel网络连接建立，以及通过Channel连接把注册请求发送出去的这些逻辑，都自己看一遍，同时好好理解我文章里画出来的图。

大家一定要注意，看源码的每一行细节是一个过程，加深你的理解，但是最终记在你脑子里的，一定是一幅一幅的图，这才是最终你自己沉淀下来的东西

如果大家有什么源码分析心得，可以发在评论区进行交流。

**End**

### 105 NameServer是如何处理Broker的注册请求的？

上一次我们分析完了Broker启动的时候是如何通过BrokerOuterAPI发送注册请求到NameServer去的

大家看下图红圈的部分，我们可以回忆一下这个Broker发送注册请求的过程。

​      ![0.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78892800_1582638671.jpg)       

今天我们就来研究一下NameServer接收到这个注册请求之后，是如何进行处理的，这里要涉及到Netty网络通信相关的东西，可能很多人没接触过Netty，但是没关系，我尽量弱化掉Netty自身的东西，主要站在通用的网络通信的角度去讲解。

大家如果对Netty不了解的，对一些Netty自己的特殊API也不用过多的去关注，主要了解他的网络通信的流程就可以了。

现在我们回到NamesrvController这个类的初始化的方法里去，也就是NamesrvController.initialize()这个方法

我们看下面的一个源码片段就可以了，我省略了一些无关紧要的代码。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/78191600_1582638755.png)

我们继续看下面的registerProcessor()方法的源码。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/79556000_1582643691.png)

大家看完了上面的源码之后，我来给大家在图里感受一下，在下图中我们可以看到NettyServer是用于接收网络请求的，那么接收到的网络请求给谁处理呢？

其实就是给DefaultRequestProcessor这个请求处理组件来进行处理的。      ![1.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/319200_1582638672.jpg)       

所以我们如果要知道Broker注册请求是如何处理的，直接就是看DefaultRequestProcessor中的代码就可以了，下面给大家看一下这个类的一些源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61581000_1582644777.png)



接着我们进入这个类里的registerBroker()方法，去看看到底如何完成Broker注册的。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/15493800_1582644790.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43254200_1582644790.png)



下面我们先在图里给大家体现一下RouteInfoManager这个路由数据管理组件，实际Broker注册就是通过他来做的。

​      ![2.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/11970800_1582638672.jpg)       

至于RouteInfoManager的注册Broker的方法，我们就不带着大家来看了。这里给大家留一个今天的源码分析小作业，大家可以自己到RouteInfoManager的注册Broker的方法里去看看，最终如何把一个Broker机器的数据放入RouteInfoManager中维护的路由数据表里去的。

其实我这里提示一下，核心思路非常简单，无非就是用一些Map类的数据结构，去存放你的Broker的路由数据就可以了，包括了Broker的clusterName、brokerId、brokerName这些核心数据。

而且在更新的时候，一定会基于Java并发包下的ReadWriteLock进行读写锁加锁，因为在这里更新那么多的内存Map数据结构，必须要加一个写锁，此时只能有一个线程来更新他们才行！

大家看完这个Broker注册的最后一个步骤之后，也就是RouteInfoManager的注册过程之后，有什么源码分析的心得体会，都可以在评论区发表出来，跟大家一起交流。

**End**

### 106 Broker是如何发送定时心跳的，以及如何进行故障感知？

昨天我们已经讲解了NameServer处理Broker注册请求的源码流程，大家已经知道了，NameServer核心其实就是基于Netty服务器来接收Broker注册请求，然后交给DefaultRequestProcessor这个请求处理组件，来处理Broker注册请求。

而真正的Broker注册的逻辑是放在RouteInfoManager这个路由数据管理组件里来进行实现的，最终Broker路由数据都会存放在RouteInfoManager内部的一些Map数据结构组成的路由数据表中。

我们看下图，就是一个示意。      ![001.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/2337500_1582722387.jpg)       

今天我们就来讲讲，Broker是如何定时发送心跳到NameServer，让NameServer感知到Broker一直都存活着，然后如果Broker一段时间没有发送心跳到NameServer，那么NameServer是如何感知到Broker已经挂掉了。

首先我们看一下Broker中的发送注册请求给NameServer的一个源码入口，其实就是在BrokerController.start()方法中，在BrokerController启动的时候，他其实并不是仅仅发送一次注册请求，而是启动了一个定时任务，会每隔一段时间就发送一次注册请求。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/10482200_1582723065.png)

上面这块代码，其实是启动了一个定时调度的任务，他默认是每隔30s就会执行一次Broker注册的过程，上面的registerNameServerPeriod是一个配置，他默认的值就是30s一次。

所以其实大家看到这里就会明白，默认情况下，第一次发送注册请求就是在进行注册，就是我们上一讲讲的内容，他会把Broker路由数据放入到NameServer的RouteInfoManager的路由数据表里去。

但是后续每隔30s他都会发送一次注册请求，这些后续定时发送的注册请求，其实本质上就是Broker发送心跳给NameServer了，我们看下图示意。

​      ![002.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/26303100_1582722387.jpg)       

那么后续每隔30s，Broker就发送一次注册请求，作为心跳来发送给NameServer的时候，NameServer对后续重复发送过来的注册请求（也就是心跳），是如何进行处理的呢？

说到这里，我今天来带大家看一下RouteInfoManager的注册方法逻辑。

上一次是给大家留了作业，想必很多人可能都已经看出一点感悟来了，今天就我们一起来分析一下，下面是RouteInfoManager的注册Broker的源码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/81720300_1582723080.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/84136400_1582723092.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38460900_1582723105.png)

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/83593100_1582723323.png)

所以我们看下图，有一个红色圈圈示意了，每隔30s你发送注册请求作为心跳的时候，RouteInfoManager里会进行心跳时间刷新的处理。      ![003.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/46879700_1582722387.jpg)       

接着我们来思考最后一个问题，那么假设Broker已经挂了，或者故障了，隔了很久都没有发送那个每隔30s一次的注册请求作为心跳，那么此时NameServer是如何感知到这个Broker已经挂掉的呢？

我们重新回到NamesrvController的initialize()方法里去，里面有一个代码是启动了RouteInfoManager中的一个定时扫描不活跃Broker的线程。

t![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/97493200_1582723407.png)

上面这段代码，就是启动一个定时调度线程，每隔10s扫描一次目前不活跃的Broker，使用的是RouteInfoManager中的scanNotActiveBroke()方法，我们去看看那个方法的逻辑，就知道他如何感知到一个Broker挂掉了。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/94452500_1582723407.png)



今天给大家留一个源码分析的小作业，就是把Broker的注册、心跳以及故障发现的相关源码都看一遍，同时结合我们画的图，深刻的理解和记忆Broker跟NameServer的交互流程，核心组件。

**End**

### 107 我们系统中使用的Producer是如何创建出来的？

在之前的学习中，我们已经讲完了NameServer启动流程的相关源码，同时也分析出了NameServer启动之后的核心架构，如下图所示

大家可以看一下回顾一下，一定要记得，牢牢地抓住他里面的一些核心组件。 

​      ![001.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/45098700_1582856056.jpg)       

简单来说，NameServer启动之后，就会有一个核心的NamesrvController组件，他就是用于控制NameServer的所有行为的，包括内部启动一个Netty服务器去监听一个9876端口号，然后接收处理Broker和客户端发送过来的请求。

接着我们还学习了Broker启动过程的相关源码，也分析出了Broker启动之后的核心架构，我们如下图所示。      ![002.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/63588600_1582856056.jpg)       

简单来说，Broker启动之后，最核心的就是有一个BrokerController组件管控Broker的整体行为，包括初始化自己的Netty服务器用于接收客户端的网络请求，包括启动处理请求的线程池、执行定时任务的线程池，初始化核心功能组件，同时还会启动之后发送注册请求到NameServer去注册自己。

同时我们之前还讲解完了Broker启动之后进行注册以及定时发送注册请求作为心跳的机制，以及NameServer有一个后台进程定时检查每个Broker的最近一次心跳时间，如果长时间没心跳就认为Broker已经故障，我们看下图。

![003.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/86007900_1582856056.jpg)       

其实大家要知道一点，在讲完这些内容过后，你可以认为在我们的RocketMQ集群里，已经启动好了NameServer，而且还启动了一批Broker，同时Broker都已经把自己注册到NameServer里去了，NameServer也会去检查这批Broker是否存活。

其实此时我们不需要去关注NameServer和Broker干了别的什么事情，这个时候我们只要知道已经有了一个可用的RocketMQ集群就可以了，然后此时我们是不是就可以让自己开发好的系统去发送消息到MQ里去了？

没错，所以此时我们就需要引入一个Producer组件了，实际上，大家要知道，我们开发好的系统，最终都是要构建一个Producer组件，然后通过Producer去发送消息到MQ的Broker上去的

所以今天开始我们就来讲一下Producer这个组件的底层原理，当然先是得从Producer的构造开始了

既然要说Producer的构造，那肯定是要先回顾一下Producer是如何构造出来的，其实我们可以回顾一下下面的这块使用Producer发送消息到MQ的代码，就能清晰的看到Producer是如何构造出来的。

DefaultMQProducer producer = new DefaultMQProducer("order_producer_group");

producer.setNamesrvAddr("localhost:9876");

producer.start();

大家可以看到，其实构造Producer很简单，就是创建一个DefaultMQProducer对象实例，在其中传入你所属的Producer分组，然后设置一下NameServer的地址，最后调用他的start()方法，启动这个Producer就可以了。

其实创建DefaultMQProducer对象实例是一个非常简单的过程，无非就是创建这么一个对象出来，然后保存一下他的Producer分组。设置NameServer地址也是一个很简单的过程，无非就是保存一下NameServer地址罢了。 

其实最核心的还是调用了这个DefaultMQProducer的start()方法去启动了这个消息生产组件，那么这个start()都干了什么呢？

这个我们下周继续讲解，今天就作为一个承上启下的过程，大家知道我们目前对RocketMQ底层原理剖析到了哪个阶段，接下去要看哪个阶段就可以了。

**End**

### 108 构建好的Producer是如何启动准备好相关资源的？

上一次我们讲了一个承上启下的过程，就是NameServer已经启动，Broker已经启动以及完成了向NameServer的注册。

NameServer也会关注集群里的每个Broker的心跳，看Broker是否出现故障，然后我们说到，此时我们初始化出来了一个Producer组件，这些过程都是没问题的。

那么今天我们重点来分析一下这个Producer组件在启动的时候是如何准备好相关资源的，因为他必须内部得有独立的线程资源，还有得跟Broker建立网络连接，这样才能把我们的消息发送出去。

首先我想告诉大家的是，其实我们在构造Producer的时候，他内部构造了一个真正用于执行消息发送逻辑的组件，就是DefaultMQProducerImpl这个类的实例对象，所以其实我们要知道，真正的生产组件其实是这个组件。

那么这个组件在启动的时候都干了什么呢？

其实这里要告诉大家的一点是，如果我们在这里贴出来这个组件启动的源码，你会发现有大量的源码篇幅，他实际上在启动的时候做了大量的工作。

但是如果我们这里逐行逐行的去分析这个组件启动的源码，可能并没有什么必要，因为大量琐碎的启动和初始化的源码即使你现在看了也没太大的意义。很多东西我们都可以在后续的各种MQ使用场景的源码分析中去讲解，不用现在来看。

所以今天这篇文章我们其实不如去分析一些有意思的事情，主要站在Producer的核心行为的角度去看。

首先我们都知道一件事儿，假设我们后续要通过Producer发送消息，必然会指定我们要往哪个Topic里发送消息。所以我们也知道，Producer必然是知道Topic的一些路由数据的，比如Topic有哪些MessageQueue，每个MessageQueue在哪些Broker上。

我们其实看下面的一幅图，图里就表示出了，这个发送消息的时候必须要知道Topic路由数据的一个概念。

​      ![01.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/36048500_1583074705.jpg)

那么现在问题来了，到底是Producer刚启动初始化的时候，就会去拉取每个Topic的路由数据呢？还是等你第一次往一个Topic发送消息的时候再拉取路由数据呢？

其实答案是显而易见的，肯定不可能是刚初始化启动的时候就拉取Topic的路由数据，因为你刚开始启动的时候，不知道要发送消息到哪个Topic去啊！

所以这个问题，一定是在你第一次发送消息到Topic的时候，才会去拉取一个Topic的路由数据，包括这个Topic有几个MessageQueue，每个MessageQueue在哪个Broker上，然后从中选择一个MessageQueue，跟那台Broker建立网络连接，发送消息过去。

所以此时我们说第二个问题，Producer发送消息必然要跟Broker建立网络，这个是在Producer刚启动的时候就立马跟所有的Broker建立网络连接吗？

那必然也不是的，因为此时你也不知道你要跟哪个Broker进行通信。

所以其实很多核心的逻辑，包括Topic路有数据拉取，MessageQueue选择，以及跟Broker建立网络连接，通过网络连接发送消息到Broker去，这些逻辑都是在Producer发送消息的时候才会有。

所以我们根本没有必要对Producer的初始化过程做太过于详细的分析，那会让人陷入一大坨源码的细节，弄的人云里雾里的！因此，大家可以等着我们后续分析Producer的发送消息逻辑的时候，我们再一步一图的去分析Producer的完整工作原理。

**End**

### 109 当我们发送消息的时候，是如何从NameServer拉取Topic元数据的？

之前我们已经给大家讲到了发送消息到Broker的时候，使用的是Producer来发送，也大概介绍了一下Producer初始化的过程

其实初始化的过程极为的复杂，但是我们却真的不用过于的深究，因为其实比如拉取Topic的路由数据，选择MessageQueue，跟Broker构建长连接，发送消息过去，这些核心的逻辑，都是封装在发送消息的方法中的。

因此我们今天就从发送消息的方法开始讲起，实际上当你调用Producer的send()方法发送消息的时候，这个方法调用会一直到比较底层的逻辑里去，最终会调用到DefaultMQProducerImpl类的sendDefaultImpl()方法里去，在这个方法里，上来什么都没干，直接就有一行非常关键的代码，如下。

TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic());

其实看到这行代码，大家就什么都明白了，每次你发送消息的时候，他都会先去检查一下，这个你要发送消息的那个Topic的路由数据是否在你客户端本地

如果不在的话，必然会发送请求到NameServer那里去拉取一下的，然后缓存在客户端本地。

所以今天我们就重点来看看，这个Producer客户端运行在你的业务系统里的时候，他如何从NameServer拉取到你的Topic的路由数据的？

我们看下图的一个简单示意

​      ![01.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/38176400_1583721759.jpg)     

其实当你进入了this.tryToFindTopicPublishInfo(msg.getTopic())这个方法逻辑之后，会发现他的逻辑非常的简单

其实简单来说，他就是先检查了一下自己本地是否有这个Topic的路由数据的缓存，如果没有的话就发送网络请求到NameServer去拉取，如果有的话，就直接返回本地Topic路由数据缓存了，如下图的逻辑演示。

​      ![02.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23769900_1583721767.jpg)      

具体的一些源码细节，我们就不给大家贴出来了，其实看源码，一个是看源码的技巧，一个就是从源码里提取核心业务逻辑和流程，之前我们已经给大家讲了很多看源码的技巧了

大家只要按我们的思路去看，都能大致看懂源码，现在开始，更重要的是，我们使用狸猫技术窝的专栏特有的风格，就是一步一图的方式，用图给大家把源码的流程讲清楚！

所以接着我们当然很想知道的是，Producer到底是如何发送网络请求到NameServer去拉取Topic路由数据的，其实这里就对应了tryToFindTopicPublishInfo()方法内的一行代码，我们看看。

this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);

通过这行代码，他就可以去从NameServer拉取某个Topic的路由数据，然后更新到自己本地的缓存里去了。

具体的发送请求到NameServer的拉取过程，其实之前都大致讲解到了，简单来说，就是封装一个Request请求对象，然后通过底层的Netty客户端发送请求到NameServer，接收到一个Response响应对象。

然后他就会从Response响应对象里取出来自己需要的Topic路由数据，更新到自己本地缓存里去，更新的时候会做一些判断，比如Topic路由数据是否有改变过，等等，然后把Topic路由数据放本地缓存就可以了，我们看下图演示。

​     ![03.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/23947000_1583721774.jpg)       

看到这里，我想大家对于Producer是如何拉取Topic路由数据的，就应该很清楚了，说白了底层主要就是基于Netty去发送网络请求而已，并没什么太难的东西，然后就是一些本地缓存更新的逻辑，大家有兴趣，可以自己去看看对应的源码，有了专栏中的思路讲解，你看懂源码应该就很容易了。

**End**

### 110 对于一条消息，Producer是如何选择MessageQueue去发送的？

上一次我们讲完了Producer发送消息的时候，上来不管三七二十一，其实会先检查一下要发送消息的Topic的路由数据是否在本地缓存，如果不在的话，就会通过底层的Netty网络通信模块去发送一个请求到NameServer去拉取Topic路由数据，然后缓存在Producer的本地。

然后今天我们就来继续讲解这里的底层逻辑，之前已经跟大家说过了，我们现在已经转换了讲解的方式，之前是通过分析源码细节来讲解，重点是教会大家读源码的一些思路和方法，不要陷于细节，抓住主要流程，而且要懂得如何推测源码的意思。

但是RocketMQ的源码量实在太大了，根本不是一本书或者一个专栏能分析完的，所以昨天开始，我们已经切换了讲解方式，开始用一步一图的方式讲解RocketMQ的底层原理了，同时辅之以 一些源码的片段，让大家在理解底层原理和流程的同时，如果有兴趣，可以自己去阅读对应的源码细节。

那么今天我们应该继续讲解的是，当你拿到了一个Topic的路由数据之后，其实接下来就应该选择要发送消息到这个Topic的哪一个MessageQueue上去了！

因为大家都知道，Topic是一个逻辑上的概念，一个Topic的数据往往是分布式存储在多台Broker机器上的，因此Topic本质是由多个MessageQueue组成的。

每个MessageQueue都可以在不同的Broker机器上，当然也可能一个Topic的多个MessageQueue在一个Broker机器上，如下图所示。

​      ![04.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/56023600_1583721933.jpg)     

所以今天我们主要就是讲解，你要发送的消息，到底应该发送到这个Topic的哪个MessageQueue上去呢？

只要你知道了要发送消息到哪个MessageQueue上去，然后就知道这个MessageQueue在哪台Broker机器上，接着就跟那台Broker机器建立连接，发送消息给他就可以了。

之前给大家说过，发送消息的核心源码是在DefaultMQProducerImpl.sendDefaultImpl()方法中的，在这个方法里，只要你获取到了Topic的路由数据，不管从本地缓存获取的，还是从NameServer拉取到的，接着就会执行下面的核心代码。

MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);

这行代码其实就是在选择Topic中的一个MessageQueue，然后发送消息到这个MessageQueue去，在这行代码里面，实现了一些Broker故障自动回避机制，但是这个我们后续再讲，先看最基本的选择MessageQueue的算法



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/4432900_1583721933.png)



上面的代码其实非常的简单，他先获取到了一个自增长的index，大家注意到没有？

接着其实他核心的就是用这个index对Topic的MessageQueue列表进行了取模操作，获取到了一个MessageQueue列表的位置，然后返回了这个位置的MessageQueue。

说实话，你只要自己去试试就知道了，这种操作就是一种简单的负载均衡的算法，比如一个Topic有8个MessageQueue，那么可能第一次发送消息到MessageQueue01，第二次就发送消息到MessageQueue02，以此类推，就是轮询把消息发送到各个MessageQueue而已！

这就是最基本的MessageQueue选择算法，但是肯定有人会说了，那万一某个Broker故障了呢？此时发送消息到哪里去呢？

所以其实这个算法里有很多别的代码，都是实现Broker规避机制的，这个后续我们再讲。

我们先看下图，给大家展示了一个最基本的MessageQueue选择算法。      ![05.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/82619400_1583721933.jpg)

**End**

### 111 我们的系统与RocketMQ Broker之间是如何进行网络通信的？

上次给大家讲到了我们的Producer是如何从Topic路由数据中选择一个MessageQueue出来的，在选择一个MessageQueue出来之后，接着其实就应该要把消息投递到那个MessageQueue所在的Broker上去了，如下图。

![1.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/74978100_1583894945.jpg)      

所以今天我们就来看看，Producer是如何把消息发送给Broker的呢？

其实这块代码就在DefaultMQProducerImpl.sendDefaultImpl()方法中，在这个方法里，先是获取到了MessageQueue所在的broker名称，如下源码片段：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/96197100_1583894114.png)

获取到了这个brokerName之后，接着其实就可以使用如下的代码把消息投递到那个Broker上去了，看下面的代码片段：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/5532900_1583894115.png)



所以今天我们重点来看看这个代码里是如何把消息投递出去，在这个方法里，先有一些较为简单的逻辑，给大家看一下，如下所示：



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/64661100_1583894100.png)



上面的代码片段其实非常简单，就是通过brokerName去本地缓存找他的实际的地址，如果找不到，就去找NameServer拉取Topic的路由数据，然后再次在本地缓存获取broker的实际地址，你有这个地址了，才能给人家进行网络通信。

接下来的源码就很繁琐细节了，其实大家不用看也行，他就是用自己的方式去封装了一个Request请求出来，这里涉及到了各种信息的封装，包括了请求头，还有一大堆所有你需要的数据，都封装在Request里了。

他在这里做的事情，大体上包括了给消息分配全局唯一ID、对超过4KB的消息体进行压缩，在消息Request中包含了生产者组、Topic名称、Topic的MessageQueue数量、MessageQueue的ID、消息发送时间、消息的flag、消息扩展属性、消息重试次数、是否是批量发送的消息、如果是事务消息则带上prepared标记，等等。

总之，这里就是封装了很多很多的数据就对了，这些东西都封装到一个Request里去，然后在底层还是通过Netty把这个请求发送出去，发送到指定的Broker上去就可以了

这里Producer和Broker之间都是通过Netty建立长连接，然后基于长连接进行持续的通信的，如下图所示。

​      ![2.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/46731600_1583894953.jpg)       

其实对于我们而言，如果大家想要研究更加细致的源码细节，可以找到我说的那块代码，然后自己仔细的分析里面如何封装Request请求的细节，包括底层的基于Netty发送请求出去的细节，但是如果你不想看这些细节，那么从原理层面而言，你只要知道这个过程就可以了，另外比较重要的就是知道他底层是基于Netty发送的，如下图。

​      ![3.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/68781400_1583894962.jpg)       

对于Producer投递消息到Broker这个过程，大家了解到这里其实就可以了，接下来明天我们就应该继续去分析Broker通过Netty服务器接收到消息之后，在底层是如何进行处理的

这个过程将会比较复杂，涉及到CommitLog、ConsumeQueue、IndexFile、Checkpoint等一系列的机制，也是RocketMQ最核心的一块机制。

**End**

### 112 当Broker获取到一条消息之后，他是如何存储这条消息的？

其实之前我们就已经给大家在原理部分讲解过一些Broker收到消息之后的处理流程，简单来说，Broker通过Netty网络服务器获取到一条消息，接着就会把这条消息写入到一个CommitLog文件里去，一个Broker机器上就只有一个CommitLog文件，所有Topic的消息都会写入到一个文件里去，如下图所示。

![1.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/29744400_1585099154.jpg)   

然后同时还会以异步的方式把消息写入到ConsumeQueue文件里去，因为一个Topic有多个MessageQueue，任何一条消息都是写入一个MessageQueue的，那个MessageQueue其实就是对应了一个ConsumeQueue文件

所以一条写入MessageQueue的消息，必然会异步进入对应的ConsumeQueue文件，如下图。

​      ![2.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/51925700_1585099154.jpg)

同时还会异步把消息写入一个IndexFile里，在里面主要就是把每条消息的key和消息在CommitLog中的offset偏移量做一个索引，这样后续如果要根据消息key从CommitLog文件里查询消息，就可以根据IndexFile的索引来了，如下图。

![3.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75815900_1585099154.jpg)

接着我们来一步一步分析一下他在这里写入这几个文件的一个流程

首先Broker收到一个消息之后，必然是先写入CommitLog文件的，那么这个CommitLog文件在磁盘上的目录结构大致如何呢？看下面

CommitLog文件的存储目录是在${ROCKETMQ_HOME}/store/commitlog下的，里面会有很多的CommitLog文件，每个文件默认是1GB大小，一个文件写满了就创建一个新的文件，文件名的话，就是文件中的第一个偏移量，如下面所示。文件名如果不足20位的话，就用0来补齐就可以了。

00000000000000000000

000000000003052631924

在把消息写入CommitLog文件的时候，会申请一个putMessageLock锁

也就是说，在Broker上写入消息到CommitLog文件的时候，都是串行的，不会让你并发的写入，并发写入文件必然会有数据错乱的问题，下面是源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/91033500_1585099154.png)



接着其实会对消息做出一通处理，包括设置消息的存储时间、创建全局唯一的消息ID、计算消息的总长度，然后会走一段很关键的源码，把消息写入到MappedFile里去，这个其实我们之前还讲解过里面的黑科技，看下面的源码。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/89390700_1585099153.png)



上面源码片段中，其实最关键的是cb.doAppend()这行代码，这行代码其实是把消息追加到MappedFile映射的一块内存里去，并没有直接刷入磁盘中，如下图所示。

![4.jpg](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/30921400_1585099155.jpg)

至于具体什么时候才会把内存里的数据刷入磁盘，其实要看我们配置的刷盘策略，这个我们后续会讲解，另外就是不管是同步刷盘还是异步刷盘，假设你配置了主从同步，一旦你写入完消息到CommitLog之后，接下来都会进行主从同步复制的。

那今天我们内容就讲到这里，其实到这里为止，就初步的讲完了Broker收到一条消息之后的处理流程了，先写入CommitLog中去。下一次我们讲解CommitLog的刷盘策略以及主从复制机制，然后接着再讲异步把消息写入ConsumeQueue和IndexFile里去。

**End**

### 113 一条消息写入CommitLog文件之后，如何实时更新索引文件？

昨天我们讲到，Broker收到一条消息之后，其实就会直接把消息写入到CommitLog里去，但是他写入刚开始仅仅是写入到MappedFile映射的一块内存里去，后续是根据刷盘策略去决定是否立即把数据从内存刷入磁盘的，我们看下图。

​      ![1131.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k89k3y2w0tci5y9bbnp.jpg)       

关于这个同步刷盘和异步刷盘的问题，我们后续再讲，今天先来说说，这个消息写入CommitLog之后，然后消息是如何进入ConsumeQueue和IndexFile的。

实际上，Broker启动的时候会开启一个线程，ReputMessageService，他会把CommitLog更新事件转发出去，然后让任务处理器去更新ConsumeQueue和IndexFile，如下图。

​      ![1132.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k89k3y2x0ezte0ctm7it.jpg)       

我们看下面的源码片段，在DefaultMessageStore的start()方法里，在里面就是启动了这个ReputMessageService线程。

这个DefaultMessageStore的start()方法就是在Broker启动的时候调用的，所以相当于是Broker启动就会启动这个线程。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/61612300_1585275571.png)

下面我们看这个ReputMessageService线程的运行逻辑，源码片段如下所示。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/27268000_1585275632.png)

也就是说，在这个线程里，每隔1毫秒，就会把最近写入CommitLog的消息进行一次转发，转发到ConsumeQueue和IndexFile里去，通过的是doReput()方法来实现的，我们再看doReput()方法里的实现逻辑，先看下面源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/17793800_1585275683.png)



这段代码意思非常的清晰明了，就是从commitLog中去获取到一个DispatchRequest，拿到了一份需要进行转发的消息，也就是从CommitLog中读取的，我们画在下面示意图里

​      ![1133.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k89k3y2x0tog5u9y3vaa.jpg)       

接着他就会通过下面的代码，调用doDispatch()方法去把消息进行转发，一个是转发到ConsumeQueue里去，一个是转发到IndexFile里去

大家看下面的源码片段，里面走了CommitLogDispatcher的循环



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/85788500_1585275717.png)

实际上正常来说这个CommitLogDispatcher的实现类有两个，分别是CommitLogDispatcherBuildConsumeQueue和CommitLogDispatcherBuildIndex，他们俩分别会负责把消息转发到ConsumeQueue和IndexFile，我画在下图中：

​      ![1134.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k89k3y2x0yqgltiyyhz.jpg)       

接着我们看一下ConsumeQueueDispatche的源码实现逻辑，其实非常的简单，就是找到当前Topic的messageQueueId对应的一个ConsumeQueue文件

一个MessageQueue会对应多个ConsumeQueue文件，找到一个即可，然后消息写入其中。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/6442600_1585275745.png)

再来看看IndexFile的写入逻辑，其实也很简单，无非就是在IndexFile里去构建对应的索引罢了，如下面的源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/55123500_1585275762.png)

因此到这里为止，我想大家基本就看明白了，当我们把消息写入到CommitLog之后，有一个后台线程每隔1毫秒就会去拉取CommitLog中最新更新的一批消息，然后分别转发到ConsumeQueue和IndexFile里去，这就是他底层的实现原理。

那么明天我们再来继续看同步刷盘和异步刷盘的实现。

**End**

### 114 RocketMQ是如何实现同步刷盘以及异步刷盘两种策略的？

上一次我们已经给大家讲解完了数据写入到Broker之后的存储流程，包括数据直接写入CommitLog，而且直接进入的是MappedFile映射的一块内存，不是直接进入磁盘，同时有一个后台线程会把CommitLog里更新的数据给写入到ConsumeQueue和IndexFile里去，如下图所示      ![1141.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k89k3y2x0ws9n7ic3lqo.jpg)

我们之前简单提过一次，写入CommitLog的数据进入到MappedFile映射的一块内存里之后，后续会执行刷盘策略

比如是同步刷盘还是异步刷盘，如果是同步刷盘，那么此时就会直接把内存里的数据写入磁盘文件，如果是异步刷盘，那么就是过一段时间之后，再把数据刷入磁盘文件里去。

那么今天我们来看看底层到底是如何执行不同的刷盘策略的。

大家应该还记得之前我们说过，往CommitLog里写数据的时候，是调用的CommitLog类的putMessage()这个方法吧？

没错的，其实在这个方法的末尾有两行代码，很关键的，大家看一下下面的源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/71966000_1585275845.png)

大家会发现在末尾有两个方法调用，一个是handleDishFlush()，一个是handleHA()

顾名思义，一个就是用于决定如何进行刷盘的，一个是用于决定如何把消息同步给Slave Broker的。

关于消息如何同步给Slave Broker，这个我们就不看了，因为涉及到Broker高可用机制，这里展开说就太多了，其实大家有兴趣可以自己慢慢去研究，我们这里主要就是讲解一些RocketMQ的核心源码原理。

所以我们重点进入到handleDiskFlush()方法里去，看看他是如何处理刷盘的。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21924200_1585276124.png)



上面代码我们就看的很清晰了，其实他里面是根据你配置的两种不同的刷盘策略分别处理的，我们先看第一种，就是同步刷盘的策略是如何处理的。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/43354200_1585527720.png)



其实上面就是构建了一个GroupCommitRequest，然后提交给了GroupCommitService去进行处理，然后调用request.waitForFlush()方法等待同步刷盘成功

万一刷盘失败了，就打印日志。具体刷盘是由GroupCommitService执行的，他的doCommit()方法最终会执行同步刷盘的逻辑，里面有如下代码。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21434100_1585276139.png)

上面那行代码一层一层调用下去，最终刷盘其实是靠的MappedByteBuffer的force()方法，如下所示。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/374800_1585276152.png)

这个MappedByteBuffer就是JDK NIO包下的API，他的force()方法就是强迫把你写入内存的数据刷入到磁盘文件里去，到此就是同步刷盘成功了。

那么如果是异步刷盘呢？我们先看CommitLog.handleDiskFlush()里的的代码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/21016300_1585276186.png)

其实这里就是唤醒了一个flushCommitLogService组件，那么他是什么呢？看下面的代码片段。

FlushCommitLogService其实是一个线程，他是个抽象父类，他的子类是CommitRealTimeService，所以真正唤醒的是他的子类代表的线程。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/52355200_1585276220.png)



具体在子类线程的run()方法里就有定时刷新的逻辑，这里就不赘述了，这里留做大家的课下作业。

其实简单来说，就是每隔一定时间执行一次刷盘，最大间隔是10s，所以一旦执行异步刷盘，那么最多就是10秒就会执行一次刷盘。

好了，到此为止，我们把CommitLog的同步刷盘和异步刷盘两种策略的核心源码也讲解完了。我们主要是讲解的核心源码，而源码里很多细节不可能一行一行进行分析，大家可以顺着文中的思路继续探究。

**End**

### 115 当Broker上的数据存储超过一定时间之后，磁盘数据是如何清理的？

上一次我们基本讲完了broker收到数据之后是如何把消息写入到磁盘文件里去的，如下图所示

![1151.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmief50e4qqwwuswkj.jpg)

那么今天我们就来探讨另外一个问题，就是broker不停的接收数据，然后磁盘上的数据越来越多，但是万一磁盘都放满了，那怎么办呢？

所以今天就要来讲一下，这个broker是如何把磁盘上的数据给删掉的。            

先简单给大家说一下，其实默认broker会启动后台线程，这个后台线程会自动去检查CommitLog、ConsumeQueue文件，因为这些文件都是多个的，比如CommitLog会有多个，ConsumeQueue也会有多个。

然后如果是那种比较旧的超过72小时的文件，就会被删除掉，也就是说，默认来说，broker只会给你把数据保留3天而已，当然你也可以自己通过fileReservedTime来配置这个时间，要保留几天的时间。

这个定时检查过期数据文件的线程代码，在DefaultMessageStore这个类里，他的start()方法中会调用一个addScheduleTask()方法，里面会每隔10s定时调度执行一个后台检查任务，我们看下面的源码片段。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/32470700_1585642601.png)



上面就可以看到了，其实他是每隔10s，就会执行一个调度任务

这个调度任务里就会执行DefaultMessageStore.this.cleanFilesPeriodically()方法，其实就是会去周期性的清理掉磁盘上的数据文件，也就是超过72小时的CommitLog、ConsumeQueue文件，如下图所示。

![1152.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmijg70ty82cepk6.jpg)       

接着我们具体看看这里的清理逻辑，他其实里面包含了清理CommitLog和ConsumeQueue的清理逻辑，如下面源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/12169300_1585642634.png)



在清理文件的时候，他会具体判断一下，如果当前时间是预先设置的凌晨4点，就会触发删除文件的逻辑，这个时间是默认的；或者是如果磁盘空间不足了，就是超过了85%的使用率了，立马会触发删除文件逻辑。

上面两个条件，第一个是说如果磁盘没有满 ，那么每天就默认一次会删除磁盘文件，默认就是凌晨4点执行，那个时候必然是业务低峰期，因为凌晨4点大部分人都睡觉了，无论什么业务都不会有太高业务量的。

第二个是说，如果磁盘使用率超过85%了，那么此时可以允许继续写入数据，但是此时会立马触发删除文件的逻辑；如果磁盘使用率超过90%了，那么此时不允许在磁盘里写入新数据，立马删除文件。这是因为，一旦磁盘满了，那么你写入磁盘会失败，此时你MQ就彻底故障了。

所以一旦磁盘满了，也会立马删除文件的。

在删除文件的时候，无非就是对文件进行遍历，如果一个文件超过72小时都没修改过了，此时就可以删除了，哪怕有的消息你可能还没消费过，但是此时也不会再让你消费了，就直接删除掉。

这就是RocketMQ的一整套文件删除的逻辑和机制。

**End**

### 116 我们系统中的Consumer作为消费者是如何创建出来的？

之前我们已经讲完了RocketMQ的Broker这块的一些源码和原理，源码没讲的太细，因为源码量实在是太多了，所以我们只能讲一些重点的片段

但是起码我们现在已经知道了，我们平时把消息写入到Broker去，他会把消息写入到CommitLog、ConsumeQueue、IndexFile里去，如下图。      ![1161.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqc8403xvdtzzski3.jpg)       

那么现在Broker上有了数据了，接着当然是某个业务系统里会启动一个Consumer，指定自己要消费哪个Topic的数据

接着Consumer就会从指定的Topic上消费数据过来了，然后消息交给你的业务代码来处理，如下图。

​      ![1162.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqc850mwm0wt38dhm.jpg)

那么这次我们来看看这个业务系统里的Cosumer是如何创建和启动的呢？

其实我们平时创建的一般都是DefaultMQPushConsumerImpl，然后会调用他的start()方法来启动他，那么今天我们就来看看启动Consumer的时候都会干什么。

首先在启动的时候，会看到如下一行源码片段：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/22684100_1585642789.png)

不知道大家对这个MQClientFactory有没有什么感觉？

说实话，你可以想一下，这个Consumer一旦启动，必然是要跟Broker去建立长连接的，底层绝对也是基于Netty去做的，建立长连接之后，才能不停的通信拉取消息

所以这个MQClientFactory底层直觉上就应该封装了Netty网络通信的东西，如下图所示。      ![1163.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqc8506yn1q16p9dq.jpg)

接着我们会看到如下的一些源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/44023400_1585642822.png)



大家看到上述源码有什么感触，是不是发现似乎在搞一个叫做RebalanceImpl的东西，还给他设置了Consumer分组，还有MQClientFactory在里面

那么这个东西，其实大家一看名字就应该知道了，他就是专门负责Consumer重平衡的。

假设你的ConsumerGroup里加入了一个新的Consumer，那么就会重新分配每个Consumer消费的MessageQueue，如果ConsumerGroup里某个Consumer宕机了，也会重新分配MessageQueue，这就是所谓的重平衡，如下图。      ![1164.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqc850be5q9mljiws.jpg)接着我们看如下源码片段。



![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/69927600_1585642852.png)



这个PullAPIWrapper大家觉得是什么呢？看起来是不是很像是专门用来拉取消息的API组件？

对的，其实这个一看就是用来拉取消息的，如下图。

​      ![1165.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqc850theqa94bwu.jpg)     

接着大家看如下的源码片段。

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/2029800_1585708579.png)



有没有发现他在弄一个叫做OffsetStore的东西呢？

这个东西一看，顾名思义，就是用来存储和管理Consumer消费进度offset的一个组件，如下图。

![1166.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8fmqcyn0cz9guda0gwm.jpg)       

接下来源码里还有一些东西，其实都不是太核心的了，最核心的无非就是这三个组件，首先Consumer刚启动，必须依托Rebalancer组件，去进行一下重平衡，自己要分配一些MessageQueue去拉取消息。

接着拉取消息，必须要依托PullAPI组件通过底层网络通信去拉取。在拉取的过程中，必然要维护offset消费进度，此时就需要OffsetStore组件。万一要是ConsumerGroup里多了Consumer或者少了Consumer，又要依托Rebalancer组件进行重平衡了。

基本就是这样一个思路，下一次我们继续分析，接下来几讲我们分析完Consumer的一些源码实现，那么对RocketMQ的核心源码的一些思路，我们就理解的差不多了。

**End**

### 117 一个消费组中的多个Consumer是如何均匀分配消息队列的？

今天来给大家讲解一下当你一个业务系统部署多台机器的时候，每个系统里都启动了一个Consumer，多个Consumer会组成一个ConsumerGroup，也就是消费组，此时就会有一个消费组内的多个Consumer同时消费一个Topic，而且这个Topic是有多个MessageQueue分布在多个Broker上的，如下图所示      ![1171.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8ix68vp0awntslx5f9p.jpg)

那么现在问题就来了，假设咱们一个业务系统部署在两台机器上，对应一个消费组里就有两个Consumer，那么现在一个Topic有三个MessageQueue，该怎么分配呢？

这就涉及到了**Consumer的负载均衡**的问题了。

不知道大家是否还记得我们上一次讲Consumer启动的时候，讲到了几个关键的组件，分别是重平衡组件、消息拉取组件、消费进度组件

其实里面有一个Balancer重平衡组件，就是在这里专门负责多个Consumer的负载均衡的，如下图。     ![1172.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8ix68vp0fd20s64ky24.jpg)

那么这个RebalancerImpl重平衡组件是如何将多个MessageQueue均匀的分配给一个消费组内的多个Consumer的呢？

实际上，每个Consumer在启动之后，都会干一件事情，就是向所有的Broker进行注册，并且持续保持自己的心跳，让每个Broker都能感知到一个消费组内有哪些Consumer，如下图。      ![1173.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8ix68vp0d1qfq1b2igu.jpg) 

上图里没法画出来Consumer向每个Broker进行注册以及心跳，只能大致示意一下，大家理解这个意思就好

然后呢，每个Consumer在启动之后，其实重平衡组件都会随机挑选一个Broker，从里面获取到这个消费组里有哪些Consumer存在，如下图。

​      ![1174.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8ix68vq0m2sf6ra7thq.jpg)       



此时重平衡组件一旦知道了消费组内有哪些Consumer之后，接着就好办了，无非就是把Topic下的MessageQueue均匀的分配给这些Consumer了，这个时候其实有几种算法可以进行分配，但是比较常用的一种算法就是简单的平均分配。

比如现在一共有3个MessageQueue，然后有2个Consumer，那么此时就会给1个Consumer分配2个MessageQueue，同时给另外1个Consumer分配剩余的1个MessageQueue。

假设有4个MessageQueue的话，那么就可以2个Consumer每个人分配2个MessageQueue了

总之，一切都是平均分配的，尽量保证每个Consumer的负载是差不多的。

这样的话，一旦MessageQueue负载确定了之后，下一步其实Consumer就知道自己要消费哪几个MessageQueue的消息了，就可以连接到那个Broker上去，从里面不停的拉取消息过来进行消费了，至于如何拉取消息消费，那就是下一次要讲的了。

讲完了拉取消息消费，我们再讲一下消费进度的管理，那么此时RocketMQ最最核心的底层原理就讲清楚了。

**End**

### 118 Consumer是如何从Broker上拉取一批消息过来处理的？

今天要讲的内容就是本专栏的最后一篇内容了，这篇内容我寻思良久，觉得其实并没有必要重新写，因为之前的文章里有一篇已经把这个消费的机制讲解的很清楚了，这里权且当做复习，放在最后一篇文章，大家再看一次，同时在最后我们将会对消费这块的源码做一点提示，大家可以自己去慢慢看。

**1、消费组到底是个什么概念？**

在理解了Broker数据存储机制以及Broker高可用主从同步机制之后，我们就可以来看一下消费者是如何从Broker获取消息，并且进行处理以及维护消费进度的。首先，我们需要了解的第一个概念，就是消费者组。

消费者组的意思，就是让你给一组消费者起一个名字。比如说我们有一个Topic叫做“TopicOrderPaySuccess”，那么假设有库存系统、积分系统、营销系统、仓储系统他们都要去消费这个Topic中的数据。

此时我们应该给那四个系统分别起一个消费组的名字，比如说：stock_consumer_group，marketing_consumer_group，credie_consumer_group，wms_consumer_group。

设置消费组的方式是在代码里进行的，类似下面这样：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/40091600_1586340313.png)

然后比如库存系统部署了4台机器，每台机器上的消费者组的名字都是“stock_consumer_group”，那么这4台机器就同属于一个消费者组，以此类推，每个系统的几台机器都是属于各自的消费者组的。

我们看一下下面的图，里面我示意了两个系统，每个系统都有2台机器，每个系统都有一个自己的消费组。      ![1181.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xwjy0wlxnn1uei7m.jpg)

然后给大家先解释一下不同消费者之间的关系，假设库存系统和营销系统作为两个消费者组，都订阅了“TopicOrderPaySuccess”这个订单支付成功消息的Topic，那么此时假设订单系统作为生产者发送了一条消息到这个Topic，如下图所示。

​      ![1182.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xwjy0fxe4m9ci77v.jpg)

那么此时这条消息是怎么被消费的呢？

其实正常情况下来说，这条消息进入Broker之后，库存系统和营销系统作为两个消费组，每个组都会拉取到这条消息。

也就是说这个订单支付成功的消息，库存系统会获取到一条，营销系统也会获取到一条，他们俩都会获取到这条消息。

但是下一个问题来了，库存系统这个消费组里，他有两台机器，是两台机器都获取到这条消息？还是说只有一台机器会获取到这条消息？

答案是，正常情况下来说，库存系统的两台机器中只有一台机器会获取到这条消息，营销系统也是同理。

我们看下面的图，示意了对于一条订单支付成功的消息，库存系统的一台机器获取到了，营销系统的一台机器也获取到了。当然为了画图方便，图里是让营销系统从Master Broker拉取的，库存系统从Slave Broker拉取的。      ![1183.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xwjz0mjgh6dthst.jpg)       

这就是在消费的时候我们要给大家介绍的第一个知识点，不同的系统应该设置不同的消费组，如果不同的消费组订阅了同一个Topic，对Topic里的一条消息，每个消费组都会获取到这条消息。

**2、集群模式消费 vs 广播模式消费**

接着我们给大家介绍下一个概念，就是对于一个消费组而言，他获取到一条消息之后，如果消费组内部有多台机器，到底是只有一台机器可以获取到这个消息，还是每台机器都可以获取到这个消息？

这个就是集群模式和广播模式的区别。

默认情况下我们都是集群模式，也就是说，一个消费组获取到一条消息，只会交给组内的一台机器去处理，不是每台机器都可以获取到这条消息的。

但是我们可以通过如下设置来改变为广播模式：

consumer.setMessageModel(MessageModel.BROADCASTING);

如果修改为广播模式，那么对于消费组获取到的一条消息，组内每台机器都可以获取到这条消息。但是相对而言广播模式其实用的很少，常见基本上都是使用集群模式来进行消费的。

**3、重温MessageQueue、CommitLog、ConsumeQueue之间的关系**

接着我们来研究一下MessageQueue与消费者的关系，通过之前的学习我们都已经知道了，一个Topic在创建的时候我们是要设置他有多少个MessageQueue的，而且我们也知道了，在Broker上MessageQueue是如何跟ConsumeQueue对应起来的。

我们先在图中展示出来这些概念，在Broker上我们会看到CommitLog文件，还有对应的多个ConsumeQueue文件。    ![1184.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xwjz0p6uezfgb1e.jpg)       

根据之前学习到的知识，我们大致可以如此理解，Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。

我们看下图中，我圈出来了ConsumeQueue，就代表了一个Topic的多个MessageQueue在物理磁盘上分别对应一个ConsumeQueue的意思。      ![1185.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xwjz0cav966vsz5.jpg)       

但是对于一个Broker机器而言，存储在他上面的所有Topic以及MessageQueue的消息数据都是写入一个统一的CommitLog的，我们看下面的图，我圈出来了CommitLog，代表的是Broker上所有消息都是往里面写的。      ![1186.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xxec0uq7q8ockwqe.jpg)       

然后对于Topic的各个MessageQueue而言，就是通过各个ConsumeQueue文件来存储属于MessageQueue的消息在CommitLog文件中的物理地址，就是一个offset偏移量，我在下面的图中标识出来了这个地址应用的关系。      ![1187.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xxek06g2sjmaiai.jpg)       

**4、MessageQueue与消费者的关系**

接着我们来想一个问题，对于一个Topic上的多个MessageQueue，是如何由一个消费组中的多台机器来进行消费的呢？

其实这里的源码实现细节是较为复杂的，但是我们可以简单的大致理解为，他会均匀的将MessageQueue分配给消费组的多台机器来消费。

举个例子，假设我们的“TopicOrderPaySuccess”有4个MessageQueue，这4个MessageQueue分布在两个Master Broker上，每个Master Broker上有2个MessageQueue。

然后库存系统作为一个消费组里有两台机器，那么正常情况下，当然最好的就是让这两台机器每个都负责2个MessageQueue的消费了，比如库存系统的机器01从Master Broker01上消费2个MessageQueue，然后库存系统的机器02从Master Broker02上消费2个MessageQueue，这样不就把消费的负载均摊到两台Master Broker上去了？

我们在下面的图里画出了这个示意。

​      ![1188.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xxjf05pxrnpuuxvk.jpg)       

所以你大致可以认为一个Topic的多个MessageQueue会均匀分摊给消费组内的多个机器去消费，这里的一个原则就是，一个MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理。

**5、Push模式 vs Pull模式**

现在我们已经知道了一个消费组内的多台机器是分别负责一部分MessageQueue的消费的，那么既然如此，每台机器都必须去连接到对应的Broker，尝试消费里面的MessageQueue对应的消息了。

此时就要涉及到两种消费模式了，之前我们也提到过，一个是Push，一个是Pull。实际上，这两个消费模式本质是一样的，都是消费者机器主动发送请求到Broker机器去拉取一批消息下来。

Push消费模式本质底层也是基于这种消费者主动拉取的模式来实现的，只不过他的名字叫做Push而已，意思是Broker会尽可能实时的把新消息交给消费者机器来进行处理，他的消息时效性会更好。

一般我们使用RocketMQ的时候，消费模式通常都是基于他的Push模式来做的，因为Pull模式的代码写起来更加的复杂和繁琐，而且Push模式底层本身就是基于消息拉取的方式来做的，只不过时效性更好而已。

Push模式的实现思路我们这里简单说一下：当消费者发送请求到Broker去拉取消息的时候，如果有新的消息可以消费那么就会立马返回一批消息到消费机器去处理，处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

所以消费机器在Push模式下会处理完一批消息，立马发起请求拉取下一批消息，消息处理的时效性非常好，看起来就跟Broker一直不停的推送消息到消费机器一样。

另外Push模式下有一个请求挂起和长轮询的机制，也要给大家简单介绍一下。

当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会让请求线程挂起，默认是挂起15秒，然后这个期间他会有后台线程每隔一会儿就去检查一下是否有的新的消息给你，另外如果在这个挂起过程中，如果有新的消息到达了会主动唤醒挂起的线程，然后把消息返回给你。

当然其实消费者进行消息拉取的底层源码是非常复杂的，涉及到大量的细节，但是他的核心思路大致就是如此，我们只要知道，其实哪怕是用常见的Push模式消费，本质也是消费者不停的发送请求到broker去拉取一批一批的消息就行了。

**6、Broker是如何将消息读取出来返回给消费机器的？**

接着我们思考一个小问题，Broker在收到消费机器的拉取请求之后，是如何将消息读取出来返回给消费机器的？其实这里要涉及到两个概念，分别是ConsumeQueue和CommitLog。

假设一个消费者机器发送了拉取请求到Broker了，他说我这次要拉取MessageQueue0中的消息，然后我之前都没拉取过消息，所以就从这个MessageQueue0中的第一条消息开始拉取好了。

于是，Broker就会找到MessageQueue0对应的ConsumeQueue0，从里面找到第一条消息的offset，如下图所示。

​      ![1189.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xxr20lu2mpryc5fk.jpg)       

接着Broker就需要根据ConsumeQueue0中找到的第一条消息的地址，去CommitLog中根据这个offset地址去读取出来这条消息的数据，然后把这条消息的数据返回给消费者机器，如下图所示。

​      ![1190.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xxxc0kb4zkc3j5y9.jpg)       

所以其实消费消息的时候，本质就是根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

**7、消费者机器如何处理消息、进行ACK以及提交消费进度？**

接着消费者机器拉取到一批消息之后，就会将这批消息回调我们注册的一个函数，如下面这样子：

![image.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/84494800_1586340409.png)



当我们处理完这批消息之后，消费者机器就会提交我们目前的一个消费进度到Broker上去，然后Broker就会存储我们的消费进度，比如我们现在对ConsumeQueue0的消费进度假设就是在offset=1的位置，那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度，如下图。

​      ![1191.jpg](http://wechatapppro-1252524126.file.myqcloud.com/apppuKyPtrl1086/image/k8r5xy720les0jyi39dg.jpg)       

那么下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

**8、如果消费组中出现机器宕机或者扩容加机器，会怎么处理？**

最后我们来看一下，如果消费组中出现机器宕机或者扩容加机器的情况，他会怎么处理？

这个时候其实会进入一个rabalance的环节，也就是说重新给各个消费机器分配他们要处理的MessageQueue。

给大家举个例子，比如现在机器01负责MessageQueue0和Message1，机器02负责MessageQueue2和MessageQueue3，现在机器02宕机了，那么机器01就会接管机器02之前负责的MessageQueue2和MessageQueue3。

或者如果此时消费组加入了一台机器03，此时就可以把机器02之前负责的MessageQueue3转移给机器03，然后机器01就仅仅负责一个MessageQueue2的消费了，这就是负载重平衡的概念。

**9、消费源码流程提示**

我们最后就对消费这块的源码流程做一点提示，首先，拉取消息的源码入口是在DefaultMQPushConsumerImpl类的pullMessage()方法中的，这个里面涉及到了拉取请求、消息流量控制、通过PullAPIWrapper与服务端进行网络交互、服务端根据ConsumeQueue文件拉取消息，等一系列的事情，大家如果要分析Consumer端拉取消息的流程，可以从这个方法入口去看看。

到此为止，其实我们的MQ专栏整体就完成了，下次最后一篇文章，我会给大家做一个总结，同时告诉大家如何深度的炼化和理解本专栏的内容，以及把里面的内容运用到面试中去。

**End**

### 119 专栏结束语：你从消息中间件专栏中学到了什么，如何运用到面试中去？

今天是我们本专栏的最后一篇文章，也是我们本专栏的结束语，最后我们来给大家总结一下，如果你认认真真的跟着专栏学下来，你都学到了什么，然后你应该如何消化吸收，总结，如何运用到自己的面试中去。

首先其实在专栏中，我想每个人都应该从0开始了解到了RocketMQ的技术原理，包括应该如何搭建一套生产级的RocketMQ集群，你要用MQ，首先你得理解MQ背后的工作原理，同时你还应该知道MQ集群是如何搭建和部署的，你接下来才能考虑如何使用。

接着我们学习了一些业务系统中的实际案例中的问题，并且针对这些实际的案例问题，设计了基于MQ的解决方案，然后教会了大家如何把这些基于MQ的技术方案落地进行编码开发，基于MQ来实现。

其实到这一步为止，大家平时去用MQ解决自己系统中的各种生产环境的问题，就已经可以做到了，并没大家想的那么难。

但是接着我们再接再厉，研究了很多RocketMQ的底层原理，接着我们针对很多MQ环境下的高阶问题进行了技术方案的设计，并且结合RocketMQ讲解了各种高阶的技术方案是如何落地的。

到这一步为止，我们对MQ的底层原理以及高阶技术方案，都有了较为深入的理解，并且知道在实践中如何落地了。

接着最后，我们深入研究了一下MQ的底层源码，当然受限于篇幅问题，我们没办法对所有源码都进行细致的分析，只能把核心源码部分提取出来给大家进行分析，但是不管如何，也帮助大家对MQ的底层原理有了更加深入的理解。

上述就是我们在本专栏中学到的东西，大家只要认认真真的跟着吃透这100多篇文章，在消息中间件这块都会功力大增

接下来我们聊聊，这个专栏里的东西，应该如何运用到面试中去呢？如果运用好了，一定是可以事半功倍的。

其实这里最最关键的一点，就是大家要认真学习和思考专栏里讲解的那些业务场景和业务问题，然后思考，你自己应该如何在自己的系统里发现一些业务场景和问题？这些问题应该如何运用MQ来设计技术方案解决？

然后你觉得你的MQ集群应该如何设计？你具体的基于MQ的技术方案如何落地？当你系统基于MQ来开发之后，可能产生哪些高阶的问题？针对那些高阶的问题，应该如何设计方案来解决？

整个MQ背后的一套基本原理，到底层原理，到源码级别，应该是什么样的？

如果你能把这些东西梳理清楚，写入你的简历，出去面试的时候，跟面试官聊你们的业务场景，业务问题，基于MQ的方案，一些高阶的技术方案，MQ底层的原理甚至源码，那么相信你一定会脱颖而出的！

好了，本专栏到这里正式结束，祝每个同学学有所成，出去面试势如破竹！

**End**

’